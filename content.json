{"posts":[{"title":"外国语学院-情况说明","text":"情况说明2018年4月23日上午，有微信公众号发布我院岳昕同学的《公开信》。学院第一时间向有关老师和同学了解情况，现作说明如下： 2018年4月22日下午和晚上，学院辅导员出于对学生的关心，通过多种方式、多次联系岳昕同学，均未能联系上，这种情况下，辅导员感到担忧。到23:30左右，该同学还未回到寝室。出于对同学安全的关心，辅导员与同学的母亲联系，询问岳昕同学是否回家等情况。该同学的母亲表示，孩子并未回家；之后，家长拨打电话未接，发微信未回，因此感到着急，随后赶到宿舍。此时，该同学已回到宿舍，母亲与其进行沟通。因担心影响其他同学休息，母亲决定和同学一起回家。 2018年4月23日上午，学院关注到网络上的相关信息后，与家长进一步沟通了情况，表达了关心。学院的老师与家长一样，都真心爱护学生、关心学生，既关心学生的学习，也关心学生的健康、安全与成长，老师和家长是善意的，态度也是一致的。 据了解，该同学已提交了毕业论文的部分初稿，指导老师也非常关心，给予了积极评价和悉心指导，并希望注意写作进度。 学院是学生成长成才的家园，老师们尽一切努力关心爱护学生，而这份关爱也意味着责任所在，学院、老师与家长都同样担负着教育的责任。与此同时，我们始终尊重每一位同学的基本权利、努力保障每一位同学的合法权益。 我们感谢师生、校友对学院工作的关心。 北京大学外国语学院2018年4月23日","link":"/posts/2018/04/24/A-Fact-Sheet-By-PKU/"},{"title":"申请公开信息事件经过（量产基地）","text":"转自公众号：量产基地文章已被4044月22日，我采访了岳昕和其他一同申请信息公开的同学，希望了解关于信息公开结果的更多情况。就在采访结束后的凌晨一点，岳昕的母亲与辅导员来到她的宿舍，将她叫醒，要求她删除手机电脑中所有与信息公开事件相关的资料，并要求她书面保证不再介入此事。随后，她被母亲带回家中，无法返校。 12个小时后，岳昕通过公众号“深约一丈”发表了面对北京大学师生及北京大学外国语学院的公开信，表示自己因学校的介入“恐惧而震怒”，并希望学院作出公开说明。 “我不能打一百分” 4月9日上午10点整，十名同学（实际到场八名）向北京大学提交关于1998年7月前后讨论沈阳‘师德’问题的系列会议记录的信息公开申请。4月20日，提交申请的同学们，及其余十五名通过邮件申请信息公开的同学均收到了来自北京大学信息公开办公室的答复。 岳昕是申请信息公开的十名同学之一。4月20日中午11时30分，她和其他同学一样，被邀请前往自己所在院系行政楼的会议室内。学工老师向她出示了校方的答复函，并问：“这个回复可以打一百分吗？” 岳昕回答：“我肯定不能打一百分。” 老师说：“如果我们学校的同学都不能打一百分，那校外的人会打多少分呢？是不是会打负分呢？” 岳昕说，她和同学们收到的答复，其实并无太多有效信息——答复函中表示，学校已在2018年4月8日向社会主动公开了《关于给予沈阳行政警告处分的决定》和中文系《关于给予沈阳警告处分的决定》，并给出查询网址链接。但对于信息申请中提及的“党委相关会议记录”“西城区公安局对此事的调查结果通报”“中文系相关会议记录”及“沈阳在大会上公开检讨的内容”，均表示现有档案并无相关信息。答复函承认当时学校和院系管理工作并不规范健全，并以“不断加强师德建设，认真落实立德树人的根本任务”作为结尾。 答复函中援引了《高等学校信息公开办法》第十八条（三）：“不属于本校职责范围的或该信息不存在的，应当告知申请人”。 岳昕想拿走答复函的纸质文件，被老师拒绝了：“给你有百害而无一利，搞不好你以后不能顺利毕业。”岳昕说，由于她参与过大量媒体工作的缘故，校方或许担心她将文件交给媒体。她从这样的拒绝中读出不信任，并再次和学工老师协商。院系学工老师在斟酌后，最终将纸质文件交给她。 在得到回复的25人中，只有岳昕和另一位同学保留了纸质文件。张震林是同样曾申请信息公开的同学之一，当他申请保留纸质文件时，院系学工老师以“规定”必须将纸质文件留在院系里为由，拒绝了他，并表示学生如有需要，可以随时来院系查看。张震林无奈同意。事后他翻阅文件，发现并无具体规定禁止。 在收取回复函期间，同学们被禁止录音。岳昕表示学工老师曾试图拿起她的手机检查，她于是将手机往自己的方向“拨了一下”。但出于种种顾虑，部分同学依然保留了当时的录音。同学们的顾虑并非全无缘由——对张震林来说，保留录音曾成为他保护自己的方式之一。“什么是明确的表态呢？” 在4月9日张震林申请信息公开的当天，院系辅导员曾给他打电话，表示学校学工部曾找自己了解学生情况，被她“挡回去了”。辅导员随后联系了张震林的同学，次日下午，同学转告张震林，称提请信息公开一事背后或有境外势力支持，让他谨言慎行。张震林“感到害怕”，于是主动联系辅导员，4月11上午9点，张震林和辅导员见面谈话。看见有两个手机倒扣在桌面上，他怀疑辅导员在现场秘密录音。因此，他也录了音。 据张震林回忆，辅导员先和他聊了聊学业及职业规划，并问他是否认识提请信息公开的其他同学。这些问题以“你显然和xxx交情不浅”的方式提出。张震林认为“这些都是诱导性提问”，他承认自己由于同处一个专业，和一位同学关系不错，但是“她一直认为我们是一个组织的”，对于这个问题，他表示否认。 辅导员向张震林提出“三条指控”：前去提交信息公开申请表的同学统一着装且佩戴口罩，极可能是有组织有预谋的行动，而张震林是他们的组织者之一；这一行动受到了境外组织的资金支持，张震林作为组织者对此知情；张震林在当天联系了境外媒体。据岳昕回忆，当时前去的同学并非统一着装，“我穿了一件米色风衣”；有部分同学因害怕被媒体拍到的缘故佩戴口罩，但并非人人如此；提交信息公开申请表的当天早上有十余家媒体在校门口以“拍摄花鸟”为由试图进校，其中确有外媒。但是张震林表示对这件事毫不知情。而是后来辅导员告诉他的。 张震林自认并不活跃，只是一同去提交信息公开申请。在被辅导员质问时，甚至觉得不可思议，“笑得很开心”——他随后否认以上指控。约谈现场有另一名他不认识的同学在场，“明显偏向辅导员一方”。这令他觉得很奇怪。 辅导员并不相信张震林，最后表示“我护你到什么程度取决于你说了多少”。事后，她再次联系张震林的同学，希望同学劝说张震林坦白实情。张震林认为她并不信任自己，因为他“说话吞吐且前后不一”“当提到有外媒时表现得很镇定，像是早已经知道了一样”。他还得知，辅导员对他的同学说，会向学校汇报关于他的约谈材料，如果他的同学“表现得不错”，材料将不会涉及被辅导员找来劝说自己的那位同学。 张震林托同学转告辅导员自己已坦白一切，但辅导员又找到另一位同学劝说张震林，再次询问他和境外组织的关系。张震林为此十分恐惧，4月12日，他向辅导员发微信表示自己的确和境外组织无任何瓜葛。辅导员以几段语音回复他：“心情差可以找我聊天”“反性侵可以从别的渠道”“给我一个明确的表态”。 “什么是明确的表态呢？”张震林问。 辅导员拒绝在微信上回答，要求面谈。 4月13日早上，辅导员给他的父亲打去电话，告知相关情况并希望他来学校。当晚八点左右，父亲到达张震林宿舍楼下，但张震林已因害怕辅导员来宿舍找自己的缘故，选择去北京八中附近的一家宾馆住宿。父亲和辅导员及张震林所在院系的党委副书记老师见面。次日，张震林见到父亲，父亲并不相信辅导员的几条指控，但依然为他担忧。“他有想调和的意思。”张震林认为，父亲是被“吓住了”。 张震林从辅导员处要到老师的电话，通话中，他表示自己有当天约谈的录音，希望学院能为此公开道歉并消除不良影响。一番协商后，老师当面向他道歉，并向他的父亲与同学澄清此事。次日，张震林和熟识的同学聊起此事，向公众号“深约一丈”讲述了这件事。4月14日，公众号“深约一丈”发表文章《沈阳事件近期情况汇总》，文章中提及这次约谈，随后以“违反网络信息安全法”为由被删除，公众号被禁言七天。 岳昕表示，通过家长渠道去做学生工作是“非常常见的威胁手法”。4月20日她收取学校答复时，也被学工老师提醒：学工部门有权不经过学生直接联系家长。 “我们对此表示遗憾” 岳昕对自己的经历感到“非常憋屈”。提请信息公开后，她同样多次收到了来自院系辅导员的约谈要求。4月10日，学工老师不断给她打电话，她因忙于毕业论文的缘故未能接听，回复短信表示已经收到消息。 4月11日晚上10点左右，岳昕在寝室换上睡衣准备开始写论文，学工老师出现在寝室门口，将她带去约谈。 此次约谈后不久，她得知张震林同被约谈的消息。正值规划毕业去向之际，母亲打来电话，她害怕母亲知情，又害怕母亲的顾虑会让她放弃自己喜爱的工作，心理压力大到一度不敢同母亲通话。她开始同朋友商量“最坏的可能性与对策”——4月15日下午，她正在思考此事时，社会学系的一位老师发来邀请，希望她能列席参与次日的“反性骚扰暂行规定学生意见征集会”，晚上10点，她收到了具体的时间地点信息。 据后来学工老师与她的约谈内容，学工老师称，在相关老师向她发出邀请时，曾有其他老师提出质疑。学工老师认为这是锻炼和成长机会，于是同意岳昕参加。张震林从岳昕处得知意见征集会的消息，向老师申请参加，得到了主办方的同意。 4月16日，意见征集会举行，该会议并未公开，而是邀请了学生常代表和部分学生。公众号“北门静悄悄”为此发文询问为何不公开征集意见。会上讨论了性骚扰的概念界定、反性骚扰专门委员会的代表产生、性骚扰的投诉与受理、调解机制、师生恋问题、资源对接、委员会监督、保护、保密及信息公开等问题。岳昕及张震林作为列席成员，提出许多问题。 主持人不希望同学在会后带走材料或将材料传到网上，但并未规定是否可以将会议内容录音或者在网上转述。4月17日晚上，张震林在未名BBS“三角地”板块和自己的公众号“境外事例”上发表了会议内容记录，并在未名BBS“校长信箱”板块提出自己的8条建议，校方回复说：“同学，你好！学校专家组会对你的意见进行研究， 感谢你的建议。” 北京大学学生会常代会回复了BBS的会议内容记录帖子：“有网友未经会议主办方确认，自行在互联网上发布本次会议‘记录整理’，我们对此表示遗憾。” 据北京大学新闻中心报道，2018年4月17日下午，学校召开第935次校长办公会议，专题研究反性骚扰暂行规定。 4月19日，老师再次约谈张震林，询问BBS的帖子是否由他发布。张震林认为他“管不着”，但又不想撒谎，于是拒绝回答。老师询问次日上午，他是否有时间来院系一趟——时间定在11时30分。次日11时25分，张震林来到院系办公楼门口，发现老师已久候多时。 他随后收到了11天前，关于信息公开申请的回复函。院长、党委书记和副书记及其他学工老师均在场。张震林认为，整个学院“精锐部队”全部出动的阵仗，可能会让很多同学觉得“压力非常大”。 老师在他吃午餐时，私下提醒他“这事到此为止，不要公开，也不要发BBS说自己收到回复的事。” 岳昕则表示，在得到回复的同时，老师们也略带威胁地同她聊天，甚至屡屡谈及“顺利毕业”的话题。 回顾从提请信息公开申请至收到回复函的过程，岳昕首先进行了反思：她对自己的表现还是不太满意，在她认为学工老师谈话有明显不合理、体现权力不对等的地方，她表现得还不够强硬。同时，她也认为自己没能发动更多同学：只有十名同学当场提交信息公开申请表，十五名同学通过邮箱提交申请表，这个数量并不算多；她也未能和更多同学交流自己的想法。但她始终认为，做事应当有始有终，才能让敢于发声的同学不致失望，也不会让其他同学们失去对自己的信任。毕业在即，她认为这才是一个爱北大的同学该做的事——“而不是为了120周年校庆歌舞升平”。 目前，同学们可以通过校长信箱或BBS其他版面向学校提出自己的意见，也可参与学生会主办的“我的校园我做主”座谈会参与校园事务，当然也可以申请信息公开。但岳昕认为这还远远不够——同学发在BBS上的意见，往往不能从根本上解决问题，“可能只是扣掉后勤工友的工资”。她曾参与过第三次“我的校园我做主”座谈会，那次座谈会针对保卫部和共享单车的管理问题，但她参与之后，却认为同学们在座谈会上很难做好充足的准备，发声较为分散，容易被校方目为幼稚，“依然是信息不对等的结果”。 由于她是4月9日上午第三名提交信息公开申请表的同学，她拿到的回复函抬头为“北大信息公开[2018]3号”。这意味着在本次申请信息公开之前，尚无其他同学就其他可能关心的事件向学校申请信息公开。 4月19日下午16时44分，她再次被学工老师约谈。最后，她们在未名湖边聊了四个多小时。老师不断劝说她考虑家人的感受，“不必走到申请信息公开的最后一步”“反正最后也会告诉你”。岳昕暂时没有回复她。 4月20日5时33分，岳昕向老师发信息，表示仍希望走完信息公开的整个流程。 7时09分，她收到短信回复：“你真的想清楚了吗？还是再想想吧。” 不是尾声的尾声 4月23日下午12时57分，公众号“深约一丈”刊载岳昕的公开信；13时18分，公众号显示“此内容因违规无法查看”。","link":"/posts/2018/04/24/A-Fact-Sheet/"},{"title":"用 Python 实现一个最简单的对象模型","text":"原文地址：A Simple Object Model 原文作者：Carl Friedrich Bolz 译文出自：掘金翻译计划 译者：Zheaoli 校对者：Yuze Ma, Gran 一个简单的对象模型Carl Friedrich Bolz 是一位在伦敦国王大学任职的研究员，他沉迷于动态语言的实现及优化等领域而不可自拔。他是 PyPy/RPython 的核心开发者之一，于此同时，他也在为 Prolog, Racket, Smalltalk, PHP 和 Ruby 等语言贡献代码。这是他的 Twitter @cfbolz 。 开篇面向对象编程是目前被广泛使用的一种编程范式，这种编程范式也被大量现代编程语言所支持。虽然大部分语言给程序猿提供了相似的面向对象的机制，但是如果深究细节的话，还是能发现它们之间还是有很多不同的。大部分的语言的共同点在于都拥有对象处理和继承机制。而对于类来说的话，并不是每种语言都完美支持它。比如对于 Self 或者 JavaScript 这样的原型继承的语言来说，是没有类这个概念的，他们的继承行为都是在对象之间所产生的。 深入了解不同语言的对象模型是一件非常有意思的事儿。这样我们可以去欣赏不同的编程语言的相似性。不得不说，这样的经历可以在我们学习新的语言的时候，利用上我们已有的经验，以便于我们快速的掌握它。 这篇文章将会带领你实现一套简单的对象模型。首先我们将实现一个简单的类与其实例，并能够通过这个实例去访问一些方法。这是被诸如 Simula 67 、Smalltalk 等早期面向对象语言所采用的面向对象模型。然后我们会一步步的扩展这个模型，你可以看到接下来两步会为你展现不同语言的模型设计思路，然后最后一步是来优化我们的对象模型的性能。最终我们所得到的模型并不是哪一门真实存在的语言所采用的模型，不过，硬是要说的话，你可以把我们得到的最终模型视为一个低配版的 Python 对象模型。 这篇文章里所展现的对象模型都是基于 Python 实现的。代码在 Python 2.7 以及 Python 3.4 上都可以完美运行。为了让大家更好的了解模型里的设计哲学，本文也为我们所设计的对象模型准备了单元测试，这些测试代码可以利用 py.test 或者 nose 来运行。 讲真，用 Python 来作为对象模型的实现语言并不是一个好的选择。一般而言，语言的虚拟机都是基于 C/C++ 这样更为贴近底层的语言来实现的，同时在实现中需要非常注意很多的细节，以保证其执行效率。不过，Python 这样非常简单的语言能让我们将主要精力都放在不同的行为表现上，而不是纠结于实现细节不可自拔。 基础方法模型我们将以 Smalltalk 中的实现的非常简单的对象模型来开始讲解我们的对象模型。Smalltalk 是一门由施乐帕克研究中心下属的 Alan Kay 所带领的小组在 70 年代所开发出的一门面向对象语言。它普及了面向对象编程，同时在今天的编程语言中依然能看到当时它所包含的很多特性。在 Smalltalk 核心设计原则之一便是：“万物皆对象”。Smalltalk 最广为人知的继承者是 Ruby，一门使用类似 C 语言语法的同时保留了 Smalltalk 对象模型的语言。 在这一部分中，我们所实现的对象模型将包含类，实例，属性的调用及修改，方法的调用，同时允许子类的存在。开始前，先声明一下，这里的类都是有他们自己的属性和方法的普通的类 友情提示：在这篇文章中，“实例”代表着“不是类的对象”的含义。 一个非常好的习惯就是优先编写测试代码，以此来约束具体实现的行为。本文所编写的测试代码由两个部分组成。第一部分由常规的 Python 代码组成，可能会使用到 Python 中的类及其余一些更高级的特性。第二部分将会用我们自己建立的对象模型来替代 Python 的类。 在编写测试代码时，我们需要手动维护常规的 Python 类和我们自建类之间的映射关系。比如，在我们自定类中将会使用 obj.read_attr(&quot;attribute&quot;) 来作为 Python 中的 obj.attribute 的替代品。在现实生活中，这样的映射关系将由语言的编译器/解释器来进行实现。 在本文中，我们还对模型进行了进一步简化，这样看起来我们实现对象模型的代码和和编写对象中方法的代码看起来没什么两样。在现实生活中，这同样是基本不可能的，一般而言，这两者都是由不同的语言实现的。 首先，让我们来编写一段用于测试读取求改对象字段的代码： 1234567891011121314151617181920212223242526def test_read_write_field(): # Python code class A(object): pass obj = A() obj.a = 1 assert obj.a == 1 obj.b = 5 assert obj.a == 1 assert obj.b == 5 obj.a = 2 assert obj.a == 2 assert obj.b == 5 # Object model code A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;a&quot;, 1) assert obj.read_attr(&quot;a&quot;) == 1 obj.write_attr(&quot;b&quot;, 5) assert obj.read_attr(&quot;a&quot;) == 1 assert obj.read_attr(&quot;b&quot;) == 5 obj.write_attr(&quot;a&quot;, 2) assert obj.read_attr(&quot;a&quot;) == 2 assert obj.read_attr(&quot;b&quot;) == 5 在上面这个测试代码中包含了我们必须实现的三个东西。Class 以及 Instance 类分别代表着我们对象中的类以及实例。同时这里有两个特殊的类的实例：OBJECT 和 TYPE。 OBJECT 对应的是作为 Python 继承系统起点的 object 类（译者注：在 Python 2.x 版本中，实际上是有两套类系统，一套被统称为 new style class , 一套被称为 old style class ，object 是 new style class 的基类）。TYPE 对应的是 Python 类型系统中的 type 。 为了给 Class 以及 Instance 类的实例提供通用操作支持，这两个类都会从 Base 类这样提供了一系列方法的基础类中进行继承并实现： 123456789101112131415161718192021222324252627class Base(object): &quot;&quot;&quot; The base class that all of the object model classes inherit from. &quot;&quot;&quot; def __init__(self, cls, fields): &quot;&quot;&quot; Every object has a class. &quot;&quot;&quot; self.cls = cls self._fields = fields def read_attr(self, fieldname): &quot;&quot;&quot; read field 'fieldname' out of the object &quot;&quot;&quot; return self._read_dict(fieldname) def write_attr(self, fieldname, value): &quot;&quot;&quot; write field 'fieldname' into the object &quot;&quot;&quot; self._write_dict(fieldname, value) def isinstance(self, cls): &quot;&quot;&quot; return True if the object is an instance of class cls &quot;&quot;&quot; return self.cls.issubclass(cls) def callmethod(self, methname, *args): &quot;&quot;&quot; call method 'methname' with arguments 'args' on object &quot;&quot;&quot; meth = self.cls._read_from_class(methname) return meth(self, *args) def _read_dict(self, fieldname): &quot;&quot;&quot; read an field 'fieldname' out of the object's dict &quot;&quot;&quot; return self._fields.get(fieldname, MISSING) def _write_dict(self, fieldname, value): &quot;&quot;&quot; write a field 'fieldname' into the object's dict &quot;&quot;&quot; self._fields[fieldname] = valueMISSING = object() Base 实现了对象类的储存，同时也使用了一个字典来保存对象字段的值。现在，我们需要去实现 Class 以及 Instance 类。在Instance 的构造器中将会完成类的实例化以及 fields 和 dict 初始化的操作。换句话说，Instance 只是 Base 的子类，同时并不会为其添加额外的方法。 Class 的构造器将会接受类名、基础类、类字典、以及元类这样几个操作。对于类来讲，上面几个变量都会在类初始化的时候由用户传递给构造器。同时构造器也会从它的基类那里获取变量的默认值。不过这个点，我们将在下一章节进行讲述。 123456789101112class Instance(Base): &quot;&quot;&quot;Instance of a user-defined class. &quot;&quot;&quot; def __init__(self, cls): assert isinstance(cls, Class) Base.__init__(self, cls, {})class Class(Base): &quot;&quot;&quot; A User-defined class. &quot;&quot;&quot; def __init__(self, name, base_class, fields, metaclass): Base.__init__(self, metaclass, fields) self.name = name self.base_class = base_class 同时，你可能注意到这点，类依旧是一种特殊的对象，他们间接的从 Base 中继承。因此，类也是一个特殊类的特殊实例，这样的很特殊的类叫做：元类。 现在，我们可以顺利通过我们第一组测试。不过这里，我们还没有定义 Type 以及 OBJECT 这样两个 Class 的实例。对于这些东西，我们将不会按照 Smalltalk 的对象模型进行构建，因为 Smalltalk 的对象模型对于我们来说太过于复杂。作为替代品，我们将采用 ObjVlisp1 的类型系统，Python 的类型系统从这里吸收了不少东西。 在 ObjVlisp 的对象模型中，OBJECT 以及 TYPE 是交杂在一起的。OBJECT 是所有类的母类，意味着 OBJECT 没有母类。TYPE 是 OBJECT 的子类。一般而言，每一个类都是 TYPE 的实例。在特定情况下，TYPE 和 OBJECT 都是 TYPE 的实例。不过，程序猿可以从 TYPE 派生出一个类去作为元类： 123456789# set up the base hierarchy as in Python (the ObjVLisp model)# the ultimate base class is OBJECTOBJECT = Class(name=&quot;object&quot;, base_class=None, fields={}, metaclass=None)# TYPE is a subclass of OBJECTTYPE = Class(name=&quot;type&quot;, base_class=OBJECT, fields={}, metaclass=None)# TYPE is an instance of itselfTYPE.cls = TYPE# OBJECT is an instance of TYPEOBJECT.cls = TYPE 为了去编写一个新的元类，我们需要自行从 TYPE 进行派生。不过在本文中我们并不会这么做，我们将只会使用 TYPE 作为我们每个类的元类。 好了，现在第一组测试已经完全通过了。现在让我们来看看第二组测试，我们将会在这组测试中测试对象属性读写是否正常。这段代码还是很好写的。 123456789101112131415def test_read_write_field_class(): # classes are objects too # Python code class A(object): pass A.a = 1 assert A.a == 1 A.a = 6 assert A.a == 6 # Object model code A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;a&quot;: 1}, metaclass=TYPE) assert A.read_attr(&quot;a&quot;) == 1 A.write_attr(&quot;a&quot;, 5) assert A.read_attr(&quot;a&quot;) == 5 isinstance 检查到目前为止，我们还没有将对象有类这点特性利用起来。接下来的测试代码将会自动的实现 isinstance 。 1234567891011121314151617181920def test_isinstance(): # Python code class A(object): pass class B(A): pass b = B() assert isinstance(b, B) assert isinstance(b, A) assert isinstance(b, object) assert not isinstance(b, type) # Object model code A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={}, metaclass=TYPE) B = Class(name=&quot;B&quot;, base_class=A, fields={}, metaclass=TYPE) b = Instance(B) assert b.isinstance(B) assert b.isinstance(A) assert b.isinstance(OBJECT) assert not b.isinstance(TYPE) 我们可以通过检查 cls 是不是 obj 类或者它自己的超类来判断 obj 对象是不是某些类 cls 的实例。通过检查一个类是否在一个超类链上工作，来判断一个类是不是另一个类的超类。如果还有其余类存在于这个超类链上，那么这些类也可以被称为是超类。这个包含了超类和类本身的链条，被称之为方法解析顺序（译者注：简称MRO）。它很容易以递归的方式进行计算： 12345678910111213class Class(Base): ... def method_resolution_order(self): &quot;&quot;&quot; compute the method resolution order of the class &quot;&quot;&quot; if self.base_class is None: return [self] else: return [self] + self.base_class.method_resolution_order() def issubclass(self, cls): &quot;&quot;&quot; is self a subclass of cls? &quot;&quot;&quot; return cls in self.method_resolution_order() 好了，在修改代码后，测试就完全能通过了 方法调用前面所建立的对象模型中还缺少了方法调用这样的重要特性。在本章我们将会建立一个简单的继承模型。 123456789101112131415161718192021222324252627def test_callmethod_simple(): # Python code class A(object): def f(self): return self.x + 1 obj = A() obj.x = 1 assert obj.f() == 2 class B(A): pass obj = B() obj.x = 1 assert obj.f() == 2 # works on subclass too # Object model code def f_A(self): return self.read_attr(&quot;x&quot;) + 1 A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;f&quot;: f_A}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;x&quot;, 1) assert obj.callmethod(&quot;f&quot;) == 2 B = Class(name=&quot;B&quot;, base_class=A, fields={}, metaclass=TYPE) obj = Instance(B) obj.write_attr(&quot;x&quot;, 2) assert obj.callmethod(&quot;f&quot;) == 3 为了找到调用对象方法的正确实现，我们现在开始讨论类对象的方法解析顺序。在 MRO 中我们所寻找到的类对象字典中第一个方法将会被调用： 12345678class Class(Base): ... def _read_from_class(self, methname): for cls in self.method_resolution_order(): if methname in cls._fields: return cls._fields[methname] return MISSING 在完成 Base 类中 callmethod 实现后，可以通过上面的测试。 为了保证函数参数传递正确，同时也确保我们事先的代码能完成方法重载的功能，我们可以编写下面这段测试代码，当然结果是完美通过测试： 123456789101112131415161718192021222324252627282930def test_callmethod_subclassing_and_arguments(): # Python code class A(object): def g(self, arg): return self.x + arg obj = A() obj.x = 1 assert obj.g(4) == 5 class B(A): def g(self, arg): return self.x + arg * 2 obj = B() obj.x = 4 assert obj.g(4) == 12 # Object model code def g_A(self, arg): return self.read_attr(&quot;x&quot;) + arg A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;g&quot;: g_A}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;x&quot;, 1) assert obj.callmethod(&quot;g&quot;, 4) == 5 def g_B(self, arg): return self.read_attr(&quot;x&quot;) + arg * 2 B = Class(name=&quot;B&quot;, base_class=A, fields={&quot;g&quot;: g_B}, metaclass=TYPE) obj = Instance(B) obj.write_attr(&quot;x&quot;, 4) assert obj.callmethod(&quot;g&quot;, 4) == 12 基础属性模型现在最简单版本的对象模型已经可以开始工作了，不过我们还需要去不断的改进。这一部分将会介绍基础方法模型和基础属性模型之间的差异。这也是 Smalltalk 、 Ruby 、 JavaScript 、 Python 和 Lua 之间的核心差异。 基础方法模型将会按照最原始的方式去调用方法： 1result = obj.f(arg1, arg2) 基础属性模型将会将调用过程分为两步：寻找属性，以及返回执行结果： 12method = obj.fresult = method(arg1, arg2) 你可以在接下来的测试中体会到前文所述的差异： 12345678910111213141516171819202122232425262728293031def test_bound_method(): # Python code class A(object): def f(self, a): return self.x + a + 1 obj = A() obj.x = 2 m = obj.f assert m(4) == 7 class B(A): pass obj = B() obj.x = 1 m = obj.f assert m(10) == 12 # works on subclass too # Object model code def f_A(self, a): return self.read_attr(&quot;x&quot;) + a + 1 A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;f&quot;: f_A}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;x&quot;, 2) m = obj.read_attr(&quot;f&quot;) assert m(4) == 7 B = Class(name=&quot;B&quot;, base_class=A, fields={}, metaclass=TYPE) obj = Instance(B) obj.write_attr(&quot;x&quot;, 1) m = obj.read_attr(&quot;f&quot;) assert m(10) == 12 我们可以按照之前测试代码中对方法调用设置一样的步骤去设置属性调用，不过和方法调用相比，这里面发生了一些变化。首先，我们将会在对象中寻找与函数名对应的方法名。这样一个查找过程结果被称之为已绑定的方法，具体来说就是，这个结果一个绑定了方法与具体对象的特殊对象。然后这个绑定方法会在接下来的操作中被调用。 为了实现这样的操作，我们需要修改 Base.read_attr 的实现。如果在实例字典中没有找到对应的属性，那么我们需要去在类字典中查找。如果在类字典中查找到了这个属性，那么我们将会执行方法绑定的操作。我们可以使用一个闭包来很简单的模拟绑定方法。除了更改 Base.read_attr 实现以外，我们也可以修改 Base.callmethod 方法来确保我们代码能通过测试。 1234567891011121314151617181920212223242526class Base(object): ... def read_attr(self, fieldname): &quot;&quot;&quot; read field 'fieldname' out of the object &quot;&quot;&quot; result = self._read_dict(fieldname) if result is not MISSING: return result result = self.cls._read_from_class(fieldname) if _is_bindable(result): return _make_boundmethod(result, self) if result is not MISSING: return result raise AttributeError(fieldname) def callmethod(self, methname, *args): &quot;&quot;&quot; call method 'methname' with arguments 'args' on object &quot;&quot;&quot; meth = self.read_attr(methname) return meth(*args)def _is_bindable(meth): return callable(meth)def _make_boundmethod(meth, self): def bound(*args): return meth(self, *args) return bound 其余的代码并不需要修改。 元对象协议除了常规的类方法之外，很多动态语言还支持特殊方法。有这样一些方法在调用时是由对象系统调用而不是使用常规调用。在 Python 中你可以看到这些方法的方法名用两个下划线作为开头和结束的，比如 __init__ 。特殊方法可以用于重载一些常规操作，同时可以提供一些自定义的功能。因此，它们的存在可以告诉对象模型如何自动的处理不同的事情。Python 中相关特殊方法的说明可以查看这篇文档。 元对象协议这一概念由 Smalltalk 引入，然后在诸如 CLOS 这样的通用 Lisp 的对象模型中也广泛的使用这个概念。这个概念包含特殊方法的集合（注：这里没有查到 coined3 的梗，请校者帮忙参考）。 在这一章中，我们将会为我们的对象模型添加三个元调用操作。它们将会用来对我们读取和修改对象的操作进行更为精细的控制。我们首先要添加的两个方法是 __getattr__ 和 __setattr__， 这两个方法的命名看起来和我们 Python 中相同功能函数的方法名很相似。 自定义属性读写操作__getattr__ 方法将会在属性通过常规方法无法查找到的情况下被调用，换句话说，在实例字典、类字典、父类字典等等对象中都找不到对应的属性时，会触发该方法的调用。我们将传入一个被查找属性的名字作为这个方法的参数。在早期的 Smalltalk4 中这个方法被称为 doesNotUnderstand: 。 在 __setattr__ 这里事情可能发生了点变化。首先我们需要明确一点的是，设置一个属性的时候通常意味着我们需要创建它，在这个时候，在设置属性的时候通常会触发 __setattr__ 方法。为了确保 __setattr__ 的存在，我们需要在 OBJECT 对象中实现 __setattr__ 方法。这样最基础的实现完成了我们向相对应的字典里写入属性的操作。这可以使得用户可以将自己定义的 __setattr__ 委托给 OBJECT.__setattr__ 方法。 针对这两个特殊方法的测试用例如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def test_getattr(): # Python code class A(object): def __getattr__(self, name): if name == &quot;fahrenheit&quot;: return self.celsius * 9\\. / 5\\. + 32 raise AttributeError(name) def __setattr__(self, name, value): if name == &quot;fahrenheit&quot;: self.celsius = (value - 32) * 5\\. / 9. else: # call the base implementation object.__setattr__(self, name, value) obj = A() obj.celsius = 30 assert obj.fahrenheit == 86 # test __getattr__ obj.celsius = 40 assert obj.fahrenheit == 104 obj.fahrenheit = 86 # test __setattr__ assert obj.celsius == 30 assert obj.fahrenheit == 86 # Object model code def __getattr__(self, name): if name == &quot;fahrenheit&quot;: return self.read_attr(&quot;celsius&quot;) * 9\\. / 5\\. + 32 raise AttributeError(name) def __setattr__(self, name, value): if name == &quot;fahrenheit&quot;: self.write_attr(&quot;celsius&quot;, (value - 32) * 5\\. / 9.) else: # call the base implementation OBJECT.read_attr(&quot;__setattr__&quot;)(self, name, value) A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;__getattr__&quot;: __getattr__, &quot;__setattr__&quot;: __setattr__}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;celsius&quot;, 30) assert obj.read_attr(&quot;fahrenheit&quot;) == 86 # test __getattr__ obj.write_attr(&quot;celsius&quot;, 40) assert obj.read_attr(&quot;fahrenheit&quot;) == 104 obj.write_attr(&quot;fahrenheit&quot;, 86) # test __setattr__ assert obj.read_attr(&quot;celsius&quot;) == 30 assert obj.read_attr(&quot;fahrenheit&quot;) == 86 为了通过测试，我们需要修改下 Base.read_attr 以及 Base.write_attr 两个方法： 12345678910111213141516171819202122class Base(object): ... def read_attr(self, fieldname): &quot;&quot;&quot; read field 'fieldname' out of the object &quot;&quot;&quot; result = self._read_dict(fieldname) if result is not MISSING: return result result = self.cls._read_from_class(fieldname) if _is_bindable(result): return _make_boundmethod(result, self) if result is not MISSING: return result meth = self.cls._read_from_class(&quot;__getattr__&quot;) if meth is not MISSING: return meth(self, fieldname) raise AttributeError(fieldname) def write_attr(self, fieldname, value): &quot;&quot;&quot; write field 'fieldname' into the object &quot;&quot;&quot; meth = self.cls._read_from_class(&quot;__setattr__&quot;) return meth(self, fieldname, value) 获取属性的过程变成调用 __getattr__ 方法并传入字段名作为参数，如果字段不存在，将会抛出一个异常。请注意 __getattr__ 只能在类中调用（Python 中的特殊方法也是这样），同时需要避免这样的 self.read_attr(&quot;__getattr__&quot;) 递归调用，因为如果 __getattr__ 方法没有定义的话，上面的调用会造成无限递归。 对属性的修改操作也会像读取一样交给 __setattr__ 方法执行。为了保证这个方法能够正常执行，OBJECT 需要实现 __setattr__ 的默认行为，比如： 123def OBJECT__setattr__(self, fieldname, value): self._write_dict(fieldname, value)OBJECT = Class(&quot;object&quot;, None, {&quot;__setattr__&quot;: OBJECT__setattr__}, None) OBJECT.__setattr__ 的具体实现和之前 write_attr 方法的实现有着相似之处。在完成这些修改后，我们可以顺利的通过我们的测试。 描述符协议在上面的测试中，我们频繁的在不同的温标之间切换，不得不说，在执行修改属性操作的时候这样真的很蛋疼，所以我们需要在 __getattr__ 和 __setattr__ 中检查所使用的的属性的名称为了解决这个问题，在 Python 中引入了描述符协议的概念。 我们将从 __getattr__ 和 __setattr__ 方法中获取具体的属性，而描述符协议则是在属性调用过程结束返回结果时触发一个特殊的方法。描述符协议可以视为一种可以绑定类与方法的特殊手段，我们可以使用描述符协议来完成将方法绑定到对象的具体操作。除了绑定方法，在 Python 中描述符最重要的几个使用场景之一就是 staticmethod、 classmethod 和 property。 在接下来一点文字中，我们将介绍怎么样来使用描述符进行对象绑定。我们可以通过使用 __get__ 方法来达成这一目标，具体请看下面的测试代码： 1234567891011121314151617181920212223def test_get(): # Python code class FahrenheitGetter(object): def __get__(self, inst, cls): return inst.celsius * 9\\. / 5\\. + 32 class A(object): fahrenheit = FahrenheitGetter() obj = A() obj.celsius = 30 assert obj.fahrenheit == 86 # Object model code class FahrenheitGetter(object): def __get__(self, inst, cls): return inst.read_attr(&quot;celsius&quot;) * 9\\. / 5\\. + 32 A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;fahrenheit&quot;: FahrenheitGetter()}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;celsius&quot;, 30) assert obj.read_attr(&quot;fahrenheit&quot;) == 86 __get__ 方法将会在属性查找完后被 FahrenheitGetter 实例所调用。传递给 __get__ 的参数是查找过程结束时所处的那个实例。 实现这样的功能倒是很简单，我们可以很简单的修改 _is_bindable 和 _make_boundmethod 方法： 12345def _is_bindable(meth): return hasattr(meth, &quot;__get__&quot;)def _make_boundmethod(meth, self): return meth.__get__(self, None) 好了，这样简单的修改能保证我们通过测试了。之前关于方法绑定的测试也能通过了，在 Python 中 __get__ 方法执行完了将会返回一个已绑定方法对象。 在实践中，描述符协议的确看起来比较复杂。它同时还包含用于设置属性的 __set__ 方法。此外，你现在所看到我们实现的版本是经过一些简化的。请注意，前面 _make_boundmethod 方法调用 __get__ 是实现级的操作，而不是使用 meth.read_attr('__get__') 。这是很有必要的，因为我们的对象模型只是从 Python 中借用函数和方法，而不是展示 Python 的对象模型。进一步完善模型的话可以有效解决这个问题。 实例优化这个对象模型前面三个部分的建立过程中伴随着很多的行为变化，而最后一部分的优化工作并不会伴随着行为变化。这种优化方式被称为 map ,广泛存在在可以自举的语言虚拟机中。这是一种最为重要对象模型优化手段：在 PyPy ，诸如 V8 现代 JavaScript 虚拟机中得到应用（在 V8 中这种方法被称为 hidden classes）。 这种优化手段基于如下的观察：到目前所实现的对象模型中，所有实例都使用一个完整的字典来储存他们的属性。字典是基于哈希表进行实现的，这将会耗费大量的内存。在很多时候，同一个类的实例将会拥有同样的属性，比如，有一个类 Point ，它所有的实例都包含同样的属性 x y。 Map 优化利用了这样一个事实。它将会将每个实例的字典分割为两个部分。一部分存放可以在所有实例中共享的属性名。然后另一部分只存放对第一部分产生的 Map 的引用和存放具体的值。存放属性名的 map 将会作为值的索引。 我们将为上面所述的需求编写一些测试用例，如下所示： 12345678910111213141516171819202122232425def test_maps(): # white box test inspecting the implementation Point = Class(name=&quot;Point&quot;, base_class=OBJECT, fields={}, metaclass=TYPE) p1 = Instance(Point) p1.write_attr(&quot;x&quot;, 1) p1.write_attr(&quot;y&quot;, 2) assert p1.storage == [1, 2] assert p1.map.attrs == {&quot;x&quot;: 0, &quot;y&quot;: 1} p2 = Instance(Point) p2.write_attr(&quot;x&quot;, 5) p2.write_attr(&quot;y&quot;, 6) assert p1.map is p2.map assert p2.storage == [5, 6] p1.write_attr(&quot;x&quot;, -1) p1.write_attr(&quot;y&quot;, -2) assert p1.map is p2.map assert p1.storage == [-1, -2] p3 = Instance(Point) p3.write_attr(&quot;x&quot;, 100) p3.write_attr(&quot;z&quot;, -343) assert p3.map is not p1.map assert p3.map.attrs == {&quot;x&quot;: 0, &quot;z&quot;: 1} 注意，这里测试代码的风格和我们之前的才是代码看起不太一样。之前所有的测试只是通过已实现的接口来测试类的功能。这里的测试通过读取类的内部属性来获取实现的详细信息，并将其与预设的值进行比较。这种测试方法又被称之为白盒测试。 p1 的包含 attrs 的 map 存放了 x 和 y 两个属性，其在 p1 中存放的值分别为 0 和 1。然后创建第二个实例 p2 ，并通过同样的方法网同样的 map 中添加同样的属性。 换句话说，如果不同的属性被添加了，那么其中的 map 是不通用的。 Map 类长下面这样： 123456789101112131415161718class Map(object): def __init__(self, attrs): self.attrs = attrs self.next_maps = {} def get_index(self, fieldname): return self.attrs.get(fieldname, -1) def next_map(self, fieldname): assert fieldname not in self.attrs if fieldname in self.next_maps: return self.next_maps[fieldname] attrs = self.attrs.copy() attrs[fieldname] = len(attrs) result = self.next_maps[fieldname] = Map(attrs) return resultEMPTY_MAP = Map({}) Map 类拥有两个方法，分别是 get_index 和 next_map 。前者用于查找对象储存空间中的索引中查找对应的属性名称。而在新的属性添加到对象中时应该使用后者。在这种情况下，不同的实例需要用 next_map 计算不同的映射关系。这个方法将会使用 next_maps 来查找已经存在的映射。这样，相似的实例将会使用相似的 Map 对象。 Figure 14.2 - Map transitions 使用 map 的 Instance 实现如下： 1234567891011121314151617181920212223class Instance(Base): &quot;&quot;&quot;Instance of a user-defined class. &quot;&quot;&quot; def __init__(self, cls): assert isinstance(cls, Class) Base.__init__(self, cls, None) self.map = EMPTY_MAP self.storage = [] def _read_dict(self, fieldname): index = self.map.get_index(fieldname) if index == -1: return MISSING return self.storage[index] def _write_dict(self, fieldname, value): index = self.map.get_index(fieldname) if index != -1: self.storage[index] = value else: new_map = self.map.next_map(fieldname) self.storage.append(value) self.map = new_map 现在这个类将给 Base 类传递 None 作为字段字典，那是因为 Instance 将会以另一种方式构建存储字典。因此它需要重载 _read_dict 和 _write_dict 。在实际操作中，我们将重构 Base 类，使其不在负责存放字段字典。不过眼下，我们传递一个 None 作为参数就足够了。 在一个新的实例创建之初使用的是 EMPTY_MAP ，这里面没有任何的对象存放着。在实现 _read_dict 后，我们将从实例的 map 中查找属性名的索引，然后映射相对应的储存表。 向字段字典写入数据分为两种情况。第一种是现有属性值的修改，那么就简单的在映射的列表中修改对应的值就好。而如果对应属性不存在，那么需要进行 map 变换（如上面的图所示一样），将会调用 next_map 方法，然后将新的值存放入储存列表中。 你肯定想问，这种优化方式到底优化了什么？一般而言，在具有很多相似结构实例的情况下能较好的优化内存。但是请记住，这不是一个通用的优化手段。有些时候代码中充斥着结构不同的实例之时，这种手段可能会耗费更大的空间。 这是动态语言优化中的常见问题。一般而言，不太可能找到一种万能的方法去优化代码，使其更快，更节省空间。因此，具体情况具体分析，我们需要根据不同的情况去选择优化方式。 在 Map 优化中很有意思的一点就是，虽然这里只有花了内存占用，但是在 VM 使用 JIT 技术的情况下，也能较好的提高程序的性能。为了实现这一点，JIT 技术使用映射来查找属性在存储空间中的偏移量。然后完全除去字典查找的方式。 潜在扩展扩展我们的对象模型和引入不同语言的设计选择是一件非常容易的事儿。这里给出一些可能的方向： 最简单的是添加更多的特殊方法方法，比如一些 __init__, __getattribute__, __set__ 这样非常容易实现和有趣的方法。 扩展模型支持多重继承。为了实现这一点，每一个类都需要一个父类列表。然后 Class.method_resolution_order 需要进行修改，以便支持方法查找。一个简单的 MRO 计算规则可以使用深度优先原则。然后更为复杂的可以采用C3 算法, 这种算法能更好的处理菱形继承结构所带来的一些问题。 一个更为疯狂的想法是切换到原型模式，这需要消除类和实例之间的差别。 总结面向对象编程语言设计的核心是其对象模型的细节。编写一些简单的对象模型是一件非常简单而且有趣的事情。你可以通过这种方式来了解现有语言的工作机制，并且深入了解面向对象语言的设计原则。编写不同的对象模型验证不同对象的设计思路是一个非常棒的方法。你也不在需要将注意力放在其余一些琐碎的事情上，比如解析和执行代码。 这样编写对象模型的工作在实践中也是非常有用的。除了作为实验品以外，它们还可以被其余语言所使用。这种例子有很多：比如 GObject 模型，用 C 语言编写，在 GLib 和 其余 Gonme 中得到使用，还有就是用 JavaScript 实现的各类对象模型。 参考文献 P. Cointe, “Metaclasses are first class: The ObjVlisp Model,” SIGPLAN Not, vol. 22, no. 12, pp. 156–162, 1987.↩ It seems that the attribute-based model is conceptually more complex, because it needs both method lookup and call. In practice, calling something is defined by looking up and calling a special attribute __call__, so conceptual simplicity is regained. This won’t be implemented in this chapter, however.)↩ G. Kiczales, J. des Rivieres, and D. G. Bobrow, The Art of the Metaobject Protocol. Cambridge, Mass: The MIT Press, 1991.↩ A. Goldberg, Smalltalk-80: The Language and its Implementation. Addison-Wesley, 1983, page 61.↩ In Python the second argument is the class where the attribute was found, though we will ignore that here.↩ C. Chambers, D. Ungar, and E. Lee, “An efficient implementation of SELF, a dynamically-typed object-oriented language based on prototypes,” in OOPSLA, 1989, vol. 24.↩ How that works is beyond the scope of this chapter. I tried to give a reasonably readable account of it in a paper I wrote a few years ago. It uses an object model that is basically a variant of the one in this chapter: C. F. Bolz, A. Cuni, M. Fijałkowski, M. Leuschel, S. Pedroni, and A. Rigo, “Runtime feedback in a meta-tracing JIT for efficient dynamic languages,” in Proceedings of the 6th Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems, New York, NY, USA, 2011, pp. 9:1–9:8.↩","link":"/posts/2016/12/15/A-Simple-Object-Model/"},{"title":"岳昕：致北大师生与北大外国语学院的一封公开信","text":"岳昕：致北大师生与北大外国语学院的一封公开信北京大学的老师和同学：你们好！我是2014级外国语学院的岳昕，是4月9日早上向北京大学递交《信息公开申请表》的八位到场同学之一。我拖着极疲惫的身躯写下这段文字，说明近来发生在我身上的一些事情。 一4月9日之后，我不断被学院学工老师、领导约谈，并两次持续到凌晨一点甚至两点。在谈话中，学工老师多次提到“能否顺利毕业”、“做这个你母亲和姥姥怎么看”、“学工老师有权不经过你直接联系你的家长”。而我近期正在准备毕业论文，频繁的打扰和后续的心理压力严重影响了我的论文写作。 二4月20日中午，我收到了校方的回复。外国语学院党委书记、学工老师、班主任在场，党委书记向我宣读了学校对于本次信息公开申请的答复： 讨论沈阳师德的会议级别不够记录 公安局调查结果不在学校的管理范围里 沈阳公开检讨的内容因中文系工作失误也没有找到 这样的回复结果令我失望。但毕业论文提交即将截止，我只能先将心思放在论文写作上。 三4月22日晚上十一点左右，辅导员突然给我打来电话，但因为时间已晚，我并没有接到。凌晨一点，辅导员和母亲突然来到我的宿舍，强行将我叫醒，要求我删除手机、电脑中所有与信息公开事件相关的资料，并于天亮后到学工老师处作出书面保证不再介入此事。有同楼层的同学可以作证。随后，我被家长带回家中，目前无法返校。我和母亲都彻夜未眠。学校在联系母亲时歪曲事实，导致母亲受到过度惊吓、情绪崩溃。因为学校强行无理的介入，我和母亲关系几乎破裂。学院目前的行动已突破底线，我感到恐惧而震怒。申请信息公开何罪之有？我没有做错任何事，也不会后悔曾经提交《信息公开申请表》，行使我作为北大学生的光荣权利。二十年孺慕情深，我爱我的母亲。面对她的嚎啕痛哭、自扇耳光、下跪请求、以自杀相胁，我的内心在滴血。在她的哀求下我只能暂时回到家中，但原则面前退无可退，妥协不能解决任何问题，我别无他法，只有写下这篇声明，陈述原委。情绪激动，请大家原谅我的语无伦次。 四在此，我正式向北京大学外国语学院提出以下诉求： 北京大学外国语学院应公开书面说明越过我向家长施压、凌晨到宿舍强行约谈我、要求我删除申请信息公开一事的相关资料所依据的规章制度，对此过程中违法违规操作予以明确，并采取措施避免此类事件再次发生。 北京大学外国语学院应立即停止一切对我家人的施压行为，向我已经遭受惊吓的母亲正式道歉并澄清事实，帮助修复因此事导致的家庭紧张关系。 北京大学外国语学院必须公开书面保证此事不会对本人毕业一事产生影响，并不会再就此事继续干扰我的论文写作进程。 北京大学外国语学院负责消除此事对本人学业、未来就业和家人的其他一切不良影响。 北京大学外国语学院应明确就以上诉求进行公开书面回复，给关注此事的大家一个交代。 我将保留通过法律手段进一步追究相关个人和单位责任的一切权利，包括但不限于向北京大学和上级主管部门举报外国语学院严重违反校纪的行为。 北京大学外国语学院14级本科生岳昕2018年4月23日","link":"/posts/2018/04/24/A-public-letter-to-PKU/"},{"title":"容器 CPU 和 Memory 限制行为简述","text":"这篇是给之前没啥容器经验的选手准备的一篇文章，主要是讲一下容器的 CPU 和 Memory 限制行为。 CPU 限制首先 Mac 或者是 Windows 选手在使用 Docker Desktop 的时候，会设置 Docker Desktop 的 CPU 限制，默认是 1，也就是说 Docker Desktop 只能使用 1 个 CPU。这是因为 Docker Desktop 裹了一层虚拟机（Windows 下应该是 WSL2/Hyper-V，Mac 下可能是 QEMU）。这相当于我们在一个特定 CPU 数量的宿主机中跑 Docker 首先提到 CPU 限制，本质上是限制进程的 CPU 使用的时间片，在 Linux 下，进程存在三种调度优先级 SCHED_NORMAL SCHED_FIFO SCHED_RR 1 用的是 Linux 中 CFS 调度器，而常见普通进程都是 SCHED_NORMAL 。OK 前提知识带过 说回容器中的 CPU 限制，目前主流语境下，容器特指以 Docker 为代表的一系列的基于 Linux 中 CGroup 和 Namespace 进行隔离的技术方案。那么在这个语境下，CPU 限制的实现利用了Linux CGroup 中三个 CPU Subsystem。我们主要关心的如下四个参数 cpu.cfs_period_us cpu.cfs_quota_us cpu.shares cpuset.cpus 现在分别来聊一下 首先说 cpu.shares，在 Docker 中的使用参数是 –cpu-shares，本质上是一个下限的软限制，用来设定 CPU 的利用率权重。默认值是 1024。这里对于相对值可能理解有点抽象。那么我们来看个例子 假如一个 1core 的主机运行 3 个 container，其中一个 cpu-shares 设置为 1024，而其它 cpu-shares 被设置成 512。当 3 个容器中的进程尝试使用 100% CPU 的时候（因为 cpu.shares 针对的是下限，只有使用 100% CPU 很重要，此时才可以体现设置值），则设置 1024 的容器会占用 50% 的 CPU 时间。那再举个例子，之前这个场景，其余的两个容器如果都没有太多任务，那么空余出来的 CPU 时间，是可以继续被第一个 1024 的容器继续使用的 接下来聊一下 cpu.cfs_quota_us 和 cpu.cfs_period_us ，这两个是需要组合使用才能生效，本质上含义是在 cpu.cfs_period_us 的单位时间内，进程最多可以利用 cpu.cfs_quota_us （单位都是 us），如果 quota 耗尽，那么进程会被内核 throttle 。在 Docker 下，你可以利用 –cpu-period 和 –cpu-quota 这两个值分别进行设置。也可以通过 –cpu 来进行设置，当我们设置 –cpu 为 2 的时候，容器会保证 cpu.cfs_quota_us 两倍于 cpu.cfs_period_us，剩下的就以此类推了（Docker 默认的 cpu.cfs_period_us 的阈值是 100ms 即 10000us） 现在已经聊了三个参数了，那么我们什么时候该用什么参数呢。通常来说，对于性能相对敏感的进程，我们可以使用 cpu.shares 来保证进程尽可能多的使用 CPU），业务进程可以利用 cpu.cfs_quota_us 和 cpu.cfs_period_us 来保证相对较好的公平分配。但是这样也带来一个问题，就是对于业务流量比较大的应用，可能会因为频繁被 throtlle 导致我们的 RT 等指标出现毛刺。Linux 5.12 之后有了一个新功能，cpu.cfs_burst_us ，即进程可以在 CPU 利用率比较低的空闲时段积累一定的 credit，然后在密集使用的时候换取一定的 buffer，实现更少的 throttle 和更高的 CPU 利用率（当然这个特性还暂时没有被主流容器所完全支持） 现在新的问题来了，无论 share 还是 cpu.cfs_quota_us 和 cpu.cfs_period_us 被 throttle 的概率都不少，如果我们想让进程更好的利用 CPU 怎么办？答案就是 cpuset.cpus ，Docker 中的参数是 –cpuset-cpus，可以让进程进行绑核处理 嗯，CPU 的部分就到这里 Mem 限制还是前提科普 首先 Mac 或者是 Windows 选手在使用 Docker Desktop 的时候，会设置 Docker Desktop 的 Mem 限制，这相当于我们在一个特定 Mem 数量的宿主机中跑 Docker 然后在我们今天的语境下，Mem 资源的限制还是依托于 CGroup 的 Memory Subsystem，参数有很多，我们目前只需要关心 memory.limit_in_bytes 含义即是容器的最大内存限制，如果设置为 -1，代表着无任何内存的限制。在 Docker 中的参数是 –memory。 行为的话分为这样两种情况 如果系统内存还有空余，但是容器内存超过了 Limit, 那么容器进程会被 OOMKiller Kill 掉 如果系统内存先于容器达到了内核阈值，那么 OOMKiller 会在整个系统范围内根据根据负载等多个因素计算一个 score，然后 rank 后从高到低进行 OOM Kill 的操作 当然实际上还有一种额外的情况。可以通过 –oom-kill-disable 参数设置 memory.oom_control 的值。如果设置为1，那么容器内存超过 Limit 就不会被 OOM Kill 掉而是会被暂停，如果设置为0，那么容器内存超过 Limit 就会被 OOM Kill 掉 嗯关于 Mem 的行为差不多就这些 总结差不多就这样吧，纯新手向的文章，水文一篇，大家别介意（","link":"/posts/2022/08/07/A-simple-introduction-about-cpu-and-memory-limit-in-docker/"},{"title":"Swift 声明式程序设计","text":"原文地址：Declarative API Design in Swift 原文作者：Benjamin Encz 译文出自：掘金翻译计划 译者：Zheaoli 校对者：luoyaqifei, Edison-Hsu 在我第一份 iOS 开发工程师的工作中，我编写了一个 XML 解析器和一个简单的布局工具，两个东西都是基于声明式接口。XML 解析器是基于 .plist 文件来实现 Objective-C 类关系映射。而布局工具则允许你利用类似 HTML 一样标签化的语法来实现界面布局（不过这个工具使用的前提是已经正确使用 AutoLayout &amp; CollectionViews）。 尽管这两个库都不完美，它们还是展现了声明式代码的四大优点： 关注点分离: 我们在使用声明式风格编写的代码时声明了意图，从而无需关注具体的底层实现，可以说这样的分离是自然发生的。 减少重复的代码: 所有声明式代码都共用一套样式实现，这里面很多属于配置文件，这样可以减少重复代码所带来的风险。 优秀的 API 设计: 声明式 API 可以让用户自行定制已有实现，而不是将已有实现做一种固定的存在看待。这样可以保证修改程度降至最小。 良好的可读性: 讲真，按照声明式 API 所写出来的代码简直优美无比。 这些天我写的大多数 Swift 代码非常适用于声明式编程风格。 不管是对于某一种数据结构的描述，或者是对某个功能的实现，在编写过程中，我最常使用的类型还是一些简单的结构体。声明不同的类型，主要是基于泛型类，然后这些东西负责实现具体的功能或者完成必要的工作。我们在 PlanGrid 开发过程中采用这种方法来编写我们得 Swift 代码。这种开发方式已经对对代码可读性的提升还有开发人员的效率提升上产生了巨大的影响。 本文我想讨论的是 PlanGrid 应用中所使用的 API 设计，它原本使用 NSOperationQueue 实现，现在使用了一种更接近声明式的方法－讨论这个 API 应该可以展示声明式编程风格在各方面的好处。 在 Swift 中构建一个声明式请求序列我们重新设计的 API 用来将本地变化（也可能是离线发生的）与 API 服务器进行同步。我不会讨论这种变化追踪方法的细节，而是将精力放在网络请求的生成和执行上。 在这篇文章里，我想专注于一个特定的请求类型上：上传本地生成的图片。出于多种因素的考虑（超出本文讨论范围），上传图片的操作包括三次请求： 向 API 服务器发起请求，API 服务器将会响应，响应内容为向 AWS 服务器上传图片所需信息。 上传图片至 AWS （使用上次请求得到的信息）。 向 API 服务器发起请求以确认图片上传成功。 既然我们有包括这些请求序列的上传任务，我们决定将其抽象成一个特殊的类型，并让我们的上传架构支持它。 定义请求序列协议我们决定引入一个单独的类型来对网络请求序列进行描述。这个类型将被我们的上传者类使用，上传者类的作用是将描述转化为实在的网络请求(要提醒你们的是我们不会在本篇文章中讨论上传者类的实现）。 接下来这个类型是我们控制流的精髓：我们有一个请求序列，序列中的每个请求都可能依赖于前一个请求的结果。 小贴士: 接下来的代码里的一些类型的命名方式看起来有点奇怪，但是它们中大多数是根据应用专属术语集来命名的（如： Operation ）。 12345678910111213141516public typealias PreviousRequestTuple = ( request: PushRequest, response: NSURLResponse, responseBody: JsonValue?)/// A sequence of push requests required to sync this operation with the server./// As soon as a request of this sequence completes,/// `PushSyncQueueManager` will poll the sequence for the next request./// If `nil` is returned for the `nextRequest` then/// this sequence is considered complete.public protocol OperationRequestSequence: class { /// When this method returns `nil` the entire `OperationRequestSequence` /// is considered completed. func nextRequest(previousRequest: PreviousRequestTuple?) throws -&gt; PushRequest?} 通过调用 nextRequest: 方法来让请求序列生成一个请求时，我们提供了一个对前一个请求的引用，包括 NSURLResponse 和 JSON 响应体（如果存在的话）。每一个请求的结果都可能在下一次请求时产生（（将会返回一个 PushRequest 对象），除了没有下一次请求（返回 nil ）或者在请求过程中发生了一些以外的情况导致没有返回必要的响应以外（请求序列在该情况下 throws ）。 值得注意的是， PushRequest 并不是这个返回值类型的理想名。这个类型只是描述一个请求的详情（结束符，HTTP 方法等等），其并不参与任何实质性的工作。这是声明式设计中很重要的一个方面。 你可能已经注意到了这个协议依赖于一个特定 class ，我们这样做是因为我们意识到 OperationRequestSequence 其是一个状态描述类型。它需要能够捕获并使用前面的请求所产生的结果（比如：在第三个请求里可能需要获取第一个请求的响应结果）。这个做法参考了 mutating 方法的结构，不得不说这样的行为貌似让这部分有关上传操作的代码变得更为复杂了（所以说重新赋值变化结构体并不是一件那么简单的事儿） 在基于 OperationRequestSequence 协议实现了我们第一个请求序列后，我们发现相比实现 nextRequest 方法来说，简单地提供一个数组来保存请求链更合适。于是我们便添加了 ArrayRequestSequence 协议来提供了一个请求数组的实现： 1234567891011121314public typealias RequestContinuation = (previous: PreviousRequestTuple?) throws -&gt; PushRequest?public protocol ArrayRequestSequence: OperationRequestSequence { var currentRequestIndex: Int { get set } var requests: [RequestContinuation] { get }}extension ArrayRequestSequence { public func nextRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { let nextRequest = try self.requests[self.currentRequestIndex](previous: previous) self.currentRequestIndex += 1 return nextRequest }} 这个时候，我们定义了一个新的上传序列，这只是很微小的一点工作。 实现请求序列协议作为一个小例子，让我们看看用来上传快照的上传序列吧（在 PlanGrid 中，快照指的是在图片中绘制的可导出的蓝图或者注释）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/// Describes a sequence of requests for uploading a snapshot.final class SnapshotUploadRequestSequence: ArrayRequestSequence { // Removed boilerplate initializer &amp; // instance variable definition code... // This is the definition of the request sequence lazy var requests: [RequestContinuation] = { return [ // 1\\. Get AWS Upload Package from API self._allocationRequest, // 2\\. Upload Snapshot to AWS self._awsUploadRequest, // 3\\. Confirm Upload with API self._metadataRequest ] }() // It follows the detailed definition of the individual requests: func _allocationRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { // Generate an API request for this file upload // Pass file size in JSON format in the request body return PushInMemoryRequestDescription( relativeURL: ApiEndpoints.snapshotAllocation(self.affectedModelUid.value), httpMethod: .POST, jsonBody: JsonValue(values: [ &quot;filesize&quot; : self.imageUploadDescription.fullFileSize ] ), operationId: self.operationId, affectedModelUid: self.affectedModelUid, requestIdentifier: SnapshotUploadRequestSequence.allocationRequest ) } func _awsUploadRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { // Check for presence of AWS allocation data in response body guard let allocationData = previous?.responseBody else { throw ImageCreationOperationError.MissingAllocationData } // Attempt to parse AWS allocation data self.snapshotAllocationData = try AWSAllocationPackage(json: allocationData[&quot;snapshot&quot;]) guard let snapshotAllocationData = self.snapshotAllocationData else { throw ImageCreationOperationError.MissingAllocationData } // Get filesystem path for this snapshot let thumbImageFilePath = NSURL(fileURLWithPath: SnapshotModel.pathForUid( self.imageUploadDescription.modelUid, size: .Full ) ) // Generate a multipart/form-data request // that uploads the image to AWS return AWSMultiPartRequestDescription( targetURL: snapshotAllocationData.targetUrl, httpMethod: .POST, fileURL: thumbImageFilePath, filename: snapshotAllocationData.filename, operationId: self.operationId, affectedModelUid: self.affectedModelUid, requestIdentifier: SnapshotUploadRequestSequence.snapshotAWS, formParameters: snapshotAllocationData.fields ) } func _metadataRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { // Generate an API request to confirm the completed upload return PushInMemoryRequestDescription( relativeURL: ApiEndpoints.snapshotAllocation(self.affectedModelUid.value), httpMethod: .PUT, jsonBody: self.snapshotMetadata, operationId: self.operationId, affectedModelUid: self.affectedModelUid, requestIdentifier: SnapshotUploadRequestSequence.metadataRequest ) }} 在实现的过程中你应该注意这样几件事情： 这里面几乎没有命令式代码。大多数的代码都通过实例变量和前次请求的结果来描述网络请求。 代码并不调用网络层，也没有任何上传操作的类型信息。它们只是对每个请求的详情进行了描述。事实上，这段代码没有能被观测到的副作用，它只更改了内部状态。 这段代码里可以说没有任何的错误处理代码。这个类型只负责处理该请求序列中发生的特定错误（比如前次请求并未返回任何结果等）。而其余的错误通常都在网络层予以处理了。 我们使用 PushInMemoryRequestDescription/AWSMultipartRequestDescription 来对我们对自己的 API 服务器或者是对 AWS 服务器发起请求的行为进行抽象。我们的上传代码将会根据情况在两者之前进行切换，对两者使用不同的 URL 会话配置，以免将我们自有 API 服务器的认证信息发送至 AWS 。 我不会详细讨论整个代码，但是我希望这个例子能充分展现我之前提到过的声明式设计方法的一系列优点： 关注点分离: 上面编写的类型只有描述一系列请求这一单一功能。 减少重复的代码: 上面编写的类型里面只包含对请求进行描述的代码，并不包含网络请求及错误处理的代码。 优秀的 API 设计: 这样的 API 设计能有效的减轻开发者的负担，他们只需要实现一个简单的协议以确保后续产生的请求是基于前一个请求结果的即可。 良好的可读性: 再次声明，以上代码非常集中；我们不需要在样板代码的海洋里游泳，就可以找到代码的意图。那也说明，为了更快地理解这段代码，你需要对我们的抽象方式有一定的了解。 现在可以想想如果利用 NSOperationQueue 来替代我们的方案会怎么样？ 什么是 NSOperationQueue ？采用 NSOperationQueue 的方案复杂了很多，所以在这篇文章里给出相对应的代码并不是一个很好的选择。不过我们还是可以讨论下这种方案。 关注点分离在这种方案中难以实现。和对请求序列进行简单抽象不同的是，NSOperationQueue 中的 NSOperations 对象将负责网络请求的开关操作。这里面包含请求取消和错误处理等特性。在不同的位置都有相似的上传代码，同时这些代码很难进行复用。在大多数上传请求被抽象成一个 NSOperation 的情况下，使用子类并不是一个好选择，虽然说我们得上传请求队列被抽象成为一个被 NSOperationQueue 所装饰的 NSOperation 。 NSOperationQueue 中的无关信息相当多。。代码中随处可见对网络层的操作和调用 NSOperation 中的特定方法，比如 main 和 finish 方法。在没有深入了解具体的 API 调用规则前，很难知道具体操作是用来做什么的 这种 API 所采用的处理方式，某种意义上让开发者的开发体验变得更差了。和简单的实现相对应的协议不同的是，在 Swift 中如果采用上述的开发方式，人们需要去了解一些约定俗成的规定，尽管这些规定可能并不强制要求你遵守。 这种处理方式将会显著增加开发者的负担。与实现一个简单协议不同的是，在新版本的 Swift 中实现这样的代码的话，我们需要去理解一些特有的约定。尽管很多被记载下来的约定并不是与编程相关的。 由于一些其他原因，该 API 可能会导致一些与网络请求的错误报告相关的 bug 。为了避免每个请求操作都执行自己的错误报告代码，我们将其集中在一个地方进行处理。错误处理代码将会在请求结束之后开始执行。然后代码将会检查请求类型中的 error 属性的值是否存在。为了及时地反馈错误信息，开发者需要及时在操作完成之前设置 NSOperation 中的 error 属性的值。由于这是一个非强制性约定导致一堆新代码忘记设置其属性的值，可能会导致诸多错误信息的遗失。 所以啊，我们很期待我们介绍的这样一种新的方式能帮助开发者们在未来编写上传及其余功能的代码。 总结声明式的编程方法已经对我们的编程技能和开发效率产生了巨大的影响。我们提供了一种受限的 API ，这种 API 用途单一且不会留下一堆迷之 Bug 。我们可以避免使用子类及多态等一系列手段，转而使用基于泛型类型的声明式风格代码来替代它。我们可以写出优美的代码。我们所编写的代码都是能很方便的进行测试的(关于这点，编程爱好者们可能觉得在声明式风格代码中测试可能不是必要的。）所以你可能想问：“别告诉我这是一种完美无瑕的编程方式？” 首先，在具体的抽象过程中，我们可能会花费一些时间与精力。不过，这种花费可以通过仔细设计 API ，并并通过提供一些测试，代替用例实现功能，为使用者提供参考。 其次，请注意，声明式编程并不是适用于任何时间任何业务的。要想适用声明式编程，你的代码库里至少要有一个用相似方法解决了多次的问题。如果你尝试在一个需要高度可定制化的应用里使用声明式编程， 然后你又对整个代码进行了错误的抽象，那么最后你会得到如同乱麻一般的半声明式代码。对于任何的抽象过程而言，过早地进行抽象都会造成一大堆令人费解的问题。 声明式 API 有效地将 API 使用者身上的压力转移至 API 开发者身上，对于命令式 API 则不需要这样。为了提供一组优秀的声明式 API ，API 的开发者必须确保接口的使用与接口的实现细节进行严格的隔离。不过严格遵循这样要求的 API 是很少的。React 和 GraphQL 证明了声明式 API 能有效提升团队编码的体验。 其实我觉得，这只是一个开端，我们会慢慢发现在复杂的库中所隐藏复杂的细节和对外提供的简单易用的接口。期待有一天，我们能利用一个基于声明式编程的 UI 库来构建我们的 iOS 程序。","link":"/posts/2016/10/25/Declarative-API-Design-in-Swift/"},{"title":"Swift 3 中的函数参数命名规范指北","text":"原文地址：Function Naming In Swift 3 原文作者：Pablo Villar 译文出自：掘金翻译计划 译者：Zheaoli 校对者：Kulbear, Tuccuay 昨天，我开始将这个 Jayme 迁移到 Swift 3。这是我第一次将一个项目从 Swift 2.2 迁移至 Swift 3。说实话这个过程十分的繁琐，由于 Swift 3 在老版本基础上发生了很多比较大的改变，我不得不承认眼前这样一个事实，除了花费较多的时间以外，没有其余的捷径可走。不过这样的经历也带来一点好处：我对 Swift 3 的理解变得更为深入，对我来讲，这可能是最好的消息了。😃 在迁移代码的过程中，我需要做出很多的选择。更为蛋疼的是，整个迁移过程并不是修改代码那么简单，你还需要用耐心去一点点适应 Swift 3 中带来的新变化。某种意义上来讲，修改代码只是整个迁移过程的开始而已。 如果你已经决定将你的代码迁移到 Swift 3 ，我建议你去看看这篇文章来作为你万里长征的第一步。 如果一切顺利的话，在不久以后，我将回去写一篇博客来记录下整个迁移过程中的点点滴滴，包括我所作出的决定等等。但是眼前，我将会把注意力集中在一个非常非常重要的问题上：怎样正确的编写函数签名. 开篇首先，让我们来看看在 Swift 3 与 Swift 2 相比函数命名方式的差异吧。 在 Swift 2 中，函数中的第一个参数的标签在调用时可以省略，这是为了遵循这样一个 good ol’ Objective-C conventions 标准。比如我们可以这样写代码： 1234// Swift 2func handleError(error: NSError) { }let error = NSError()handleError(error) // Looks like Objective-C 在 Swift 3 中调用函数时，其实也是有办法省略第一个参数的标签的，但默认情况下不是这样： 12345// Swift 3func handleError(error: NSError) { }let error = NSError()handleError(error) // Does not compile!// ⛔ Missing argument label 'error:' in call 当遇到这样的情况时，我们第一反应可能是下面这样的： 123456// Swift 3func handleError(error: NSError) { }let error = NSError()handleError(error: error) // Had to write 'error' three times in a row!// My eyes already hurt 🙈 当然如果这样做，你肯定会很快意识到你的代码将将会变得有多坑爹。 如同前面所说的一样，在 Swift 3 中，我们是可以在调用函数时，将第一个参数的标签省略的，但是记住，你要去明确的告诉编译器这一点： 12345// Swift 3func handleError(_ error: NSError) { }// 🖐 Notice the underscore!let error = NSError()handleError(error) // Same as in Swift 2 你可能在使用 Xcode 自带的迁移工具进行迁移时遇到这样的情况。 注意，在函数签名中的下划线的意思是：告诉编译器，我们在调用函数时第一个参数不需要外带标签。这样，我们可以按照 Swift 2 中的方式去调用函数。 此外，你需要意识到，Swift 3 之所以修改了函数编写方式，是为了保证其一致性与可读性：我们不在需要对不同的参数区别对待。我想这可能是你遇到的第一个问题。 好了，现在代码可以编译运行了，但是你必须知道，你需要反复的去阅读 Swift 3 API design guidelines 一文。 ☝️ 一点微小的人生经验：你需要随时去诵读 Swift 3 API design guidelines 一文，这会为你解锁 Swift 开发的新体位。 第二步，精简你的代码 让我们再来看看之前的代码: 为了精简我们的代码，你可以将你的代码进行修剪一番，比如去除函数名里的类型信息等。 12345// Swift 3func handle(_ error: NSError) { /* ... */ }let error = NSError()handle(error) // Type name has been pruned// from function name, since it was redundant 如果你想让你的代码变得更短，更精悍，更明了的话，我给你们讲，作为一个钦定的开发者，一定要去反复诵读这篇 Swift 3 API design guidelines 文章到可以默写为止。 要注意让函数的调用过程是清晰、明确的，我们根据以下两点来确定函数的的命名和参数： 我们知道函数的返回类型 我们知道参数所对应的类型（比如在上面这个例子中，我们毫无疑问的知道其参数所属的类型是 NSError）。 更多的一些问题现在请睁大眼睛看清楚我们下面所讨论的东西。 ⚠️ 上面我们所讲的东西并没有包括所有可能出现的情况，换句话说，你可能遇到这样一种特殊情况，即，一个参数的类型没有办法直观的体现其作用。 让我们考虑下面这样一种情况： 123// Swift 2func requestForPath(path: String) -&gt; URLRequest { }let request = requestForPath(&quot;local:80/users&quot;) 如果你想将代码迁移到 Swift 3 ，那么根据已有的知识，你可能会这么做： 123// Swift 3func request(_ path: String) -&gt; URLRequest { }let request = request(&quot;local:80/users&quot;) 讲真，这段代码看起来可读性很差，让我们稍微修改下： 123// Swift 3func request(for path: String) -&gt; URLRequest { }let request = request(for: &quot;local:80/users&quot;) OK，现在看起来舒服多了，但是并没有解决我上面提到的问题。 在我们调用这个函数的时候，我们怎样很直观的知道我们需要给这个参数传递一个 Web Url 呢？你所能提前知道的是你需要传递一个 String 类型的变量进去，但是你并不清楚你需要传递一个 Web Url 进去。 同理，我们在一个大型项目中，我们需要很清楚的明白每个参数的作用所在，但是很明显，目前我们还没有解决这个大问题，比如: 你怎么知道一个 String 类型的变量代表着 Web Url。 你怎么知道一个 Int 类型的变量代表着 Http 状态码。[String: String] 你怎么知道一个 [String: String] 类型的变量代表着 Http Header。 等等…。 ⚠️ 综上，我给你们一点微小的人生经验吧: 谨慎精简你的代码 ✄ 回到代码上，我们可以给参数添加上相对应的标签来解决这个问题，好了看看下面这个代码： 12func request(forPath path: String) -&gt; URLRequest { }let request = request(forPath: &quot;local:80/users&quot;) 好了，现在代码看起来是不是更清楚，可读性更强了呢？ 🎉 恭喜~ 讲真，看到这里其实你可以关闭浏览器了，但是事实上，下面才是最精华的部分。 好了，让我们来看看关于函数参命名的用词问题： 12func request(forPath path: String) -&gt; URLRequest { }// The word 'path' appears twice 这段代码看起来不错，但是如果你想让其变得更好，那么请看接下来的部分。 你所不知道的小技巧这个小技巧很简单：在上下文中反映参数的类型及作用，这样你就可以无脑的精简你的代码了。 呐，我们来看看下面这段代码。 1234typealias Path = String // To the rescue!func request(for path: Path) -&gt; URLRequest { }let request = request(for: &quot;local:80/users&quot;) 在这个例子中，参数的类型和参数的作用表达达成了一个完美的统一，因为你在上下文中为 String 赋予了一个别名叫做 Path。 现在，你的函数看起来还是依旧的精简，可读性较高，但是却不重复。 以此类推，你可以使用同样的方式来书写一些优美的代码，比如： 1234typealias Path = Stringtypealias StatusCode = Inttypealias HTTPHeader = [String: String]// etc... 如你所见，你可以尽情的写精简而优美的代码了。 不过，请记住，凡事走向极端便变了味了：这个小技巧会为你的代码添加额外的负担，特别是你们代码存在多重嵌套的情况下。因此请记住，如果你无脑的使用这样的小技巧的话，那么你可能会付出一些惨痛的代价。 结论很多时候，你在使用 Swift 3 时，命名函数的时候你会遇到很多困难。 积累一些代码片段可能会帮助你很多： 123456789101112131415func remove(at position: Index) -&gt; Element { }employees.remove(at: x)func remove(_ member: Element) -&gt; Element? { }allViews.remove(cancelButton)func url(forPath path: String) -&gt; URL { }let url = url(forPath: &quot;local:80/users&quot;)typealias Path = String // Alternativefunc url(for path: Path) -&gt; URL { }let url = url(for: &quot;local:80/users&quot;)func entity(from dictionary: [String: Any]) -&gt; Entity { /* ... */ }let entity = entity(from: [&quot;id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;John&quot;])","link":"/posts/2016/10/09/Function-Naming-In-Swift-3/"},{"title":"详解模板引擎工作机制","text":"原文地址：How a template engine works 原文作者：Shipeng Feng 译文出自：掘金翻译计划 译者： Zheaoli 校对者：Kulbear, hpoenixf 我已经使用各种模版引擎很久了，现在终于有时间研究一下模版引擎到底是如何工作的了。 简介简单的说，模版引擎是一种可以用来完成涉及大量文本数据的编程任务的工具。一般而言，我们经常在一个 web 应用中利用模板引擎来生成 HTML 。在 Python 中，当你想使用模板引擎的时候，你会发现你有不少的选择，比如jinja 或者是mako。从现在开始，我们将利用 tornado 中的模板引擎来讲解模板引擎的工作原理，在 tornado 中，自带的模板引擎相对的简单，能方便我们去深入的剖析其原理。 在我们研究（模版引擎）的实现原理之前，先让我们来看一个简单的接口调用例子。 123456789101112131415from tornado import templatePAGE_HTML = &quot;&quot;&quot;&lt;html&gt; Hello, {{ username }}! &lt;ul&gt; {% for job in job_list %} &lt;li&gt;{{ job }}&lt;/li&gt; {% end %} &lt;/ul&gt;&lt;/html&gt; &quot;&quot;&quot;t = template.Template(PAGE_HTML)print t.generate(username='John', job_list=['engineer']) 这段代码里的 username 将会动态的生成，job 列表也是如此。你可以通过安装 tornado 并运行这段代码来看看最后的效果。 详解如果你仔细观察 PAGE_HTML ，你会发现这段模板字符串由两个部分组成，一部分是固定的字符串，另一部分是将会动态生成的内容。我们将会用特殊的符号来标注动态生成的部分。在整个工作流程中，模板引擎需要正确输出固定的字符串，同时需要将正确的结果替换我们所标注的需要动态生成的字符串。 使用模板引擎最简单的方式就是像下面这样用一行 python 代码就可以解决： 12deftemplate_engine(template_string, **context):# process herereturn result_string 在整个工作过程中，模板引擎将会分为如下两个阶段对我们的字符串进行操作： 解析 渲染 在解析阶段，我们将我们准备好的字符串进行解析，然后格式化成可被渲染的格式，其可能是能被 rendered.Consider 所解析的字符串，解析器可能是一个语言的解释器或是一个语言的编译器。如果解析器是一种解释器的话，在解析过程中将会生成一种特殊的数据结构来存放数据，然后渲染器会遍历整个数据结构来进行渲染。例如 Django 的模板引擎中的解析器就是一种基于解释器的工具。除此之外，解析器可能会生成一些可执行代码，渲染器将只会执行这些代码，然后生成对应的结果。在 Jinja2 ， Mako ，Tornado 中，模板引擎都在使用编译器来作为解析工具。 编译如同上面所说的一样，我们需要解析我们所编写的模板字符串，然后 tornado 中的模板解析器将会将我们所编写的模板字符串编译成可执行的 Python 代码。我们的解析工具负责生成Python代码，而仅仅由单个Python函数构成： 123def parse_template(template_string): # compilation return python_source_code 在我们分析 parse_template 的代码之前，让我们先看个模板字符串的例子： 12345678&lt;html&gt; Hello, { { username } }! &lt;ul&gt; { % for job in jobs % } &lt;li&gt;{ { job.name } }&lt;/li&gt; { % end % } &lt;/ul&gt;&lt;/html&gt; 模板引擎里的 parse_template 函数将会将上面这个字符串编译成 Python 源码，最简单的实现方式如下： 12345678910111213def _execute(): _buffer = [] _buffer.append('\\n&lt;html&gt;\\n Hello, ') _tmp = username _buffer.append(str(_tmp)) _buffer.append('!\\n &lt;ul&gt;\\n ') for job in jobs: _buffer.append('\\n &lt;li&gt;') _tmp = job.name _buffer.append(str(_tmp)) _buffer.append('&lt;/li&gt;\\n ') _buffer.append('\\n &lt;/ul&gt;\\n&lt;/html&gt;\\n') return''.join(_buffer) 现在我们在 _execute 函数里处理我们的模版。这个函数将可以使用全局命名空间里的所有有效变量。这个函数将创建一个包含多个 string 的列表并将他们合并后返回。显然找到一个局部变量比找一个全局变量要快多了。同时，我们对于其余代码的优化也在这个阶段完成，比如： 12345_buffer.append('hello')_append_buffer = _buffer.append# faster for repeated use_append_buffer('hello') 在 { { ... } } 中的表达式将会被提取出来，然后添加进 string 列表中。在 tornado 模板模块中，在 { { ... } } 所编写的表达式没有任何的限制，if 和 for 代码块都可以准确地转换成为 Python 代码。 让我们来看看具体的代码实现吧让我们来看看模板引擎的具体实现吧。我们在 Template 类中编声明核心变量，当我们创建一个 Template 对象后，我们便可以编译我们所编写的模板字符串，随后我们便可以根据编译的结果来对其进行渲染。我们只需要对我们所编写的模板字符串进行一次编译，然后我们可以缓存我们的编译结果，下面是 Template 类的简化版本的构造器： 1234class Template(object): def__init__(self, template_string): self.code = parse_template(template_string) self.compiled = compile(self.code, '&lt;string&gt;', 'exec') 上段代码里的 compile 函数将会将字符串编译成为可执行代码，我们可以稍后调用 exec 函数来执行我们生成的代码。现在，让我们来看看 parse_template 函数的实现，首先，我们需要将我们所编写的模板字符串转化成一个个独立的节点，为我们后面生成 Python 代码做好准备。在这过程中，我们需要一个 _parse 函数，我们先把它放在一边，等下在回来看看这个函数。现，我们需要编写一些辅助函数来帮助我们从模板文件里读取数据。现在让我们来看看 _TemplateReader 这个类，它用于从我们自定义的模板中读取数据： 123456789101112131415161718192021222324252627282930313233343536373839class _TemplateReader(object): def __init__(self, text): self.text = text self.pos = 0 def find(self, needle, start=0, end=None): pos = self.pos start += pos if end is None: index = self.text.find(needle, start) else: end += pos index = self.text.find(needle, start, end) if index != -1: index -= pos return index def consume(self, count=None): if count is None: count = len(self.text) - self.pos newpos = self.pos + count s = self.text[self.pos:newpos] self.pos = newpos return s def remaining(self): return len(self.text) - self.pos def __len__(self): return self.remaining() def __getitem__(self, key): if key &lt; 0: return self.text[key] else: return self.text[self.pos + key] def __str__(self): return self.text[self.pos:] 为了生成 Python 代码，我们需要去看看 _CodeWriter 这个类的源码，这个类可以编写代码行和管理缩进，同时它也是一个 Python 上下文管理器： 123456789101112131415161718192021222324252627class _CodeWriter(object): def __init__(self): self.buffer = cStringIO.StringIO() self._indent = 0 def indent(self): return self def indent_size(self): return self._indent def __enter__(self): self._indent += 1 return self def __exit__(self, *args): self._indent -= 1 def write_line(self, line, indent=None): if indent == None: indent = self._indent for i in xrange(indent): self.buffer.write(&quot; &quot;) print self.buffer, line def __str__(self): return self.buffer.getvalue() 在 parse_template 函数里，我们先要创建一个 _TemplateReader 对象： 123456def parse_template(template_string): reader = _TemplateReader(template_string) file_node = _File(_parse(reader)) writer = _CodeWriter() file_node.generate(writer) return str(writer) 然后，我们将我们所创建的 _TemplateReader 对象传入 _parse 函数中以便生成节点列表。这里生成的所有节点都是模板文件的子节点。接着，我们创建一个 _CodeWriter 对象，然后 file_node 对象会把生成的 Python 代码写入 _CodeWriter 对象中。然后我们返回一系列动态生成的 Python 代码。_Node 类将会用一种特殊的方法去生成 Python 源码。这个先放着，我们等下再绕回来看。 现在先让我们回头看看前面所说的 _parse 函数： 123456789101112131415161718192021222324252627def _parse(reader, in_block=None): body = _ChunkList([]) while True: # Find next template directive curly = 0 while True: curly = reader.find(&quot;{&quot;, curly) if curly == -1 or curly + 1 == reader.remaining(): # EOF if in_block: raise ParseError(&quot;Missing { %% end %% } block for %s&quot; % in_block) body.chunks.append(_Text(reader.consume())) return body # If the first curly brace is not the start of a special token, # start searching from the character after it if reader[curly + 1] not in (&quot;{&quot;, &quot;%&quot;): curly += 1 continue # When there are more than 2 curlies in a row, use the # innermost ones. This is useful when generating languages # like latex where curlies are also meaningful if (curly + 2 &lt; reader.remaining() and reader[curly + 1] == '{' and reader[curly + 2] == '{'): curly += 1 continue break 我们将在文件中无限循环下去来查找我们所规定的特殊标记符号。当我们到达文件的末尾处时，我们将文本节点添加至列表中然后退出循环。 123# Append any text before the special tokenif curly &gt; 0: body.chunks.append(_Text(reader.consume(curly))) 在我们对特殊标记的代码块进行处理之前，我们先将静态的部分添加至节点列表中。 1start_brace = reader.consume(2) 在遇到 { { 或者 { % 的符号时，我们便开始着手处理相应的的表达式： 1234567891011# Expressionif start_brace == &quot;{ {&quot;: end = reader.find(&quot;} }&quot;) if end == -1 or reader.find(&quot;\\n&quot;, 0, end) != -1: raise ParseError(&quot;Missing end expression } }&quot;) contents = reader.consume(end).strip() reader.consume(2) if not contents: raise ParseError(&quot;Empty expression&quot;) body.chunks.append(_Expression(contents)) continue 当遇到 { { 之时，便意味着后面会跟随一个表达式，我们只需要将表达式提取出来，并添加至 _Expression 节点列表中。 1234567891011121314151617181920212223# Blockassert start_brace == &quot;{ %&quot;, start_braceend = reader.find(&quot;% }&quot;)if end == -1 or reader.find(&quot;\\n&quot;, 0, end) != -1: raise ParseError(&quot;Missing end block % }&quot;)contents = reader.consume(end).strip()reader.consume(2)if not contents: raise ParseError(&quot;Empty block tag ({ % % })&quot;)operator, space, suffix = contents.partition(&quot; &quot;)# End tagif operator == &quot;end&quot;: if not in_block: raise ParseError(&quot;Extra { % end % } block&quot;) return bodyelif operator in (&quot;try&quot;, &quot;if&quot;, &quot;for&quot;, &quot;while&quot;): # parse inner body recursively block_body = _parse(reader, operator) block = _ControlBlock(contents, block_body) body.chunks.append(block) continueelse: raise ParseError(&quot;unknown operator: %r&quot; % operator) 在遇到模板里的代码块的时候，我们需要通过递归的方式将代码块提取出来，并添加至 _ControlBlock 节点列表中。当遇到 { % end % } 时，意味着这个代码块的结束，这个时候我们可以跳出相对应的函数了。 好了现在，让我们看看之前所提到的 _Node 节点，别慌，这其实是很简单的： 123456789101112class _Node(object): def generate(self, writer): raise NotImplementedError()class _ChunkList(_Node): def __init__(self, chunks): self.chunks = chunks def generate(self, writer): for chunk in self.chunks: chunk.generate(writer) _ChunkList 只是一个节点列表而已。 12345678910class _File(_Node): def __init__(self, body): self.body = body def generate(self, writer): writer.write_line(&quot;def _execute():&quot;) with writer.indent(): writer.write_line(&quot;_buffer = []&quot;) self.body.generate(writer) writer.write_line(&quot;return ''.join(_buffer)&quot;) 在 _File 中，它会将 _execute 函数写入 CodeWriter。 1234567891011121314151617class _Expression(_Node): def __init__(self, expression): self.expression = expression def generate(self, writer): writer.write_line(&quot;_tmp = %s&quot; % self.expression) writer.write_line(&quot;_buffer.append(str(_tmp))&quot;)class _Text(_Node): def __init__(self, value): self.value = value def generate(self, writer): value = self.value if value: writer.write_line('_buffer.append(%r)' % value) _Text 和 _Expression 节点的实现也非常简单，它们只是将我们从模板里获取的数据添加进列表中。 123456789class _ControlBlock(_Node): def __init__(self, statement, body=None): self.statement = statement self.body = body def generate(self, writer): writer.write_line(&quot;%s:&quot; % self.statement) with writer.indent(): self.body.generate(writer) 在 _ControlBlock 中，我们需要将我们获取的代码块按 Python 语法进行格式化。 现在让我们看看之前所提到的模板引擎的渲染部分，我们通过在 Template 对象中实现 generate 方法来调用从模板中解析出来的 Python 代码。 123456def generate(self, **kwargs): namespace = { } namespace.update(kwargs) exec self.compiled in namespace execute = namespace[&quot;_execute&quot;] return execute() 在给予的全局命名空间中， exec 函数将会执行编译过的代码对象。然后我们就可以在全局中调用 _execute 函数了。 最后经过上面的一系列操作，我们便可以尽情的编译我们的模板并得到相对应的结果了。其实在 tornado 模板引擎中，还有很多特性是我们没有讨论到的，不过，我们已经了解了其最基础的工作机制，你可以在此基础上去研究你所感兴趣的部分，比如： 模板继承 模板包含 其余的一些逻辑控制语句，比如 else , elfi , try 等等 空白控制 特殊字符转译 更多没讲到的模板指令（译者注：请参考 tornado 官方文档","link":"/posts/2016/08/13/How-a-template-engine-works/"},{"title":"她曾以为自己能逃开教授的手丨人间","text":"她曾以为自己能逃开教授的手丨人间 《不能说的夏天》剧照 之前听舍友笑薇被教授性骚扰时，小柯还以为那只是老师对好学生的亲昵行为，安慰她说：“这应该是老师表达欣赏你的一种方式吧。”但几天后她就也被教授性骚扰了。 陈静越来越焦虑。 她又梦见去上课，楼梯里遇到教授张鹏，转身想跑，对方一把手抓住她，恶毒地问：你为什么举报我？你把我逼急了，我也不让你活…… 在惊恐中醒来，陈静大汗淋漓。 早在今年“五四”青年节，她们五个女生给中大纪委发去了举报信，指控张鹏从2011年到2017年持续性骚扰女学生和女老师，是田野中名副其实的“叫兽”。 张鹏，中山大学社会学与人类学学院（下称“社人院”）兼生命科学大学院教授，跨学科博士生导师（生态学、社会学方向），兼任国际自然保护联盟（IUCN）物种生存委员会委员，2016年青年长江学者。在网络上搜索他的名字，无论是文艺青年的社交网路，或者是著名的科普网站，他会经常跟“灵长类动物研究”出现在一起。 1陈静的噩梦始于2016年1月底的内伶仃猕猴种群数量田野调查。 田野调查是中大社人院每年组织的特色研究活动。张鹏每年寒假都会带队去不同的岛“蹲点”和“环岛”，以此来评估岛上有多少猕猴。陈静喜欢观察猴子的行为模式，田野中一直在蹲点观测。 田野调查最后一天，学员们相互体验，陈静从蹲点观测转为环岛两圈。第一圈时，她遇到在其中一段路蹲点观测的张鹏。 注意到独自环岛的陈静，张鹏说：“我陪你走一段吧。”当时的陈静觉得，陈教授“真亲民”——大一时她听过张鹏的人类学系列讲座，感觉“内容丰富，氛围活跃”，对灵长类研究产生了浓烈的兴趣，也对授课的张鹏心怀敬意。 路上，陈静话不多，张鹏一会儿称赞陈静的长相，“你长得真可爱”，一会儿又分析她的性格，“有南方姑娘温柔气质，又有北方人的豪爽”。逐渐地，张鹏向陈静靠得越来越近，聊得越来越具体，“你的头发真好”，顺势把玩起陈静的长发，还时不时闻一下说，“真香啊。” 陈静先是觉得尴尬，后来越来越不舒服，她隐隐觉得这不应该是一个教授应有的举动，却又不知如何是好，只能加快脚步赶到下一段有同学在的地方。 环岛第二圈，陈静再次遇上张鹏时，张鹏又上前提出“一起走吧”，还很自然地把手搭在陈静肩膀上。陈静不适，碍于师生情面，并未明确拒绝。走过泥泞路段，张鹏突然拉着陈静比起了身高，“感觉你没1米6啊”，比完身高又说“想看看你有多重”，就在陈静不知要如何回答还未及时拒绝时，张鹏一把抱起了陈静，还顺势把头埋到陈静胸口深深吸了一口气。 “那时候整个人完全懵了，不敢相信。”陈静说，想起他在自己胸口闻的那个动作，至今仍想干呕，有一股强烈的羞耻感。陈静挣扎起来，张鹏才把她放下，一放下，陈静就快步跑开，拿出手机，紧张地给她姐姐以及姐姐的男朋友所在的群发信息：“感觉张教授是‘叫兽’。” 陈静姐姐的男友、中山大学2013届历史系学生陈翰元，向我佐证了陈静的讲述。他坦言，当时看到陈静的那条信息，并没太当回事，“以为张鹏也就是在她面前讲了个黄色笑话”。等陈静回到住处，详细和他们讲起具体细节时，陈翰元才意识到这是性骚扰。当时的陈翰元也只是从男性的角度猜测，问：“张鹏是不是很喜欢你啊？” 陈翰元建议陈静去了解一下张鹏的为人，他听过张鹏不少光鲜的头衔，主持如国家自然科学基金及中日国际交流等不少国家级科研项目，也听说过张鹏还有个被广为流传事迹：据说，做研究时，张鹏曾给一个猴群里最漂亮的一只母猴以他女朋友的名字命名，靠此来排解在山上几个月里对女朋友的思念。后来者也跟着张鹏叫起这个名字，于是就在观察笔记写：xxx（张鹏女朋友名字）今天和一只雄猴打情骂俏，明天和另一只雄猴交配。张鹏也津津乐道。 陈翰元担心陈静因为张鹏一次偶尔的“低级错误”影响她对学业的追求，曾尝试劝解。但是，后来好几次聚餐，陈静都会跟姐姐和陈翰元谈起被张鹏性骚扰的经历，越发变得压抑，情绪低落，还持续做噩梦。 陈静无法理解和接受张鹏的举动，也不能让别人理解自己的感受，“感觉自己在孤岛里转”，只好选择了暂时性遗忘。此后她再遇到张鹏，就能躲多远就躲多远，即便上曾经最喜欢的“灵长类进化论”，都变成一种煎熬。 后来，陈静陆陆续续听到张鹏性骚扰其他女生的事情。 2其实，张鹏对女学生更频繁的性骚扰集中在2015年。 那一年暑假，张鹏带队到海南南湾猴岛进行为期一个月的野外实习。当时大二的笑薇和小柯在这次田野实习中，先后遭受了张鹏的性骚扰。 电话里寒暄时，笑薇说话轻快爽朗；谈到张鹏，她的语速变得缓慢而坚定：“张鹏真的伤害了一群女生，他没有资格做中大教授。” 笑薇向我缓缓了讲述她遭遇张鹏性骚扰的经历： 一天开完组会后，约深夜11点，张鹏喊住她“来办公室改论文”。笑薇想，白天都在外观察猴群，晚上讨论也是情理之中；而且张鹏从日本回来，看起来对科研十分严格，于是毫无担忧地去了。 笑薇原本坐在张鹏对面，张鹏指着电脑屏幕招呼她坐到自己同侧。开始笑薇还跟他保持了半米的礼貌距离，张鹏又叫她坐近一点，“坐过来看得清楚”。 出于对张鹏教授身份的尊敬，笑薇没多想，论文讲到一半，张鹏指出她论文问题，同时右手环住她的肩膀久久不拿开，笑薇感到窘迫。 “他先是说了论文这里不对、那里有问题，然后拍拍我的肩膀，拍完手就放着不走了。”笑薇当时思绪复杂，一边想着论文，一边莫名害怕，“还安慰自己，长辈拍晚辈肩膀是常有的，是自己胡思乱想”。 然而，张鹏的话暧昧起来，“你长得真漂亮啊”，手还拍起笑薇的手背。“那时我真是害怕，紧张，窘迫，他却表现得很自然。”笑薇说，回想起来她才发现张鹏的恐怖，“他一边指出你论文各种问题，让你害怕，一边又似乎安慰你，挽肩膀拍手，让你难以理解他动作的真实含义；他控制着你的情绪，让你的注意力都在论文问题上，一时辨识不了他行为的性质。” 在笑薇此前的认知中，张鹏教授常年在野外，年轻有为，风趣幽默——“眼前这个人跟课堂上谈笑风生的让人尊敬的教授完全不是一个人啊！怎么会这样？他不是日本回来的学者吗？” 在坐立不安中笑薇艰难度过了讲论文的一个小时。回到寝室，她跟几个舍友说起张鹏对她的举动。 舍友小柯问她：“是不是你想多了？” 小柯后来在接受采访时向我解释，她当时这样问，并不是质疑笑薇——她从小学到高中，一直都幸运地遇上各种好老师，所以一直觉得老师都是高山仰止，会爱护学生，形象正直高大。所以当笑薇说张鹏“有点不对劲”、对自己动手动脚的时候，小柯还以为那只是老师对好学生的亲昵行为，安慰她说：“这应该是老师表达欣赏你的一种方式吧。” 笑薇的另一位舍友对我说，那晚她听到笑薇说张鹏讲暧昧的话、还摸手搭背时十分惊讶，“我虽没听过张鹏的课，但很多人都说他课讲得好，没想到是这样的老师”。田野回来后，笑薇也曾多次向她透露对张鹏的反感，“她说不想写这个田野报告，不喜欢张鹏，不想见到他”。 笑薇和小柯都坦率告诉我，她们那时未曾意识到、或者意识到了却不愿意相信备受尊敬的教授会性骚扰自己，“如果是陌生人，他随便搭着你的肩膀，摸你的背，拍你的手，闻你的头发，又说‘你很漂亮’此类的话我肯定知道这是性骚扰，但这个人是老师啊，是自己原本尊敬的教授，他那么威严，怎么去辨识他的行为呢？” 女孩们讨论最终得出的解决方案是：不再单独与张鹏相处，找张鹏改论文的时候舍友们要在门口等着。 3然而，小柯很快就“被现实啪啪啪地打脸了”：张鹏也性骚扰了她。 几天后的一次组会结束，小柯让两舍友在门外等，自己带着报告进去张鹏办公室。问完问题小柯想走，张鹏却开始跟她聊起无关田野实习和论文的事，还紧挨着她坐下，一边笑着说奉承话，一边抓起她手腕看，“你的手好细啊”，等小柯把手抽开，张鹏又摸起小柯的头发，“你的头发发质好好啊”，还抓起她一缕头发把玩起来。 小柯尴尬极了，忐忑不已，却手足无措。突然，张鹏起身，走到门口探出身子左右张望——直到后来，小柯才意识到他当时探出头是为了看外面有没有人。 当时小柯还安慰自己，舍友就在外面，不用害怕。然而，她却见张鹏以“外面虫子多”为由把门关上了。关上门的那一刻，小柯懵了。后来她才知道，由于张鹏拖得时间过长，舍友们先行回了宿舍。 关了门后的张鹏言语越发露骨：“我看你这么努力，总让我想到我小时候，也这么努力。你就像一个小妹妹……让我抱一下……”不等小柯拒绝，张鹏便一把抱住了她，“我脑袋一片空白，他一松开的时候，我就赶紧跑走了”。 小柯满脸通红跑出张鹏办公室的一幕，正好被路过的笑薇看见。两个女孩难以理解，张鹏作为一名已婚教授，行为为何如此不堪？她们也不懂，事后张鹏为什么可以没有丝毫避讳，还毫无廉耻之心出现在女学生们面前。 “他看起来那么理直气壮，那么自然，让你怀疑，是不是自己太敏感了？”小柯曾猜测，女学生或许是张鹏心血来潮的戏弄对象？——可她后来才知道是自己后知后觉，张鹏对她的骚扰行为其实早就有了苗头。2015年春，小柯曾和同学们跟随张鹏到上川岛进行一个“小田野”。傍晚休息时，同学们和张鹏商量看第二天的日出，小柯应和并着手查询次日的天气。就在此时，张鹏走到她后面，把手心贴在小柯后背心上，久久不曾放开。当时，小柯按下内心的不适，将这一动作看作长辈对晚辈的一种亲昵，“没想到他是一步步试探”。 即便看到了张鹏的“叫兽”面目，女生们也不知道如何是好：岛上只有他们一个调研组，只有张鹏一个教授，她们不知道该向谁申诉。而且，田野报告需要张鹏打分，她们还有张鹏的课，甚至已经选了张鹏做论文指导导师。 她们能做的，只是不再与张鹏正面接触。 “那段日子有巨大的阴影笼罩在身上，世界仿佛到处都是黑暗。”电话那头，原本激动着控诉张鹏恶行的小柯突然放低了声音，“你知道吗？黄记者，那时感觉自己在地狱。” 4其实，女生们也曾做出力所能及的反抗。 因为项目和论文，小柯还是需要时常与张鹏接触，她曾认为掌握了张鹏的“套路”，“感觉可以保护自己了”。 每次要向张鹏当面汇报时，小柯都会提高警惕：“他把手放在了我大腿上，我直接把腿移开了；他用眼神从上到下扫一遍，那种眼神让人很不舒服，但我没办法控制他的眼神；他问你买了新衣服了？是不是烫头发了、变漂亮了？我都会说‘不是’，并且把话题立刻转移到论文或项目上。” 强硬起来对抗老师，小柯觉得结果没有想象中那么糟糕——那时，她并不知道还有更多女生受到了张鹏更为放肆的性骚扰。 张鹏也并未收敛。 小柯清楚记得，2017年4月的一个夜里，晚上8点多，张鹏走到她自习的桌前当面邀约，“再谈谈论文”，小柯不好当面拒绝，也不好当张鹏的面拿出手机提前给舍友发信息通风，只能跟着他去了办公室。 在办公室里留到快10点，张鹏就坐到了小柯身边，她担心起来，偷偷地快速给舍友发信息：“等我。” 张鹏看到小柯发信息的动作，立刻火了：“老师专门辅导，你竟玩手机？！”然后，张鹏开始用带有侮辱性言语攻击她：“没教养，自私自利”，“老师为了你的论文到现在都没吃饭，你呢？为老师做了什么？我把实验室的资源都提供给你，你又为实验室做了什么？” 小柯被张鹏的翻脸吓坏了，只得道歉，然而，张鹏并未停止责骂：“老师为你付出那么多，你是不是把老师当工具？是不是想快毕业了，可以远走高飞，翻脸不认人，什么都不为我做了？” 小柯难以相信，一个教授，为人师表，竟然说出这样的话。她后来和其他女生交流才得知，张鹏会抓住不同的女生们犯下的各种“错误”，在性骚扰不遂或被拒绝后都复制着一样的骂人模式，试图控制女生们的思想。 “他骂很多女生自私自利，可是，这个实验室本身就是人类学系系里为学生教学投资的，我们应当都有权力正当使用。然而，每次在实验室时，张鹏都要让我们觉得，（能使用实验室）这都是因为他的好心和慷慨，这个道德包袱太重了。” 后来，被张鹏性骚扰过的女生们聚集起来才发现，张鹏通常会选择性格温和、家庭背景普通、独立无援的女学生为骚扰对象。他的性骚扰行为在多人身上重复出现，呈现某种模式化特点： 他不明目张胆地胁迫，而是策划和利用情境（如修改论文、做田野项目），逐步拉近距离；他还会操控受害者心理，找到不同理由和借口严厉训斥，先打击、摧毁女生的自尊自信，使得学生战战兢兢；然后柔声抚慰，诉说欣赏、喜欢之情，打着“师长的关爱”的幌子借机拍背、捏手、拥抱、甚至亲吻，让惊慌的女生无法辨识其动作的真实目的。 小柯和笑薇感到庆幸——她俩及时毕业了，并未遭遇张鹏更为严重的骚扰。 5一次偶然借书机会，陈静认识了师姐小柯。熟悉后，小柯叮嘱师妹，“要小心张鹏”。两人细聊才知道，原来张鹏的性骚扰对象涉及各个年级的女生。 这个结论让她们感到更大的震惊，不约而同萌发了举报张鹏的念头，尤其是后来听到消息后：张鹏性骚扰了2017届的大一师妹，情节严重，接近性侵害。女孩告知了父母，其父亲来到中大评理，因有视频佐证，张鹏无法抵赖，被党内处分。 两个中大2017届的人类学专业的学生向我佐证了这个消息，他们承认“级里都在传”。其中一位学生透露，事发2018年4月3日晚上约10点半，张鹏与受害者女生单独在实验室，张鹏关了灯，对女生进行了严重的性骚扰。第二天，受害者女生父亲气冲冲地来到学校，他们刚好那时在同一栋大楼，看到有两名保安前往实验室取证，还调取了实验室走廊的视频。有当时在实验室的学生匿名向我证实，确实有保安前来调取视频。 知情学生透露说，视频画面里，张鹏先是从他办公室出来，到其他办公室敲了敲门，然后关了灯，又回到了自己的办公室，近半小时后，张鹏先走出办公室，就在走廊里提了提裤子，并把露在外面的衣角重新塞进裤子里，随后女生出来，两人一同离开。 办公室里具体发生了什么，该学生并不清楚，但知道第二天女孩的父亲就来了学校，去了纪委办公室。 张鹏终于被处分，这让学生们看到了一丝希望；但一个“党内处分”，并未平息学生们的愤怒。 “张鹏性骚扰学生的消息就没停过，但是他还是一直在性骚扰学生，而且情况越来越严重”。陈静说，她感觉“不能发生了不可逆转的性侵才举报，那就太迟了”。 小柯也气愤学校的保守处理：“张鹏的行为越来越大胆，一年比一年严重，真的要造成性侵这样实实在在的伤害、有视频证据，才能处罚他吗？” 得知情况的陈翰元也坐不住了：“他不是一次性的冲动，而是一而再再而三性骚扰学生，是一个惯犯，中大怎么能容忍这样的教授？” 受过张鹏性骚扰的女学生们自觉组成举报联盟，笑薇在班级群里实名告知师妹们：“如果选了张鹏的课和‘田野’，一定要格外小心保护自己。” 这引来了更多当事人响应。举报人们收集到了4封实名举报信和1封匿名举报信，让她们没想到的是，举报信中，竟然有一封是一名女老师写的。 6女老师的举报把张鹏最初的性骚扰行为时点提前到2011年，而且张鹏对这位女老师的性骚扰更为直接、严重。 因为这位女老师已经给纪委实名举报交代，并签下协议不再向外透露其他信息，我只能引用此前早已掌握的举报信材料。 女老师在举报信里称，2011年她刚入职中大外国语学院，在往返于中大南校区与东校区（大学城）的校车上，张鹏与她搭讪，“（他）坐我旁边座位，没说几句就开始摸，先是肩膀，再到大腿和大腿内侧，那个时候我很怕，车上有老师有学生，我不敢喊。只能闪避，比如背对他或者甩开他的手。” 当天晚上，她坐校车返回南校时，又遇到张鹏。张鹏借机坐在她身边，“他先是不断找我攀谈，讲述家里的烦心事，妻子不了解他等等。看我没有怎么搭理，就又开始动手动脚，把我的头拉向他的肩膀，并试图亲吻我的耳朵，并继续向胸部和大腿内侧摸。我跟他说，张老师，您这么做可不妥当。他说，我就是很喜欢你啊！当时校车已近校门，我赶紧甩开他下车了！” 之后，张鹏尾随她，并变本加厉进行骚扰，“上课下课都跟着我，找到机会就凑过来……动作越来越过分，往耳朵吹气，抚摸胸部，语言上多次要求发生男女关系，我没有办法，只好每次课都尽可能地约学生陪同搭车。当时我认识了一个住在南校、跟我一样需要搭校车往返的女学生。下课后，留意到张又在尾随我，为了不引起他的注意，我都用外语小声告诉学生这件事，希望她保护我。至此之后，该女生便一直陪同我，每次都坐在我的座位旁边。她也亲眼看到了张鹏的一些性骚扰行为。” 经多方打听，我只得知这位随行的女生是当时旅游管理学院的一位学生，但后来去了法国留学，没有留下有效的联系方式，我至今没有联系到对方进行佐证。 张鹏的性骚扰后来越发露骨和恶劣。另一知情的学生透露，女老师在写举报信前曾跟她说过：2012年初，张鹏平均每天发两三条短信或者打电话给她，言辞暧昧，直接要求去酒店开房或去办公室约会。 2012年春季，女老师调到珠海校区上课，张鹏不知道通过什么方式找到她在珠海校区的教师公寓住址，来到她门口不断敲门，“一敲就敲差不多一个小时，教师公寓的住客比较少，周围没什么人，我吓得不敢动弹。他又不断给我发短信，用词非常露骨，我只能不断地删除，并且把他拉黑名单，因为他，我换了三次号码。” 这段骚扰持续了几年，女老师已经结婚生子，直到2017年，张鹏依然尝试添加她的微信，纠缠不止。 72017届女生和女老师的遭遇给学子们敲醒了警钟：若继续沉默，只能成为待宰羔羊。2018年5月4日，青年节的时候，女生们实名给中大纪委寄出了五个当事人的举报信。 女生们的举报在学院里传开，支持当事人的学生们自主成立了“中山大学人类学系反性骚扰小组”（下称小组），草拟了建议信并半公开征集联署，很快把《人类学系学子关于本系的舆论事件及加快建立本院反性骚扰机制的建议信》发送至院长、副院长、系主任的邮箱。 学生们提出，他们查阅了《中山大学学生手册》、《中山大学学生申诉处理暂行办法》以及附录中的《高等学校校园秩序管理若干规定》和《学生伤害事故处理办法》，都没有找到关于性骚扰问题向何处申诉、哪个机构/部门负责处理、如何处理等信息。“文件多，却都没有实质操作意义”。 他们还查阅了《中山大学关于建立健全师德建设长效机制的实施办法》，文件中确实有指出纪检监察部门负责接收师德相关的举报：“只是，我们发现，里面邮箱负责人都不知道是谁；查到的联系电话，大多是党政办公室的电话，也是外部联系社人院的联系通路，其繁忙程度可想而知。在无专人负责、事务繁杂的情况下，我们有理由质疑，通过这部电话进行的申诉能否得到重视和处理。” 小组建议，尽快出台有效的校园反性骚扰机制，进行性骚扰的师生教育、田野行前培训；在院系层面设立公开渠道接受关于性骚扰的投诉举报、设立专门的负责人受理相关事宜等。 还有中大学生在“为学校发展规划建言献策之‘十大提案’活动”中上交了“关于中山大学师风师德规范细则的建议”的提案，提案详细分析了《中山大学关于建立健全师德建设长效机制的实施办法》和《中山大学教师考核实施办法（试行）》，指出，“中山大学在制度层面上已经有师德建设与师德考核制度，但既存的不同制度之间的重叠、区分甚至相互矛盾，不同渠道的程序的复杂，都让受害者望而却步，让既有的好制度失去其应有的作用。” 然而，提案上交后，一些校领导多次找了提交提案的学子们谈话，表示提案做得很好，但“这个话题太敏感不宜公开讨论”，甚至拒绝了让学生公开对提案进行答辩，在提案优秀奖的奖状上，也不能出现提案具体名称。 我联系中大相关部门对张鹏性骚扰事件以及学子们对反性骚扰机制的建议和提案进行回应，对方表示“不能接受采访，所有采访通过中大宣传部”。我于7月5日、6日的上午和下午的办公时间，分别打了4次电话致电中大宣传部，一直无人接听。 8陈翰元告诉我，最终触发他实名站出来的原因是，他看到张静的焦虑以及她对学术的失望。 “她以前对灵长类很有兴趣的，说起猴子的属性，观察它们群居生活，给它们一一命名，说起来她都是神飞色舞的。她去岛上田野，住得不好，吃得不好，蚊子多，被咬得一身包，她一句怨言都没有，都是兴致勃勃的。现在呢，因为张鹏，她都放弃了原本最感兴趣的研究。对学术的热爱一下子被打破了。张鹏这样性骚扰女生，是断了女孩子一条学术路，伤害女孩子平等受教育的权利。”陈翰元十分惋惜，“她那么聪明、勤奋，如果遇到的是个好导师，肯定会继续做科研的。” 陈静坦言，后来张鹏叫她参加暑假期间印尼苏门答腊的一个研究项目，她因不想再与张鹏接触，放弃了机会。 小柯原本也想尝试在同一个方向做科研，但课题结束后，完全失去了对灵长类的兴趣。她发现自己对张鹏有很强的心理抵触，“国内的灵长类研究领域里张鹏有一定的权威性，教授都这样了，还有什么意思呢？”多种原因下，她放弃了这个研究方向，甚至放弃了在中大的读研机会。 笑薇也告诉我，虽然张鹏的性骚扰不是导致她放弃灵长类研究的唯一原因，但也是主要原因之一。2017级的女孩子，在被张鹏性侵害后，同样选择了放弃。 “你说，他伤害了那么多的女学生，为什么还能继续留在中大？”陈静久久不能释怀，“为什么学校会认为‘老师对学生的捏捏抱抱、亲亲吻吻是小事一桩’呢？” 举报两个月了。中大纪委与女孩们一一座谈了，但对张鹏的处罚仍是未知之数。 女生们说，纪委调查期间也问话了张鹏，张鹏把一切都否认了。 我打电话采访张鹏，问他女孩们举报信上的内容是否属实？张鹏说了一句“你没有工作单位，我不认识你”，便挂掉了我的电话，不再回应。 张鹏仍如往常一样，在实验室里来来回回，若无其事。张鹏的妻子也走进了实验室，要求实验室学生们写一份“张鹏老师无不当行为”的证明，但遭到实验室学生的拒绝。 陈静和其他还留在学校的当事人担心事件再一次被压下去，她们害怕如果学校继续纵容，那以后张鹏必然会更加肆无忌惮伤害更多的人。 更让她们寒心的是，在一个课程群里，仍有老师把名为《你还敢报中山大学人类学的在职研究生吗》的帖子贴到群里，还公开发表了“不要过分纠结”、“有些社会对带色笑话能够容忍，可以舒缓工作压力”等言论。 举报者们好不容易积累起来的勇气和信心正在一点点流逝。 “难道真的需要用生命做祭品，像庆阳女孩一样，才能让他的行为看起来恶劣吗？我们该怎么办？” 陈静又焦虑起来，这一天她的噩梦里，张鹏拿着刀，准备杀了她。 （应受访者要求：陈静、笑薇、小柯为化名） 编辑：许智博 点击联系人间编辑","link":"/posts/2018/07/07/She-thought-she-can-survive-from-sexual-assualt/"},{"title":"听说你会 Python （2）：Python 高阶数据结构解析","text":"前言之前写过一篇《听说你会 Python ？》的文章，大家反响都还不错，那么我想干脆把这个文章做成一个系列，继续讲解一下 Python 当中那些不为人知的细节吧。然后之前在和师父川爷讨论面试的时候，川爷说了一句“要是我，我就考考你们怎么去实现一个 namedtuple ，好用，方便，又能区分人”，说者无心，听者有意，我于是决定在这次的文章中，和大家聊一聊 Python 中一个特殊的高阶数据结构， namedtuple 的实现。 Let’s beginnamedtuple介绍tuple 是 Python 中 build-in 的一种特殊的数据结构，它是一种 immutable 的数据集合，我们经常会这样使用它 123456789def test(): a = (1, 2) print(a) return aif __name__ == '__main__': b, c = test() print(a) Right，很多时候我们会直接使用 tuple 来进行一些数据的 packing/unpacking 的操作。OK，关于 tuple 的科普就到这里。那么什么是 namedtuple 呢，恩，前面不是说了 tuple 是一种特殊的数据集合么，那么 namedtuple 是其一个进阶（这不是废话么）。它将会基础的 tuple 抽象成一个类，我们将自行定义变量的名称和类的名称，这样我们可以很方便的将其复用并管理。具体的用法我们可以看看下面这个例子 12345if __name__ == '__main__': fuck=namedtuple(&quot;fuck&quot;, ['x', 'y']) a=fuck(1,2) print(a.x) print(a.y) 恩，这样看起来貌似更直观了点，但是，但是，但是，我猜你肯定想知道 namedtuple 是怎么实现的，那么我们先来看看代码吧 详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150_class_template = '''\\class {typename}(tuple): '{typename}({arg_list})' __slots__ = () _fields = {field_names!r} def __new__(_cls, {arg_list}): 'Create new instance of {typename}({arg_list})' return _tuple.__new__(_cls, ({arg_list})) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new {typename} object from a sequence or iterable' result = new(cls, iterable) if len(result) != {num_fields:d}: raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result)) return result def __repr__(self): 'Return a nicely formatted representation string' return '{typename}({repr_fmt})' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values' return OrderedDict(zip(self._fields, self)) def _replace(_self, **kwds): 'Return a new {typename} object replacing specified fields with new values' result = _self._make(map(kwds.pop, {field_names!r}, _self)) if kwds: raise ValueError('Got unexpected field names: %r' % kwds.keys()) return result def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) __dict__ = _property(_asdict) def __getstate__(self): 'Exclude the OrderedDict from pickling' pass{field_defs}'''_repr_template = '{name}=%r'_field_template = '''\\ {name} = _property(_itemgetter({index:d}), doc='Alias for field number {index:d}')'''def namedtuple(typename, field_names, verbose=False, rename=False): &quot;&quot;&quot;Returns a new subclass of tuple with named fields. &gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y']) &gt;&gt;&gt; Point.__doc__ # docstring for the new class 'Point(x, y)' &gt;&gt;&gt; p = Point(11, y=22) # instantiate with positional args or keywords &gt;&gt;&gt; p[0] + p[1] # indexable like a plain tuple 33 &gt;&gt;&gt; x, y = p # unpack like a regular tuple &gt;&gt;&gt; x, y (11, 22) &gt;&gt;&gt; p.x + p.y # fields also accessible by name 33 &gt;&gt;&gt; d = p._asdict() # convert to a dictionary &gt;&gt;&gt; d['x'] 11 &gt;&gt;&gt; Point(**d) # convert from a dictionary Point(x=11, y=22) &gt;&gt;&gt; p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22) &quot;&quot;&quot; # Validate the field names. At the user's option, either generate an error # message or automatically replace the field name with a valid name. if isinstance(field_names, basestring): field_names = field_names.replace(',', ' ').split() field_names = map(str, field_names) typename = str(typename) if rename: seen = set() for index, name in enumerate(field_names): if (not all(c.isalnum() or c=='_' for c in name) or _iskeyword(name) or not name or name[0].isdigit() or name.startswith('_') or name in seen): field_names[index] = '_%d' % index seen.add(name) for name in [typename] + field_names: if type(name) != str: raise TypeError('Type names and field names must be strings') if not all(c.isalnum() or c=='_' for c in name): raise ValueError('Type names and field names can only contain ' 'alphanumeric characters and underscores: %r' % name) if _iskeyword(name): raise ValueError('Type names and field names cannot be a ' 'keyword: %r' % name) if name[0].isdigit(): raise ValueError('Type names and field names cannot start with ' 'a number: %r' % name) seen = set() for name in field_names: if name.startswith('_') and not rename: raise ValueError('Field names cannot start with an underscore: ' '%r' % name) if name in seen: raise ValueError('Encountered duplicate field name: %r' % name) seen.add(name) # Fill-in the class template class_definition = _class_template.format( typename = typename, field_names = tuple(field_names), num_fields = len(field_names), arg_list = repr(tuple(field_names)).replace(&quot;'&quot;, &quot;&quot;)[1:-1], repr_fmt = ', '.join(_repr_template.format(name=name) for name in field_names), field_defs = '\\n'.join(_field_template.format(index=index, name=name) for index, name in enumerate(field_names)) ) if verbose: print class_definition # Execute the template string in a temporary namespace and support # tracing utilities by setting a value for frame.f_globals['__name__'] namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename, OrderedDict=OrderedDict, _property=property, _tuple=tuple) try: exec class_definition in namespace except SyntaxError as e: raise SyntaxError(e.message + ':\\n' + class_definition) result = namespace[typename] # For pickling to work, the __module__ variable needs to be set to the frame # where the named tuple is created. Bypass this step in environments where # sys._getframe is not defined (Jython for example) or sys._getframe is not # defined for arguments greater than 0 (IronPython). try: result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__') except (AttributeError, ValueError): pass return result 这，这，这，这特么什么玩意儿啊！没事,我们慢慢来看。首先，下面这一部分代码，将会校验我们传入的数据是否符合要求 1234567891011121314151617181920212223242526272829303132333435if isinstance(field_names, basestring): field_names = field_names.replace(',', ' ').split()field_names = map(str, field_names)typename = str(typename)if rename: seen = set() for index, name in enumerate(field_names): if (not all(c.isalnum() or c=='_' for c in name) or _iskeyword(name) or not name or name[0].isdigit() or name.startswith('_') or name in seen): field_names[index] = '_%d' % index seen.add(name)for name in [typename] + field_names: if type(name) != str: raise TypeError('Type names and field names must be strings') if not all(c.isalnum() or c=='_' for c in name): raise ValueError('Type names and field names can only contain ' 'alphanumeric characters and underscores: %r' % name) if _iskeyword(name): raise ValueError('Type names and field names cannot be a ' 'keyword: %r' % name) if name[0].isdigit(): raise ValueError('Type names and field names cannot start with ' 'a number: %r' % name)seen = set()for name in field_names: if name.startswith('_') and not rename: raise ValueError('Field names cannot start with an underscore: ' '%r' % name) if name in seen: raise ValueError('Encountered duplicate field name: %r' % name) seen.add(name) 接着，便是我们 namedtuple 的核心代码 12345678910111213141516171819202122class_definition = _class_template.format( typename = typename, field_names = tuple(field_names), num_fields = len(field_names), arg_list = repr(tuple(field_names)).replace(&quot;'&quot;, &quot;&quot;)[1:-1], repr_fmt = ', '.join(_repr_template.format(name=name) for name in field_names), field_defs = '\\n'.join(_field_template.format(index=index, name=name) for index, name in enumerate(field_names)))if verbose: print class_definition# Execute the template string in a temporary namespace and support# tracing utilities by setting a value for frame.f_globals['__name__']namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename, OrderedDict=OrderedDict, _property=property, _tuple=tuple)try: exec class_definition in namespaceexcept SyntaxError as e: raise SyntaxError(e.message + ':\\n' + class_definition)result = namespace[typename] 你是不是想说，what the fuck！我知道，class_definition 、 _repr_template 和 _field_template 是前面所定义的字符串模板 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253_class_template = '''\\class {typename}(tuple): '{typename}({arg_list})' __slots__ = () _fields = {field_names!r} def __new__(_cls, {arg_list}): 'Create new instance of {typename}({arg_list})' return _tuple.__new__(_cls, ({arg_list})) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new {typename} object from a sequence or iterable' result = new(cls, iterable) if len(result) != {num_fields:d}: raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result)) return result def __repr__(self): 'Return a nicely formatted representation string' return '{typename}({repr_fmt})' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values' return OrderedDict(zip(self._fields, self)) def _replace(_self, **kwds): 'Return a new {typename} object replacing specified fields with new values' result = _self._make(map(kwds.pop, {field_names!r}, _self)) if kwds: raise ValueError('Got unexpected field names: %r' % kwds.keys()) return result def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) __dict__ = _property(_asdict) def __getstate__(self): 'Exclude the OrderedDict from pickling' pass{field_defs}'''_repr_template = '{name}=%r'_field_template = '''\\ {name} = _property(_itemgetter({index:d}), doc='Alias for field number {index:d}')''' 但是其余的是什么鬼啊！别急，字符串模板我们先放在一边，我们先来看看后面的一段代码 1234567namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename, OrderedDict=OrderedDict, _property=property, _tuple=tuple)try: exec class_definition in namespaceexcept SyntaxError as e: raise SyntaxError(e.message + ':\\n' + class_definition)result = namespace[typename] 在这段代码中，首先 namespace 变量是一个字典，里面设置了一些变量的存在，紧接就是 exec class_definition in namespace 。众所周知，Python 是一门动态语言，在 Python 中，解释器允许我们在运行时，生成一些包含了符合 Python 语法语句的字符串，并用 exec 将其作为 Python 代码进行执行。同时在我们生成一些语句字符串的时候，我们可能会使用一些自定义的变量，于是，我们需要提供一个 dict 供其进行变量的查找。知道前面这些知识点后，exec class_definition in namespace 的作用是不是就很清楚了捏。好了，我们再回过头去看 class_definition 定义。不过我们直接看未格式化之前的模板未免的太过于枯燥和难懂了，我们干脆以前面举过的一个例子来看看格式化后的 class_definition 吧~ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class fuck(tuple): 'fuck(x, y)' __slots__ = () _fields = ('x', 'y') def __new__(_cls, x, y): 'Create new instance of fuck(x, y)' return _tuple.__new__(_cls, (x, y)) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new fuck object from a sequence or iterable' result = new(cls, iterable) if len(result) != 2: raise TypeError('Expected 2 arguments, got %d' % len(result)) return result def __repr__(self): 'Return a nicely formatted representation string' return 'fuck(x=%r, y=%r)' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values' return OrderedDict(zip(self._fields, self)) def _replace(_self, **kwds): 'Return a new fuck object replacing specified fields with new values' result = _self._make(map(kwds.pop, ('x', 'y'), _self)) if kwds: raise ValueError('Got unexpected field names: %r' % kwds.keys()) return result def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) __dict__ = _property(_asdict) def __getstate__(self): 'Exclude the OrderedDict from pickling' pass x = _property(_itemgetter(0), doc='Alias for field number 0') y = _property(_itemgetter(1), doc='Alias for field number 1') 好了，让我们一点点来分析，首先 class fuck(tuple) 指明我们创建的 fuck 类是继承自 tuple 。紧接着 __new__ 是 Python 对象系统中的一个特殊方法，用于我们的实例化的操作，其在 __init__ 之前便被触发，其是一个特殊的静态方法，我们可以将其用于实例缓存等特殊的功能。在这里，__new__ 将会返回一个 tuple 的实例。接下来的是是一些特殊的私有方法，代码很好懂，我们就不细讲了，接着我们来看看这样一段代码 123x = _property(_itemgetter(0), doc='Alias for field number 0')y = _property(_itemgetter(1), doc='Alias for field number 1') 你可能还不知道这两段代码用来是干什么的233，没事儿，我们慢慢来。还记得前面我们举过的一个例子么 12345if __name__ == '__main__': fuck=namedtuple(&quot;fuck&quot;, ['x', 'y']) a=fuck(1,2) print(a.x) print(a.y) 你可能会突发奇想，要是我们执行 a.x=1 这样的操作会怎样呢？OK，你会发现，Python 会抛出一个异常叫做 AttributeError: can't set attribute ，嗯哼，讲到这里，你可能就知道前面提到的包含 property 的两行代码作用就是保证 namedtuple 的 immutable 的特性。那么你可能还是不知道这是为什么。这和 Python 增加的描述符机制有关 扩展（1）：Python 中的描述符首先我们要明确一点，描述符指的是实现了描述符协议的特殊的类，三个描述符协议指的是 __get__ , ‘set‘ , __delete__ 以及 Python 3.6 中新增的 __set_name__ 方法，其中实现了 __get__ 以及 __set__ / __delete__ / __set_name__ 的是 Data descriptors ，而只实现了 __get__ 的是 Non-Data descriptor 。那么有什么区别呢，前面说了， 我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。 但是，这里没有考虑描述符的因素进去，如果将描述符因素考虑进去，那么正确的表述应该是我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。其中如果在类实例字典中的该属性是一个 Data descriptors ，那么无论实例字典中存在该属性与否，无条件走描述符协议进行调用，在类实例字典中的该属性是一个 Non-Data descriptors ，那么优先调用实例字典中的属性值而不触发描述符协议，如果实例字典中不存在该属性值，那么触发 Non-Data descriptor 的描述符协议。 可能这讲完了，你还是不清楚和前面问题有什么关联，没事儿，我们接下来会讲讲 property 的实现 扩展（2）：Property 详解首先我们来看看关于 Property 的实现 1234567891011121314151617181920212223242526272829303132333435class Property(object): &quot;Emulate PyProperty_Type() in Objects/descrobject.c&quot; def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(&quot;unreadable attribute&quot;) return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError(&quot;can't set attribute&quot;) self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(&quot;can't delete attribute&quot;) self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__) 当我们执行完这两句语句时 123x = _property(_itemgetter(0), doc='Alias for field number 0')y = _property(_itemgetter(1), doc='Alias for field number 1') 我们的 x 和 y 就变成了一个 property 对象的实例，它们也是一个描述符，还记得我们前面讲的么，当一个变量/成员成为一个描述符后，它将改变正常的调用逻辑，现在当我们 a.x=1 的时候，因为我们的x是一个 Data descriptors ，那么不管我们的实例字典中是否有 x 的存在，我们都会触发其 __set__ 方法，由于在我们初始化 x 和 y 两个变量时，没有给予其传入 fset 的方法，因此，我们 __set__ 方法在运行过程中将会抛出 AttributeError(&quot;can't set attribute&quot;) 的异常，这也保证了 namedtuple 遵循了 tuple 的 immutable 的特性！是不是很优美！Amazing！ 吐槽向其实很多人不知道我为什么选择 namedtuple 来作为本期的主题，其实很简单呀，namedtuple 中预定义模板，格式化，然后用 exec 函数进行执行这一套方法，是目前 Python 中主流模板引擎的核心原理。某种意义上讲，你在吃透这一点后，你也掌握了怎样去实现一个简易模板引擎的方法，如果大家有兴趣，我们可以下次一起来写一个简单的模板引擎。还有就是在 namedtuple 对于 Python 中的一些高阶特性使用的简直优美无比，这也是我们学习的好例子。 最后的最后，作为另一个写的非常优美的例子，我将 orderdict 的代码贴出来，大家可以下来看看，然后评论区我们讨论一个！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207class OrderedDict(dict): 'Dictionary that remembers insertion order' # An inherited dict maps keys to values. # The inherited dict provides __getitem__, __len__, __contains__, and get. # The remaining methods are order-aware. # Big-O running times for all methods are the same as regular dictionaries. # The internal self.__map dict maps keys to links in a doubly linked list. # The circular doubly linked list starts and ends with a sentinel element. # The sentinel element never gets deleted (this simplifies the algorithm). # Each link is stored as a list of length three: [PREV, NEXT, KEY]. def __init__(*args, **kwds): '''Initialize an ordered dictionary. The signature is the same as regular dictionaries, but keyword arguments are not recommended because their insertion order is arbitrary. ''' if not args: raise TypeError(&quot;descriptor '__init__' of 'OrderedDict' object &quot; &quot;needs an argument&quot;) self = args[0] args = args[1:] if len(args) &gt; 1: raise TypeError('expected at most 1 arguments, got %d' % len(args)) try: self.__root except AttributeError: self.__root = root = [] # sentinel node root[:] = [root, root, None] self.__map = {} self.__update(*args, **kwds) def __setitem__(self, key, value, dict_setitem=dict.__setitem__): 'od.__setitem__(i, y) &lt;==&gt; od[i]=y' # Setting a new item creates a new link at the end of the linked list, # and the inherited dictionary is updated with the new key/value pair. if key not in self: root = self.__root last = root[0] last[1] = root[0] = self.__map[key] = [last, root, key] return dict_setitem(self, key, value) def __delitem__(self, key, dict_delitem=dict.__delitem__): 'od.__delitem__(y) &lt;==&gt; del od[y]' # Deleting an existing item uses self.__map to find the link which gets # removed by updating the links in the predecessor and successor nodes. dict_delitem(self, key) link_prev, link_next, _ = self.__map.pop(key) link_prev[1] = link_next # update link_prev[NEXT] link_next[0] = link_prev # update link_next[PREV] def __iter__(self): 'od.__iter__() &lt;==&gt; iter(od)' # Traverse the linked list in order. root = self.__root curr = root[1] # start at the first node while curr is not root: yield curr[2] # yield the curr[KEY] curr = curr[1] # move to next node def __reversed__(self): 'od.__reversed__() &lt;==&gt; reversed(od)' # Traverse the linked list in reverse order. root = self.__root curr = root[0] # start at the last node while curr is not root: yield curr[2] # yield the curr[KEY] curr = curr[0] # move to previous node def clear(self): 'od.clear() -&gt; None. Remove all items from od.' root = self.__root root[:] = [root, root, None] self.__map.clear() dict.clear(self) # -- the following methods do not depend on the internal structure -- def keys(self): 'od.keys() -&gt; list of keys in od' return list(self) def values(self): 'od.values() -&gt; list of values in od' return [self[key] for key in self] def items(self): 'od.items() -&gt; list of (key, value) pairs in od' return [(key, self[key]) for key in self] def iterkeys(self): 'od.iterkeys() -&gt; an iterator over the keys in od' return iter(self) def itervalues(self): 'od.itervalues -&gt; an iterator over the values in od' for k in self: yield self[k] def iteritems(self): 'od.iteritems -&gt; an iterator over the (key, value) pairs in od' for k in self: yield (k, self[k]) update = MutableMapping.update __update = update # let subclasses override update without breaking __init__ __marker = object() def pop(self, key, default=__marker): '''od.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. ''' if key in self: result = self[key] del self[key] return result if default is self.__marker: raise KeyError(key) return default def setdefault(self, key, default=None): 'od.setdefault(k[,d]) -&gt; od.get(k,d), also set od[k]=d if k not in od' if key in self: return self[key] self[key] = default return default def popitem(self, last=True): '''od.popitem() -&gt; (k, v), return and remove a (key, value) pair. Pairs are returned in LIFO order if last is true or FIFO order if false. ''' if not self: raise KeyError('dictionary is empty') key = next(reversed(self) if last else iter(self)) value = self.pop(key) return key, value def __repr__(self, _repr_running={}): 'od.__repr__() &lt;==&gt; repr(od)' call_key = id(self), _get_ident() if call_key in _repr_running: return '...' _repr_running[call_key] = 1 try: if not self: return '%s()' % (self.__class__.__name__,) return '%s(%r)' % (self.__class__.__name__, self.items()) finally: del _repr_running[call_key] def __reduce__(self): 'Return state information for pickling' items = [[k, self[k]] for k in self] inst_dict = vars(self).copy() for k in vars(OrderedDict()): inst_dict.pop(k, None) if inst_dict: return (self.__class__, (items,), inst_dict) return self.__class__, (items,) def copy(self): 'od.copy() -&gt; a shallow copy of od' return self.__class__(self) @classmethod def fromkeys(cls, iterable, value=None): '''OD.fromkeys(S[, v]) -&gt; New ordered dictionary with keys from S. If not specified, the value defaults to None. ''' self = cls() for key in iterable: self[key] = value return self def __eq__(self, other): '''od.__eq__(y) &lt;==&gt; od==y. Comparison to another OD is order-sensitive while comparison to a regular mapping is order-insensitive. ''' if isinstance(other, OrderedDict): return dict.__eq__(self, other) and all(_imap(_eq, self, other)) return dict.__eq__(self, other) def __ne__(self, other): 'od.__ne__(y) &lt;==&gt; od!=y' return not self == other # -- the following methods support python 3.x style dictionary views -- def viewkeys(self): &quot;od.viewkeys() -&gt; a set-like object providing a view on od's keys&quot; return KeysView(self) def viewvalues(self): &quot;od.viewvalues() -&gt; an object providing a view on od's values&quot; return ValuesView(self) def viewitems(self): &quot;od.viewitems() -&gt; a set-like object providing a view on od's items&quot; return ItemsView(self) 参考目录 Descriptor HowTo Guide Python 描述符入门指北 collections","link":"/posts/2016/12/28/Someone-tell-me-that-you-think-Python-is-simple-2/"},{"title":"听说你会 Python ？","text":"前言最近觉得 Python 太“简单了”，于是在师父川爷面前放肆了一把：“我觉得 Python 是世界上最简单的语言！”。于是川爷嘴角闪过了一丝轻蔑的微笑（内心 OS：Naive！，作为一个 Python 开发者，我必须要给你一点人生经验，不然你不知道天高地厚！）于是川爷给我了一份满分 100 分的题，然后这篇文章就是记录下做这套题所踩过的坑。 1.列表生成器描述下面的代码会报错，为什么？ 1234567class A(object): x = 1 gen = (x for _ in xrange(10)) # gen=(x for _ in range(10))if __name__ == &quot;__main__&quot;: print(list(A.gen)) 答案这个问题是变量作用域问题，在 gen=(x for _ in xrange(10)) 中 gen 是一个 generator ,在 generator 中变量有自己的一套作用域，与其余作用域空间相互隔离。因此，将会出现这样的 NameError: name 'x' is not defined 的问题，那么解决方案是什么呢？答案是：用 lambda 。 12345678class A(object): x = 1 gen = (lambda x: (x for _ in xrange(10)))(x) # gen=(x for _ in range(10))if __name__ == &quot;__main__&quot;: print(list(A.gen)) 或者这样 1234567class A(object): x = 1 gen = (A.x for _ in xrange(10)) # gen=(x for _ in range(10))if __name__ == &quot;__main__&quot;: print(list(A.gen)) 补充感谢评论区几位提出的意见，这里我给一份官方文档的说明吧：The scope of names defined in a class block is limited to the class block; it does not extend to the code blocks of methods – this includes comprehensions and generator expressions since they are implemented using a function scope. This means that the following will fail: 123class A: a = 42 b = list(a + i for i in range(10)) 参考链接 Python2 Execution-Model:Naming-and-Binding ， Python3 Execution-Model:Resolution-of-Names。据说这是 PEP 227 中新增的提案，我回去会进一步详细考证。再次拜谢评论区 @没头脑很着急 @涂伟忠 @Cholerae 三位的勘误指正。 2.装饰器描述我想写一个类装饰器用来度量函数/方法运行时间 1234567891011import timeclass Timeit(object): def __init__(self, func): self._wrapped = func def __call__(self, *args, **kws): start_time = time.time() result = self._wrapped(*args, **kws) print(&quot;elapsed time is %s &quot; % (time.time() - start_time)) return result 这个装饰器能够运行在普通函数上： 12345678@Timeitdef func(): time.sleep(1) return &quot;invoking function func&quot;if __name__ == '__main__': func() # output: elapsed time is 1.00044410133 但是运行在方法上会报错，为什么？ 1234567891011class A(object): @Timeit def func(self): time.sleep(1) return 'invoking method func'if __name__ == '__main__': a = A() a.func() # Boom! 如果我坚持使用类装饰器，应该如何修改？ 答案使用类装饰器后，在调用 func 函数的过程中其对应的 instance 并不会传递给 __call__ 方法，造成其 mehtod unbound ,那么解决方法是什么呢？描述符赛高 123456789class Timeit(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print('invoking Timer') def __get__(self, instance, owner): return lambda *args, **kwargs: self.func(instance, *args, **kwargs) 3.Python 调用机制描述我们知道 __call__ 方法可以用来重载圆括号调用，好的，以为问题就这么简单？Naive！ 12345678class A(object): def __call__(self): print(&quot;invoking __call__ from A!&quot;)if __name__ == &quot;__main__&quot;: a = A() a() # output: invoking __call__ from A 现在我们可以看到 a() 似乎等价于 a.__call__() ,看起来很 Easy 对吧，好的，我现在想作死，又写出了如下的代码， 1234567a.__call__ = lambda: &quot;invoking __call__ from lambda&quot;a.__call__()# output:invoking __call__ from lambdaa()# output:invoking __call__ from A! 请大佬们解释下，为什么 a() 没有调用出 a.__call__() (此题由 USTC 王子博前辈提出) 答案原因在于，在 Python 中，新式类（ new class )的内建特殊方法，和实例的属性字典是相互隔离的，具体可以看看 Python 官方文档对于这一情况的说明 For new-style classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object’s type, not in the object’s instance dictionary. That behaviour is the reason why the following code raises an exception (unlike the equivalent example with old-style classes): 同时官方也给出了一个例子： 123456789101112class C(object): passc = C()c.__len__ = lambda: 5len(c)# Traceback (most recent call last):# File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;# TypeError: object of type 'C' has no len() 回到我们的例子上来，当我们在执行 a.__call__=lambda:&quot;invoking __call__ from lambda&quot; 时，的确在我们在 a.__dict__ 中新增加了一个 key 为 __call__ 的 item，但是当我们执行 a() 时，因为涉及特殊方法的调用，因此我们的调用过程不会从 a.__dict__ 中寻找属性，而是从 tyee(a).__dict__ 中寻找属性。因此，就会出现如上所述的情况。 4.描述符描述我想写一个 Exam 类，其属性 math 为 [0,100] 的整数，若赋值时不在此范围内则抛出异常，我决定用描述符来实现这个需求。 123456789101112131415161718192021222324252627282930class Grade(object): def __init__(self): self._score = 0 def __get__(self, instance, owner): return self._score def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: self._score = value else: raise ValueError('grade must be between 0 and 100')class Exam(object): math = Grade() def __init__(self, math): self.math = mathif __name__ == '__main__': niche = Exam(math=90) print(niche.math) # output : 90 snake = Exam(math=75) print(snake.math) # output : 75 snake.math = 120 # output: ValueError:grade must be between 0 and 100! 看起来一切正常。不过这里面有个巨大的问题，尝试说明是什么问题为了解决这个问题，我改写了 Grade 描述符如下： 12345678910111213class Grad(object): def __init__(self): self._grade_pool = {} def __get__(self, instance, owner): return self._grade_pool.get(instance, None) def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: _grade_pool = self.__dict__.setdefault('_grade_pool', {}) _grade_pool[instance] = value else: raise ValueError(&quot;fuck&quot;) 不过这样会导致更大的问题，请问该怎么解决这个问题？ 答案1.第一个问题的其实很简单，如果你再运行一次 print(niche.math) 你就会发现，输出值是 75 ，那么这是为什么呢？这就要先从 Python 的调用机制说起了。我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。好的，现在回到我们的问题，我们发现，在我们的类 Exam 中，其 self.math 的调用过程是，首先在实例化后的实例的 __dict__ 中进行查找，没有找到，接着往上一级，在我们的类 Exam 中进行查找，好的找到了，返回。那么这意味着，我们对于 self.math 的所有操作都是对于类变量 math 的操作。因此造成变量污染的问题。那么该则怎么解决呢？很多同志可能会说，恩，在 __set__ 函数中将值设置到具体的实例字典不就行了。那么这样可不可以呢？答案是，很明显不得行啊，至于为什么，就涉及到我们 Python 描述符的机制了，描述符指的是实现了描述符协议的特殊的类，三个描述符协议指的是 __get__ , ‘set‘ , __delete__ 以及 Python 3.6 中新增的 __set_name__ 方法，其中实现了 __get__ 以及 __set__ / __delete__ / __set_name__ 的是 Data descriptors ，而只实现了 __get__ 的是 Non-Data descriptor 。那么有什么区别呢，前面说了， 我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。 但是，这里没有考虑描述符的因素进去，如果将描述符因素考虑进去，那么正确的表述应该是我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。其中如果在类实例字典中的该属性是一个 Data descriptors ，那么无论实例字典中存在该属性与否，无条件走描述符协议进行调用，在类实例字典中的该属性是一个 Non-Data descriptors ，那么优先调用实例字典中的属性值而不触发描述符协议，如果实例字典中不存在该属性值，那么触发 Non-Data descriptor 的描述符协议。回到之前的问题，我们即使在 __set__ 将具体的属性写入实例字典中，但是由于类字典中存在着 Data descriptors ，因此，我们在调用 math 属性时，依旧会触发描述符协议。 2.经过改良的做法，利用 dict 的 key 唯一性，将具体的值与实例进行绑定，但是同时带来了内存泄露的问题。那么为什么会造成内存泄露呢，首先复习下我们的 dict 的特性，dict 最重要的一个特性，就是凡可 hash 的对象皆可为 key ，dict 通过利用的 hash 值的唯一性（严格意义上来讲并不是唯一，而是其 hash 值碰撞几率极小，近似认定其唯一）来保证 key 的不重复性，同时（敲黑板，重点来了），dict 中的 key 引用是强引用类型，会造成对应对象的引用计数的增加，可能造成对象无法被 gc ，从而产生内存泄露。那么这里该怎么解决呢？两种方法第一种： 1234567891011121314class Grad(object): def __init__(self): import weakref self._grade_pool = weakref.WeakKeyDictionary() def __get__(self, instance, owner): return self._grade_pool.get(instance, None) def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: _grade_pool = self.__dict__.setdefault('_grade_pool', {}) _grade_pool[instance] = value else: raise ValueError(&quot;fuck&quot;) weakref 库中的 WeakKeyDictionary 所产生的字典的 key 对于对象的引用是弱引用类型，其不会造成内存引用计数的增加，因此不会造成内存泄露。同理，如果我们为了避免 value 对于对象的强引用，我们可以使用 WeakValueDictionary 。第二种：在 Python 3.6 中，实现的 PEP 487 提案，为描述符新增加了一个协议，我们可以用其来绑定对应的对象： 123456789101112class Grad(object): def __get__(self, instance, owner): return instance.__dict__[self.key] def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: instance.__dict__[self.key] = value else: raise ValueError(&quot;fuck&quot;) def __set_name__(self, owner, name): self.key = name 这道题涉及的东西比较多，这里给出一点参考链接，invoking-descriptors , Descriptor HowTo Guide , PEP 487 , [what`s new in Python 3.6](https://docs.python.org/3.6/whatsnew/3.6.html#pep-487-descriptor-protocol-enhancements) 。 5.Python 继承机制描述试求出以下代码的输出结果。 12345678910111213141516171819202122232425262728293031class Init(object): def __init__(self, value): self.val = valueclass Add2(Init): def __init__(self, val): super(Add2, self).__init__(val) self.val += 2class Mul5(Init): def __init__(self, val): super(Mul5, self).__init__(val) self.val *= 5class Pro(Mul5, Add2): passclass Incr(Pro): csup = super(Pro) def __init__(self, val): self.csup.__init__(val) self.val += 1p = Incr(5)print(p.val) 答案输出是 36 ，具体可以参考 New-style Classes , multiple-inheritance 6. Python 特殊方法描述我写了一个通过重载 new 方法来实现单例模式的类。 1234567891011121314class Singleton(object): _instance = None def __new__(cls, *args, **kwargs): if cls._instance: return cls._instance cls._isntance = cv = object.__new__(cls, *args, **kwargs) return cvsin1 = Singleton()sin2 = Singleton()print(sin1 is sin2)# output: True 现在我有一堆类要实现为单例模式，所以我打算照葫芦画瓢写一个元类，这样可以让代码复用： 12345678910111213141516171819class SingleMeta(type): def __init__(cls, name, bases, dict): cls._instance = None __new__o = cls.__new__ def __new__(cls, *args, **kwargs): if cls._instance: return cls._instance cls._instance = cv = __new__o(cls, *args, **kwargs) return cv cls.__new__ = __new__class A(object): __metaclass__ = SingleMetaa1 = A() # what`s the fuck 哎呀，好气啊，为啥这会报错啊，我明明之前用这种方法给 __getattribute__ 打补丁的，下面这段代码能够捕获一切属性调用并打印参数 123456789101112131415161718192021class TraceAttribute(type): def __init__(cls, name, bases, dict): __getattribute__o = cls.__getattribute__ def __getattribute__(self, *args, **kwargs): print('__getattribute__:', args, kwargs) return __getattribute__o(self, *args, **kwargs) cls.__getattribute__ = __getattribute__class A(object): # Python 3 是 class A(object,metaclass=TraceAttribute): __metaclass__ = TraceAttribute a = 1 b = 2a = A()a.a# output: __getattribute__:('a',){}a.b 试解释为什么给 getattribute 打补丁成功，而 new 打补丁失败。如果我坚持使用元类给 new 打补丁来实现单例模式，应该怎么修改？ 答案其实这是最气人的一点，类里的 __new__ 是一个 staticmethod 因此替换的时候必须以 staticmethod 进行替换。答案如下： 1234567891011121314151617181920class SingleMeta(type): def __init__(cls, name, bases, dict): cls._instance = None __new__o = cls.__new__ @staticmethod def __new__(cls, *args, **kwargs): if cls._instance: return cls._instance cls._instance = cv = __new__o(cls, *args, **kwargs) return cv cls.__new__ = __new__class A(object): __metaclass__ = SingleMetaprint(A() is A()) # output: True 结语感谢师父大人的一套题让我开启新世界的大门，恩，博客上没法艾特，只能传递心意了。说实话 Python 的动态特性可以让其用众多 black magic 去实现一些很舒服的功能，当然这也对我们对语言特性及坑的掌握也变得更严格了，愿各位 Pythoner 没事阅读官方文档，早日达到装逼如风，常伴吾身的境界。","link":"/posts/2016/11/18/Someone-tell-me-that-you-think-Python-is-simple/"},{"title":"Python 描述符入门指北","text":"很久都没写 Flask 代码相关了，想想也真是惭愧，然并卵，这次还是不写 Flask 相关，不服你来打我啊（就这么贱，有本事咬我啊这次我来写一下 Python 一个很重要的东西，即 Descriptor （描述符） 初识描述符老规矩，Talk is cheap,Show me the code. 我们先来看看一段代码 1234567891011121314151617181920212223class Person(object): &quot;&quot;&quot;&quot;&quot;&quot; #---------------------------------------------------------------------- def __init__(self, first_name, last_name): &quot;&quot;&quot;Constructor&quot;&quot;&quot; self.first_name = first_name self.last_name = last_name #---------------------------------------------------------------------- @property def full_name(self): &quot;&quot;&quot; Return the full name &quot;&quot;&quot; return &quot;%s %s&quot; % (self.first_name, self.last_name)if __name__==&quot;__main__&quot;: person = Person(&quot;Mike&quot;, &quot;Driscoll&quot;) print(person.full_name) # 'Mike Driscoll' print(person.first_name) # 'Mike' 这段代大家肯定很熟悉，恩，property 嘛，谁不知道呢，但是 property 的实现机制大家清楚么？什么不清楚？那还学个毛的 Python 啊。。。开个玩笑，我们看下面一段代码 1234567891011121314151617181920212223242526272829303132333435class Property(object): &quot;Emulate PyProperty_Type() in Objects/descrobject.c&quot; def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(&quot;unreadable attribute&quot;) return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError(&quot;can't set attribute&quot;) self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(&quot;can't delete attribute&quot;) self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__) 看起来是不是很复杂，没事，我们来一步步的看。不过这里我们首先给出一个结论：Descriptors 是一种特殊 的对象，这种对象实现了 __get__ ，__set__ ，__delete__ 这三个特殊方法。 详解描述符说说 Property在上文，我们给出了 Propery 实现代码，现在让我们来详细说说这个 1234567891011121314151617181920212223class Person(object): &quot;&quot;&quot;&quot;&quot;&quot; #---------------------------------------------------------------------- def __init__(self, first_name, last_name): &quot;&quot;&quot;Constructor&quot;&quot;&quot; self.first_name = first_name self.last_name = last_name #---------------------------------------------------------------------- @Property def full_name(self): &quot;&quot;&quot; Return the full name &quot;&quot;&quot; return &quot;%s %s&quot; % (self.first_name, self.last_name)if __name__==&quot;__main__&quot;: person = Person(&quot;Mike&quot;, &quot;Driscoll&quot;) print(person.full_name) # 'Mike Driscoll' print(person.first_name) # 'Mike' 首先，如果你对装饰器不了解的话，你可能要去看看这篇文章，简而言之，在我们正式运行代码之前，我们的解释器就会对我们的代码进行一次扫描，对涉及装饰器的部分进行替换。类装饰器同理。在上文中，这段代码 123456@Propertydef full_name(self): &quot;&quot;&quot; Return the full name &quot;&quot;&quot; return &quot;%s %s&quot; % (self.first_name, self.last_name) 会触发这样一个过程，即 full_name=Property(full_name) 。然后在我们后面所实例化对象之后我们调用 person.full_name 这样一个过程其实等价于 person.full_name.__get__(person) 然后进而触发__get__() 方法里所写的 return self.fget(obj) 即原本上我们所编写的 def full_name 内的执行代码。 这个时候，同志们可以去思考下 getter() ,setter() ,以及 deleter() 的具体运行机制了=。=如果还是有问题，欢迎在评论里进行讨论。 关于描述符还记得之前我们所提到的一个定义么：Descriptors 是一种特殊的对象，这种对象实现了 __get__ ，__set__ ，__delete__ 这三个特殊方法。然后在 Python 官方文档的说明中，为了体现描述符的重要性，有这样一段话：“They are the mechanism behind properties, methods, static methods, class methods, and super(). They are used throughout Python itself to implement the new style classes introduced in version 2.2. ” 简而言之就是 先有描述符后有天，秒天秒地秒空气。恩，在新式类中，属性，方法调用，静态方法，类方法等都是基于描述符的特定使用。 OK，你可能想问，为什么描述符是这么重要呢？别急，我们接着看 使用描述符首先请看下一段代码 123456class A(object): #注：在 Python 3.x 版本中，对于 new class 的使用不需要显式的指定从 object 类进行继承，如果在 Python 2.X（x&gt;2)的版本中则需要 def a(self): passif __name__==&quot;__main__&quot;: a=A() a.a() 大家都注意到了我们存在着这样一个语句 a.a() ，好的，现在请大家思考下，我们在调用这个方法的时候发生了什么？OK？想出来了么？没有？好的我们继续首先我们调用一个属性的时候，不管是成员还是方法，我们都会触发这样一个方法用于调用属性 __getattribute__() ,在我们的 __getattribute__() 方法中，如果我们尝试调用的属性实现了我们的描述符协议，那么会产生这样一个调用过程 type(a).__dict__['a'].__get__(b,type(b))。好的这里我们又要给出一个结论了：“在这样一个调用过程中，有这样一个优先级顺序，如果我们所尝试调用属性是一个 data descriptors ，那么不管这个属性是否存在我们的实例的 __dict__ 字典中，优先调用我们描述符里的 __get__ 方法，如果我们所尝试调用属性是一个 non data descriptors，那么我们优先调用我们实例里的 __dict__ 里的存在的属性，如果不存在，则依照相应原则往上查找我们类，父类中的 __dict__ 中所包含的属性，一旦属性存在，则调用 __get__ 方法，如果不存在则调用 __getattr__() 方法”。理解起来有点抽象？没事，我们马上会讲，不过在这里，我们先要解释下 data descriptors 与 non data descriptors，再来看一个例子。什么是 data descriptors 与 non data descriptors 呢？其实很简单，在描述符中同时实现了 __get__ 与 __set__ 协议的描述符是 data descriptors ，如果只实现了 __get__ 协议的则是 non data descriptors 。好了我们现在来看个例子： 123456789101112131415161718192021222324252627import mathclass lazyproperty: def __init__(self, func): self.func = func def __get__(self, instance, owner): if instance is None: return self else: value = self.func(instance) setattr(instance, self.func.__name__, value) return valueclass Circle: def __init__(self, radius): self.radius = radius pass @lazyproperty def area(self): print(&quot;Com&quot;) return math.pi * self.radius * 2 def test(self): passif __name__=='__main__': c=Circle(4) print(c.area) 好的，让我们仔细来看看这段代码，首先类描述符 @lazyproperty 的替换过程，前面已经说了，我们不在重复。接着，在我们第一次调用 c.area 的时候，我们首先查询实例 c 的 __dict__ 中是否存在着 area 描述符，然后发现在 c 中既不存在描述符，也不存在这样一个属性，接着我们向上查询 Circle 中的 __dict__ ，然后查找到名为 area 的属性，同时这是一个 non data descriptors ，由于我们的实例字典内并不存在 area 属性，那么我们便调用类字典中的 area 的 __get__ 方法，并在 __get__ 方法中通过调用 setattr 方法为实例字典注册属性 area 。紧接着，我们在后续调用 c.area 的时候，我们能在实例字典中找到 area 属性的存在，且类字典中的 area 是一个 non data descriptors，于是我们不会触发代码里所实现的 __get__ 方法，而是直接从实例的字典中直接获取属性值。 描述符的使用描述符的使用面很广，不过其主要的目的在于让我们的调用过程变得可控。因此我们在一些需要对我们调用过程实行精细控制的时候，使用描述符，比如我们之前提到的这个例子 12345678910111213141516171819202122232425262728293031323334class lazyproperty: def __init__(self, func): self.func = func def __get__(self, instance, owner): if instance is None: return self else: value = self.func(instance) setattr(instance, self.func.__name__, value) return value def __set__(self, instance, value=0): passimport mathclass Circle: def __init__(self, radius): self.radius = radius pass @lazyproperty def area(self, value=0): print(&quot;Com&quot;) if value == 0 and self.radius == 0: raise TypeError(&quot;Something went wring&quot;) return math.pi * value * 2 if value != 0 else math.pi * self.radius * 2 def test(self): pass 利用描述符的特性实现懒加载，再比如，我们可以控制属性赋值的值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Property(object): &quot;Emulate PyProperty_Type() in Objects/descrobject.c&quot; def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(&quot;unreadable attribute&quot;) return self.fget(obj) def __set__(self, obj, value=None): if value is None: raise TypeError(&quot;You can`t to set value as None&quot;) if self.fset is None: raise AttributeError(&quot;can't set attribute&quot;) self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(&quot;can't delete attribute&quot;) self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__)class test(): def __init__(self, value): self.value = value @Property def Value(self): return self.value @Value.setter def test(self, x): self.value = x 如上面的例子所描述的一样，我们可以判断所传入的值是否有效等等。 总结Python 中的描述符可以说是新式类调用链中的根基，所有的方法，成员，变量调用时都将会有描述符的介入。同时我们可以利用描述符的特性来将我们的调用过程变得更为可控。这一点，我们可以在很多著名框架中找到这样的例子。 参考1.《Python Cookbook》 8.10 章 P2712.《Descriptor HowTo Guid》3.《Python 黑魔法》","link":"/posts/2016/10/12/Something-about-Descriptor/"},{"title":"Swift 3.0 新增安全特性的一点普及","text":"原文链接 : WWDC 2016: Increased Safety in Swift 3.0 原文作者 : Matt Mathias 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: llp0574, thanksdanny 在 Swift 发布之后，Swift 的开发者一直在强调，安全性与可选择类型是 Swift 最为重要的特性之一。他们提供了一种nil的表示机制，并要求有一个明确的语法在可能为nil的实例上使用。 可选择类型主要以下两种: Optional ImplicitlyUnwrappedOptional 第一种做法是一种安全的做法：它要求我们去拆解可选类型变量是为了访问基础值。第二种做法是一种不安全的做法：我们可在不拆解可选择类型变量的情况下直接访问其底层值。比如，如果在变量值为 nil 的时候，使用 ImplicitlyUnwrappedOptional 可能会导致一些异常。 下面将展示一个关于这个问题的例子： 1234let x: Int! = nilprint(x) // Crash! `x` is nil! 在 Swift 3.0 中，苹果改进了 ImplicitlyUnwrappedOptional 的实现，使其相对于以前变得更为安全。这里我们不禁想问，苹果到底在 Swift 3.0 对 ImplicitlyUnwrappedOptional 做了哪些改进，从而使 Swift 变得更为安全了呢。答案在于，苹果在编译器对于 ImplicitlyUnwrappedOptional 进行类型推导的过程中进行了优化。 在 Swift 2.x 中的使用方式让我们来通过一个例子来理解这里面的变化。 123456789101112struct Person { let firstName: String let lastName: String init!(firstName: String, lastName: String) { guard !firstName.isEmpty &amp;&amp; !lastName.isEmpty else { return nil } self.firstName = firstName self.lastName = lastName }} 这里我们创建了一个初始化方法有缺陷的结构体 Person 。如果我们在初始化中不给实例提供 first name 和 last name 的值的话，那么初始化将会失败。 在这里 init!(firstName: String, lastName: String) ，我们通过使用 ! 而不是 ? 来进行初始化的。不同于 Swift 3.0，在 Swift 2.x 中，我们用过利用 init! 来使用 ImplicitlyUnwrappedOptional 。不管我们所使用的 Swift 版本如何，我们应该谨慎的使用 init!。一般而言，如果你能允许在引用生成的为nil的实例时所产生的异常，那么你可以使用 init! 。因为如果对应的实例为 nil 的时候，你使用 init! 会导致程序的崩溃。 在 .* 中，这个初始化方法将会生成一个 ImplicitlyUnwrappedOptional&lt;Person&gt; 。如果初始化失败，所有基于 Person 的实例将会产生异常。 比如，在 Swift 2.x 里，下面这段代码在运行时将崩溃。 12345// Swift 2.xlet nilPerson = Person(firstName: &quot;&quot;, lastName: &quot;Mathias&quot;)nilPerson.firstName // Crash! 请注意，由于在初始化器中存在着隐式解包，因此我们没有必要使用类型绑定（译者注1： optional binding ）或者是自判断链接（译者注2： optional chaining ）来保证 nilPerson 能被正常的使用。 在 Swift 3.0 里的新姿势在 Swift 3.0 中事情发生了一点微小的变化。在 init! 中的 ! 表示初始化可能会失败，如果成功进行了初始化，那么生成的实例将被强制隐式拆包。不同于 Swift 2.x ，init! 所生成的实例是 optional 而不是 ImplicitlyUnwrappedOptional 。这意味着你需要针对不同的基础值对实例进行类型绑定或者是自判断链接处理。 123456// Swift 3.0let nilPerson = Person(firstName: &quot;&quot;, lastName: &quot;Mathias&quot;)nilPerson?.firstName 在上面这个示例中，nilPerson 是一个 Optional&lt;Person&gt; 类型的实例。这意味着如果你想正常的访问里面的值，你需要对 nilPerson 进行拆包处理。这种情况下，手动拆包是个非常好的选择。 安全的类型声明这种变化可能会令人疑惑。为什么使用的 init! 的初始化会会生成 Optional 类型的实例？不是说在 init! 中的 ! 表示生成 ImplicitlyUnwrappedOptional 么？ 答案是安全性与声明之间的依赖关系。在上面这段代码里（ let nilPerson = Person(firstName: &quot;&quot;, lastName: &quot;Mathias&quot;) ）将依靠编译器对 nilPerson 的类型进行推断。 在 Swift 2.x 中，编译器将会把 nilPerson 作为 ImplicitlyUnwrappedOptional&lt;Person&gt; 进行处理。讲道理，我们已经习惯了这种编译方式，而且它在某种程度上也是有道理的。总之一句话，在 Swift 2.x 中，想要使用 ImplicitlyUnwrappedOptional 的话，就需要利用 init! 对实例进行初始化。 然而，某种程度上来讲，上面这种做法是很不安全的。说实话，我们从没有任何钦定 nilPerson 应该是 ImplicitlyUnwrappedOptional 实例的意思，因为如果将来编译器推导出一些不安全的类型信息导致程序运行出了偏差，等于，你们也有责任吧。 Swift 3.0 解决这类安全问题的方式是在我们不是明确的声明一个 ImplicitlyUnwrappedOptional 时，会将 ImplicitlyUnwrappedOptional 作为 optional 进行处理。 限制 ImplicitlyUnwrappedOptional 的实例传递这种做法很巧妙的一点在于限制了隐式解包的 optional 实例的传递。参考下我们前面关于 Person 的代码，同时思考下我们之前在 Swift 2.x 里的一些做法： 1234567// Swift 2.xlet matt = Person(firstName: &quot;Matt&quot;, lastName: &quot;Mathias&quot;)matt.firstName // `matt` is `ImplicitlyUnwrappedOptional&lt;person&gt;`; we can access `firstName` directly&lt;/person&gt;let anotherMatt = matt // `anotherMatt` is also `ImplicitlyUnwrappedOptional&lt;person&gt;`&lt;/person&gt; anotherMatt 是和 matt 一样类型的实例。你可能已经预料到这种并不是很理想的情况。在代码里，ImplicitlyUnwrappedOptional 的实例已经进行了传递。对于所产生的新的不安全的代码，我们务必要多加小心。 比如，在上面的代码中，我们如果进行了一些异步操作，情况会怎么样呢？ 1234567// Swift 2.xlet matt = Person(firstName: &quot;Matt&quot;, lastName: &quot;Mathias&quot;)matt.firstName // `matt` is `ImplicitlyUnwrappedOptional&lt;person&gt;`, and so we can access `firstName` directly&lt;/person&gt;... // Stuff happens; time passes; code executes; `matt` is set to nillet anotherMatt = matt // `anotherMatt` has the same type: `ImplicitlyUnwrappedOptional&lt;person&gt;`&lt;/person&gt; 在上面这个例子中，anotherMatt 是一个值为 nil 的实例，这意味着任何直接访问他基础值的操作，都会导致崩溃。这种类型的访问确切来说是 ImplicitlyUnwrappedOptional 所推荐的方式。那么我们如果把anotherMatt 换成 Optional&lt;Person&gt; ，情况会不会好一些呢？ 让我们在 Swift 3.0 中试试同样的代码会怎样。 1234567// Swift 3.0let matt = Person(firstName: &quot;Matt&quot;, lastName: &quot;Mathias&quot;)matt?.firstName // `matt` is `Optional&lt;person&gt;`&lt;/person&gt;let anotherMatt = matt // `anotherMatt` is also `Optional&lt;person&gt;`&lt;/person&gt; 如果我们没有显示声明我们生成的是 ImplicitlyUnwrappedOptional 类型的实例，那么编译器会默认使用更为安全的 Optional。 类型推断应该是安全的在这个变化中，最大的好处在于编译器的类型推断不会使我们代码的安全性降低。如果在必要的情况下，我们选择的一些不太安全的方式，我们必须进行显示的声明。这样编译器不会再进行自动的判断。 在某些时候，如果我们的确需要使用 ImplicitlyUnwrappedOptional 类型的实例，我们仅仅需要进行显示声明。 1234// Swift 3.0let runningWithScissors: Person! = Person(firstName: &quot;Edward&quot;, lastName: &quot;&quot;) // Must explicitly declare Person!let safeAgain = runningWithScissors // What`s the type here? runningWithScissors 是一个值为 nil 的实例，因为我们在初始化的时候，我们给 lastName 了一个空字符串。 请注意，我们所声明的 runningWithScissors 实例是一个 ImplicitlyUnwrappedOptional&lt;Person&gt; 的实例。在 Swift 3.0 中，Swift 允许我们同时使用 Optional 和 ImplicitlyUnwrappedOptional 。不过我们必须进行显示声明，从而告诉编译器我们所使用的是 ImplicitlyUnwrappedOptional 。 不过幸运的是，编译器不再自动将 safeAgain 作为一个 ImplicitlyUnwrappedOptionalThankfully 实例进行处理。相对应的是，编译器将会把 safeAgain 变量作为 Optional 实例进行处理。这个过程中，Swift 3.0 对不安全的实例的传播进行了有效的限制。 一些想说的话ImplicitlyUnwrappedOptional 的改变可能是处于这样一种原因：我们通常在 macOS 或者 iOS 上操作利用 Objective-C 所编写的API，在这些API中，某些情况下，它们的返回值可能是为 nil，对于 Swift 来讲，这种情况是不安全的。 因此，Swift 正在避免这样的不安全的情况发生。非常感谢 Swift 开发者对于 ImplicitlyUnwrappedOptional 所进行的改进。我们现在可以非常方便的去编写健壮的代码。也许在未来某一天，ImplicitlyUnwrappedOptional 可能会彻底的从我们视野里消失。= 写在最后的话如果你想知道更多关于这方面的知识，你可以从这里this proposal获取一些有用的信息。你可以从 issue 里获得这个提案的作者的一些想法，同时通过具体的变化来了解更多的细节。同时那里也有相关社区讨论的链接。","link":"/posts/2016/07/22/Swift-3-0-%E6%96%B0%E5%A2%9E%E5%AE%89%E5%85%A8%E7%89%B9%E6%80%A7%E7%9A%84%E4%B8%80%E7%82%B9%E6%99%AE%E5%8F%8A/"},{"title":"聊聊 sk_buff 中一个冷门字段: nohdr","text":"今天遇到一个很有意思的问题，“nohdr 字段到底有什么用”，在这里写个水文简单记录一下 正文前情提要首先来说，不管介绍再冷门的字段，既然涉及到 SKBUFF ，那么就得先来对 sk_buff 做个简单的介绍 简而言之，sk_buff 是 Linux 网络子系统的核心数据结构，从链路层到我们最终对数据包的操作，背后都离不开 sk_buff sk_buff 要完全讲解基本就相当于把 Linux 网络系统完全讲解了，所以讲完是不可能讲完的，这辈子都不可能的！ 简单聊几个关键，可能会帮助大家理解我们本文提到的冷门字段 nohdr 的关键字段吧 首先来讲，最重要的三个字段：data ，mac 和 nh ，分别代表着当前 sk_buff 的数据区的起始地址，L2 header 的起始地址，L3 Header 的起始地址。用一个图方便大家理解 看了图的同学可能会有点明白了，实际上在内核里，也是一层一层的通过指针偏移，不断的添加新的 header 来处理网络请求。和我们直觉相符。可能有同学会问，我既然知道 L3 Header 的起始地址，IP 之类的 L3 协议的 header 长度是固定的。我是不是可以算出 L4 的偏移，然后手动处理。 Bingo，内核里有 tcphdr 的数据结构（对应 IP 是 iphdr ），你根据偏移，手动 cast 就可以手动处理。不过详细做法以后再聊 接着两个比较重要的字段，是 len 和 data_len ，这两个字段都是标识数据长度，但是简要来说，len 代表着当前 sk_buff 所有数据的长度（即包含当前协议的 header 和 payload），data_len 代表当前有效数据长度（即当前协议 payload 长度） OK，前情提要到此结束 关于 nohdr花开两朵，各表一支。聊了 sk_buff 一些预备知识，我们来聊一下 nohdr 这个字段。说实话这个字段真的很冷门 首先官方对此有对应描述 The ‘nohdr’ field is used in the support of TCP Segmentation Offload (‘TSO’ for short). Most devices supporting this feature need to make some minor modifications to the TCP and IP headers of an outgoing packet to get it in the right form for the hardware to process. We do not want these modifications to be seen by packet sniffers and the like. So we use this ‘nohdr’ field and a special bit in the data area reference count to keep track of whether the device needs to replace the data area before making the packet header modifications. 嗯，这段属实有点拗口。首先 TSO 大家肯定有所所了解。利用网卡来对大数据包进行分段（具体 Linux 下 GSO/TSO 的实现可以改天鸽一篇文章来聊），那么在这种情况下，网卡可能会需要对 header 部分进行一点小的修改来完成分片的操作。 但是有些时候，我们对于 L4 这一层的包，我并不需要关心其被修改的 Header ，只需要关心其 payload，那么怎么搞。这个时候就是 nohdr 发挥作用了。 在这里， nohdr 生效还需要配合另外一个字段，dataref 。 dataref 是一个计数字段，其具体的含义是指当前 data 字段所指向的数据区，被多少个 sk_buff 所引用。在这里有两种情况 在 nohdr 为 0 的情况下，dataref 值为数据区的引用计数 在 nohdr 为 1 的情况下，高16位，是数据区中 payload 数据区的引用计数，低16位是数据区的引用计数 对此官方有这样的描述 12345678/* We divide dataref into two halves. The higher 16 bits hold references * to the payload part of skb-&gt;data. The lower 16 bits hold references to * the entire skb-&gt;data. It is up to the users of the skb to agree on * where the payload starts.* * All users must obey the rule that the skb-&gt;data reference count must be * greater than or equal to the payload reference count.* * Holding a reference to the payload part means that the user does not * care about modifications to the header part of skb-&gt;data.*/ #define SKB_DATAREF_SHIFT 16 #define SKB_DATAREF_MASK ((1 &lt;&lt; SKB_DATAREF_SHIFT) - 1) 实际上这里也不太难理解为什么这么设计。首先来说，我们在内核里去获取数据包的时候，有些时候不需要去关心具体的 header，只需要关心具体的 payload。 而我们对于 payload 的引用计数，也需要单独的处理来保证其正确性。这样确保我们的数据还没处理完的时候。数据片不会被内核提前释放。当然这里需要大家在处理这块的时候需要保证数据区的引用计数要大于 payload 的引用计数（感觉这里像约定大于配置的做法？（当然这里不遵守约定的后果就是你内核 dump 了2333 在最后，我们的内核也通过 dataref 来在合适的时机释放数据区的内存空间，释放条件是满足以下其一即可 !skb-&gt;cloned: skb 没有 被 clone !atomic_sub_return(skb-&gt;nohdr ? (1 &lt;&lt; SKB_DATAREF_SHIFT) + 1 : 1, &amp;skb_shinfo(skb)-&gt;dataref) 即在 nohdr 为 1 的时候通过 dataref-(1 &lt;&lt; SKB_DATAREF_SHIFT) + 1) 判断是否需要释放数据区。而 nohdr 为 0 的时候通过 dataref-1 来决定是否需要释放数据区 总结水文差不多就这样。。nohdr 真的是个很冷门的字段。嗯，因为这篇水文的一些 reference 是在地铁上查的。。我就懒得列在文章里了。。差不多这样。。写题去了。。","link":"/posts/2021/11/22/a-litte-introduction-about-nohdr-filed-in-skbuff/"},{"title":"日常辣鸡水文:一个关于 Sanic 的小问题的思考","text":"日常辣鸡水文:一个关于 Sanic 的小问题的思考睡不着，作为一个 API 复制粘贴工程师来日常辣鸡水文一篇 正文最近迁移组内代码到 Sanic ，遇到一个很有意思的情况 首先标准的套路应该是这样的 1234567891011121314151617181920212223242526272829303132333435363738394041from sanic import Sanic,reponseapp=Sanic(__name__)def return_value(controller_fun): &quot;&quot;&quot; 返回参数的装饰器 :param controller_fun: 控制层函数 :return: &quot;&quot;&quot; async def __decorator(*args, **kwargs): ret_value = { &quot;version&quot;: server_current_config.version, &quot;success&quot;: 0, &quot;message&quot;: u&quot;fail query&quot; } ret_data, code = await controller_fun(*args, **kwargs) if is_blank(ret_data): ret_value[&quot;data&quot;] = {} else: ret_value[&quot;success&quot;] = 1 ret_value[&quot;message&quot;] = u&quot;succ query&quot; ret_value[&quot;data&quot;] = ret_data ret_value[&quot;update_time&quot;] = convert_time_to_time_str(get_now()) print(ret_value) return response.json(body=ret_value, status=code) return __decoratorasync def test1(): return {&quot;a&quot;:1&quot;}@return_valueasync def test2(): return await test1(),200@app.route(&quot;/wtf&quot;)async def test3(): return await test2() 中规中举，没什么太大问题 不过如果上面的代码变成下面这样 12345678910111213from sanic import Sanic,reponseapp=Sanic(__name__)async def test1(): return {&quot;a&quot;:1&quot;}@return_valueasync def test2(): return await test1()@app.route(&quot;/wtf&quot;)def test3(): return test2() 一般会以为这样会产生报错的，因为没有 await test2() ，直接 return test2() 的话，返回的是一个 Coroutine 的对象，这样应该是会抛错的，但是实际上是正常运行的，最开始很迷，不过后面看了下 Sanic 中关于 handle_request 的部分，有点意思 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172async def handle_request(self, request, write_callback, stream_callback): &quot;&quot;&quot;Take a request from the HTTP Server and return a response object to be sent back The HTTP Server only expects a response object, so exception handling must be done here :param request: HTTP Request object :param write_callback: Synchronous response function to be called with the response as the only argument :param stream_callback: Coroutine that handles streaming a StreamingHTTPResponse if produced by the handler. :return: Nothing &quot;&quot;&quot; try: # -------------------------------------------- # # Request Middleware # -------------------------------------------- # request.app = self response = await self._run_request_middleware(request) # No middleware results if not response: # -------------------------------------------- # # Execute Handler # -------------------------------------------- # # Fetch handler from router handler, args, kwargs, uri = self.router.get(request) request.uri_template = uri if handler is None: raise ServerError( (&quot;'None' was returned while requesting a &quot; &quot;handler from the router&quot;)) # Run response handler response = handler(request, *args, **kwargs) if isawaitable(response): response = await response except Exception as e: # -------------------------------------------- # # Response Generation Failed # -------------------------------------------- # try: response = self.error_handler.response(request, e) if isawaitable(response): response = await response except Exception as e: if self.debug: response = HTTPResponse( &quot;Error while handling error: {}\\nStack: {}&quot;.format( e, format_exc())) else: response = HTTPResponse( &quot;An error occurred while handling an error&quot;) finally: # -------------------------------------------- # # Response Middleware # -------------------------------------------- # try: response = await self._run_response_middleware(request, response) except: log.exception( 'Exception occured in one of response middleware handlers' ) # pass the response to the correct callback if isinstance(response, StreamingHTTPResponse): await stream_callback(response) else: write_callback(response) 核心代码是这样一段 1234567891011handler, args, kwargs, uri = self.router.get(request)request.uri_template = uriif handler is None: raise ServerError( (&quot;'None' was returned while requesting a &quot; &quot;handler from the router&quot;))# Run response handlerresponse = handler(request, *args, **kwargs)if isawaitable(response): response = await response 大概就是，首先按照 route-&gt;add_route 的顺序注册对应的处理函数和 URL 到一个映射里，然后当请求发过来时，取出对应的 handler ，然后进一步处理 最开始正常的中规中矩的做法里 123@app.route(&quot;/wtf&quot;)async def test3(): return await test2() 注册的 handler 是 test3 这个函数，然后执行 response = handler(request, *args, **kwargs) ，初始化了一个 Coroutine 对象，紧接着这个对象是 awaitable 的，于是进入后面的 response = await response 流程。 好了，来看看非主流的做法 123@app.route(&quot;/wtf&quot;)def test3(): return test2() 老规矩，先注册，然后取出 test3 这个函数作为 handler ，然后执行，因为是普通函数，于是 response 的值便是 test3 中初始化的那个 Coroutine 对象，然后同样是 awaitable 的，进入后面的 response = await response 流程。 两种方式殊途同归，这也解释了为什么第二中不清真的方式也能得到正确的结果 思考Sanic 这样的处理方式，相当于增强了整个框架的容错性。也可能让用户写出向之前那样不清真的代码。不过我也没法说这个是好是坏，各有看法吧。不过有一点是肯定的，在 debug 模式下，如果用户利用 app.route 添加了一个非 async 的函数，是有必要抛出一个 warning 的，不过，Sanic 还有，PR 已经提出，就不知道合不合了。。。 好了，就先这样吧。。明天还得搬砖，溜了，溜了。。","link":"/posts/2018/02/22/a-little-idea-about-sanic/"},{"title":"关于 Node.js 中 execSync 的一点问题","text":"很久没写水文了，昨天帮人查了一个 Node.js 中 execSync 这个函数特殊行为的问题，很有趣，所以大概记录下来水一篇文章 背景首先老哥给了一张截图 首先基本问题可以抽象为在 Node.js 中利用 execSync 这个函数执行 ps -Af | grep -q -E -c &quot;\\\\-\\\\-user-data-dir=\\\\.+App&quot; 这样一条命令的时候，Node.js 时不时会报错。具体堆栈大概为 12345678910Uncaught Error: Command failed: ps -Af | grep -q -E -c &quot;\\-\\-user-data-dir=\\.+App&quot; at checkExecSyncError (child_process.js:616:11) at Object.execSync (child_process.js:652:15) { status: 1, signal: null, output: [ null, &lt;Buffer &gt;, &lt;Buffer &gt; ], pid: 89073, stdout: &lt;Buffer &gt;, stderr: &lt;Buffer &gt;} 但是同样的命令在终端上并不会有类似的现象。所以这个问题有点困扰人 分析首先先看一下 Node.js 文档中对 execSync 的描述 The child_process.execSync() method is generally identical to child_process.exec() with the exception that the method will not return until the child process has fully closed. When a timeout has been encountered and killSignal is sent, the method won’t return until the process has completely exited. If the child process intercepts and handles the SIGTERM signal and doesn’t exit, the parent process will wait until the child process has exited.If the process times out or has a non-zero exit code, this method will throw. The Error object will contain the entire result from child_process.spawnSync().Never pass unsanitized user input to this function. Any input containing shell metacharacters may be used to trigger arbitrary command execution. 大意就是，这个函数通过子进程来执行一个命令，在命令执行超时之前会一直等待。OK 没有问题。那接下来，我们先来看一下上面提到的报错堆栈以及 execSync 的实现代码 12345678910111213141516171819202122232425262728293031323334function execSync(command, options) { const opts = normalizeExecArgs(command, options, null); const inheritStderr = !opts.options.stdio; const ret = spawnSync(opts.file, opts.options); if (inheritStderr &amp;&amp; ret.stderr) process.stderr.write(ret.stderr); const err = checkExecSyncError(ret, opts.args, command); if (err) throw err; return ret.stdout;}function checkExecSyncError(ret, args, cmd) { let err; if (ret.error) { err = ret.error; } else if (ret.status !== 0) { let msg = 'Command failed: '; msg += cmd || ArrayPrototypeJoin(args, ' '); if (ret.stderr &amp;&amp; ret.stderr.length &gt; 0) msg += `\\n${ret.stderr.toString()}`; // eslint-disable-next-line no-restricted-syntax err = new Error(msg); } if (err) { ObjectAssign(err, ret); } return err;} 我们能看到，这里 execSync 在执行完命令执行代码后，会进入 checkExecSyncError 来检查子进程的 Exit Status Code 是否为0，不为0则认为命令执行出错，然后抛出异常。 看起来没有问题，那么也就是我们执行命令的时候出错了？那我们验证下吧 对于这种涉及 Linux 下 Syscall 问题排查的工具（这个问题在 Mac 等环境下也存在，不过我为了方便排查，跑去 Linux 上复现了），除了 strace 好像也暂时找不到更成熟方便的工具了（虽然基于 eBPF 也能做，但是说实话自己现撸绝对没 strace 的效果好。 那么上命令 1sudo strace -t -f -p $PID -o error_trace.txt tips: 在使用 strace 的时候可以利用 -f 参数，可以 trace 被 trace 进程创建的子进程 好了执行命令，成功拿到整个 syscall 的调用链路，OK 开始分析 首先我们将目光很快定位到了最关键的部分（因为整个文件太长，有将近 4K 行，我就直接挑重点部分分析了） 12345678910...894259 13:21:23 clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f12d9465a50) = 896940...896940 13:21:23 execve(&quot;/bin/sh&quot;, [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;ps -Af | grep -E -c \\&quot;\\\\-\\\\-user-da&quot;...], 0x4aae230 /* 40 vars */ &lt;unfinished ...&gt;...896940 13:21:24 &lt;... wait4 resumed&gt;[{WIFEXITED(s) &amp;&amp; WEXITSTATUS(s) == 1}], 0, NULL) = 896942896940 13:21:24 --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=896942, si_uid=1000, si_status=1, si_utime=0, si_stime=0} ---896940 13:21:24 rt_sigreturn({mask=[]}) = 896942896940 13:21:24 exit_group(1) = ?896940 13:21:24 +++ exited with 1 +++ 首先这里科普一下，Node.js 中没有直接使用 fork 来创建新的进程，而是使用 clone 来创建新的进程，至于两者之间的差别，要详细说的话，可以单独水一篇超长文了（我先立个 flag）这里先用官方的说法大概描述下 These system calls create a new (“child”) process, in a manner similar to fork(2).By contrast with fork(2), these system calls provide more precise control over what pieces of execution context are shared between the calling process and the child process. For example, using these system calls, the caller can control whether or not the two processes share the virtual address space, the table of file descriptors, and the table of signal handlers. These system calls also allow the new child process to be placed in separate namespaces(7). 用简短的概括性的话来描述就是,clone 提供了 fork 近似的语义,不过通过 clone ,开发者能更细粒度的控制进程/线程创建过程中的细节 OK, 这里我们看到 894259 这个主进程通过 clone 创建了 896940 这个进程。在执行过程中, 896940 这个进程利用 execve 这个 syscall 通过 sh (这是 execSync 的默认行为)我们的命令 ps -Af | grep -q -E -c &quot;\\\\-\\\\-user-data-dir=\\\\.+App&quot;。 OK，我们也看到了，896940 在退出的时候，的确是以 1 的 exit code 退出的，和我们之前的分析一致。那么换句话说，在我们执行命令的时候，有 error 的出现。那么这里的 error 出现在哪呢？ 我们分析一下命令，如果熟悉常见 shell 的同学可能发现了，我们的命令中实际上使用了管道操作符 | ，不精确的来说，当这个操作符出现的时候，前后两个命令将分别在两个进程执行，然后通过 pipe 进行 IPC。那么换句话说，我们可以很快定位这两个进程，直接快速搜了一下文本 12345678910...896941 13:21:23 execve(&quot;/bin/ps&quot;, [&quot;ps&quot;, &quot;-Af&quot;], 0x564c16f6ec38 /* 40 vars */) = 0...896942 13:21:23 execve(&quot;/bin/grep&quot;, [&quot;grep&quot;, &quot;-E&quot;, &quot;-c&quot;, &quot;\\\\-\\\\-user-data-dir=\\\\.*&quot;], 0x564c16f6ecb0 /* 40 vars */ &lt;unfinished ...&gt;...896941 13:21:24 &lt;... exit_group resumed&gt;) = ?896941 13:21:24 +++ exited with 0 +++...896942 13:21:24 exit_group(1) = ?896942 13:21:24 +++ exited with 1 +++ OK，我们发现 896942 即执行 grep 的进程直接以 exit code 1 退出了。那么这是为什么呢？？看了下 grep 的官方文档，，卧操，差点吐血 Normally, the exit status is 0 if selected lines are found and 1 otherwise. But the exit status is 2 if an error occurred, unless the -q or –quiet or –silent option is used and a selected line is found. Note, however, that POSIX only mandates, for programs such as grep, cmp, and diff, that the exit status in case of error be greater than 1; it is therefore advisable, for the sake of portability, to use logic that tests for this general condition instead of strict equality with 2. 如果 grep 没有匹配到数据，那么会以 1 作为 exit code 退出进程。。如果匹配到了，则0退出。。但是，但是，卧操，卧操。。按照标准语义，exit code 1 的含义难道不是 Operation not permitted 吗？？完全不按基本法出牌！ 总结实际上通篇看了下来，我们可以总结出两个原因 Node.js 在对 POSIX 相关 API 进行抽象封装的时候，直接按照了标准语义，给用户兜底了。虽然从理论上讲这应该是个应用自决的行为 grep 没有按照基本法办事 说实话我也不知道怎么去评价这两方面谁更坑一点。按照前面所说么处理子进程的 exit code 从理论上讲这应该是个应用自决的行为，但是 Node.js 自己做了一层封装，在节省用户心智的同时，遇到一些非标场景，也会有不小的隐患了。。 只能说不断根据不同的场景做 trade-off 吧 好了，这篇文章就到这里了，因为是临时起义，所以我就懒得将相关 Reference 列在文里了。差不多这样吧，水文目标达成.jpg","link":"/posts/2021/08/24/a-little-problem-about-posix-node-js-execsync/"},{"title":"为什么 Python 的 Type Hint 没有流行起来","text":"在知乎上看到一个很有意思的问题，为什么TypeScript如此流行，却少见有人写带类型标注的Python？ 虽然我没忍住在知乎上输出了答案，但是为了以防万一，我在博客上扩展，与更新一下 BTW 最近上线真的心力憔悴，写个文章放松下 开始其实这个答案很简单，历史包袱与 ROI，在了解为什么有这样的现象之前，首先我们要去了解 Type Hint 能给我们带来什么，然后我们需要去了解 Type Hint 的前世今生 在现在这个时间点（2020.03）来看，Type Hint 能给我们带来肉眼可见的收益是 通过 annotation ，配合 IDE 的支持，能让我们在代码编辑的时候的体验更好 通过 mypy/pytype 等工具的支持，我们能在 CI/CD 流程中去集成静态类型检查 通过 pydantic 以及很多新式框架的支持，我们能够减少很多重复的工作 可能大家以为从 Python 3.5 引入 PEP 484 开始，Python Type Hint 便已经成熟。但是实际上，这个时间比大家想象的短的多 好了，我们现在要去回顾一下整个 Type Hint 发展史上的关键节点 PEP 3107 Function Annotations PEP 484 Type Hints PEP 526 Syntax for Variable Annotations PEP 563 Postponed Evaluation of Annotations PEP 3107如同前面所说，大家最开始认识 Type Hint 的时间应该是14 年 9 月提出，15 年 5 月通过的 PEP 484 。但是实际上雏形早的多，PEP 484 的语法实际上来自于 06 年提出，3.0 引入的 PEP 3107 所设计的语法，参见 PEP 3107 – Function Annotations 在 PEP 3107 中，对于这个提案的目标，有这样一段描述 Because Python’s 2.x series lacks a standard way of annotating a function’s parameters and return values, a variety of tools and libraries have appeared to fill this gap. Some utilise the decorators introduced in “PEP 318”, while others parse a function’s docstring, looking for annotations there.This PEP aims to provide a single, standard way of specifying this information, reducing the confusion caused by the wide variation in mechanism and syntax that has existed until this point. 说人话就是，为了能够给函数的参数或者返回值添加额外的元信息，大家五花八门各显神通，有用 PEP 318 装饰器的，有用 docstring 来做的。社区为了缓解这个现象，决定推出新的语法糖，来让用户能够方便的为参数签名和返回值添加额外的信息 最后形成的语法如下 12def foo(a: 'x', b: 5 + 6, c: list) -&gt; max(2, 9): pass 是不是很眼熟？ 没错，3107 实际上奠定了后续 Type Hint 的基调 可标注 作为 function/method 信息的一部分，可 inspect runtime 但是新的疑惑就来了，为什么这个提案经常被人忽略？还是，我们需要放在具体的时间点来看 这个提案提出时间最早可以追溯至06年，在 PEP3000 这个可能是 Python 历史上最著名的提案（即宣告 Python 3 的诞生）中确定在 Python 3 中引入，08年正式发布 在这个时间点下，3107 面临着两个问题： 在06-08这个时间点上，社区最主要的精力都在友(ji)好(lie)的讨(si)论(bi)，我们为什么要 Python 3？以及为什么我们要迁到 Python 3 3107 实际上只是告诉大家，你可以标注，你可以方便的获取标注信息，但是怎么样去抽象一个类型的表示，如一个 int 类型的 list ，这种事，还是依靠社区自行发展，换句话说，叫做放养 问题1，无解，只能依靠时间去慢慢推动。问题2，促成了 PEP 484 的诞生 PEP 484PEP 484 这个提案大家应该都有一定程度上的了解了，在此不再描述提案的具体内容 PEP 484 最大的意义在于， 在继承了 PEP 3107 奠定的语法和基调之上，将 Python 的类型系统进行了合理的抽象，这也是重要的产物 typing，直到这时，Python 中的 type hint 才有了基本的官方规范，同时达到了基本的可用性，这个时间点是 15 年 9 月（9月13，Python 3.5.0 正式 Release） 但是实际上 PEP 484 在这个时间点也只能说基本满足使用，我来举几个被诟病的例子 首先看一段代码 12345from typing import Optionalclass Node: left: Optional[Node] right: Optional[Node] 这段代码实际上很简单对吧，一个标准的二叉树节点的描述，但是放在 PEP 484 中，这段代码暴露出两个问题 无法对变量进行标注。如同我前面所说的一样，PEP 484 本质上是 PEP 3107 的一个扩展，这个时候 hint 的范围仅限于 function/method ，而在上面的代码中，在 3.5 时期，我是无法对我的 left 和 right 的变量进行标注的，一个编程语言的基本要素之一的变量，无法被 Type Hint ，那么一定程度上我们可以说这样一个 type hint 的功能没有闭环 循环引用，字面意义，在社区/StackOverflow 上如何解决 Type Hint 中的循环引用这个问题，一度让人十分头大。社区：What the fuck? 所幸，Python 社区意识到了这个问题，推出了两个提案来解决这样的问题 PEP 526问题1 促成了 PEP 526 – Syntax for Variable Annotations 的诞生，16 年 8 月提出，16 年 9 月被接受。16 年 9 月在 BPO-27985 实现。在我印象里，这应该是 Python 社区中数的出来的争议小，接收快，实现快的 PEP 了 在 526 中，Python 正式允许大家对变量进行标注，无论是 class attribute 还是普通的 variable 12class Node: left: str 这样是可以的， 12def abc(): a:int = 1 这样也是可以的 在这个提案的基础上，Python 官方也推动了 PEP 557 – Data Classes 的落地，当然这是后话 话说回来，526 只解决了上面的问题1，没有解决问题2，这个事情，将会由 PEP 563 来解决 PEP 563为了解决循环引用的问题，Python 引入了 PEP 563 – Postponed Evaluation of Annotations，17 年 9 月社区提出，17 年 11 月被接受，18 年 1 月在 GH-4390 中实现。 在 563 之后，我们上面的代码可以这么写了 12345from typing import Optionalclass Node: left: Optional[&quot;Node&quot;] right: Optional[&quot;Node&quot;] 嗯，484 中的两个问题，终于被解决了 总结以 PEP 563 作为重要分割点，Python 最早在 18 年 1 月之后才初步具备完整的生态和生产可用性，如果考虑 release version，那么应该是 18 年 6 月，Python 3.7 正式发布之后的事了。 在 Python 3.6/7 之后，社区也才开始围绕 Type Hint 去构建一套生态体系， 比如利用 PEP 526 来高效的验证数据格式，参见 pydantic 顺带一提，这货也是目前很火的一个新型框架（也是我目前最喜欢的一个框架）FastAPI 的根基 各大公司也开始跟进，例如 Google 的 pytype ，微软推出了 pyright 来提供在 VSCode 上的支持 还有许许多多优秀的如 starlette 这样库 直到这时，Python + Type Hint 的真正的威力才开始挥发出来。这样才开始能回答大家这样一个问题：“我为什么要切换到 Type Hint”，我猜在 IDE 里写的爽肯定不是一个重要原因 要知道，我们在做技术决策时候，一定是因为这个决策能给我们带来足够的 benefit，换句话说，有足够的 ROI，而不是单纯的因为，我们喜欢它 这样看起来，到现在，满打满算一年半不超过两年的时间。对于一个用户习惯养成周期来说，这太短了。更何况还有一大堆的 Python 2 代码在那放着23333 话说回来，作为对比，TypeScript Release 时间可以上溯至 12 年 10 月，发布 0.8 版本，当时的 TS 应该是具备了相对完整地类型系统。 TS 用了 8 年，Python 可能也还有很长的路要走 当然，这个答案也只是从技术和历史的角度聊聊这个问题。至于其余的很多因素，包括社区的博奕与妥协等，暂还不在这个答案的范围内，大家有兴趣的话，可以去 python-idea，python-dev，discuss-python 这几个地方去找一找历史上关于这几个提案的讨论，非常有意思。 最后，TS 成功还有一个原因，它有个好爸爸&amp;&amp;它爸爸有钱（逃 嗯，差不多就这样吧，最近干活干的心里憔悴的我，也就只能写点垃圾水文了压压惊，平复心情了。。","link":"/posts/2020/03/20/a-simple-history-about-type-hint-in-python/"},{"title":"当我们在聊 CI&#x2F;CD 时，我们在聊什么？","text":"本文实际上是在群内第二次分享的内容。这次其实想来聊聊，关于 CI/CD 的一些破事和演进过程中我们所需要遇到的一些问题，当然本文中是一个偏新手向的文章和一点点爆论，随便看看就好。 开宗明义，定义先行在我们谈论一个事物之前，我们需要对这个事物给出一个定义，那我们先来看一下我们今天要聊的 CI 与 CD 的定义。 首先，CI 指 Continuous Integration ，在中文语境中的表述是持续集成。而 CD 在常见语境下可能是两种意思：Continuous Delivery 或 Continuous Deployment，与之对应的表述是持续交付/持续部署。这里借用一下 Brent Laster 在 What is CI/CD?1 中给出的定义 Continuous integration (CI) is the process of automatically detecting, pulling, building, and (in most cases) doing unit testing as source code is changed for a product. CI is the activity that starts the pipeline (although certain pre-validations—often called “pre-flight checks”—are sometimes incorporated ahead of CI).The goal of CI is to quickly make sure a new change from a developer is “good” and suitable for further use in the code base.Continuous deployment (CD) refers to the idea of being able to automatically take a release of code that has come out of the CD pipeline and make it available for end users. Depending on the way the code is “installed” by users, that may mean automatically deploying something in a cloud, making an update available (such as for an app on a phone), updating a website, or simply updating the list of available releases. 光看定义，可能大家还是会很懵逼，那么下面我们用一些实际的例子来给大家从头捋一遍 CI/CD 那些事 Re：从0开始构建流程这个标题好像起的有点草，不过不管了。首先我们假定这样一个最简单的需求 我们基于 Hexo 构建了一个个人的博客系统。其中包含我们所需要发布的文章，我们配置的主题。我们需要将其发布到具体的 Repo 上。 好了，基于这个需求，我们来从0到0玩一圈吧（笑（ 构建原生之初可能这里有很多人会问，为啥会选择 Hexo 来作为我们的切入点。原因很简单啊！因为它够简单啊！ 言归正传，首先 Hexo 有两个命令 hexo g &amp;&amp; hexo d ，分别是根据当前目录下的 Markdown 文件来生成静态的网页。然后将生成的产物根据配置推送到对应的 repo 上 OK，那么我们在最原始的阶段一个构建的流程就是这样 用一个编辑器，开开心心的写文章 然后在本地终端执行 hexo g &amp;&amp; hexo d 问题来了，现在有些时候提交了博客，但是忘了执行生成命令怎么办？或者是我每次都需要敲重复的命令很麻烦怎么办？那就让我们把整个过程自动化一下吧。Let’s rock! 更进一步的构建OK，我们先来假设一下，我们如果完成了自动化，我们现在发布一个博客的工作流应该编程什么样的 我们编写一个 Markdown 文件，推送到 GitHub 仓库里的 Master 分支上 我们的自动任务开始构建我们博客，生成一系列静态文件和样式 将我们的静态文件和样式推送到我们的站点 Repo/CDN 等目标位置 好了，那么这里有两个核心的问题 在我们推送代码的时候，自动开始构建 在构建完成后，推送产物 那我们现在基于 GitHub Action 来配置一套我们的自动化构建任务 1234567891011121314151617181920212223name: Build And Publish Blogon: push: branches: [ master ]jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v1 with: node-version: '12.x' - name: Install Package run: npm install -g hexo-cli &amp;&amp; npm install - name: Generate Html File run: hexo g - name: Deploy 🚀 uses: JamesIves/github-pages-deploy-action@3.7.1 with: GITHUB_TOKEN: ${{ secrets.PUBLSH_TOKEN }} BRANCH: gh-pages # The branch the action should deploy to. FOLDER: public # The folder the action should deploy. 我们能看到这段配置实际上完成了这样一些事情 在我们往 master 分支提交代码的时候触发构建 拉取代码 安装构建所需依赖 构建生成静态文件 推送静态文件 如果上面任何一个步骤失败了，都将取消后面步骤的执行。实际上这样一个简单的任务已经包含了一个 CI &amp;&amp; CD 所包含的基础要素（这里 CD 我并未严格区分 Continuous Delivery/Continuous Deployment) 与已有的代码持续的构建与集成 集成中区分多个 phase。每个 phase 将依赖上个 phase 结果。 将构建产物交付/部署出去。交付/部署的成功依赖于集成的成功 那么在这里，我们将博客系统换成一个我们工程中的例子。将 Hexo 换成我们的 Python 服务。将新增博文换成我们的新增的代码。将构建命令换成 mypy/pylint 等检查工具。你看，CI/CD 实际上和你想象的复杂的系统，是不是有很大差别？ 这里可能有很多人会提出这样一个问题，如果说这里我们将这些命令，不用线上的形式触发。而在本地用 Git Hook 等形式进行实现。那么这算不算一种 CI 与 CD 呢？我觉得毫无疑问算的，从我的视角来看，CI/CD 核心的要素在于通过可以重复，自动化的任务，来尽早暴露缺陷，减轻人为因素所带来的不必要的事故发生。 这个开发过份傻逼却不谨慎首先抛出一个最基础的爆论，然后我们接着往下谈 所有人都有傻逼的时候，而且这个傻逼的时候可能还会很多。 在这样一个爆论的情况下，我们来回顾一下上面举基于 Hexo 去构建一个个人博客系统的例子中，如果我们不选择通过一种收敛的，自动化的系统去解决我们的构建，发布需求。那么我们哪些环节会出现风险 最基础的，写完博客，忘了构建，忘了发布 比如我们升级一下依赖中的 Hexo 版本或者主题版本，我们没有测试，导致构建出来的样式失效 我们的 Markdown 有问题，导致构建失败 比如多个人维护一个博客的情况下，我们每个人都需要保存目标仓库/CDN的密钥等信息。导致信息泄漏等 将基于 Hexo 去构建一个个人博客系统的例子切换成我们日常开发的场景，那么我们可能遇到的问题会更多。简单举几个 没法很快速的回滚 没法溯源具体的构建/发布记录 没有自动化的任务，研发懒得跑测试或者 lint 导致代码腐化 高峰期上线导致事故 嗯，这些问题大家是不是都很熟悉？大概就是，我起了，构建了，出事故了，有啥好说的23333 讲到这里的大家实际上有没有发现一个问题？我在这篇文章中，没有对 CI 与 CD 进行区分？从我的视角来看，CI/CD 本质上是践行的同一个事。即 对于研发流程与交付流程的收敛 从我的视角来看，去构建一个 CI/CD 系统核心的目标在于 通过收敛入口以及自动化的任务触发，尽可能减轻人为因素所带来的系统不稳定性 通过快速，多次，可重复，无感知的任务，尽可能的在较早阶段暴露系统中的问题 在这样两个大目标的前提下，我们便会根据不同的业务场景，采用不同的手段与形式丰富我们 CI/CD 中的内容，包括不仅限于 在 CI 阶段自动化的单元测试，E2E 测试等 在 CI 阶段周期性的 Nighty Build 等 在 CD 阶段进行发布管控等 不过无论我们怎么样去构建一个 CI/CD 系统，或者选择什么样的粒度去进行 CI/CD。我觉得一个合格的 CI/CD 系统与机制 都需要遵照这样几个原则（个人向总结） 入口的收敛，SOP 的建立。如果不达成这点共识，研发能够通过技术手段绕过 CI/CD 系统那么便又回到的了我们本章的标题（这个研发过份傻逼却不谨慎） 对于业务代码无侵入 集成任务/发布任务一定要是自动化，可重复的 可回溯的历史记录与结果 可回溯的构建集成产物 从上到下的支持 那么遵照我总结的这样几个原则，我们来迭代一下我们之前的博客的发布过程 1234567891011121314151617181920212223242526272829303132333435name: Build And Publish Blogon: push: branches: [ master ] pull_request: branches: [ master ]jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v1 with: node-version: '12.x' - name: Install Package run: npm install -g hexo-cli &amp;&amp; npm install - name: Generate Html File run: hexo g - name: Deploy To Repo🚀 if: ${{ github.ref == 'refs/heads/master'}} uses: JamesIves/github-pages-deploy-action@3.7.1 with: GITHUB_TOKEN: ${{ secrets.PUBLSH_TOKEN }} BRANCH: gh-pages # The branch the action should deploy to. FOLDER: public # The folder the action should deploy. - name: Upload to Collect Repo uses: JamesIves/github-pages-deploy-action@3.7.1 with: GITHUB_TOKEN: ${{ secrets.PUBLSH_TOKEN }} BRANCH: build-${{ github.run_id }} # The branch the action should deploy to. FOLDER: public # The folder the action should deploy. 在这段更改后的构建流程中，我选择以 PR 为粒度去触发 CI 流程，并将历史产物进行存储，而在合并主分支后，新增发布流程。在这样一来，我在博客构建发布的时候，便能够通过回溯的历史产物来验证我框架升级，新增博文等操作的正确性。同时依托 GitHub Action，我能很好的完成历史构建的回溯 嗯，这样便可以尽可能避免我傻逼的操作所带来的各种副作用（逃 进击的构建：终章好了，啥都没有，傻眼了吧。。。。 只是开个玩笑。实际上本文到这差不多就可以告一段落了。实际上大家通过这篇文章可以发现一个问题。就是实际上构建一个 CI/CD 系统可能并不会涉及很多，很高深的技术问题(极少数的场景除外）无论是传统的 Jenkins，还是新生的 GitHub Action，GitLab-CI，亦或者是云厂商提供的服务都能很好的帮助我们去构建一套贴合业务的 CI/CD 系统。但我之前在推特上发表了的一个爆论“CI/CD 的建立往往不是一个技术问题，而是一个制度问题，更可以称为是一个想法问题”。 所以，我希望我们每个人都能认识到我们都会犯错这样一个事实。然后尽可能的将自己所负责的系统的开发流程与交付流程尽可能的收敛与自动化。让一个 CI/CD 真正称为我们日常工作中的一部分。 差不多这样，溜了，溜了。","link":"/posts/2021/04/11/a-simple-introduce-about-ci-cd/"},{"title":"简单聊聊 IaC：Infrastructure as Code","text":"实际上 IaC 这个概念的出现已经很久了，所以写篇水文来简单聊聊 IaC 的过去，现在，和将来 IaC 的过去实际上 IaC 的历史其实足够悠久。首先来看一下 IaC 的核心的特征 最终的产物是 machine readable 的的产物。可能是一份代码，也可能是一份配制文件 基于 machine readable 的产物，可以进一步依赖已有的 VCS 系统（SVN，Git）等做版本管理 基于 machine readable 的产物，可以进一步依赖已有的 CI/CD 系统（Jenkins，Travis CI）等做持续集成/持续交付 状态的一致性，或者称为幂等性。即理论上来讲，基于同样一份 Code，同一套参数构建出的产物，其最终的行为应该是一致的 实际上通过 IaC 这样的一些核心特征，我们现在能明白 IaC 兴起的原因。IaC 实际上的兴起，大背景是在千禧年之后，互联网世界迭代的速度愈发的快速，这个时候传统的手工式的维护面临着几个问题 交互式变更所引入的人的因素太大，导致了变更的不可控性 人工变更面对愈发快速的 Infra 迭代力有不逮 交互式的变更导致管控的难做，让版本控制之类的手段变为空谈 在这样的时代背景下，大家都在追求用更技术，更优雅的手段来解决这些问题。于是，IaC 这个概念就出现了 如果说要将 IaC 分为几个阶段的话，那么我觉得可以分为以下几个阶段 刀根火种阶段 现代化的 IaC 如同前面所说，IaC 实际上是一个自发的驱动，在面对不确定的时候，我们选择用代码来尽可能的消泯掉不确定性（实际上这个原则一直贯穿到现在） 那么在最早期，人们选择用最基础的代码的形式，来完成 IaC 的工作。其特征是对于之前的各种交互式的手段的精确化，程序化的描述。人们可能会选择直接用 bash 来解决这一切（祖传的来路不明的 bash 脚本.jpg），也可能会基于 Python Fabric 这样的框架进行简单的封装来完成所需的程序化描述的工作。 但是我们回头去看这一阶段，我们能直观的感受到一些缺陷 代码复用性较差 各家都有一套祖传的 IaC 基建，没有统一的行业标准，导致新人入门门槛较高 所以在面对这样一套的问题的时候。更现代化的 IaC 设施应运而生。其中典型的一些产物是 Ansible Chef Puppet 实际上这些工具，可能设计上各有所取舍（比如 Pull/Push 模型的取舍），但是其核心的特征不会变化 框架内部提供了常见的比如 SSH 链接管理，多机并行执行，auto retry 等功能 基于上面描述的这一套基础功能，提供了一套 DSL 封装。让开发者更专注于 IaC 的逻辑，而非基础层面的细节 其开源开放，并形成了一套完善的插件机制。社区可以基于这一套提供更丰富的生态。比如 SDN 社区基于 ANSIBLE 提供了各种交换机的 playbook 等 那么截至到现在，实际上 IaC 的发展其实到了一个相对完备的程度。其中不少工具，也依旧贯穿到了现在。 新生代的 IaC从2006年8月25日，Amazon 正式宣布提供了 EC2 服务开始。整个基础设施开始快步向 Cloud 时代迈进。截止到目前，各家云厂商提供了各种各样的服务。通过十多年的演进，也诞生出了诸如 IaaS，PaaS，DaaS，FaaS 等等各种各样的服务模式。这些服务模式，让我们的基础设施的构建，变得更加的简单，更加的快速。但是这些服务模式，也带来了一些新的问题。 可能写到这里，有同学已经能意识到了问题的所在：在获取算力，获取资源越来越快捷的当下。我们怎么样去管理这样一些资源？ 那么要解决这样的问题，我们似乎又需要去考虑怎么样用代码或者可声明式的配置来管理这些资源。有没有一点眼熟，历史始终就是一个圈圈.jpg 在起初的时候，我们各自会选择基于各家云厂商提供的 API 与 SDK 自行封装一套 IaC 工具，如同前面所说的一样。这样会带来一些额外的问题： 代码复用性较差 各家都有一套祖传的 IaC 基建，没有统一的行业标准，导致新人入门门槛较高 那么这个时候，云时代的，面向云资源管理的新型 IaC 工具的需求也愈发的迫切。这个时候，Terraform 这样的新型工具应运而生 在 Terraform 里，可能一台 EC2 Instance 的开启可能就是这样的一段简短的定义 12345678910111213141516171819202122232425262728293031323334353637383940resource &quot;aws_vpc&quot; &quot;my_vpc&quot; { cidr_block = &quot;172.16.0.0/16&quot; tags = { Name = &quot;tf-example&quot; }}resource &quot;aws_subnet&quot; &quot;my_subnet&quot; { vpc_id = aws_vpc.my_vpc.id cidr_block = &quot;172.16.10.0/24&quot; availability_zone = &quot;us-west-2a&quot; tags = { Name = &quot;tf-example&quot; }}resource &quot;aws_network_interface&quot; &quot;foo&quot; { subnet_id = aws_subnet.my_subnet.id private_ips = [&quot;172.16.10.100&quot;] tags = { Name = &quot;primary_network_interface&quot; }}resource &quot;aws_instance&quot; &quot;foo&quot; { ami = &quot;ami-005e54dee72cc1d00&quot; # us-west-2 instance_type = &quot;t2.micro&quot; network_interface { network_interface_id = aws_network_interface.foo.id device_index = 0 } credit_specification { cpu_credits = &quot;unlimited&quot; }} 这个基础上，我们可以继续将我们诸如 Database，Redis，MQ 等基础设施都进行代码化/描述式配置化，进而提升我们对资源维护的有效性。 同时，随着各家 SaaS 的发展，研发人员也尝试着将这些 SaaS 服务也进行代码化/描述式配置化。以 Terraform 为例，我们可以通过 Terraform 的 Provider 来进行对接。比如 newrelic 提供的 Provider，Bytebase 提供的 Provider 等等 同时，在 IaC 工具帮助我们完成基础设施描述的标准化之后，我们在此基础上能做更多有趣的事情。比如我们可以基于 Infracost 来计算每次资源变更所带来的资源花费变更。基于 atlantis 来完成集中式的资源变更等等进阶的工作。 那么到现在为止，我们已有的 IaC 产品的选择足够多，能满足我们大部分需求。那么是不是 IaC 整个产品的发展实际上就已经到了一个相对完备的程度呢？答案很明显是否定的 未来的 IaC所以这张主要来聊聊当下 IaC 产品所面临的一些问题，以及我对未来的一些思考吧 缺陷一：现有基于 DSL 的语法体系的缺陷先给大家看一个例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155locals { dns_records = { # &quot;demo0&quot; : 0, &quot;demo1&quot; : 1, &quot;demo2&quot; : 2 &quot;demo3&quot; : 3, } lb_listener_port = 80 instance_rpc_port = 9545 default_target_group_attr = { backend_protocol = &quot;HTTP&quot; backend_port = 9545 target_type = &quot;instance&quot; deregistration_delay = 10 protocol_version = &quot;HTTP1&quot; health_check = { enabled = true interval = 15 path = &quot;/status&quot; port = 9545 healthy_threshold = 3 unhealthy_threshold = 3 timeout = 5 protocol = &quot;HTTP&quot; matcher = &quot;200-499&quot; } }}module &quot;alb&quot; { source = &quot;terraform-aws-modules/alb/aws&quot; version = &quot;~&gt; 6.0&quot; name = &quot;alb-demo-internal-rpc&quot; load_balancer_type = &quot;application&quot; internal = true enable_deletion_protection = true http_tcp_listeners = [ { protocol = &quot;HTTP&quot; port = local.lb_listener_port target_group_index = 0 action_type = &quot;forward&quot; } ] http_tcp_listener_rules = concat([ for rec, pos in local.dns_records : { http_tcp_listener_index = 0 priority = 105 + tonumber(pos) actions = [ { type = &quot;forward&quot; target_group_index = tonumber(pos) } ] conditions = [ { host_headers = [&quot;${rec}.manjusaka.me&quot;] } ] } ], [{ http_tcp_listener_index = 0 priority = 120 actions = [ { type = &quot;weighted-forward&quot; target_groups = [ { target_group_index = 0 weight = 95 }, { target_group_index = 5 weight = 4 }, ] } ] conditions = [ { host_headers = [&quot;demo0.manjusaka.me&quot;] } ] }]) target_groups = [ merge( { name_prefix = &quot;demo0&quot; targets = { &quot;demo0-${module.ec2_instance_demo[0].tags_all[&quot;Name&quot;]}&quot; = { target_id = module.ec2_instance_demo[0].id port = local.instance_rpc_port } } }, local.default_target_group_attr, ), merge( { name_prefix = &quot;demo1&quot; targets = { &quot;demo1-${module.ec2_instance_demo[0].tags_all[&quot;Name&quot;]}&quot; = { target_id = module.ec2_instance_demo[0].id port = local.instance_rpc_port } } }, local.default_target_group_attr, ), merge( { name_prefix = &quot;demo2&quot; targets = { &quot;demo2-${module.ec2_family_c[0].tags_all[&quot;Name&quot;]}&quot; = { target_id = module.ec2_family_c[0].id port = local.instance_rpc_port }, } }, local.default_target_group_attr, ), merge( { name_prefix = &quot;demo3&quot; targets = { &quot;demo3-${module.ec2_family_d[0].tags_all[&quot;Name&quot;]}&quot; = { target_id = module.ec2_family_d[0].id port = local.instance_rpc_port }, } }, local.default_target_group_attr, ), # target_group_index_3 merge( { name_prefix = &quot;demonew&quot; targets = { &quot;demo0-${module.ec2_instance_reader[0].tags_all[&quot;Name&quot;]}&quot; = { target_id = module.ec2_instance_reader[0].id port = local.instance_rpc_port } } }, local.default_target_group_attr, ), ]} 这段 TF 配置描述虽然看起来长，但是实际上做的事很简单，根据不同的域名 *.manjusaka.me 将流量转发到不同的 instance 上。然后对于 demo0.manjusaka.me 这个域名，进行单独的流量灰度处理。 我们能发现，Terrafrom 这种 DSL 的解决方案所需要面临的问题就是在对于这种动态灵活的场景下，其表达能力将会有很大的局限性。 社区也充分意识到了这个问题。所以类似 Pulumi 这种基于 Python/Lua/Go/TS 等完整的编程语言的 IaC 产品就应运而生了。比如我们用 Pulumi + Python 改写上面的例子(此处由 ChatGPT 提供技术支持) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137from pulumi_aws import albdns_records = { # &quot;demo0&quot; : 0, &quot;demo1&quot;: 1, &quot;demo2&quot;: 2, &quot;demo3&quot;: 3,}lb_listener_port = 80instance_rpc_port = 9545default_target_group_attr = { &quot;backend_protocol&quot;: &quot;HTTP&quot;, &quot;backend_port&quot;: 9545, &quot;target_type&quot;: &quot;instance&quot;, &quot;deregistration_delay&quot;: 10, &quot;protocol_version&quot;: &quot;HTTP1&quot;, &quot;health_check&quot;: { &quot;enabled&quot;: True, &quot;interval&quot;: 15, &quot;path&quot;: &quot;/status&quot;, &quot;port&quot;: 9545, &quot;healthy_threshold&quot;: 3, &quot;unhealthy_threshold&quot;: 3, &quot;timeout&quot;: 5, &quot;protocol&quot;: &quot;HTTP&quot;, &quot;matcher&quot;: &quot;200-499&quot;, },}alb_module = alb.ApplicationLoadBalancer( &quot;alb&quot;, name=&quot;alb-demo-internal-rpc&quot;, load_balancer_type=&quot;application&quot;, internal=True, enable_deletion_protection=True, http_tcp_listeners=[ { &quot;protocol&quot;: &quot;HTTP&quot;, &quot;port&quot;: lb_listener_port, &quot;target_group_index&quot;: 0, &quot;action_type&quot;: &quot;forward&quot;, } ], http_tcp_listener_rules=[ { &quot;http_tcp_listener_index&quot;: 0, &quot;priority&quot;: 105 + pos, &quot;actions&quot;: [ { &quot;type&quot;: &quot;forward&quot;, &quot;target_group_index&quot;: pos, } ], &quot;conditions&quot;: [ { &quot;host_headers&quot;: [f&quot;{rec}.manjusaka.me&quot;], } ], } for rec, pos in dns_records.items() ] + [ { &quot;http_tcp_listener_index&quot;: 0, &quot;priority&quot;: 120, &quot;actions&quot;: [ { &quot;type&quot;: &quot;weighted-forward&quot;, &quot;target_groups&quot;: [ {&quot;target_group_index&quot;: 0, &quot;weight&quot;: 95}, {&quot;target_group_index&quot;: 5, &quot;weight&quot;: 4}, ], } ], &quot;conditions&quot;: [{&quot;host_headers&quot;: [&quot;demo0.manjusaka.me&quot;]}], } ], target_groups=[ alb.TargetGroup( f&quot;demo0-{module.ec2_instance_demo[0].tags_all['Name'].apply(lambda x: x)}&quot;, name_prefix=&quot;demo0&quot;, targets=[ { &quot;target_id&quot;: module.ec2_instance_demo[0].id, &quot;port&quot;: instance_rpc_port, } ], **default_target_group_attr, ), alb.TargetGroup( f&quot;demo1-{module.ec2_instance_demo[0].tags_all['Name'].apply(lambda x: x)}&quot;, name_prefix=&quot;demo1&quot;, targets=[ { &quot;target_id&quot;: module.ec2_instance_demo[0].id, &quot;port&quot;: instance_rpc_port, } ], **default_target_group_attr, ), alb.TargetGroup( f&quot;demo2-{module.ec2_family_c[0].tags_all['Name'].apply(lambda x: x)}&quot;, name_prefix=&quot;demo2&quot;, targets=[ { &quot;target_id&quot;: module.ec2_family_c[0].id, &quot;port&quot;: instance_rpc_port, } ], **default_target_group_attr, ), alb.TargetGroup( f&quot;demo3-{module.ec2_family_d[0].tags_all['Name'].apply(lambda x: x)}&quot;, name_prefix=&quot;demo3&quot;, targets=[ { &quot;target_id&quot;: module.ec2_family_d[0].id, &quot;port&quot;: instance_rpc_port, } ], **default_target_group_attr, ), alb.TargetGroup( f&quot;demo0-{module.ec2_instance_reader[0].tags_all['Name'].apply(lambda x: x)}&quot;, name_prefix=&quot;demonew&quot;, targets=[ { &quot;target_id&quot;: module.ec2_instance_reader[0].id, &quot;port&quot;: instance_rpc_port, } ], **default_target_group_attr, ), ],) 你看，整体的用法是不是更贴近于我们的使用习惯，其表达力也更好 缺陷二：和业务需求之间的 Gap实际上在云时代的 IaC 工具，更多的去解决的是基础设施的存在性的问题。而对于已有基础设施的编排与更合理的利用实际上是存在比较大的 Gap 的。我们怎么样将应用部署到这些基础资源上。怎么样去调度这些资源。实际上是个很值得玩味的一个问题。 实际上可能出乎人们的意料，实际上 Kubernetes/Nomad 实际上就在尝试解决这样的问题。可能有人在思考，什么？这个也算是 IaC 工具？毫无疑问的是嘛，不信你对照一下我们前面列的 IaC 的几个核心特征 最终的产物是 machine readable 的的产物。可能是一份代码，也可能是一份配制文件（YAML 工程师表示认可） 基于 machine readable 的产物，可以进一步依赖已有的 VCS 系统（SVN，Git）等做版本管理（manifest 随着仓库走） 基于 machine readable 的产物，可以进一步依赖已有的 CI/CD 系统（Jenkins，Travis CI）等做持续集成/持续交付（argocd 等平台提供了进一步的支持） 同时我们在对应的配置文件里，可以声明我们所需要 CPU/Mem，需要的磁盘/远程盘，需要的网关等。同时这一套框架实际上将计算 Infra 进行了一个相对通用性的抽象，让业务百分之八十的场景下并不需要去考虑底层 Infra 的细节。 但是实际上这套已经存在的方案又会存在一些问题。比如其复杂度的飙升，self-hosted 的运维成本，以及一些抽象泄漏带来的问题。 缺陷三：质量性的偏差云时代新生的 IaC，其 scope 相较于传统的诸如 ansible 之类的 IaC 工具范围更大，野心也更大。所带来的副作用就是其质量的偏差。这个话题可以分为两方面说 第一点来看，诸如 Terraform 这样的 IaC 工具，通过官方提供的 Provider 实现了对 AWS/Azure/GCP 等平台的支持。但是即便是官方支持，其 Provider 里设计的一些逻辑，和平台侧在交互式界面里的设计逻辑并不一致。比如我之前吐槽过“比如 Aurora DB Instance 的 delete protection 在 Console 创建时默认打开，而 TF 里是默认关闭”。这实际上会在使用的时候，给开发者带来额外的心智负担 第二点来看，IaC 工具极度依赖社区（此处的社区饱含开源社区和各类商业公司）。不同于 Ansible 等老前辈，其周边设施的质量相对稳定。Terraform 等新生代的 IaC 周边的质量一言难尽。比如国内诸如福报云，华为云，腾讯云等厂商提供的 Provider 一直被人诟病。而不少大型的面向研发者的 SaaS 平台没有官方提供的 Provider 等（比如 Newrelic） 同时，云厂商所提供的一些功能实际上是和通用性 IaC 工具所冲突的。比如 AWS 的 WAF 工具，其中有一个功能是基于 IPSet 进行拦截，这个时候如果 IPSet 非常大，那么使用通用性的 IaC 工具进行描述将会是一个灾难性的存在。这个时候对于类似的场景，只能基于云厂商自己的 SDK 进行封装，云厂商提供的 SDK 质量合格还好。如果像福报云这样的神奇的 SDK 设计的话，那就只能自求多福了。。 缺陷四：面对开发者体验的不足开发者体验实际上现在是一个比较热门的话题。毕竟没有人愿意将自己宝贵的生命来做重复的工作。就目前而言，主要的 IaC 工具都是 For Production Server 的，而不是 For Developer Experience 的，导致我们用的时候，其体验就很一般。 比如我们现在有一个场景，我们需要在 AWS 上给研发的同学批量开一批 EC2 Instance 作为开发机。怎么样保证研发同学在这些机器上开箱即用，就是很大的一个问题。 虽然我们可以通过预制镜像等方式提供相对统一的环境。不过我们可能会需要更进一步的去细调环境的话，那么就会比较蛋疼。 针对于类似的场景，老一点的有 Nix，新一点的有 envd 来解决这样一些问题。但是目前来讲，还是和已有的 IaC 产品有一些 gap。后续怎么样进行对接可能会是个很有趣的话题。 缺陷五：面对新型技术栈的一些不足最典型的是 Serverless 的场景。比如我举个例子，我现在有个简单的需求，就是用 Lambda 来实现一个简单的 SSR 的渲染 12345678910export default function BlogPosts({ posts }) { return posts.map(post =&gt; &lt;BlogPost key={post.id} post={post} /&gt;)}export async function getServerSideProps() { const posts = await getBlogPosts(); return { props: { posts } }} 函数本身非常简单，但是如果我们要将这个函数部署到 Production Enviorment 里将会是一个比较麻烦的事。比如我们来思考下我们现在需要为这个简单的函数准备什么样的 infra 一个 lambda 实例 一个 S3 bucket 一个 APIGateway 及路由规则 接入 CDN （可选） DNS 准备 那么在 IaC Manifest + 业务代码彼此分离的情况下，我们的变更以及资源的管理将会是一个很大的问题。Vercel 在最近的 Blog Framework-defined infrastructure 也描述了这样的问题。我们怎么样能进一步发展为 Domain Code as Infrastructure 将会是未来的一个挑战 总结这篇文章写了两天，差不多作为自己对于 IaC 这个事物的一些碎碎念（而不是 Terraform Tutorial！（逃。祝大家读的开心","link":"/posts/2023/03/12/a-simple-introduction-about-iac/"},{"title":"简单聊聊 Maglev ，来自 Google 的软负载均衡实践","text":"好久没博客了，来写个简单的读论文笔记吧，这篇文章是来自 Google 2016 年发表的一篇论文 Maglev: A Fast and Reliable Software Network Load Balancer 分享了他们内部从08年开始大规模使用的软负载均衡系统的实现。里面很多很有趣的细节，我看我能写多少，算多少吧 背景负载均衡的概念大家肯定都比较熟悉了，再次不再赘述。现在我们需要考虑 Google 的场景。设计之初，Google 需要一种高性能的 LB 来承担 Google 一些重头服务的流量，比如 Google 搜索，Gmail 等等。由于流量非常庞大，那么 LB 需要非常强大的性能来处理大量的流量。 在这里，传统的想法可能说，我直接上专业的硬件负载均衡，能用钱解决的问题，都不算事（笑。但是这样的方案有着不小的问题 硬件负载均衡单点的性能决定了整个网络能承担的请求 在 HA 上存在缺陷。为了保证单点失效的时候，整个网络集群不陷入瘫痪。那么我们通常需要 1:1 的做冗余 灵活性和编程性欠缺，想做骚操作的时候没有切入点 太贵了。贵到 Google 都承受不了（逃 在这样一种情况下，Google 开始考虑自行构建一种 SLB (Software Load Balance) 系统。去构建这样一种系统。好处也很明显。比如方便的 Scale ，为了保证 HA 所需的冗余从之前的 1:1 可以降至 N+1 ，方便的定制性等。架构就演变成下图了 但是挑战也很明显。首先需要有足够的性能，这样保证集群有足够的吞吐。同时需要做 connection tracking ，这样保证同一个连接的数据包能妥投到同一个机器上。也许要保证能有透明的 failover 的能力。 这样一些要件结合起来，这也就是我们今天要聊的 Maglev。Google 从 08 年开始大规模的应用的 LB 系统 Maglev 初窥背景知识在继续聊 Maglev 之前，我们需要去了解 Google 现在怎么样去去使用 Maglev 的，下面是一个简化后的示意图 同时这里我们需要介绍一个很重要的概念叫做 VIP(Virtual IP Address) 。 用过 Kubernetes 的同学肯定对这个概念并不陌生。VIP 并不是一个实际与网卡绑定的物理 IP。近似来讲它可以作为后端一组 Endpoint 的抽象，当你访问这个 VIP 的时候，实际上是在访问后端的 Endpoint 。这里举个更方便理解的例子，以 Kubernetes 为例，我们在创建完一组 Pod 后，为了暴露 Pod 中提供的服务，我们通常会创建一个 Service 来关联对应的 Pod。Service 通常会有一个 IP，那么这个 IP 就是一个 VIP 。当我们访问 Service 的 IP 的时候，通常会随机从后面的 Pod 中选择一个承接请求。 好了，回到 Maglev ，我们现在来看下整个的一个流程。Maglev 会和 VIP 关联，然后将 VIP 透传给一组 Router。 当用户在浏览器中输入 https://www.google.com 并按下回车的时候，浏览器会进行 DNS 解析。而 DNS 解析将由 Google 的 DNS 服务器进行处理。DNS 服务器会根据用户的区域选择一个最近集群的 VIP 返回给用户，然后浏览器会根据获取到的 VIP 建立连接。 当 Router 收到对应包时，会将包转发给 VIP 所属的 Maglev 集群中的任意节点。集群中的每个节点权重都是平衡。Maglev 节点在接受到包的时候，会利用 GRE(Generic Routing Encapsulation) 进行封包。然后传输给对应的后端端点。 当后端端点接收到数据包的时候，会进行接包并处理请求。当响应数据准备就绪的时候，会进行封包操作，会将 VIP 的作为源地址，用户的 IP 作为目标地址，然后响应数据作为数据包操作。这个时候，后端端点会利用 DSR(Direct Server Return) 将数据包绕过 Maglev 直接返回。这样避免响应过大的时候对 Maglev 造成额外的负担。实际上 DSR 在 L4 的 LB 实现，如 HAProxy，Envoy 等都得到了比较多的应用。改天有时间写篇博客来聊聊。 Maglev 配置如前面所说， Maglev 接收来自 Router 的 VIP 请求，然后将对应流量转发到对应的后端端点上。每个 Maglev 将由 Controller 和 Forwarder 组成，其架构如下所示 而 Controller 和 Forwarder 都利用 Configuration Object 管理相关 VIP。Configuration Object 这一套实际上又是另外一套系统（可以近似的认为是注册中心），彼此之间通过 RPC 来通信。 在 Maglev 机器上，Controller 会定期对 Forwarder 进行检查。根据检查结果来确定是否通过 BGP 提交/撤回所有 VIP 的注册（要么全部成功，要么全部失败，其实还是为了保障系统的一致性）。这样确保从 Router 过来的流量都能扔到健康的机器上 而从 Router 过来的 VIP 流量将会由 Forwarder 进行处理。在 Forwarder 中，每个 VIP 都会和一个或多个 backend pool 关联。除非特殊处理，Maglev 中的 backend 都是服务端点。一个 backend pool 可以包含一组服务端点的的物理 IP ，也可以是其余的 backend pool。每个 backend pool 都会根据其特定需求，设计若干个监控检查器，数据包只会转发给健康的服务。如之前所说，同一个服务可能会被包含在多个 backend pool 中，因此 Forwarder 将会根据具体的地址进行去重，避免额外的开销。 Forwarder 的 Config Manager 将负责从 Configuration Object 中拉取，解析并验证相关的配置。所有配置的提交都是具备原子性（要么全部成功，要么全部失败）。在推送和解析到生效的过程中，存在一个非常短暂的 gap，在此期间，一个 Maglev 集群之间的配置可能存在不同步的情况。不过因为一致性 Hash 的存在，在这个非常短的 Gap 内，大部分请求还是能成功妥投。 Maglev 实现好了，扯了这么多，来看一下 Maglev 整个系统的一些实践细节 概述总所周知（如前面所说），Maglev 由 Forwarder 来实际承担流量相关的转发工作，我们用一张图来说明一下它的结构 Forwarder 将直接从 NIC(Network Interface Card) 拿到数据包，然后直接扔入 NIC 转发到后端。期间所有操作都不会过内核（实际上过内核会有额外的 cost） 从 NIC 中捞出的包，会先由 Steering Module 进行处理，在处理过程中，Steering Module 将会根据五元组（协议，目标地址，目标端口，源地址，源端口）进行 hash 计算。然后将其转入对应的 Receiving Queue 中。每个 Receiving Queue 都会对应一个处理线程。处理线程将过滤掉目标 VIP 和本机注册 VIP 不匹配的包。然后会重新计算五元组 hash，然后从 Connection Tracking Table 中查找对应的值。 在 Connection Tracking Table 中存放之前五元组 Hash 所对应的 Backend，然后如果查找命中，那么直接复用，如果未命中，则为这个包选择一个新的 Backend, 然后将键值对加入 Connection Tracking Table。如果此时没有 Backend 可用，那么这个包会被丢弃。当这个包完成查找操作后，如前面所说，会改写这个包，然后将其放入 transmission queue 中去。最后将 muxing module 会将 transmission queue 的包直接通过 NIC 发送出去。 这里有个问题，在 Steering Module 中为啥不考虑根据 round-robin 这种常见的策略来做？大家都知道每个线程的处理速度是不一致的，如果直接裸 round-robin ，那么面对这种情况，可能会导致数据包重排的情况发生，如果是引入权重的概念来改良，又会引入新的复杂度，毕竟线程的处理速度是动态变化的。另外一种是 connnection tracking 的情况，假设我们有个需要持久化的连接，我们需要保证每个包都能扔到同样的机器上，这个时候用 round-robin 就会引入额外的复杂性。不过对于一些特殊情况，比如 receive queue 满了，一致性 Hash 处理不过来的时候，我们会利用 round-robin 作为 backup 的手段来替代一致性 Hash，这种情况对于同时存在同样5元组包的时候比较好用。 高效处理数据包前面已经花了很多时间讲述了，Maglev 是直接对 TCP 的数据包进行操作，同时因为 Google 的流量极为庞大，那么这个时候实际上是需要 Maglev 有着良好的转发性能。不然在大规模场景下，其吞吐能力会无法满足需求。Google 怎么做的？答：直接对网卡操作。。 我们都知道在 Linux 中进行网络编程的时候，将数据包从内核态拷贝到用户态实际上是一件开销非常大的事，所以对于一些极端需求性能的场景，如 L4 的负载均衡等，大家可能更倾向于将东西做到内核里，避免跨态拷贝。这也是 LVS 等工具的思路。但是实际上对于更大规模的流量，来讲，从网卡到内核，经过内核中的一堆 filter 处理也是一件开销非常大的事，而如同前面所说，Maglev 只依赖数据包中的五元组，对于包序列号，包 payload ，都不需要关心。于是 Google：我有一个大胆的想法！好了，来看张图 Google 选择直接在 NIC (即网卡) 上进行编程。让 Forwarder 和 NIC 共享一片内存。内存中维护的是一个环状的数据包池子。然后 Forwarder 中的 steering module 和 muxing module 各自维护三个指针来处理这些数据包，下面详细描述一下 首先而言 steering module 维护了三个指针 received ，管理接收数据包 reserved, 管理已接收未处理的数据包 processed, 管理处理完成的数据包 那么流程是这样的，当 NIC 接受到新的数据包后，那么 received 指针指向的内存会被修改。然后当一个数据包被分发给线程完成相关操作后，那么 processed 指针指向的内存地址会被修改。因为是个环状结构嘛， received 和 processed 中间存在的数据包就是已接收但未完成处理的包，由 reserved 指针进行管理。 于此对应的，muxing module 也维护了三个指针 sent，管理已发送完毕的数据包 ready，管理已经就绪等待发送的数据包 recycled, 管理已回收的数据包 那么对应的流程是这样的，当 steering module 完成相关包的处理的时候，ready 指针指向的内存会被修改，然后等待发送。当一个数据包发送后，sent 指向的内存地址被修改。在 ready 和 sent 之外有另一个状态 recycled 管理已经回收的数据包。 我们可以看到在这个过程中，没有发生数据拷贝的操作，实际上这减小了一部分复制数据带来的时延。不过这种方法存在的问题就是，当指针越界后，会带来很大的额外开销。所以 Google 采用的做法是批处理，比如接收 3000 个小包集中处理一次，这样的骚操作 另外需要做一些额外的优化，比如包处理线程之间不共享数据以避免竞态。比如需要将线程与具体 CPU Core 绑定来保证性能等等 目前来看，Google 这一套的做法效率非常的出色，平均每个包的处理只需要 300 ns($10^{-9}$s)。如同前面所说，Google 采用批处理的方式来处理包，这样的问题是每当一些例如硬件中断的情况发生的时候，可能到达处理阈值的时间会比大部分情况长很多，所以 Google 设计了一个 50μs($10^{-6}$s) 的 Timer 来处理这种情况。换句话说，当因为硬件或者其余问题时，整体的包处理时长可能会增加 50μs 的时间（其实这里感觉 Google 怎么是在得瑟，你看我们性能超棒的噢，只有硬件是我们的瓶颈喔（逃 后端选择如同前面所说的一样，Forwarder 会为数据包选择一个后端。对于 TCP 这种常见来说，将相同五元组的数据包转发到同一个后端节点上非常重要。Google 采取在 Maglev 中维护一个 connction tracking table 来解决这个问题。当一个包抵达的时候，Maglev 会计算其五元组 Hash ，然后确定在 table 中是否存在，如果不存在，则选择一个节点作为后端，然后将记录值添加到 table 中。如果存在则直接复用 这样看起来没有问题了对吧？Google：不，不是，还有问题！ 我们首先考虑这样一种场景：如前面所说，Maglev 前面挂了一个/组 Router，而 Router 是不提供连接亲和的，即不保证把同一个连接的包发送到同一个机器上。所以可能存在的情况是同一个连接的不同数据包会被仍在不同的机器上。再比如，我们假设 Router 是具有连接亲和的，但是也会存在如果机器发生重启后，connection tracking table 被清空的情况。 再来一个例子，我们都知道 connection tracking table 它所能使用的内存，必定是有一个阈值的。这样在面对一些流量非常大，或者 SYN Flood 这种非正常情景的时候。当 connection tracking table 的容量到达阈值的时候，我们势必会清理一些数据。那么在这个时候，一个连接的 tracking 信息就很有可能被清理。那么在这种情况下，我们怎么样去做 connection tracking ？ Google 选择的做法是引入一致性 Hash 一致性 Hash：Maglev Hash整体算法其实有很多细节，这里只说明大概，具体细节大家可以去阅读原文查找 首先，我们要确定经过预处理后的产物 lookup table 的长度 M。所有 Key 都会被 hash 到这个 lookup table 中去，而 lookup table 中的每个元素都会被映射到一个 Node 上 而计算 lookup table 的计算分为两步 计算每一个 node 对于每一个 lookup table 项的一个取值（也就是原文中提到的 permutation）； 根据这个值，去计算每一个 lookup table 项所映射到的 node（放在 entry 中，此处 entry 用原文的话来讲就是叫做 the final lookup table）。 permutation 是一个 M×N 的矩阵，列对应 lookup table，行对应 node。 为了计算 permutation，需要挑选两个 hash 算法，分别计算两个值 offset 与 skip 。最后根据 offset 和 skip 的值来填充 permutation，计算方式描述如下： offset ← h 1 (name[i]) mod M skip ← h 2 (name[i]) mod (M − 1)+ 1 permutation[i][j] ← (offset+ j × skip) mod M 其中 i 是 Node Table 中 Node 的下标，j 是 lookup table 下标 在计算完 permutation 后，我们就可以计算最后的 lookup table 了，这个 table 用一维的数组表示 这里贴一张图，大家可以配合下面的代码一起看一下 123456789101112131415161718192021222324252627from typing import List# 根据已经计算好的 permutation 来计算 lookup_tabledef calculate_lookup_table(n: int, m: int, permutation: List[List[int]]) -&gt; List[int]: # result 是最终记录分布的 Hash 表 result: List[int] = [-1] * m # next 是用来解决冲突的，在遍历过程中突然想要填入的 entry 表已经被占用， # 则通过 next 找到下一行。一直进行该过程直到找到一个空位。 # 因为每一列都包含有 0~M-1 的每一个值，所以最终肯定能遍历完每一行。 # 计算复杂度为 O(M logM) ~ O(M^2) next: List[int] = [0] * n flag = 0 while True: for i in range(n): x = permutation[i][next[i]] while True: # 找到空位，退出查找 if result[x] == -1: break next[i] += 1 x = permutation[i][next[i]] result[x] = i next[i] += 1 flag += 1 # 表已经填满，退出计算 if flag == m: return result 在这里我们能看到，这段循环代码必然结束，而最坏情况下，复杂度会非常高，最坏的情况可能会到 O(M^2)。原文中建议找一个远大于 N 的 M （To avoid this happening we always choose M such that M ≫ N.）可以使平均复杂度维持在 O(MlogM) 而 Maglev 中 Google 自研的一致性算法性能怎么样呢？论文中也做了测试 可以看到，对于不同大小的 lookup table，Maglev 表现出了更好的均衡性 说实话，Maglev 在我看来本质上是一个带虚节点的 Hash，说实话，我没想到为什么 Google 不用 Dynamo 等已经比较成熟的 Hash ？难道是因为政策原因？（毕竟 Dynamo 是 AWS 家的嘛（逃。BTW Enovy 也实现了 Maglev 。参见 Evaluate other consistent hash LB algorithms ，而且引入了权重，实现的挺不错，有兴趣的同学可以去看看（逃 说实话，Maglev Hash 还有很多细节没有讲，不过实在懒得写了，，等后面出一个一致性 Hash 的分析博客吧，Flag++ Maglev 优化前面我们已经把 Maglev 这一套的基本原理讲的差不多了。但是如果作为一个生产上大规模使用的 LB ，那么势必还需要针对细节做很多优化，由于这里涉及到很多方面，我这里只简单介绍一下，剩下的还是建议大家直接去读原文 分段数据包的处理熟悉网络的同学都知道，在基于 IP 协议传输报文的时候，受限于 MTU 的大小，在传输的时候，可能会存在数据分片传输的情况，而这些分片后的数据不一定会带有完整的五元组信息。比如一个数据被切分为两段，那么第一段将带有 L3 和 L4 的头部信息，而第二段只带有 L3 的信息。而在传输过程中，因为网络关系，Maglev 无法完全保证对接收到的数据作出正确的处理 这样问题就大了，因为数据分段的情况实际上是非常场景的。那么对于这样的场景，Maglev 应该怎么样去处理？首先我们需要确定怎么样才能保证所有数据都能妥投 保证一个数据报文的不同数据段都需要由同一个 Maglev 实例处理 对于同一个数据报文的不同数据段需要能保证后端选择结果是一致的 OK，那么我们来看看 Google 是怎么解决这个问题的。 首先，每个 Maglev 实例中都会有一个特殊的 backend pool ，池子中是该 Maglev 集群中所有的实例。当接收到数据后，Maglev 会先根据三元组（源地址，目标地址，协议簇）计算 hash ，然后选择一个 Maglev 实例进行转发，这样就能保证同一数据报文的不同分段能传输到同一个 Maglev 实例上。当然这里需要利用 GRE 的递归控制来避免无限循环。 好了我们来看看条件2怎么满足。在每个 Maglev 实例上会维护一个特殊的表，记录数据分片后第一个数据端的转发结果。以前面的例子为例，当一个报文的第二个分段抵达的时候，Maglev 会查询表中是否存在第一个数据段的转发结果。如果存在则直接转发，如果不存在，则将这个数据段缓存，直到第一个数据段抵达，或者到达超时阈值 监控与调试真正的用时都是不需要调试（划掉）（笑，Google 为了这一套系统设计了辅助的监控与调试手段来帮助日常的开发迭代。 在监控这边，分为黑盒和白盒两种监控手段。比如遍布全球的特定监控节点，以确认 VIP 的健康状态。当然与之配套的还有一整套白盒监控。Google 会监控具体的服务器指标，同时会监控 Maglev 本身的指标 当然与之配套的还有一些调试工具。比如 Google 开发了一套类似 X-Trace 的 packettracer。可以通过 packettracer 来发送一些带有特定标头和 payload 的信息。当 Maglev 接到这样一些特殊的数据包后，除了照常转发数据包以外，也会讲一些关键信息上报到指定位置 这其实也体现了软负载均衡相较于硬件负载均衡的一个好处，无论可调试性还是可迭代性都是硬件负载均衡无法媲美的 总结这篇文章其实我读了挺久，里面很多细节挺值得慢慢深究的，所以再次建议大家一定要去找原文读一下，非常不错。另外顺便推荐一篇文章，是美团技术团队的作品，他们也参考了 Maglev 来实现自己的高性能 L4 负载均衡，参见MGW——美团点评高性能四层负载均衡 好了，这篇文章，就先到这里吧，这篇文章应该是我写的最耗时的一篇文章了。。不过想想后面还有几篇文章要写，头就很大 溜了溜了","link":"/posts/2020/05/22/a-simple-introduction-about-maglev/"},{"title":"简单聊聊在 Linux 内核中的网络质量监控","text":"这可能是2021年最后一篇文章（农历年），也可能是2022年第一篇文章，不过这完全取决于我什么时候写完。这次来简单聊聊 Linux 中的网络监控 开篇这篇文章，既是一篇水文，又不是一篇水文。不过还是新手向的一个文章。这篇文章实际上在我的草稿箱里呆了一年多的时间了，灵感最初源自我在阿里的一些工作（某种意义上算是国内领先的（但也是比较小众的工作（XD 随着技术的发展，大家对于服务的稳定性要求越来越高，而保证服务质量的前提就是有着合格的监控的覆盖面（阿里对于服务稳定性的要求叫做 “1-5-10” 即，一分钟发现，五分钟处理，十分钟自愈，而这样一个对于稳定性的要求没有足够的覆盖面的监控的话，那么一切等于圈圈）。而在这其中，网络质量的监控是重中之重 在讨论网络质量的监控之前，我们需要来明确网络质量这个定义的覆盖范围。 网络链路上的异常情况 服务端网络的处理能力 在明确这样的覆盖范围后，我们可以来思考什么样的指标代表着网络质量的降低。（注：本文主要分析 TCP 及 over TCP 协议的监控，后续不再赘述） 毫无疑问，如果我们存在丢包的情况 发送/接收队列阻塞 超时 那么我们可以再来看下具体细节 如 RFC7931 提出的 RTO，RFC62982 提出的 Retransmission Timer 等指标，可以衡量包传送时间。一个粗略的概括是，这两个指标越大代表着网络质量越低 如 RFC20183 提出的 SACK，一个不精确的概括是 SACK 越多，代表着丢包越多 如果我们的链接频繁的被 RST，那么也代表着我们的网络质量存在问题 当然在实际的生产过程中，我们还可以从很多其余的指标来辅助衡量网络质量，不过因为本文主要是介绍思路以 prototype 为主，所以不做过多赘述 在明确我们这篇文章中要获取什么指标后，我们再来分析一下我们怎么样去获取这些指标 内核网络质量监控暴力版从内核中获取网络的 metric ，本质上来说是从内核获取运行状态。说道这点，对 Linux 有所了解的同学第一反应肯定是说从 The Proc Filesystem4 看一下能不能拿到具体的指标。Yep， 不错的思路，实际上的确可以拿到一部分的指标（这也是 netstat 等一些网络工具的原理) 在 /proc/net/tcp 中，我们可以获取到内核吐出的 Metric，现在包括这样一些 连接状态 本地端口，地址 远程端口，地址 接收队列长度 发送队列长度 慢启动阈值 RTO 值 连接所属的 socket 的 inode id uid delay ack 软时钟 完整的解释可以参考 proc_net_tcp.txt5 这样的做法针对于 prototype 可能说是可以的，不过其固有的几个弊端限制了在生产上大规模使用 内核已经明确不推荐使用 proc_net_tcp.txt5，换句话说，并不保证未来的兼容性与维护 内核直接提供的 metric 信息还是太少，一些关于 RTT，SRTT 这样的指标还是没法获取，也没法获取 SACK 等一些特定事件。 根据内核输出的 metric。存在的问题是实时性和精度的问题，换句话说，我们在不考虑精度的情况下可以去做这方面的尝试 proc_net_tcp.txt5 是和 network namespace 进行绑定的，换句话说，在容器的场景下，我们需要遍历可能存在的多个 network namespace ，不断的走 nsenter 去获取对应的 Metric 所以在这样的背景下，proc_net_tcp.txt5 并不太适合比较大规模的使用场景。所以我们需要对其做更近一步的优化 优化 1.0 版在上文里，我们提到了关于直接从 The Proc Filesystem4 中获取数据的弊端。其中一条很重要的是提到了 内核已经明确不推荐使用 proc_net_tcp.txt5，换句话说，并不保证未来的兼容性与维护 那么推荐的做法是什么呢？答案是 netlink+sock_diag 简单介绍下 netlink6 是 Linux 2.2 引入的一种 Kernel Space 与 User Space 进行通信的机制，最早由 RFC35497 提出。官方对于 netlink6 的描述大概是这样 Netlink is used to transfer information between the kernel anduser-space processes. It consists of a standard sockets-based interface for user space processes and an internal kernel API for kernel modules.The internal kernel interface is not documented in this manual page. There is also an obsolete netlink interface via netlink character devices; this interface is not documented here and is provided only for backward compatibility. 简而言之大概是用户可以利用 netlink6 很方便的与内核中的不同的 Kernel Module 进行数据交互 而在我们这样的场景下，我们就需要利用到 sock_diag8，官方对此的描述是 The sock_diag netlink subsystem provides a mechanism for obtaining information about sockets of various address families from the kernel. This subsystem can be used to obtain information about individual sockets or request a list of sockets. 这里简而言之是说我们可以利用 sock_diag7 来获取不同 socket 的连接状态及相应的指标。（我们能获取到上文提到的所有指标，也能获得更细的 RTT 等指标）啊对了，这里要注意，netlink6 可以通过设置参数来从所有的 Network Namespace 获取指标。 在使用 netlink6 时，可能直接用 Pure C 来写比较繁琐。所幸，社区已经有不少封装成熟的 Lib，比如这里我选用 vishvananda 所封装的 netlink 库8，这里我给一个 Demo 1234567891011121314151617181920package mainimport ( &quot;fmt&quot; &quot;github.com/vishvananda/netlink&quot; &quot;syscall&quot;)func main() { results, err := netlink.SocketDiagTCPInfo(syscall.AF_INET) if err != nil { return } for _, item := range results { if item.TCPInfo != nil { fmt.Printf(&quot;Source:%s, Dest:%s, RTT:%d\\n&quot;, item.InetDiagMsg.ID.Source.String(), item.InetDiagMsg.ID.Destination.String(), item.TCPInfo.Rtt) } }} 运行示例大概是这样 OK，现在我们能用官方推荐的 Best Practice 来获取到更全更细的指标，也无需操心 Network namespace 的问题，但是我们最开始的几个问题还有一个比较棘手，就是实时性的问题。 因为如果我们选择周期性的轮询，那么如果在我们的轮询间隔中发生了网络波动，我们将丢失掉对应的现场。所以我们怎么样去解决实时性的问题呢？ 优化 2.0 版如果要在具体的比如重传，connection reset 等事件发生的时候，直接触发我们的调用。看过我之前博客的同学，可能第一时间考虑使用 eBPF + kprobe 的组合，在一些诸如 tcp_reset ，tcp_retransmit_skb 之类的关键调用上打点来获取实时的数据。Sounds good！ 不过实际上还是有一些小小的问题 kprobe 的开销在高频的情况下，相对来说会比较大一些 如果我们仅仅需要一些诸如 source_address, dest_address, source_port, dest_port 之类的信息，我们直接走 kprobe 拿完整地 skb 再来 cast 属实有点浪费 所以我们有什么更好的方法吗？有的！ 在 Linux 中，对于一系列的类似我们需求这样的特殊事件的触发与回调的场景，有一套基础设施叫做 Tracepoint9。这套设施，能够很好的帮我们处理监听事件并回调的需求。而在 Linux 4.15 以及 4.16 之后，Linux 新增了6个 tcp 相关的 Tracepoint9 分别是 tcp:tcp_destroy_sock tcp:tcp_probe tcp:tcp_receive_reset tcp:tcp_retransmit_skb tcp:tcp_retransmit_synack tcp:tcp_send_reset 这些 Tracepoint9 的含义，大家看名字可能就能明白了 而在这些 Tracepoint9 触发的时候，他们会给注册回调函数传入若干参数，这里我也给大家列一下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657tcp:tcp_retransmit_skb const void * skbaddr; const void * skaddr; __u16 sport; __u16 dport; __u8 saddr[4]; __u8 daddr[4]; __u8 saddr_v6[16]; __u8 daddr_v6[16];tcp:tcp_send_reset const void * skbaddr; const void * skaddr; __u16 sport; __u16 dport; __u8 saddr[4]; __u8 daddr[4]; __u8 saddr_v6[16]; __u8 daddr_v6[16];tcp:tcp_receive_reset const void * skaddr; __u16 sport; __u16 dport; __u8 saddr[4]; __u8 daddr[4]; __u8 saddr_v6[16]; __u8 daddr_v6[16];tcp:tcp_destroy_sock const void * skaddr; __u16 sport; __u16 dport; __u8 saddr[4]; __u8 daddr[4]; __u8 saddr_v6[16]; __u8 daddr_v6[16];tcp:tcp_retransmit_synack const void * skaddr; const void * req; __u16 sport; __u16 dport; __u8 saddr[4]; __u8 daddr[4]; __u8 saddr_v6[16]; __u8 daddr_v6[16];tcp:tcp_probe __u8 saddr[sizeof(struct sockaddr_in6)]; __u8 daddr[sizeof(struct sockaddr_in6)]; __u16 sport; __u16 dport; __u32 mark; __u16 length; __u32 snd_nxt; __u32 snd_una; __u32 snd_cwnd; __u32 ssthresh; __u32 snd_wnd; __u32 srtt; __u32 rcv_wnd; 嗯，看到这里，大家可能心里应该有个数了，那么我们还是来写一下示例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071from bcc import BPFbpf_text = &quot;&quot;&quot;BPF_RINGBUF_OUTPUT(tcp_event, 65536);enum tcp_event_type { retrans_event, recv_rst_event,};struct event_data_t { enum tcp_event_type type; u16 sport; u16 dport; u8 saddr[4]; u8 daddr[4]; u32 pid;};TRACEPOINT_PROBE(tcp, tcp_retransmit_skb){ struct event_data_t event_data={}; event_data.type = retrans_event; event_data.sport = args-&gt;sport; event_data.dport = args-&gt;dport; event_data.pid=bpf_get_current_pid_tgid()&gt;&gt;32; bpf_probe_read_kernel(&amp;event_data.saddr,sizeof(event_data.saddr), args-&gt;saddr); bpf_probe_read_kernel(&amp;event_data.daddr,sizeof(event_data.daddr), args-&gt;daddr); tcp_event.ringbuf_output(&amp;event_data, sizeof(struct event_data_t), 0); return 0;}TRACEPOINT_PROBE(tcp, tcp_receive_reset){ struct event_data_t event_data={}; event_data.type = recv_rst_event; event_data.sport = args-&gt;sport; event_data.dport = args-&gt;dport; event_data.pid=bpf_get_current_pid_tgid()&gt;&gt;32; bpf_probe_read_kernel(&amp;event_data.saddr,sizeof(event_data.saddr), args-&gt;saddr); bpf_probe_read_kernel(&amp;event_data.daddr,sizeof(event_data.daddr), args-&gt;daddr); tcp_event.ringbuf_output(&amp;event_data, sizeof(struct event_data_t), 0); return 0;}&quot;&quot;&quot;bpf = BPF(text=bpf_text)def process_event_data(cpu, data, size): event = bpf[&quot;tcp_event&quot;].event(data) event_type = &quot;retransmit&quot; if event.type == 0 else &quot;recv_rst&quot; print( &quot;%s %d %d %s %s %d&quot; % ( event_type, event.sport, event.dport, &quot;.&quot;.join([str(i) for i in event.saddr]), &quot;.&quot;.join([str(i) for i in event.daddr]), event.pid, ) )bpf[&quot;tcp_event&quot;].open_ring_buffer(process_event_data)while True: bpf.ring_buffer_consume() 我这里使用了 tcp_receive_reset 和 tcp_retransmit_skb 来监控我们机器上的程序。为了演示具体的效果，我先用 Go 写了一个访问 Google 的程序，然后通过 sudo iptables -I OUTPUT -p tcp -m string --algo kmp --hex-string &quot;|c02bc02fc02cc030cca9cca8c009c013c00ac014009c009d002f0035c012000a130113021303|&quot; -j REJECT --reject-with tcp-reset 来给这个 Go 程序注入 Connection Reset （这里的注入原理是 Go 默认库的发起 HTTPS 链接的 Client Hello 特征是固定的，我用 iptables 识别出方向流量，然后重置链接） 效果如下 嗯，写到这里，你可能想明白了，我们可以将 Tracepoint9 和 netlink6 结合使用来满足我们实时性的需求 优化 3.0 版实际上写到现在，也更多的是讲一些 Prototype 和思路上的介绍。而为了能满足生产上的需要，还有很多的工作要做（这也是我之前所做的工作的一部分），包括不仅限于： 工程上的性能优化，避免影响服务 Kubernetes 等容器平台的兼容 对接 Prometheus 等数据监控平台 可能需要嵌入 CNI 来获取更简便的监控路径等等 实际上社区在这一块也有很多很有意思的工作，比如 Cilium 等，大家有兴趣也可以关注下。而我后续拾掇拾掇代码，也会在合适的时候将我之前的一些实现路径给开源出来。 总结这篇文章差不多就写到这里，内核的网络监控终归是比较小众的领域。希望我这里面的一些经验能够帮助上大家。嗯，祝大家新年快乐！虎年大吉！（下一篇文章就是写去年的年终总结了） Reference RFC793: https://datatracker.ietf.org/doc/html/rfc793 RFC6298：https://datatracker.ietf.org/doc/html/rfc6298 RFC2018：https://datatracker.ietf.org/doc/html/rfc2018 The /proc Filesystem：https://www.kernel.org/doc/html/latest/filesystems/proc.html proc_net_tcp.txt：https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt netlink：https://man7.org/linux/man-pages/man7/netlink.7.html sock_diag：https://man7.org/linux/man-pages/man7/sock_diag.7.html vishvananda/netlink：https://github.com/vishvananda/netlink9: Linux Tracepoint：https://www.kernel.org/doc/html/latest/trace/tracepoints.html","link":"/posts/2022/01/31/a-simple-introduction-about-network-monitoring-in-linux-kernel/"},{"title":"简单聊聊进程中的信号处理 V2","text":"上次写了一个水文简单聊聊进程中的信号处理 ，师父看了后把我怒斥了一顿，表示上篇水文中的例子太 old style, too simple ,too naive。如果未来出了偏差，我也要负泽任的。吓得我连和妹子周年庆的文章都没写，先赶紧来重新水一篇文章，聊聊更优秀，更方便的信号处理方式 前情提要首先来看看，之前那篇文章中的例子 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); deletejob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;}int main(int argc, char **argv) { int pid; sigset_t mask_all, prev_all; sigfillset(&amp;mask_all); signal(SIGCHLD, handler); while (1) { if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); addjob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); }} 再来复习下几个关键的 syscall signal1: 信号处理函数，使用者可以通过这个函数为当前进程指定具体信号的 Handler。当信号触发时，系统会调用具体的 Handler 进行对应的逻辑处理。 sigfillset2: 用于操作 signal sets（信号集）的函数之一，这里的含义是将系统所有支持的信号量添加进一个信号集中 fork3: 大家比较熟悉的一个 API 了，创建一个新的进程，并返回 pid 。如果是在父进程中，返回的 pid 是对应子进程的 pid。如果子进程中，pid 为0 execve4: 执行一个特定的可执行文件 sigprocmask5：设置进程的信号屏蔽集。当传入第一个参数为 SIG_BLOCK 时，函数会将当前进程的信号屏蔽集保存在第三个参数传入的信号集变量中，并将当前进程的信号屏蔽集设置为第二个参数传入的信号屏蔽集。当第一个参数为 SIG_SETMASK 时，函数会将当前进程的信号屏蔽集设置为第二个参数设置的值。 wait_pid6: 做一个不精确的概括，回收并释放已终止的子进程的资源。 好了，复习完关键点之后，开始进入本文的关键部分。 更优雅的信号处理手段更优雅的 handler首先再来看看上面信号处理部分的代码 123456789101112131415void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); deletejob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;} 这里我们为了保证 handler 不被其余的信号打断，所以我们在处理的时候使用 sigprocmask + SIG_BLOCK 来做信号屏蔽。这样看起来逻辑上没啥问题，但是有个问题。当我们有其余很多不同 handler 的时候，我们势必会生成很多重复冗余的代码。那么我们有没有更优雅的方法来保证我们的 handler 的安全呢？ 有（超大声（好，很有精神！（逃。隆重介绍一个新的 syscall -&gt; sigaction7 废话不多说，先上代码 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { deletejob(pid); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;}int main(int argc, char **argv) { int pid; sigset_t mask_all, prev_all; sigfillset(&amp;mask_all); struct sigaction new_action; new_action.sa_handler=handler; new_action.sa_mask=mask_all; signal(SIGCHLD, handler); while (1) { if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); addjob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); }} 好！很有精神！大家可能发现了，我们这段代码相较于之前的代码增加了关于 sigaction 相关的设置。难道？ yep，在 sigaction 中，我们可以通过设置 sa_mask 来设置当信号处理函数执行期间，进程将阻塞哪些信号。 你看，这样我们的代码是不是相较于之前更为优雅了。当然，sigaction 还有很多其余很有用的设置项，大家可以下来了解一下。 更快速的信号处理方式在我们上面的例子中，我们已经解决了优雅的设置信号处理函数这样的问题，那么我们现在又面临了一个全新的问题。 如上面所说，我们信号处理函数在执行时，我们选择阻塞了其余的信号。那么这里存在一个问题，当我们在信号处理函数中的逻辑耗时较长，且不需要原子性（即需要和信号处理函数保持同步），而且系统中的信号发生频率较高。那么我们这样的做法将会导致进程的信号队列不断增加，进而导致不可预料的后果。 那么我们这里有什么更好的方法来处理这件事呢？ 假设，我们打开一个文件，在信号处理函数中只完成一件事，就是往这个文件中写一个特定的值。然后我们轮询这个文件，如果一旦发生变化，那么我们读取文件中的值，判断具体的信号，做具体的信号处理，这样是不是既保证了信号的妥投，又保证我们信号处理逻辑将阻塞信号的代价降至最低了？ 当然，当然，社区知道大家嫌写代码难，所以专门给大家提供了一个船新的 syscall -&gt; signalfd8 老规矩，先来看看例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/signalfd.h&gt;#include &lt;sys/wait.h&gt;#define MAXEVENTS 64void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }int main(int argc, char **argv) { int pid; struct epoll_event event; struct epoll_event *events; sigset_t mask; sigemptyset(&amp;mask); sigaddset(&amp;mask, SIGCHLD); if (sigprocmask(SIG_SETMASK, &amp;mask, NULL) &lt; 0) { perror(&quot;sigprocmask&quot;); return 1; } int sfd = signalfd(-1, &amp;mask, 0); int epoll_fd = epoll_create(MAXEVENTS); event.events = EPOLLIN | EPOLLEXCLUSIVE | EPOLLET; event.data.fd = sfd; int s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); while (1) { int n = epoll_wait(epoll_fd, events, MAXEVENTS, 1); if (n == -1) { if (errno == EINTR) { fprintf(stderr, &quot;epoll EINTR error\\n&quot;); } else if (errno == EINVAL) { fprintf(stderr, &quot;epoll EINVAL error\\n&quot;); } else if (errno == EFAULT) { fprintf(stderr, &quot;epoll EFAULT error\\n&quot;); exit(-1); } else if (errno == EBADF) { fprintf(stderr, &quot;epoll EBADF error\\n&quot;); exit(-1); } } printf(&quot;%d\\n&quot;, n); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (sfd == events[i].data.fd) { struct signalfd_siginfo si; ssize_t res = read(sfd, &amp;si, sizeof(si)); if (res &lt; 0) { fprintf(stderr, &quot;read error\\n&quot;); continue; } if (res != sizeof(si)) { fprintf(stderr, &quot;Something wrong\\n&quot;); continue; } if (si.ssi_signo == SIGCHLD) { printf(&quot;Got SIGCHLD\\n&quot;); int child_pid = waitpid(-1, NULL, 0); deletejob(child_pid); } } } if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } addjob(pid); }} 好了，我们来介绍下这段代码中的一些关键点 signalfd 是一类特殊的文件描述符，这个文件可读，可 select 。当我们指定的信号发生时，我们可以从返回的 fd 中读取到具体的信号值。 signalfd 优先级比信号处理函数低。换句话说，假设我们为信号 SIGCHLD 注册了信号处理函数，同时也为其注册了 signalfd 那么当信号发生时，将优先调用信号处理函数。所以我们在使用 signalfd 时，需要利用 sigprocmask 设置进程的信号屏蔽集。 如前面所说，该文件描述符可 select ，换句话说，我们可以利用 select9, poll10, epoll1112 等函数来对 fd 进行监听。在上面的的代码中，我们就利用 epoll 对 signalfd 进行监听 当然，这里额外要注意的一点是，很多语言不一定提供了官方的 signalfd 的 API（如 Python），但是也有可能提供了等价的替代品，典型的例子就是 Python 中的 signal.set_wakeup_fd13 在这里也给大家留一个思考题：除了利用 signalfd ，还有什么方法可以实现高效，安全的信号处理？ 总结私以为信号处理是作为一个研发的基本功，我们需要安全，可靠的处理在程序环境中遇到的各种信号。而系统也提供了很多设计很优秀的 API 来减轻研发的负担。但是我们要知道，信号本质上是通讯手段的一种。而其天生的弊端便是携带的信息较少。很多时候，当我们有很多高频的信息传递需要去做的时候，这个时候可能利用信号并不是一个很好的选择。当然这个并没有定论。只能 case by case 的去做 trade-off。 差不多就这样吧，本周第二篇水文混完（逃 Reference [1]. Linux man page: signal [2]. Linux man page: sigfillset [3]. Linux man page: fork [4]. Linux man page: execve [5]. Linux man page: sigprocmask [6]. Linux man page: waitpid [7]. Linux man page: sigaction [8]. Linux man page: signalfd [9]. Linux man page: select [10]. Linux man page: poll [11]. Linux man page: epoll_ctl [12]. Linux man page: epoll_wait [13]. Python Documentation: signal.set_wakeup_fd","link":"/posts/2020/11/07/a-simple-introduction-about-signal-process-in-linux-v2/"},{"title":"简单聊聊进程中的信号处理","text":"最近在某个技术群里帮人分析了 Linux 编程下信号处理的一段代码。我自己觉得这段代码是挺不错的一个例子，所以写个简单的水文，用这段代码聊聊 Linux 中的信号处理 正文我们首先来看一看这一段代码 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); deletejob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;}int main(int argc, char **argv) { int pid; sigset_t mask_all, prev_all; sigfillset(&amp;mask_all); signal(SIGCHLD, handler); while (1) { if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); addjob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); }} 实际上这段代码是比较典型的信号处理的代码，为了引出后续的内容，我们先来复习一下，这段代码中几个关键的 syscall signal1: 信号处理函数，使用者可以通过这个函数为当前进程指定具体信号的 Handler。当信号触发时，系统会调用具体的 Handler 进行对应的逻辑处理。 sigfillset2: 用于操作 signal sets（信号集）的函数之一，这里的含义是将系统所有支持的信号量添加进一个信号集中 fork3: 大家比较熟悉的一个 API 了，创建一个新的进程，并返回 pid 。如果是在父进程中，返回的 pid 是对应子进程的 pid。如果子进程中，pid 为0 execve4: 执行一个特定的可执行文件 sigprocmask5：设置进程的信号屏蔽集。当传入第一个参数为 SIG_BLOCK 时，函数会将当前进程的信号屏蔽集保存在第三个参数传入的信号集变量中，并将当前进程的信号屏蔽集设置为第二个参数传入的信号屏蔽集。当第一个参数为 SIG_SETMASK 时，函数会将当前进程的信号屏蔽集设置为第二个参数设置的值。 wait_pid6: 做一个不精确的概括，回收并释放已终止的子进程的资源。 OK 了解完这样一些关键的 syscall 后，这段代码那么基本上不难理解了。但是要吃透这段代码，我们还需要去复习一下一些 Linux 或者说 POSIX 中的机制： 由 fork 创建出来的子进程，会继承父进程中的很多东西。就本文中聊的信号一部分来说，子进程会继承父进程的信号屏蔽集和信号处理函数的相关设置 execve 执行后，会重设当前进程的程序段与堆栈。所以在上面的代码中我们执行 /bin/date 后，子进程会被重设。信号处理函数等设置也会被重设 每个进程都有信号屏蔽集，在信号屏蔽集中的信号被触发时，会进入一个队列，暂时不会触发进程的信号处理，此时信号处于 pending 状态。在取消对应信号的屏蔽与阻塞后，再次触发进程的信号处理机制。如果进程显式声明忽略信号，那么不会触发信号的处理。（Tips：关于信号队列这一点，这是一个 POSIX 1. 的约定。在 POSIX 中将这种机制称为可靠信号，当阻塞期间，有多个信号发生时，会进入一个可靠队列确保信号能被妥投。 Linux 支持可靠信号，其余 Unix/类 Unix 不一定支持） 子进程退出后，会给所属的父进程传递一个 SIGCHLD1 信号，父进程在接受到这种信号后，需要调用 wait_pid6 函数对子进程进行处理。否则未被回收的子进程，会成为一个僵尸进程，也就是通常说的 Z 进程 OK，到现在，大家在掌握这些东西后，对于上面的代码应该能完整明白了。不过可能大家还有一个疑惑，为什么在这段代码中需要调用 sigprocmask5 设置进程的信号屏蔽集来阻塞信号呢？这涉及到另一个问题。 如前面所说，信号在触发时，进程会”跳转“对应的信号处理函数进行处理。但是信号处理函数处理完后的行为会怎么样呢？依照 Linux 中的设计，可能会出现两种情况 对于可重入函数而言，信号处理函数返回后会继续处理 对于不可重入函数而言，会返回 EINTR1 OK 大家这里应该对我们为什么会在这里使用 sigprocmask5 有具体的了解了，实际上是为了保证我们的一些函数能够正常的执行完，不会被信号处理所打断。当然这里也有其余的问题，如果信号触发特别密集的情况下，这里的处理会带来额外的 cost。所以还是需要根据不同的场景做 trade-off 了。 好了。差不多就这样吧，福报久了真没力气写文章，💊。下一篇文章应该就是我最近做内核协议栈监控的一些吃屎记录了（flag++（逃。 Reference [1]. Linux man page: signal [2]. Linux man page: sigfillset [3]. Linux man page: fork [4]. Linux man page: execve [5]. Linux man page: sigprocmask [6]. Linux man page: waitpid","link":"/posts/2020/10/24/a-simple-introduction-about-signal-process-in-linux/"},{"title":"简单聊聊容器中的一号进程","text":"新年了，决定趁着有时间的时候多写几篇技术水文。今天的话，准备来简单聊聊容器中我们每天都会接触，但是时常又会被我们忽略的一号进程 正文容器技术发展到现在，其实形态上已经发生了很大的变化。根据不同的场景，既有传统的 Docker1, containterd2 这样传统基于 CGroup + Namespace 的容器形态，也有像 Kata3 这样基于 VM 的新型的容器形态。本文主要着眼在传统容器中一号进程上。 我们都知道，传统容器依赖的 CGroup + Namespace 进行资源隔离，本质上来说，还是 OS 内的一个进程。所以在继续往下聊容器相关的内容之前，我们需要先来简单聊聊 Linux 中的进程管理 Linux 中的进程管理简单聊聊进程Linux 中的进程实际上是个非常大的话题，如果要展开聊，实际上这个话题可以聊一整本书= =，\b所以为了时间着想，我们还是把目光聚集在最核心的一部分上面（实际上是因为很多东西我也不懂。 首先来讲，在内核中利用一个特殊的结构体来维护进程有关的相关信息，比如常见的 PID，进程状态，打开的文件描述符等信息。在内核代码中，这个结构体是 task_struct4, 其大概结构大家可以看一下下图 而通常而言，我们会在系统上跑很多个进程。所以内核用一个进程表(实际上 Linux 中管理进程表的有多个数据结构，这里我们用 PID Hash Map 来举例）来维护所有 Process Descriptor 相关的信息，详见下图 OK， 这里我们大概了解了进程中的基本结构，现在我们来看我们常见使用进程的一个场景：父子进程。我们都知道，我们有时会在一个进程中，通过 fork5 这个 sys call 来创建出一个新的进程。通常来说，我们创建的新的进程是当前进程的子进程。那么在内核中怎么表达这种父子关系呢？ 回到刚刚提到 task_struct, 在这个结构体中存在这样几个字段来描述父子关系 real_parent：一个 task_struct 指针，指向父进程 parent: 一个 task_struct 指针，指向父进程。在大多数情况下，这个字段的值和 real_parent 一致。在有进程对当前进程使用 ptrace6 等情况的时候，和 real_parent 字段不一致 children：list_head, 其指向一个由当前进程所创建的所有子进程的双向链表 这里大家可能还有点抽象的话，给大家一个图就能看清楚了 实际上，我们发现，不同进程之间的父子关系，反应到具体的数据结构之上，就形成了一个完整的树形结构（先记住这点，我们稍后会再提到这里） 到现在为止，我们已经对 Linux 中的进程，有了最简单一个概念，那么接下来，我们会聊聊我们在进程使用中常遇到的两个问题：孤儿进程&amp;&amp;僵尸进程 孤儿进程 &amp;&amp; 僵尸进程首先来聊聊 僵尸进程 这个概念。 如前面所说，我们内核有进程表来维护 Process Descriptor 相关信息。那么在 Linux 的设计中，当一个子进程退出后，将保存自己的进程相关的状态以供父进程使用。而父进程将调用 waitpid7 来获取子进程状态，并清理相关资源。 那么如上所说，父进程是有可能需要拿到子进程相关的状态的。那么也就导致为了满足这一设计，内核中的进程表将一直保存相关资源。当僵尸进程多了以后，那么将造成很大的资源浪费。 首先来看一个简单的僵尸进程的例子 1234567891011121314#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main() { int pid; if ((pid = fork()) == 0) { printf(&quot;Here's child process\\n&quot;); } else { printf(&quot;the child process pid is %d\\n&quot;, pid); sleep(20); } return 0;} 然后我们编译执行这段代码，然后配合 ps 命令查看一下，发现我们的确造了一个 z 进程 OK 我们再来看一个正确处理子进程退出的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/signalfd.h&gt;#include &lt;sys/wait.h&gt;#define MAXEVENTS 64void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }int main(int argc, char **argv) { int pid; struct epoll_event event; struct epoll_event *events; sigset_t mask; sigemptyset(&amp;mask); sigaddset(&amp;mask, SIGCHLD); if (sigprocmask(SIG_SETMASK, &amp;mask, NULL) &lt; 0) { perror(&quot;sigprocmask&quot;); return 1; } int sfd = signalfd(-1, &amp;mask, 0); int epoll_fd = epoll_create(MAXEVENTS); event.events = EPOLLIN | EPOLLEXCLUSIVE | EPOLLET; event.data.fd = sfd; int s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); while (1) { int n = epoll_wait(epoll_fd, events, MAXEVENTS, 1); if (n == -1) { if (errno == EINTR) { fprintf(stderr, &quot;epoll EINTR error\\n&quot;); } else if (errno == EINVAL) { fprintf(stderr, &quot;epoll EINVAL error\\n&quot;); } else if (errno == EFAULT) { fprintf(stderr, &quot;epoll EFAULT error\\n&quot;); exit(-1); } else if (errno == EBADF) { fprintf(stderr, &quot;epoll EBADF error\\n&quot;); exit(-1); } } printf(&quot;%d\\n&quot;, n); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (sfd == events[i].data.fd) { struct signalfd_siginfo si; ssize_t res = read(sfd, &amp;si, sizeof(si)); if (res &lt; 0) { fprintf(stderr, &quot;read error\\n&quot;); continue; } if (res != sizeof(si)) { fprintf(stderr, &quot;Something wrong\\n&quot;); continue; } if (si.ssi_signo == SIGCHLD) { printf(&quot;Got SIGCHLD\\n&quot;); int child_pid = waitpid(-1, NULL, 0); deletejob(child_pid); } } } if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } addjob(pid); }} OK, 我们现在都知道了，子进程退出后需要由父进程正确的回收相关的资源。那么问题来了，我们父进程先于子进程退出了怎么办。实际上这是一个很常见的场景。比如说大家去用两次 fork 实现守护进程。 我们常规的认知来说，我们父进程退出后，这个进程所属的所有子进程会进行 re-parent 到当前 PID Namespace 的一号进程上，那么这样的答案是正确的么？对，也不对，我们首先来看一个例子 1234567891011121314151617181920212223#include &lt;stdio.h&gt;#include &lt;sys/prctl.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main() { int pid; int err = prctl(PR_SET_CHILD_SUBREAPER, 1); if (err != 0) { return 0; } if ((pid = fork()) == 0) { if ((pid = fork()) == 0) { printf(&quot;Here's child process1\\n&quot;); sleep(20); } else { printf(&quot;the child process pid is %d\\n&quot;, pid); } } else { sleep(40); } return 0;} 这是一个很典型的两次 fork 创建守护进程的代码（除了我没写 SIGCHLD 处理（逃）。我们来看下这段代码的输出 我们能看到守护进程的 PID 是 449920 然后我们执行 ps -efj 和 ps auf 两个命令看一下结果 我们能看到，449920 这个进程在父进程退出后没有 re-parent 到当前空间的一号进程上。这是为什么呢？可能眼尖的同学已经注意到，这段代码中一个特殊的 sys call prctl8。我们给当前进程设置了 PR_SET_CHILD_SUBREAPER 的属性。 这里我们来看一下内核里的实现 1234567891011121314151617181920212223242526272829303132333435363738394041/* * When we die, we re-parent all our children, and try to: * 1. give them to another thread in our thread group, if such a member exists * 2. give it to the first ancestor process which prctl'd itself as a * child_subreaper for its children (like a service manager) * 3. give it to the init process (PID 1) in our pid namespace */static struct task_struct *find_new_reaper(struct task_struct *father, struct task_struct *child_reaper){ struct task_struct *thread, *reaper; thread = find_alive_thread(father); if (thread) return thread; if (father-&gt;signal-&gt;has_child_subreaper) { unsigned int ns_level = task_pid(father)-&gt;level; /* * Find the first -&gt;is_child_subreaper ancestor in our pid_ns. * We can't check reaper != child_reaper to ensure we do not * cross the namespaces, the exiting parent could be injected * by setns() + fork(). * We check pid-&gt;level, this is slightly more efficient than * task_active_pid_ns(reaper) != task_active_pid_ns(father). */ for (reaper = father-&gt;real_parent; task_pid(reaper)-&gt;level == ns_level; reaper = reaper-&gt;real_parent) { if (reaper == &amp;init_task) break; if (!reaper-&gt;signal-&gt;is_child_subreaper) continue; thread = find_alive_thread(reaper); if (thread) return thread; } } return child_reaper;} 这里我们总结一下，当父进程退出后，所属的子进程，将按照如下顺序进行 re-parent 线程组里其余可用线程（这里的线程有所不一样，可以暂时忽略） 在当前所属的进程树上不断寻找设置了 PR_SET_CHILD_SUBREAPER 进程 在前面两者都无效的情况下，re-parent 到当前 PID Namespace 中的 1 号进程上 到这里，我们关于 Linux 中进程管理的基础介绍就完成了。那么我们将来聊聊容器中的情况 容器中的一号进程这里，我们将利用，Docker 作为背景聊聊这个话题。首先，在 Docker 1.11 之后，其架构发生了比较大的变化，如下图所示 那么我们拉起一个容器的的流程如下 Docker Daemon 向 containerd 发送指令 containerd 创建一个 containterd-shim 进程 containerd-shim 创建一个 runc 进程 runc 进程将根据 OCI 标准，设置相关环境（创建 cgroup，创建 ns 等），然后执行 entrypoint 中的设定的命令 runc 在执行完相关设置后，将自我退出，此时其子进程（即容器命名空间内的1号进程）将被 re-parent 给 containerd-shim 进程。 OK，上面 step 5 操作，就需要依赖我们上节中讲到的 prctl 和 PR_SET_CHILD_SUBREAPER 。 自此，containerd-shim 将承担容器内进程相关的操作，即便其父进程退出，子进程也会根据 re-parent 的流程托管到 containerd-shim 进程上。 那么，这样是不是就没有问题了呢？ 答案很明显不是。来给大家举一个实际上的场景：假设我一个服务需要实现一个需求叫做优雅下线。通常而言，我们会在暴力杀死进程之前，利用 SIGTERM 信号实现这个功能。但是在容器时期有个问题，我们一号进程，可能不是程序本身（比如大家习惯性的会考虑在 entrypoint 中用 bash 去裹一层），或者经过一些特殊场景，容器中的进程，全部已经托管在 containerd-shim 上了。而 contaninerd-shim 是不具备信号转发的能力的。 所以在这样一些场景下，我们就需要考虑额外引入一些组件来完成我们的需求。这里以一个非常轻量级的专门针对容器的设计的一号进程项目 tini9 来作为介绍 我们这里看一下核心的一些代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455int register_subreaper () { if (subreaper &gt; 0) { if (prctl(PR_SET_CHILD_SUBREAPER, 1)) { if (errno == EINVAL) { PRINT_FATAL(&quot;PR_SET_CHILD_SUBREAPER is unavailable on this platform. Are you using Linux &gt;= 3.4?&quot;) } else { PRINT_FATAL(&quot;Failed to register as child subreaper: %s&quot;, strerror(errno)) } return 1; } else { PRINT_TRACE(&quot;Registered as child subreaper&quot;); } } return 0;}int wait_and_forward_signal(sigset_t const* const parent_sigset_ptr, pid_t const child_pid) { siginfo_t sig; if (sigtimedwait(parent_sigset_ptr, &amp;sig, &amp;ts) == -1) { switch (errno) { case EAGAIN: break; case EINTR: break; default: PRINT_FATAL(&quot;Unexpected error in sigtimedwait: '%s'&quot;, strerror(errno)); return 1; } } else { /* There is a signal to handle here */ switch (sig.si_signo) { case SIGCHLD: /* Special-cased, as we don't forward SIGCHLD. Instead, we'll * fallthrough to reaping processes. */ PRINT_DEBUG(&quot;Received SIGCHLD&quot;); break; default: PRINT_DEBUG(&quot;Passing signal: '%s'&quot;, strsignal(sig.si_signo)); /* Forward anything else */ if (kill(kill_process_group ? -child_pid : child_pid, sig.si_signo)) { if (errno == ESRCH) { PRINT_WARNING(&quot;Child was dead when forwarding signal&quot;); } else { PRINT_FATAL(&quot;Unexpected error when forwarding signal: '%s'&quot;, strerror(errno)); return 1; } } break; } } return 0;} 这里我们能很清楚看到两个核心点 tini 会通过 prctl 和 PR_SET_CHILD_SUBREAPER 来接管容器内的孤儿进程 tini 在收到信号后，会将信号转发给子进程或者是所属的子进程组 当然其实 tini 本身也有一些小问题（不过比较冷门）这里留一个讨论题：假设我们有这样一个服务，在创建10个守护进程后自己退出。在这十个守护进程中，我们都会设置一个全新的进程组 ID （所谓进程组逃逸）。那么我们怎么样将信号转发到这十个进程上（仅供讨论，生产上这么干的人早被打死了） 总结可能看到这里，可能有人要喷我不讲武德，说好的容器内一号进程，但是花了大半篇幅来讲 Linux 进程233333. 实际上传统容器基本可以认为是在 OS 中执行的一个完整进程。讨论容器中的一号进程离不开讨论 Linux 中进程管理的相关知识点。 希望通过这篇技术水文能帮大家对容器中一号进程有个大概的认知，并能正确的使用和管理他。 最后祝大家新年快乐！（希望新年我能不以写水文为生，呜呜呜呜） Reference [1]. Docker [2]. containerd [3]. kata [4]. task_struct [5]. Linux Man Page: fork [6]. Linux Man Page: ptrace [7]. Linux man page: waitpid [8]. Linux man page: prctl [9]. tini","link":"/posts/2021/02/13/a-simple-introduction-about-the-init-process-in-container/"},{"title":"简单聊聊容器中的 UID 中的一点小坑","text":"今天不太舒服，在家请假了一天。突然想起最近因为一些小问题，看了下关于容器中 UID 的东西。所以简单来聊聊这方面的东西。算个新手向的文章 开篇最近帮 FrostMing 把他的 tokei-pie-cooker 部署到我的 K8S 上做成一个 SaaS 服务。Frost 最开始给我了一个镜像地址。然后我啪的一下复制粘贴了一个 Deployment 出来 123456789101112131415161718192021222324252627282930313233apiVersion: apps/v1kind: Deploymentmetadata: name: tokei-pie namespace: tokei-pie labels: app: tokei-piespec: replicas: 12 selector: matchLabels: app: tokei-pie template: metadata: labels: app: tokei-pie spec: containers: - name: tokei-pie image: frostming/tokei-pie-cooker:latest imagePullPolicy: Always resources: limits: cpu: &quot;1&quot; memory: &quot;2Gi&quot; ephemeral-storage: &quot;3Gi&quot; requests: cpu: &quot;500m&quot; memory: &quot;500Mi&quot; ephemeral-storage: &quot;1Gi&quot; securityContext: allowPrivilegeEscalation: false runAsNonRoot: true 啪的一下，很快嘛，很简单对吧，限制下 Storage 用量，限制一下 NonRoot ，以免我被人打穿。Fine，kubectl apply -f 一下。Ops， 1Error: container has runAsNonRoot and image has non-numeric user (tokei), cannot verify user is non-root (pod: &quot;tokei-pie-6c6fd5cb84-s4bz7_tokei-pie(239057ea-fe47-40a9-8041-966c65344a44)&quot;, container: tokei-pie) 噢，被 K8$ 拦截了，拦截点在 pkg/kubelet/kuberruntime/security_context_others.go 中。 1234567891011121314151617181920212223func verifyRunAsNonRoot(pod *v1.Pod, container *v1.Container, uid *int64, username string) error { effectiveSc := securitycontext.DetermineEffectiveSecurityContext(pod, container) // If the option is not set, or if running as root is allowed, return nil. if effectiveSc == nil || effectiveSc.RunAsNonRoot == nil || !*effectiveSc.RunAsNonRoot { return nil } if effectiveSc.RunAsUser != nil { if *effectiveSc.RunAsUser == 0 { return fmt.Errorf(&quot;container's runAsUser breaks non-root policy (pod: %q, container: %s)&quot;, format.Pod(pod), container.Name) } return nil } switch { case uid != nil &amp;&amp; *uid == 0: return fmt.Errorf(&quot;container has runAsNonRoot and image will run as root (pod: %q, container: %s)&quot;, format.Pod(pod), container.Name) case uid == nil &amp;&amp; len(username) &gt; 0: return fmt.Errorf(&quot;container has runAsNonRoot and image has non-numeric user (%s), cannot verify user is non-root (pod: %q, container: %s)&quot;, username, format.Pod(pod), container.Name) default: return nil }} 简而言之，K8$ 先会从镜像的 manifact 中拿镜像的 Runing Username. 如果你镜像里有设置 Runing Username 且你设置了 runAsNoneRoot ，同时你没设置 Run uid，那么会报错。Make Sense，如果你指定的用户名的 uid 是0，那么实际上还是打穿了 SecurityContext 的限制 找 Frost 要了下他的 Dockerfile，如下 12345678910111213141516FROM python:3.10-slimRUN useradd -m tokeiUSER tokeiWORKDIR /appCOPY requirements.txt .RUN pip install -r requirements.txtCOPY templates /app/templatesCOPY app.py .COPY gunicorn_config.py .ENV PATH=&quot;/home/tokei/.local/bin:$PATH&quot;EXPOSE 8000CMD [&quot;gunicorn&quot;, &quot;-c&quot;, &quot;gunicorn_config.py&quot;] OK, 平平淡淡，没有异常。OK，那我啪的一下改了 Deployment，新版如下 12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: tokei-pie namespace: tokei-pie labels: app: tokei-piespec: replicas: 12 selector: matchLabels: app: tokei-pie template: metadata: labels: app: tokei-pie spec: containers: - name: tokei-pie image: frostming/tokei-pie-cooker:latest imagePullPolicy: Always resources: limits: cpu: &quot;1&quot; memory: &quot;2Gi&quot; ephemeral-storage: &quot;3Gi&quot; requests: cpu: &quot;500m&quot; memory: &quot;500Mi&quot; ephemeral-storage: &quot;1Gi&quot; securityContext: allowPrivilegeEscalation: false runAsNonRoot: true runAsUser: 10086 这里选了我自己的 Magic Number， 10086，这下总没问题了吧，我又 duang 的一下执行了 kubectl apply -f。Oooops，船新的报错 1/usr/local/bin/python: can't open file '/home/tokei/.local/bin/gunicorn': [Errno 13] Permission denied OK，那我抛弃我的 Magic Number，换成传说中的数字，1000 来看一下。OK，Works！ 那么这一切到底是为什么呢？那么接下来小编会来告诉你（XD 简单的介绍，完整的快乐容器中的 UID首先讲一点前置的知识。首先在 Linux 中的 UID 分配规律。首先在一个 Linux UserNamespace 中，UID 默认的范围是从 0 - 60000。其中 UID 0 是 Root 的保留 UID。从理论上来讲，用户 UID/GID 的创建的范围是从 1 到 60000 但是实际上可能会更复杂一些，通常各发行版的内置的一些服务，可能会自带一些特殊的用户，比如经典的 www-data （之前没事喜欢搭博客的同学对这个肯定不陌生）。所以实践中，一个 User Namespace 内，一个 UID 的起始，通常是 500 或者 1000。具体的设置，取决于一个特殊文件的设置，login.defs，路径是 /etc/login.defs 官方文档中描述如下： Range of user IDs used for the creation of regular users by useradd or newusers. The default value for UID_MIN (resp. UID_MAX) is 1000 (resp. 60000). 在我们调用 useradd 来在构建 Dockerfile 时添加用户。这个时候，在相关操作执行完毕后，会在 /etc/passwd 这个特殊文件中添加对应的用户信息。以 Frost 的 Dockerfile 为例，最终的 passwd 文件内容如下 1234567891011121314151617181920root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologinsync:x:4:65534:sync:/bin:/bin/syncgames:x:5:60:games:/usr/games:/usr/sbin/nologinman:x:6:12:man:/var/cache/man:/usr/sbin/nologinlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologinmail:x:8:8:mail:/var/mail:/usr/sbin/nologinnews:x:9:9:news:/var/spool/news:/usr/sbin/nologinuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologinproxy:x:13:13:proxy:/bin:/usr/sbin/nologinwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologinbackup:x:34:34:backup:/var/backups:/usr/sbin/nologinlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologinirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologingnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologinnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin_apt:x:100:65534::/nonexistent:/usr/sbin/nologintokei:x:1000:1000::/home/tokei:/bin/sh 那么构建文件结束后，我们来看一下我们常见的容器运行时之一的 Docker 对此相关的处理。 这里还要科普一点前置的知识，现在 Docker 实际上只能算一个 Daemon+CLI，它核心的功能是调用其背后的 containerd。而 containerd 最终通过 runc 来创建相关的容器 那我们这里看一下 runc 对此相关的处理 在 runc 创建容器的时候，会调用 runc/libcontainer/init_linux.go.finalizeNamespace 这个函数完成一些设置，而在这个函数中，会调用 runc/libcontainer/init_linux.go.setupUser 这个函数来完成 Exec User 的设置，我们来看下源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990func setupUser(config *initConfig) error { // Set up defaults. defaultExecUser := user.ExecUser{ Uid: 0, Gid: 0, Home: &quot;/&quot;, } passwdPath, err := user.GetPasswdPath() if err != nil { return err } groupPath, err := user.GetGroupPath() if err != nil { return err } execUser, err := user.GetExecUserPath(config.User, &amp;defaultExecUser, passwdPath, groupPath) if err != nil { return err } var addGroups []int if len(config.AdditionalGroups) &gt; 0 { addGroups, err = user.GetAdditionalGroupsPath(config.AdditionalGroups, groupPath) if err != nil { return err } } // Rather than just erroring out later in setuid(2) and setgid(2), check // that the user is mapped here. if _, err := config.Config.HostUID(execUser.Uid); err != nil { return errors.New(&quot;cannot set uid to unmapped user in user namespace&quot;) } if _, err := config.Config.HostGID(execUser.Gid); err != nil { return errors.New(&quot;cannot set gid to unmapped user in user namespace&quot;) } if config.RootlessEUID { // We cannot set any additional groups in a rootless container and thus // we bail if the user asked us to do so. TODO: We currently can't do // this check earlier, but if libcontainer.Process.User was typesafe // this might work. if len(addGroups) &gt; 0 { return errors.New(&quot;cannot set any additional groups in a rootless container&quot;) } } // Before we change to the container's user make sure that the processes // STDIO is correctly owned by the user that we are switching to. if err := fixStdioPermissions(config, execUser); err != nil { return err } setgroups, err := ioutil.ReadFile(&quot;/proc/self/setgroups&quot;) if err != nil &amp;&amp; !os.IsNotExist(err) { return err } // This isn't allowed in an unprivileged user namespace since Linux 3.19. // There's nothing we can do about /etc/group entries, so we silently // ignore setting groups here (since the user didn't explicitly ask us to // set the group). allowSupGroups := !config.RootlessEUID &amp;&amp; string(bytes.TrimSpace(setgroups)) != &quot;deny&quot; if allowSupGroups { suppGroups := append(execUser.Sgids, addGroups...) if err := unix.Setgroups(suppGroups); err != nil { return err } } if err := system.Setgid(execUser.Gid); err != nil { return err } if err := system.Setuid(execUser.Uid); err != nil { return err } // if we didn't get HOME already, set it based on the user's HOME if envHome := os.Getenv(&quot;HOME&quot;); envHome == &quot;&quot; { if err := os.Setenv(&quot;HOME&quot;, execUser.Home); err != nil { return err } } return nil} 大家看注释应该差不多能理解这段代码在干啥，在这段代码将会调用 runc/libcontainer/user/user.go.GetExecUserPath 和 runc/libcontainer/user/user.go.GetExecUser 来获取 exec 时的 UID，我们来看一下这块的实现（下面代码我精简了一部（ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374func GetExecUser(userSpec string, defaults *ExecUser, passwd, group io.Reader) (*ExecUser, error) { if defaults == nil { defaults = new(ExecUser) } // Copy over defaults. user := &amp;ExecUser{ Uid: defaults.Uid, Gid: defaults.Gid, Sgids: defaults.Sgids, Home: defaults.Home, } // Sgids slice *cannot* be nil. if user.Sgids == nil { user.Sgids = []int{} } // Allow for userArg to have either &quot;user&quot; syntax, or optionally &quot;user:group&quot; syntax var userArg, groupArg string parseLine([]byte(userSpec), &amp;userArg, &amp;groupArg) // Convert userArg and groupArg to be numeric, so we don't have to execute // Atoi *twice* for each iteration over lines. uidArg, uidErr := strconv.Atoi(userArg) gidArg, gidErr := strconv.Atoi(groupArg) // Find the matching user. users, err := ParsePasswdFilter(passwd, func(u User) bool { if userArg == &quot;&quot; { // Default to current state of the user. return u.Uid == user.Uid } if uidErr == nil { // If the userArg is numeric, always treat it as a UID. return uidArg == u.Uid } return u.Name == userArg }) if err != nil &amp;&amp; passwd != nil { if userArg == &quot;&quot; { userArg = strconv.Itoa(user.Uid) } return nil, fmt.Errorf(&quot;unable to find user %s: %v&quot;, userArg, err) } var matchedUserName string if len(users) &gt; 0 { // First match wins, even if there's more than one matching entry. matchedUserName = users[0].Name user.Uid = users[0].Uid user.Gid = users[0].Gid user.Home = users[0].Home } else if userArg != &quot;&quot; { // If we can't find a user with the given username, the only other valid // option is if it's a numeric username with no associated entry in passwd. if uidErr != nil { // Not numeric. return nil, fmt.Errorf(&quot;unable to find user %s: %v&quot;, userArg, ErrNoPasswdEntries) } user.Uid = uidArg // Must be inside valid uid range. if user.Uid &lt; minID || user.Uid &gt; maxID { return nil, ErrRange } // Okay, so it's numeric. We can just roll with this. }} 这里看着很复杂，实际上总结下来就这样 首先从 /etc/passwd 读取已知的所有的用户 如果用户启动时传入的是用户名，那么判断是否有用户名和启动参数传入的相等，没有则启动失败 如果用户启动传入的是 UID，那么如果在已知用户中有对应的用户，那么设置为该用户。如果没有，则将进程的 UID 设置为传入的 UID 如果用户什么都没传入，那么以 /etc/passwd 中第一个用户来作为 exec 用户。默认情况下第一个用户通常是指 UID 为 0 的 root 用户。 OK 那么回到我们的 Deployment 中，那我们不难得出如下的结论 如果我们没有设置 runAsUser ，且镜像里也没指定启动用户，那么我们容器中的进程将以当前 user namespace 中 uid 为 0 的 root 用户启动 如果在 Dockerfile 中设定了启动时的用户，且没有设置 runAsUser，那么将以我们在 Dockerfile 中的用户启动 如果我们设置了 runAsUser 且 Dockerfile 中也指定了相关的用户，那么将以 runAsUser 所指定的 UID 启动进程 OK 那么，到这里看似问题解决了。但是这里有个新的疑问。通常来说，我们创建文件之类的操作，默认的权限都是 755 ，即对于非当前用户，也非当前用户组内的成员，有可读可执行权限。按道理说不应该出现前文所说的 [Errno 13] Permission denied 情况。 我进容器看了下报错的文件，的确也和我估计的一样，是 755 权限 那么问题出在哪呢？问题出在 ~/.local/ 这个文件夹， 是的没错，这里的 .local 是 700 权限，即对于非当前用户，也非当前用户组内的成员，没有对当前目录的可执行权限。这里大家可能有点迷惑，目录的可执行权限是什么？这里引用下官方文档 Understanding Linux File Permissions 中的描述 execute – The Execute permission affects a user’s capability to execute a file or view the contents of a directory. OK，好吧，如果没有对应的目录的可执行权限，那么我们也没法执行该目录里的文件，即便我们有文件的可执行权限。 而我这里翻了一下 pip 的源码。发现 pip 在用户态安装的时候，如果不存在 .local 目录，那么会创建 .local 目录并将权限设置为 700。 OK 到这里我们的整个问题的因果链就已经完全建立了 在 dockerfile 中创建并设置用户 tokei，uid 1000 -&gt; pip 创建了 700 的 .local， .local 归属 UID 1000 的用户-&gt; 我们 runAsUser 设置为 非 1000 的数字 -&gt; 无 .local 的可执行权限 -&gt; 报错 说实话，我能理解 pip 为什么这么设计，但是我觉得这样的设计是有一点 broke 了一些约定俗成的规矩的，其合理性有待商榷 总结这个问题其实不算难查，但是发生的位置是我有点没有想到的，从我的角度来看，归根结底还是在与 pip 不遵守基本法造成的23333 这里留个题目大家有兴趣可以思考下。我们都知道 Docker 有个命令是 docker cp 是从宿主机往运行的容器中拷贝文件/从容器中往宿主机中拷贝文件。有个参数是 -a ，即保留原文件的 UID/GID，那么如果我们用这个参数从宿主机/容器往容器/宿主机中拷贝文件，那么我们 ls -lh 时，可以看到怎样的 User/UserGroup 信息。 OK，这篇水文就先写到这里，写水文真快乐。周末要是有时间的话，可以再写个水文简单聊聊一个关于最近遇到的一个很有趣的根据特征封锁 SSL 流量的手法分析 好了，溜了溜了","link":"/posts/2021/12/03/a-simple-introduction-about-uid-docker/"},{"title":"2018，我，2019，未来","text":"2018，我，2019，未来本来以为年前没有机会发这篇年终总结以及新年展望。不过目前去向已经确定，所以准备还是写一篇文章纪念下2018这一年，赶在农历戊戌年的末尾发一篇文章出来吧。 2018，我2018 的这一年算是很特殊的一年，我的本命年，24岁，第二次有意识的状态下过农历狗年。但是可能因为我一整年都没穿红内裤吧。所以整个人生算是起起伏伏，跌跌撞撞。 原本以为自己可能有很多话想说，但是真正要写的时候却发现不知道从何说起。所以干脆随意记点流水账吧 工作一整年的工作算是有条不紊的进行，去年，阿不，前年一整年都在做基础组件和快照这类基础服务的开发。今年因为搭档出差和转组的关系接触了下业务。虽然期间发生了不少的意外，但是也算是有一个不错的结果。也认识了不少的朋友。 但是最大的收获还是在于对自己的认知发生了改变。曾经以为自己能妥善的处理业务与基础服务的不同场景。过早的把自己定位在一个万金油的角色上，但是就目前看来，当时的认知是错误的。我自己对于业务可能兴趣并不是太大，可能需要去专注的做基础服务的开发学习，才是比较好的出路。 而这样的认知的改变也直接造成了我后续的计划的改变和未来走向的变动 在工作之余，我选择建立了一些私人项目来进行练手，在这样一个从0到1的过程中，进一步确认了我的想法。 不过今年学到最深的一句话还是“做一件事，做一件有结果的事” 不过今年的认知的改变和身体原因直接造成了我选择裸辞在家休息，也直接让我存款降为0，这也是年初没有想到的。 感情11月，在光棍节之前脱单，算是在知乎上认识的一位很优秀的女孩子，山东人，中国音乐学院的钢琴生。 她是一个很优秀的女孩子，至少比我优秀的多。至少我现在都还在思考一个问题：“当时她怎么眼瞎看上我的” 最开始我有点担心，某种意义上讲，我们算是两个不同世界的人，这样凑在一起是不是不太好，会不会缺少话题。但是目前看起来，我们彼此都在适应对方，我会告诉她互联网圈的趣闻，她告诉我艺术圈的趣闻，一起看话剧/音乐剧，一起吐槽分析一起做菜，一起买糖葫芦，目前看来这样的日子很舒服，很幸福。 对了，可能有朋友在担心，她会不会需要我照顾人之类的，实话实说，她是一个很独立的女孩子，也很会照顾我。在离职后两个月里，我几次生病（有几次还比较惨）都是她在照顾我，所以请大家放心啦 至于照片，她说你敢放就打死你，出于男女朋友间和谐生活的角度考虑，我就不单独放啦！ 技术2018年初给自己立了几个目标 《Unix 网络编程卷一》刷一遍 配合 15-213 刷《深入理解计算机系统》 CPython 源码……也争取吃一下吧…… 目前来看，完成度都还不错 UNP 刷了 CSAPP 没有刷，但是刷了 AUP（Unix 高级编程》 CPython 源码刷了一部分，并且作为作业向 CPython 提交两个代码补丁，一个文档补丁 另外还有些奇奇怪怪的收获 作为 PyCon China 2018 的主要参与者，筹办了北京场，成都场 作为讲师参加成都场的活动，做了一次关于黑 asyncio 的演讲 另外再去阅读了一些奇奇怪怪的源码，比如 Redis/Nginx 之类的。也写了点 Golang/Java 服务。 所以整体来看，预期达到并有所超出，但是缺陷也很明显，局限于某一部分分支，而缺乏方法论，系统设计方面的学习，可能这也是一位 HR 对我说我的格局可能还是相对较小的原因所在。 2019，未来在18年11月因为个人规划选择离开上家公司后，经过两个月休息，目前去向已经定了。去饿了么基础框架部 Huskar 组师从张江阁（松鼠奥利奥/tongseek）老师做服务注册治理平台方向的开发。整体薪资和定级出于隐私原因，我就不在这里透露了。只能说比我预期的还要好一些。所以在昨晚得到消息的时候很明确的告诉 HR 选择接受 Offer。 所以未来几年的发展方向应该会是在中间件和分布式服务这一块深入耕耘，同时也会在业余时间折腾不同的东西。 关于2019，首先争取多回家几次，陪陪父母。然后和女朋友好好的走下去，多去玩不同的地方，吃不同的好吃的，看不同音乐剧。 关于职业和技术方面。在我看来这次 Offer 算是一个破例 Offer 了（因为小跳大，而且只有两年经验非科班，不过因为三轮面试官对我的评价都蛮高，所以最后结果让我有一些惊喜）。所以先在饿了么立足，拿一个不错的绩效这会是一个首要的目标，为了我自己，也为了三位面试官不会后悔他们给出的评价。 社区方面，我会持续为 CPython 提交 Patch，争取有天能晋升成为核心开发者。同时我也会全程参与 PyCon China 2019 的筹办，恩，继续去学习一些技术之外的事 其余的话，就是继续补补计算机基础，补一些这次面试暴露出来的薄弱点，然后去按部就班学习新的东西。 结束前天饿了么三面结束后，我给师父说谢谢你带我不断地学习。师父说谢你自己啊白痴。其实年前一个月的面试不管结果如何，我最开心可能还是我终于有资格能认为自己是个合格的开发者了吧。算不上优秀，但是能算及格。这也是市场对我的肯定吧 我问师父，你徒弟没给你丢脸吧？师父说丢什么？你很厉害的。恩所以下一步的目标大概就是朝着有资格说自己是个不错的开发者努力了吧。 这一年，跌跌撞撞的走过来，哭过很多次，也难受过很多次，但是最后算是有个 happy ending，也算是一种幸运了。期间承蒙许多人的照顾，父母，女朋友，师父，leader，同事，我的密友们。至少在我想哭的时候，会有人陪着我。也在不断地支持我去看看更大的世界。 不管2019年这一年，会怎么样，会不会如同18年一样的不顺，我还是得努力让自己的每一天都变得精彩一点，有意义的一点。为我自己，也为所有照顾帮助过我的人。我不想让我和他们失望。 有道是，诗酒趁年华","link":"/posts/2018/12/31/about-me-2019/"},{"title":"继续爆论容器中的一号进程","text":"上周的文章聊了关于容器中的一号进程的一些概况后，在我师父某川(可以去 GitHub 找他玩，jschwinger23) 的指导与配合下，我们一起对目前主流的被广泛使用的两个容器中一号进程的实现 dumb-init 和 tini 做了一番探究，继续写个水文来爆论一番。 正文我们为什么需要一个一号进程，我们希望的一号进程需要承担怎样的职责？在继续聊关于 dumb-init 和 tini 的相关爆论之前，我们需要来 review 一个问题。我们为什么需要一个一号进程？以及我们所选择的一号进程需要承担怎么样的职责 其实我们在容器场景下需要一号进程托管在前面实际上有两种主要的场景， 对于容器内 Graceful Upgrade 二进制这种场景，主流的一种做法之一是 fork 一个新的进程，exec 新的二进制文件，新进程处理新链接，老进程处理老链接。（Nginx 就采用这种方案） 没有正确的处理信号转发以及进程回收的情况 一些如同 calico-node 的场景么，我们出于方便打包的考虑，将多个二进制运行在同一容器中 对于第一种其实需要说的没有太多，我们来看一下第二点的测试 我们先准备一个最简单 Python 文件，demo1.py 123import timetime.sleep(10000) 然后依照常规，我们开始用一个 bash 脚本裹一下 123#!/bin/bashpython /root/demo1.py 最后编写 Dockerfile 123456FROM python:3.9ADD demo1.py /root/demo1.pyADD demo1.sh /root/demo1.shENTRYPOINT [&quot;bash&quot;, &quot;/root/demo1.sh&quot;] 构建后开始执行，我们先来看一下进程结构 没有问题，现在我们用 strace 来 trace 一下，2049962、2050009 这两个进程，然后对 2049962 这个 bash 进程发 *SIGTERM＊ 信号 我们来看下结果 我们能清晰看到 2049962 进程在接到 SIGTERM 的时候，没有将其转发给 2050009 进程。在我们手动 SIGKILL 2049962 后， 2050009 也随即退出，这里可能有人会有点疑惑，为什么 2049962 退出后，2050009 也会退出呢？ 这里是由于 pid namespace 本身的特性，我们来看看，pid_namespaces 中的相关介绍 If the “init” process of a PID namespace terminates, the kernel terminates all of the processes in the namespace via a SIGKILL signal. 当当前 pid ns 内的一号进程退出的时候，内核直接 SIGKILL 伺候该 pid ns 内的剩余进程 OK，在我们结合容器调度框架后，那么在生产上实际会出现很多的坑，来看一段我之前的吐槽 我们一个测试服务，Spring Cloud 的，在下线后，节点无法从注册中心摘除，然后百思不得其解，最后查到问题，，本质上是这样，POD 被摘除的时候，K8S Scheduler 会给 POD 的 ENTRYPOINT 发一个 SIGTERM 信号，然后等待三十秒（默认的 graceful shutdown 超时实践)，还没响应就会 SIGKILL 直接杀问题在于，我们 Eureka 版的服务是通过 start.sh 来启动的，ENTRYPOINT [“/home/admin/start.sh”]，容器里默认是 /bin/sh 是 fork/exec 模式，导致我服务进程没法正确的收到 SIGTERM 信号，然后一直没结束就被 SIGKILL 了 刺激不刺激。除了信号转发无法正常处理以外，我们应用程序常见的一个常见处理的问题就是 Z 进程的出现，即子进程结束之后，无法正确的回收。比如早期 puppeteer 臭名昭著的 Z 进程问题。 在这种情况下，除了应用程序本身的问题以外，另外可能的原因是在守护进程这样的场景下，孤儿进程 re-parent 之后的进程，不具备回收子进程的功能 OK 在回顾完上面我们常见的问题后，我们来 review 一下我们对于容器内一号进程所需要承担的职责 信号的转发 Z 进程的回收 而在目前，在容器场景下，大家主要使用两个方案来作为自己的容器内一号进程，dumb-init 和 tini。这两个方案对于容器内孤儿与 Z 进程的处理都算是 OK。但是信号转发的实现上一言难尽。那么接下来 爆论时间！ 拉跨的 dumb-init某种程度上来说，dumb-init 这货完全是属于虚假宣传的典范。代码实现非常糙 来看看官方的宣传 dumb-init runs as PID 1, acting like a simple init system. It launches a single process and then proxies all received signals to a session rooted at that child process. 这里，dumb-init 说自己使用了 Linux 中的进程 Session，我们都知道，一个进程 Session 在默认情况下，共享一个 Process Group Id 。那么我们这里可以理解为，dumb-init 能将信号完全转发到进程组中的每个进程上。听起来很美好是不是？ 我们先来测试一下吧 测试代码如下，demo2.py 1234567import osimport timepid = os.fork()if pid == 0: cpid = os.fork()time.sleep(1000) Dockerfile 如下 12345678910FROM python:3.9RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.5/dumb-init_1.2.5_x86_64RUN chmod +x /usr/local/bin/dumb-initADD demo2.py /root/demo2.pyENTRYPOINT [&quot;/usr/local/bin/dumb-init&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo2.py&quot;] 构建，开跑，先来看下进程结构 然后老规矩，strace 2103908、2103909、2103910 这三个进程，然后我们对 dumb-init 的进程做一下发送 SIGTERM 的操作吧 诶？dumb-init 老师，发生了甚么事？为什么 2103909 直接被 SIGKILL 了，而没有收到 SIGTERM 这里我们要来看下 dumb-init 的关键实现 123456789101112131415161718192021222324252627282930313233void handle_signal(int signum) { DEBUG(&quot;Received signal %d.\\n&quot;, signum); if (signal_temporary_ignores[signum] == 1) { DEBUG(&quot;Ignoring tty hand-off signal %d.\\n&quot;, signum); signal_temporary_ignores[signum] = 0; } else if (signum == SIGCHLD) { int status, exit_status; pid_t killed_pid; while ((killed_pid = waitpid(-1, &amp;status, WNOHANG)) &gt; 0) { if (WIFEXITED(status)) { exit_status = WEXITSTATUS(status); DEBUG(&quot;A child with PID %d exited with exit status %d.\\n&quot;, killed_pid, exit_status); } else { assert(WIFSIGNALED(status)); exit_status = 128 + WTERMSIG(status); DEBUG(&quot;A child with PID %d was terminated by signal %d.\\n&quot;, killed_pid, exit_status - 128); } if (killed_pid == child_pid) { forward_signal(SIGTERM); // send SIGTERM to any remaining children DEBUG(&quot;Child exited with status %d. Goodbye.\\n&quot;, exit_status); exit(exit_status); } } } else { forward_signal(signum); if (signum == SIGTSTP || signum == SIGTTOU || signum == SIGTTIN) { DEBUG(&quot;Suspending self due to TTY signal.\\n&quot;); kill(getpid(), SIGSTOP); } }} 这是 dumb-init 老师处理信号的代码，在收到信号后，将除 SIGCHLD 的信号做转发（注意 SIGKILL 是不可 handle 信号），我们来看看信号转发的逻辑 123456789void forward_signal(int signum) { signum = translate_signal(signum); if (signum != 0) { kill(use_setsid ? -child_pid : child_pid, signum); DEBUG(&quot;Forwarded signal %d to children.\\n&quot;, signum); } else { DEBUG(&quot;Not forwarding signal %d to children (ignored).\\n&quot;, signum); }} 默认情况下直接 kill 发送信号，其中 -child_pid 是这样一个特性： If pid is less than -1, then sig is sent to every process in the process group whose ID is -pid. 直接转发进程组，看起来没啥问题啊？那么上面是甚么原因呢？我们再来复习下上一段话，kill 给进程组发信号的逻辑是 sig is sent to every process ，懂了，一个 O(N) 的遍历嘛。没啥问题啊？好了，不卖关子，这里 dumb-init 的实现存在一个 race-condition 我们刚刚说了，kill 进程组的行为是一个 O(N) 的遍历，那么必然会有进程先收到信号，而有进程后收到信号。以 SIGTERM 为例，假设我们 dumb-init 的子进程先收到 SIGTERM，优雅退出后，dumb-init 收到 SIGCHLD 的信号，然后 wait_pid 拿到子进程 ID，判断是自己直接托管的进程后，自杀退出。好了，由于 dumb-init 是我们当前 pid ns 内的 init 进程，再来复习下 pid ns 的特性。 If the “init” process of a PID namespace terminates, the kernel terminates all of the processes in the namespace via a SIGKILL signal. 在 dumb-init 自杀以后，剩余进程将直接被内核 SIGKILL 伺候。也就导致了我们上面看到的，子进程没有收到转发的信号！ 所以这里加粗处理一下，dumb-init 所承诺的，能将信号转发到所有进程上，完全是虚假宣传！ 而且请注意，dumb-init 宣称自己能管理一个 Session 内的进程！但是实际上他们只做了一个进程组的信号转发！完全是虚假宣称！Fake News！ 而且如上面所提到的，在我们热更新二进制这样的场景下，dumb-init 在进程退出后直接自杀。和不使用一号进程完全没有差别！ 我们可以来测试一下，测试代码 demo3.py 12345import osimport timepid = os.fork()time.sleep(1000) fork 一个进程，总共两个进程 Dockerfile 如下 12345678910FROM python:3.9RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.5/dumb-init_1.2.5_x86_64RUN chmod +x /usr/local/bin/dumb-initADD demo3.py /root/demo3.pyENTRYPOINT [&quot;/usr/local/bin/dumb-init&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo3.py&quot;] 构建，执行，先看看进程结构 然后模拟老进程退出，我们直接 SIGKILL 掉 2134836，然后我们看看 2134837 的 strace 的结果 如预期一样，在 dumb-init 自杀后，2134837 被内核 SIGKILL 了 所以跟我复习一遍 dumb-init 拉跨！好了，我们接着聊 tini 的实现 态度友好的聊聊 tini平心而论，tini 的实现，虽然也还有坑，但是比 dumb-init 细腻到不知道哪里去了，我们直接来先看下代码 12345678910111213141516while (1) { /* Wait for one signal, and forward it */ if (wait_and_forward_signal(&amp;parent_sigset, child_pid)) { return 1; } /* Now, reap zombies */ if (reap_zombies(child_pid, &amp;child_exitcode)) { return 1; } if (child_exitcode != -1) { PRINT_TRACE(&quot;Exiting: child has exited&quot;); return child_exitcode; }} 首先 tini 没有设置 signal handler ，不断循环 wait_and_forward_signal 和 reap_zombies 这两个函数 12345678910111213141516171819202122232425262728293031323334353637383940int wait_and_forward_signal(sigset_t const* const parent_sigset_ptr, pid_t const child_pid) { siginfo_t sig; if (sigtimedwait(parent_sigset_ptr, &amp;sig, &amp;ts) == -1) { switch (errno) { case EAGAIN: break; case EINTR: break; default: PRINT_FATAL(&quot;Unexpected error in sigtimedwait: '%s'&quot;, strerror(errno)); return 1; } } else { /* There is a signal to handle here */ switch (sig.si_signo) { case SIGCHLD: /* Special-cased, as we don't forward SIGCHLD. Instead, we'll * fallthrough to reaping processes. */ PRINT_DEBUG(&quot;Received SIGCHLD&quot;); break; default: PRINT_DEBUG(&quot;Passing signal: '%s'&quot;, strsignal(sig.si_signo)); /* Forward anything else */ if (kill(kill_process_group ? -child_pid : child_pid, sig.si_signo)) { if (errno == ESRCH) { PRINT_WARNING(&quot;Child was dead when forwarding signal&quot;); } else { PRINT_FATAL(&quot;Unexpected error when forwarding signal: '%s'&quot;, strerror(errno)); return 1; } } break; } } return 0;} 用 sigtimedwait 这个函数来接收信号，然后过滤掉 SIGCHLD 转发。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364int reap_zombies(const pid_t child_pid, int* const child_exitcode_ptr) { pid_t current_pid; int current_status; while (1) { current_pid = waitpid(-1, &amp;current_status, WNOHANG); switch (current_pid) { case -1: if (errno == ECHILD) { PRINT_TRACE(&quot;No child to wait&quot;); break; } PRINT_FATAL(&quot;Error while waiting for pids: '%s'&quot;, strerror(errno)); return 1; case 0: PRINT_TRACE(&quot;No child to reap&quot;); break; default: /* A child was reaped. Check whether it's the main one. If it is, then * set the exit_code, which will cause us to exit once we've reaped everyone else. */ PRINT_DEBUG(&quot;Reaped child with pid: '%i'&quot;, current_pid); if (current_pid == child_pid) { if (WIFEXITED(current_status)) { /* Our process exited normally. */ PRINT_INFO(&quot;Main child exited normally (with status '%i')&quot;, WEXITSTATUS(current_status)); *child_exitcode_ptr = WEXITSTATUS(current_status); } else if (WIFSIGNALED(current_status)) { /* Our process was terminated. Emulate what sh / bash * would do, which is to return 128 + signal number. */ PRINT_INFO(&quot;Main child exited with signal (with signal '%s')&quot;, strsignal(WTERMSIG(current_status))); *child_exitcode_ptr = 128 + WTERMSIG(current_status); } else { PRINT_FATAL(&quot;Main child exited for unknown reason&quot;); return 1; } // Be safe, ensure the status code is indeed between 0 and 255. *child_exitcode_ptr = *child_exitcode_ptr % (STATUS_MAX - STATUS_MIN + 1); // If this exitcode was remapped, then set it to 0. INT32_BITFIELD_CHECK_BOUNDS(expect_status, *child_exitcode_ptr); if (INT32_BITFIELD_TEST(expect_status, *child_exitcode_ptr)) { *child_exitcode_ptr = 0; } } else if (warn_on_reap &gt; 0) { PRINT_WARNING(&quot;Reaped zombie process with pid=%i&quot;, current_pid); } // Check if other childs have been reaped. continue; } /* If we make it here, that's because we did not continue in the switch case. */ break; } return 0;} 然后在 reap_zombies 函数中，不断利用 waitpid 这个函数来处理进程，在没有子进程等待处理或者遇到其余系统错误时退出循环。 注意这里 tini 和 dumb-init 的的实现差异，dumb-init 在回收自己的入口子进程后便会自杀。而 tini 将会在所有自己的子进程退出之后，结束循环，然后判断是否自杀。 那么我们这里来测试一下 还是 demo2 的例子，我们来测试一下孙进程的例子 123456789FROM python:3.9ADD demo2.py /root/demo2.pyENV TINI_VERSION v0.19.0ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tiniRUN chmod +x /tiniENTRYPOINT [ &quot;/tini&quot;,&quot;-s&quot;, &quot;-g&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo2.py&quot;] 然后构建，执行，进程结构如下 然后，老规矩，strace , kill 发 SIGTERM 看一下， 嗯，如预期一样，那么 tini 的实现是不是没有问题了呢，我们再来准备一个例子,demo4.py 12345678import osimport timeimport signalpid = os.fork()if pid == 0: signal.signal(15, lambda _, __: time.sleep(1)) cpid = os.fork()time.sleep(1000) 这里我们用 time.sleep(1) 来模拟，程序接到 SIGTERM 后需要优雅处理，然后我们还是准备下 dockefile 123456789FROM python:3.9ADD demo4.py /root/demo4.pyENV TINI_VERSION v0.19.0ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tiniRUN chmod +x /tiniENTRYPOINT [ &quot;/tini&quot;,&quot;-s&quot;, &quot;-g&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo4.py&quot;] 然后构建，允许，看进程结构，啪的一下很快啊 然后 strace ，发 SIGTERM 一条龙服务， 然后我们发现，2173316 和 2173317 这两个进程，成功接收到 SIGTERM 的信号后，在处理中，被 SIGKILL 了。那么这是为甚么呢？实际上这里也存在一个潜在的 race condition 当我们开启 tini 的使用。2173315 退出后，2173316 将被 re-parent ， 按照内核的 re-parent 流程，2173317 re-parent 到 tini 进程。 但是，tini 在使用 waitpid 的时候，使用了 WNOHANG 这个选项，那么这里如果在执行 waitpid 时，子进程还未结束，那么将立刻返回0。从而退出循环，开始自杀流程。 刺激不刺激，关于这点，我师父和我提了一个 issue: tini Exits Too Early Leading to Graceful Termination Failure 然后，我也做了一版修复，具体可以参考use new threading to run waipid（还在 PoC，没写单测，处理也有点糙） 实际上思路很简单 ，我们不使用 waitpid 中的 WNOHANG 选项，将其变为阻塞的调用，然后用一个新的线程来做 waitpid 的处理 构建一版测试效果如下 嗯，如预期一样，测试没有问题。 当然这里实际上可能细心的朋友发现，原本的 tini 也没法处理二进制更新的情况，原因和 demo5 里的原因一致。这里大家可以去测试一下 实际上这里我的处理很过于粗糙和暴力，我们实际上只要保证让 tini 的退出条件变成一定要等到 waitpid()=-1 &amp;&amp; errno==EHILD再退出。具体的实现手段大家可以一起来思考（实际上还不少 最后来总结一下问题的核心： 无论是 dumb-init 还是 tini 在现行的实现里，都犯了同一个错误，即在容器这个特殊的场景下，都没有等待所有子孙进程的退出再退出。其实解决方案很简单，退出条件一定要是 waitpid()=-1 &amp;&amp; errno==EHILD 总结本文吐槽了 dumb-init 和 tini。dumb—init 实现属实拉跨，tini 的实现细腻了很多。但是 tini 依旧存在不可靠的行为，以及我们所期待的 fork 二进制更新这种使用一号进程的场景在 dumb-init 和 tini 上都没法实现。而且 dumb-init 和 tini 目前也还有一个共通的局限性。即无法处理子进程进程组逃逸的情况。（比如十个子进程各自逃逸到一个进程组中）。 而且在文中的测试中，我们用 time.sleep(1) 来模拟 Graceful Shutdown 的行为，tini 也已经无法满足需求了。。So。。。。 所以归根到底一句话，应用的信号，进程回收这些基础行为应该应用自决。任何管杀不管埋而寄托于一号进程的行为，都是对于生产的不负责任。（如果你们实在想要一个一号进程，还是用 tini 吧，千万别用 dumb-init) 所以 exec 裸起大法好，不用一号进程平安保！ 差不多水文就这样吧，这篇水文从提出问题到验证结论，到 patch PoC 报销了我快一个星期的业余时间（本文初稿在凌晨4点过写完）。最后感谢某川同学和我一起搞了几个凌晨三点过。最后，祝大家看的愉快。","link":"/posts/2021/02/27/damn-the-init-process/"},{"title":"但行好事，莫问前程","text":"这篇文章，实际上本应该在 2020 年结束时写完的。不过我是一个拖延症患者，而且写这篇文章时会想起一些已经离去的人，所以一直不愿动笔，拖到了现在。不过在农历新年的末尾，还是得写出来，给自己去过一年一个总结吧。 开篇怎么说呢，2020其实是很操蛋的一年，我无数次的在内心问候”2020，我日你仙人“（山本大佐表示很赞） 但是2020呢，其实又是挺好的一年，让我估计后面会无数次的从2020这一年里学到的东西中受益。 所以吧，我把这暂且称为薛定谔2020？（算了，这 TM 就是 Manjusaka 的2020 工作大年三十了，先说点难过的吧，熟悉我的朋友都知道，我其实2020的职业生涯从职级的角度上来说可以用职场败犬来形容 Yep，没错，我被降级了，而且晋升还他娘挂了。你说我气不气 你要说我心态没有炸实际上是不可能的，在19年调转时刚得知降级结果那段时间心态简直爆炸。当时我老板说了一句印象非常深刻的话”你怎么焦虑成这样了，我记得你当时刚进来时不是这样的啊？“ 在心里接受降级后，开始继续努力搬砖争取晋升回来后，然后，晋升又挂了。我\b？？？？？？ 不过说回来，其实这段经历，可能在刚刚发生那一会儿乃至后续的一段时间，让我一度焦虑狂躁。但是从我现在的视角去看，其实也能反思很多东西。 实际上经历过这一段很特殊的经历后，我自己的心态已经算是相对平和，某种意义上比刚去阿里云时来得更为沉得住气。如同前段时间我给老板说过的一样，现在的我更希望去做一些自己认可的事，而不是纯粹的为了半年后的晋升去做什么事。往往机关算尽，到头却功亏一篑。不值当 嘛，当时在调转之初，我老板送我的一句话，我觉得非常受用： 你现在所经历的事情在当下你看来可能无法接受，但是如果将时间放大到整个一生这几十年的尺度来看，其实可能也不算什么大事了。 \b抛开职级不谈，说实话在阿里云一年多时间里，虽然也经历了客服支持，连续加班等非常惨痛的经历。不过就我而言，可以说还是蛮享受以及这段时间的工作。 在云这边我主要的工作是和同事一起从0开始做了一款公有云上的网关产品。在去年组织结构变动后，又跑来做监控服务了。 在这么样的一个过程中，我的角色从在饿了么时期的一个纯粹的一个对内的 infra RD 的角色切换成了一个云产品的研发，我觉得这样一个角色的转变其实是能让我从一个不同的视角去审视 infra 这件事。如果说在对内时期，你做的东西可以通过一些非技术上的手段去强行 push 落地的话。那么在做云产品的时候，如果你产品无法提供足够 OK 的成本，特性，是不会有客户给你产品买单的（当然，你爸要是客户老板当我没说）。 这样一个观点的转变，让我更能从客户的视角去系统的思考我们做的事的意义。而通过客户支持的经历让我能贴近客户真实的使用场景与业务。我觉得这样一段经历对于我来说是非常重要的。 另外一个方面是我在云这段时间，有足够的场景和驱动力，去对内核，eBPF ，SystemTap 等一些相对冷门和深入的技术进行探究。我觉得也是非常不错的。 不过提到工作我就没法逃避的一个事实（也是我迟迟不想写这篇文章的原因）就是，2020，我失去的我最早期刚进入职场时的领路人，也是我最好的朋友。如果说，我师父在我职业早期送我最重要的礼物是说”学习任何一个东西需要系统化“，那么他送给我的礼物就是正确的职业态度，正确的提问方式等很多很细碎，但是足够让我受益终生的东西。So，RIP &amp;&amp; 2020 我日你仙人*2。 感情和荆澈同学的感情步入了第三年，依旧非常甜蜜。荆澈同学一如既往的照顾我，比如我身上现在的行头都是荆澈同学给我买的，而此文由荆澈同学给我买的 HHKB Hybrid 写出！ 而且我们第一次一起去拍了美美的情侣照！有一说一，情侣照成片拿到手后，我最喜欢干的一件事就是换微信头像Hhhh（逃 不过说实话，很多时候觉得我自己还不算是一个合格的男朋友，很多时候小事和小细节非常不注意（= =我也很绝望）会无意间让她非常不开心，用她吐槽我的话来说，就是”败兴大王“，而且之前自己作死导致19年身体一直时好时坏，也让荆澈同学操了不少心。 所以，有些时候我也在想，荆澈同学没把我扔出去也是神奇Hhhh。不过我自己也定了很多目标，会一点点的改掉自己很多不好的习惯！不过这里突然想引用一下每年给荆澈同学一封信中我经常说的一句话 亲爱的，我感激并享受着你的爱 啊，对了，在年初的时候，和荆澈同学一起，列了不少今年要一起做的事，希望能一起好好的走下去。（再次表白荆澈同学！） 技术嘛，实际上去年年初的时候，因为各种原因没有给自己立下什么 flag，所以大概聊聊自己过去一年多做的一些事吧。 读书方面的话，我自己比较印象深刻的有这样几本 Design Data Intensive Application Kubernetes in Action BPF Performance Tools 然后因为各种原因，复习/新读了几篇论文，比如印象比较深刻的 Cloud Programming Simplified A Berkeley View on Serverless Computing, Maglev A Fast and Reliable Software Network Load Balancer。 社区方面的话，今年一如既往的参与了 PyCon China 2021 的筹办，参与了两次 Meetup 分享。收获了微软的 MVP ，算是完成了一个软粉的夙愿。 整体来看吧，可能输出没有之前多，而且也有点偷懒了，具体原因后面会说。，不过可能还算是一个合格的答卷。希望21年能够继续在技术上勇猛精进吧 生活嘛，实际上生活上这一年多的变化还是蛮大的，首先正儿八经的讲，我没有穿迷彩了！（实际上是荆澈同学看不下去了包办了我全身的行头！） 在年初的时候，家里加入了一个新的成员，一只“憨厚“”老实“”稳重“的红虎斑缅因猫年年，这样一来，我们家就有了四只猫！年年说实话给我们的生活带来了很多的乐趣（除了踩在我肚子上的时候） 而过去一年，在生活上比较大的一个变化是，我第一次意识到了，除了技术和睡觉，生活也很重要。开始时不时的和亲爱的荆澈同学一起打打游戏（一起动森，一起分手厨房），一起看看剧，逛逛淘宝啥的。说实话，非常幸福，这也是我之前从未有过的体验。Hhhhhh 另外一方面，2020年，我做了一件对我来说可能是比较重要的一个事，在17年确诊抑郁后，当时分析成因可能是因为来源于自己小时候被性侵后的 PTSD。所以我选择将我自己的经历不断以文字的形式公开出来。而去年，我选择参与进华师的一个研究，我面向研究者，完整的，深入的直面了当时我被性侵的细节，心里状况，反思及建议 这件事实际上，对于我来说意义也非常重要。我希望能通过自己和其余人的努力，能让国内被性侵儿童心里干涉相关的研究能够进一步发展。这里非常非常谢谢亲爱的荆澈同学对我的支持，几次因为小时候的事极度丧的时候，都是荆澈同学一直在给我拥抱并安慰。而且荆澈同学也非常支持并鼓励我讲自己的经历分享出来。同时我们决定，将自己参加研究所得到的相关收入捐赠给公益基金会。 但行好事，莫问前程嗯，差不多流水帐就这样一些，说实话，2020年，对于我来说其实是很特殊的一年，有过哭泣，有过坚强，想过放弃，但是又继续走了下去 这一年，或者说过去四年，能跌跌撞撞的走到现在，有很多的人想感谢，亲爱的荆澈同学，师父，几位密友，捕蛇者说的搭档，历任的 Leader，推上认识的朋友。感谢你们一路相伴，带我看了更大的世界。然后，其实我想第一次在这里谢谢自己，谢谢自己在无数次想过一了百了之后，还是坚持到了今天。 前两天，我看着我招行账单，上面显示说，我自己的收入比19年高了不少，当时心里百味杂陈，给亲爱的说“我们日子会越过越好的” 嗯，Everything is gonna be OK 如果说四年前，在17年的总结中，我送给自己的话是 “诗酒趁年华”。那么在经历过挫折，反思，成长，可能不算丰富，但是对我来说却意义重大的四年后。我选择送给自己的话，也是这篇记录的标题 但行好事，莫问前程","link":"/posts/2021/02/11/do-anything-you-want/"},{"title":"写在一周年","text":"说实话，突然体会到了老祖宗说的『光阴似箭，日月如梭』的感觉了，感觉告白还在昨天，但是转眼就一年了。所以来写篇文章纪念这短短却有很丰富的一年吧 起初说实话，我现在都没搞清楚是她撩的我，还是我撩的她，估计暂时也不会搞清楚了？ 在去年的11月7号凌晨，当时聊着双十一，然后她突然说，“要是双十一能脱单就好了”，然后我直接没过大脑的来了一句 ”我保佑你啊“，然后你能以肉眼可见的速度感觉到屏幕对面的人不开心了 然后我就纳闷了，这女生咋这么难以琢磨呢？咋这就生气了呢？我说错啥了我？等等，她不会是喜欢我吧？我这么一个沙雕程序员咋会被她看上呢？喵？然后在我反复逼问下她承认了，然后就顺其自然的在一起了 后续她吐槽我： 你咋能拷问一个女孩子喜不喜欢你呢？ 当时你说”我保佑你啊“，我第一反应，这男的白撩了 相处她是一个很棒的钢琴专业的学生，非常棒，而且堪称学霸（偶然间知道她当年高考的时候即便缺席了半年文化课，高考成绩也超一本线50多分，当时就给跪了），而我是一个屌丝学渣程序员，按道理说我们两个人是毫无交集的，所以可能很多人想问，你们两是怎么相处的？ 嘛，首先，我日常被嫌弃这是必然的，然后她有时会傻傻的，然后我也会吐槽她 讲两个故事 某天我给她讲，亲爱的你知道么？我们金牛座超屌的，她惊了：哟，你还信星座的啊？然后我兴致勃勃的给她说，你知道么，人类历史上最早有记录的超新星爆发之一就在金牛座（SN 1054，公元1054年），她无语了十分钟 某天晚上，我哄她去睡觉，我说，亲爱的，快睡了，现在快十二点了，她当时直接来了句”现在日本才11点“，我当时愣了十分钟（喵？？ 好了，其实开个玩笑啦。虽然看起来是两个世界的人，但是我们彼此都还是有不少共同点，同时我们日常也会刻意的去靠近对方的习惯。比如我们会一起去看音乐剧（音乐剧看多了的结果就是钱包抗不住），然后一起吐槽比如法扎那场演员不咋样，比如堂吉诃德的演员有啥瑕疵，当然我也会日常给她普及某些电子产品有多好用，然后给她安利哪些软件比较好用（当然她日常吐槽我：你这个之前就给我说过啦白痴！）（然后我成功把她带入了 RSS 订阅的坑） 当然，要问她为什么能看上我，唯一的理由：她眼瞎（逃 日常其实作为普通人的我们，日常其实也还是如同大部分普通人一样，一部分是温暖与惊喜，一部分是嫌弃与包容吧？ 我自己来说，作为一个程序员，特别是在这两年经历了很多重大的改变，也经历不少的关键时候，同时因为我自己职业的关系，虽不至于996，但是每天清早出门，晚上回家，一天下来精疲力竭也是常态，而且说实话我自己的身体一直欠佳，去年到今年，大大小小的毛病也一堆，在这样一个时候，有一个人陪在你的身边，我觉得这对于我来说意义重大 我之前给她说过，她给我带来最直接改变就是让我对于这个城市有了归属感，每天早晨出门的时候有一个抱抱，晚上回家的时候也有一个抱抱，这样一个颇具有仪式感的行为让我对北京这个城市有了难得归属感。让我曾经在不同的城市之间飘荡的数字游民式的想法彻底消失。 而且她也是一个擅长个给我小惊喜和有着生活情趣的人，无论是在入职之前给我准备的一束花，还是有些时候在家里养的小花，满天星，茉莉，让你觉得家里有了生机，而有时你觉得很你难过的时候，她也会duang的一下跳出来，带着你最爱吃得东西来犒劳你，让你继续安心前行 当然一段感情中，除了温暖与惊喜，就是不断的嫌弃与包容吧 比如我因为工作繁忙+自己实在是懒，日常会把家里搞的很乱，然后她一遍嘟囔着我要和你分手，一遍收拾家里。然后之前她调雪球酒，然后我贪杯多喝了几杯在床上装死狗的时候，她一边嫌弃的骂我是傻狗，一遍隔一会儿来确认下我是否有事 嘛，所以日子，等等，我突然忘了说了，其实我们两人的日常还有一个很重要的组成部分！那就是虐狗！ 啊，每当我在技术群里有意/无意的虐狗后，她总是很担(xin)心(xi)的问，你这样会不会被打啊！我说会，然后她说噢，那我就放心了 233333！ 宠物嘛，宠物是我们日常中很重要的一部分，所以我就开个单节来聊聊 首先来介绍下家里四只猫 小舅子大雄，现居于山东，喜欢唱、抓、rap、捞鱼，目前在背《沁园春.雪》 懂事的大闺女，肚兜，前流浪猫，极为懂事，在她/我身体不舒服的时候，会过去看着，也导致她经常吐槽：”你还没肚兜爱我！“ 叛逆期的外孙女，秋千，颜值波动极大，有时喜欢捣乱 我们当时因为这三只猫结实，我记得我们俩最开始的话题是如何给大雄做绝育，借着给大雄做绝育的契机，我们俩加深了感情，加深了对于彼此的认识，啊，伟大的大雄同志，万岁！ 然后突然想起关于秋千还有一个故事，去年10月，当时做保洁。秋千因为害怕躲在了家里一个未知的角落，回家的我一度以为秋千丢了，哭着满小区找秋千，亲爱的一直在安慰，别哭，别哭，我来陪你找 嗯，这三只猫，某种意义上是我们俩的媒人。嗯，纪念日开罐头 最后一只小猫，对我们来讲，是第一只一起救下来的小猫，嗯在儿童节那天，亲爱的说，这是我收到的最好的儿童节礼物 曾经的点点 现在的点点 睡死了的点点 嘛，这四只猫，分别出现在不同的时候，对于我们有着不一样的意义，当然家里养猫的日常是，当猫偷吃/打碎化妆品后，亲爱的总会对着我抱怨：”看你养的好猫！你赔我”，欲哭无泪.jpg 结束在过去这一年里，很幸运的是虽然大大小小的争端不断，没有吵架。一起养猫，一起看书，一起做陶艺，然后一起走到纪念日。在这一年里面，我们也一起立下了很多的 Flag 和对未来的希望，比如一起去加拿大读书，一起工作赚钱买 Dream House，一起去日本京都 Gap Year，等等。 虽然这句话说过很多次，但是我是还想再说一次：“亲爱的，我感激并享受着你的爱” 爱你亲爱的，一周年快乐！ 啊对了，很多人都会问，你文中的她是谁呀，再次介绍下，她叫荆澈（也是我的花名），山东人，这里两位荆澈一起，在这里向大家问好啦！","link":"/posts/2019/11/04/for-one-year-anniversary/"},{"title":"云原生时代的几个爆论","text":"从去年调转到现在，做了一段时间的云原生，我突发奇想，想发表几个爆论来论述下我眼中的云原生来作为今年最后一篇技术博客。本文纯属个人向吐槽，与本人公司立场无关 概述云原生大概在 2014-2015 年开始左右，开始正式的提出了这个概念。2015 年 Google 主导成立了云原生计算基金会（Cloud Native Computing Foundation aka CNCF)。在 2018 年，CNCF 在 CNCF Cloud Native Definition v1.01 首次对云原生的概念有了一个认定 Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil. 其中文翻译如下： 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 从官方的定义来看，我更愿意将其称为一个愿景(vision/landscape)而不是一个定义(definition)，因为在上述的表达中，并没有清晰明确的表述出云原生这一新生概念的具体的范围与边界，也没有阐述清楚 Cloud Native 和 Non-Cloud Native 之间的差异。 如果以个人的视角来看，一个云原生应用具备以下特质 容器化 服务化 而一个践行云原生的组织，那么应该具备以下特质 重度 Kubernetes 或其余容器调度平台（如 Shopee 自研的 eru22 具备完整的监控体系 具备完整的 CI/CD 体系 在这个基础上，最近看到很多人都在讨论云原生这一新生概念，所以我想在这里聊聊个人向的四个爆论（爆论中的数据是个人主观判断，轻喷） 百分之95以上的公司，没有完成 CI/CD 体系的建立。也没有完成线上服务进程的收敛 百分之90以上的公司，没有能微服务化的技术储备 百分之90以上的公司，没有能撑起容器化的技术储备 开始爆论1. 百分之95以上的公司，没有完成 CI/CD 体系的建立。也没有完成线上服务进程的收敛CI 指持续集成（Continuous Integration aka CI），而 CD 指持续交付(Continuous Delivery aka CD)，通常来讲 CI 与 CD 的定义如下（此处引用 Brent Laster 在 What is CI/CD?3 中给出的定义 Continuous integration (CI) is the process of automatically detecting, pulling, building, and (in most cases) doing unit testing as source code is changed for a product. CI is the activity that starts the pipeline (although certain pre-validations—often called “pre-flight checks”—are sometimes incorporated ahead of CI).The goal of CI is to quickly make sure a new change from a developer is “good” and suitable for further use in the code base.Continuous deployment (CD) refers to the idea of being able to automatically take a release of code that has come out of the CD pipeline and make it available for end users. Depending on the way the code is “installed” by users, that may mean automatically deploying something in a cloud, making an update available (such as for an app on a phone), updating a website, or simply updating the list of available releases. 通常在我们的实践中，CI 和 CD 的边界并不明显。以常见的基于 Jenkins 的实践为例，我们通常的一套路径是 创建一个 Jenkins 的项目，设定一个 Pipeline（其中包含代码拉取，构建，单元测试等 task），设置触发条件 当指定代码仓库存在主分支代码合入等操作时，执行 Pipeline ，然后生成产物 在生成产物后的，常见有两种做法 在生成产物的下一个阶段触发自动的 deploy 流程，按照 deploy script 直接将生成的产物/镜像直接部署到目标服务器上 将生成的产物上传到中间平台，由人通过部署平台手动触发部署任务 在上面描述的过程中，如果有着完备的流程的公司还会有着其余的辅助流程（如 PR/MR 时的 CI 流程，CR 流程等） 而在面对目标平台的部署时，我自己的另外一个观点是大部分的公司没有完成线上服务进程的收敛。讲个笑话： Q: 你们怎么部署线上服务呀？A；nohup，tmux，screen 对于当下而言，一个规范化的 CI/CD 流程，收口的线上的服务进程的管理，至少在当下，有着可以遇见的几个好处 尽可能的降低人为手动变更带来的风险 能够较好的完成基础运行依赖配置的收口 依托目前主流的开源的 systemd, supervisor, pm2 等进程管理工具，能对进程提供基础的 HA 的保证（包括进程探活，进程重拉等） 为后续的服务化，容器化等步骤打下基础 2. 百分之90以上的公司，没有能微服务化的技术储备如果说，对于爆论1 提到的 CI/CD 等手段，我更多的觉得这是一个制度障碍大于技术障碍的现实。那么接下来的几个爆论，我更愿意用没有技术储备来形容 先来说说爆论2: 百分之90以上的公司，没有能微服务化的技术储备 首先来聊聊微服务的概念吧，微服务实际上在计算机历史上有着不同的论述，在2014年 Martin Fowler 和 James Lewis 正式在 Microservices a definition of this new architectural term4 一文中正式的提出了微服务（Microservice）这一概念。此处引用维基百科的一段概述 微服务是由以单一应用程序构成的小服务，自己拥有自己的行程与轻量化处理，服务依业务功能设计，以全自动的方式部署，与其他服务使用HTTP API通信。同时服务会使用最小的规模的集中管理 (例如 Docker) 能力，服务可以用不同的编程语言与数据库等组件实现 那么我们来用研发的话来尝试描述下关于微服务和与之对应的传统单体服务（Monolith） 之间显著性的差异 微服务的 scope 更小，其更多的专注在某一个功能，或者某一类的功能上 由于其 scope 更小的特性，其变更，crash 所带来的影响相较于传统的单体来说更小 对于多语言多技术栈团队来说更为友好 ”符合“现在互联网所需求的小步快跑，快速迭代的大目标 那么我们这里需要思考一下，微服务这一套体系，如果我们想要去进行落地和实践，那么我们需要怎么样的技术储备？我觉得主要是两个方面，架构和治理 首先来聊聊架构吧，我觉得对于微服务来说，最麻烦的一个问题在于从传统单体应用上进行拆分（当然要是最开始创始之初就开始搞微服务的当我没说，虽然这样也有其余的问题） 如前面所说，微服务相较于传统的单体应用来说，，其 scope 更小，更专注在某一个功能或者某一类的功能上。那么这里所引申出来我觉得做微服务最大的一个问题在于合理的划分功能边界并进行拆分 如果拆分不合理那么将导致服务之间相互耦合，比如我将用户鉴权放置在商城服务中，导致我论坛服务需要依赖其不需要的商城服务。如果拆分的过细，那么将导致出现一个很有趣的现象，一个规模不大的业务拆了100多个服务 repo 出来（我们把这种情况称为： 微服务难民2333） 我们践行落地微服务这一套理念，是因为我们在业务和团队规模扩大后，面对多样化的需求与团队成员技术栈时，传统单体应用在其持续维护上的成本将会是一个不小的开支。我们希望引入微服务来尽可能减少维护成本，降低风险。但是不合理的拆分，将会重新让我们的维护成本远超继续践行单体化的方案 而我觉得阻碍微服务继续践行的另外一个问题是治理问题。我们来看一下在微服务化后我们所面临的几个问题 可观测性的问题。如前面所说，微服务化后的单个服务 scope 更小，更多的专注在某一个功能或者某一类功能上。那么这可能导致的问题是，我们在完成一个业务请求所需要经历的请求链路更长。那么按照通用的观点来看，链路更长，其风险更大。那么在在当服务存在异常时（比如业务 RT 的突然增高）我们怎么样去定位具体服务的问题？ 配置框架的收口。在微服务化的场景中，我们可能会选择将一些基础的功能下沉至具体的内部框架中（如服务注册，发现，路由等），那么意味着我们需要维护自己的框架，同时完成配置的收敛 老生常谈的服务治理（注册、发现、熔断）等 由于微服务化后，对于一个完备 CI/CD 机制的需求将变得更为迫切。那么如果存在爆论1的情况，将会成为践行微服务这一理念的障碍 诚然，目前无论开源社区（如 Spring Cloud，Go—Micro 等）还是四大云厂商（AWS，Azure，阿里云，GCP）都在尝试提供一种开箱即用的微服务方案，但是除了没法很好的解决如上面所说的诸如架构这样的问题外，其也存在自己的问题 无论是依赖开源社区的方案，还是云厂商的方案，都需要使用者具备一定的技术素养，来定位特定情况下框架中的问题 Vendor Lock-in，目前开箱即用的微服务方案并没有一个通用的开源事实标准。那么依赖某一个开源社区或者云厂商的方案将存在 vendor lock-in 的问题 无论是开源社区的方案还是云厂商的方案，都存在多语言不友好的问题（大家貌似现在都喜欢 Java 一点（Python 没人权.jpg 所以爆论2想表明的一个最核心的观点就是：微服务化并不是一个无代价的行为，与之相反的是一个需要不低技术储备与人力投入的的行为。所以请不要认为微服务是万能良药。请按需使用 3. 百分之90以上的公司，没有能撑起容器化的技术储备目前很主流的一个观点，是能上容器尽可能上容器，说实话这个想法实际上是有一定的合理性的，去 review 这个想法，我们需要去看一下容器这个东西，给我们带来了什么样的改变 容器首先毫无疑问，会给我们带来非常多的好处： 真正意义上让开发与生产环境保持一致是一种非常方便的事，换句话说，开发说的“这个服务在我本地没啥问题”是一句有用的话了 让部署一些服务变的更为方便，无论是分发，还是部署， 能做到一定程度上的资源隔离与分配 那么，看起来我们是不是可以无脑用容器？不，不是，我们需要再来 Review 一下，容器化后我们可能所要面临的一些弊端： 容器安全性问题，目前最主流的容器实现（此处点名 Docker）本质上而言还是基于 CGroups + NS 来进行资源与进程隔离。那么其安全性将会是一个非常值得考量的问题。毕竟 Docker 越权与逃逸漏洞年年有，年年新。那么这意味着我们是需要去有一个系统的机制去规范我们容器的使用，来保证相关的越权点能被把控在一个可控的范围内。而另一个方向是镜像安全问题，大家都是面向百度/CSDN/Google/Stackoverflow 编(fu)程(zhi)选手，那么势必会出现一个情况，当我们遇到一个问题，搜索一番，直接复制点 Dockerfile 下来，这个时候，将会存在很大的风险点，毕竟谁也不知道 base image 里加了啥料不是？ 容器的网络问题。当我们启动若干个镜像后，那么容器之间的网络互通怎么处理？而大家生产环境，肯定不止一个机器那么少，那么跨主机的情况下，怎么样去进行容器间的通信，同时保证网络的稳定性？ 容器的调度与运维的问题，当我一个机器高负载的时候，怎么样去将该机器上的一些容器调度到其余的机器上？而怎么样去探知一个容器是否存活？如果一个容器 crash 了，怎么样重新拉起？ 容器具体的细节问题，比如镜像怎么样构建与打包？怎么样上传？（又回到了爆论1）乃至说怎么样去排查一些 corner case 的问题？ 对于一些特定的 large size 的镜像（如机器学习同学常用的 CUDA 官方镜像，打包了字典模型等大量数据的镜像等）怎么样去快速下载，快速发布？ 可能这里又会有一种观点，没事，我们上 Kubernetes 就好啦，上面这些很多问题就能解决啦！好吧，我们再来聊聊这个问题 首先我已经忽略掉自建 Kubernetes 集群的场景了，因为那不是一般人能 Hold 住的。那么我们来看一下，依托公有云使用的情况吧，以阿里云为例，点开页面，然后我们见到这样张图 好了，提问： VPC 是什么？ Kubernetes 1.16.9 和 1.14.8 有什么区别 Docker 19.03.5 和阿里云安全沙箱 1.1.0 是什么，有什么区别 专有网络是什么？ 虚拟交换机是什么？ 网络插件是什么？Flannel 和 Terway 又是什么？有什么区别？当你翻了翻文档，然后文档告诉你，Terway 是阿里云基于 Calico 魔改的 CNI 插件。那么 CNI 插件是什么？Calico 是什么？ Pod CIDR 是什么怎么设？ Service CIDR 是什么怎么设？ SNAT 是什么怎么设？ 安全组怎么配置？ Kube-Proxy 是什么？iptables 和 IPVS 有什么区别？怎么选？ 大家能看到上面的问题涵盖了这样几方面 Kubernetes 本身的深入了解（CNI，runtime，kube-proxy 等） 一个合理网络规划 对于云厂商特定功能的熟悉 在我看来，这三方面任何一方面对于一个技术团队的技术储备以及对于业务的理解（广义的技术储备）都需要有一个不浅的需求。 当然这里在碎碎念一下，实际上搞 Kubernetes 这一套开销实际上很大的（有点偏题，但是还是继续说吧） 你得有个镜像仓库吧，不贵，中国区基础版780一个月 你集群内的服务需要暴露出去用吧？行叭，买个最低规格的 SLB，简约型，每个月200 好了，你每个月日志得花钱吧？假设你每个月20G日志，不多吧？行，39.1 你集群监控要不要？好，买，每天50w条日志上报吧？行，不贵，975 一个月 算一下，一个集群吧，(780+200+39.1+975)*12=23292.2一年，不算集群基础的 ENI，ECS 等费用，美滋滋 而且 Kubernetes 会有很多的玄学的问题，也需要技术团队有足够的技术储备来进行排查（我想想啊，我遇到过 CNI 一号进程 crash 了没重拉，特定版本上的内核 cgroup 泄漏，ingress OOM 等问题），大家可以去 Kubernetes 的 Issue 区看一下盛况（说多了都是泪） 总结我知道这篇文章写出来会存在很多的争议。但是我始终想表述的一个观点是对于云原生时代这一套东西（实际上也更多是之前传统技术的延伸），他们的引入并不是无代价，并不是无成本的。对于有着足够规模与痛点的公司来说，这样的成本对于他们的业务增长来说是一个正向的促进，而对于更多中小企业来说，可能这一套对于业务的提升将会是非常小乃至说是负作用。 我希望我们技术人员在做技术决策的时候，一定是在评估自己的团队的技术储备乃至对于业务的收益后再引入某一种技术与理念，而不是引入一个技术只是因为它看起来够先进，够屌，能够为我的简历背书 最后用之前我分享过的一句话来作为本文的结尾吧 一个企业奔着技术先进性去搞技术，就是死 Reference CNCF Cloud Native Definition v1.0 projecteru2 What is CI/CD? Microservices a definition of this new architectural term","link":"/posts/2020/12/31/fuck-the-cloud-native/"},{"title":"利用动态 tracing 技术来 trace 内核中的网络请求","text":"这周帮朋友用 eBPF/SystemTap 这样的动态 tracing 工具做了一些很有趣的功能。这篇文章算是一个总结 开篇实际上这周的一些想法，最开始是实际上来源于某天一个朋友问我的一个问题 我们能不能监控机器上哪些进程在发出 ICMP 请求？需要拿到 PID，ICMP 包出口地址，目标地址，进程启动命令 很有趣的问题。实际上首先拿到这个问题时候，我们第一反应肯定是 “让机器上的进程在发 ICMP 包的时候”直接往一个地方写日志不就好了，emmmm，用一个 meme 镇楼吧 嗯，可能大家都知道我想说什么了，我们这种场景实际上只能选择旁路，无侵入的方式去做。 那么涉及到包的旁路的 trace，大家第一反应肯定是 tcpdump 去抓包。但是在我们今天的问题下，tcpdump 只能拿到包信息， 但是拿不到具体的 PID，启动命令等信息。 所以我们可能需要用另外一些方式去实现我们的需求 在需求最开始之初，我们还可能的选择的方式有这样一些 走 /proc/net/tcp 去拿具体的 socket 的 inode 信息，然后反查 pid 关联 eBPF + kprobe 内核打点做监控 SystemTap + kprobe 内核打点做监控 第一种方式，实际上只能拿到 TCP 一层的信息，但是 ICMP 并不是 TCP 协议啊（衰（虽然同属 L4 那么看到最后，我们貌似就只有用 eBPF/SystemTap 配合 kprobe 的一条路可以走了 基础的 traceKprobe在继续下面的代码实际操作之前，我们首先要来认识一下 Kprobe 先引用一段官方文档的介绍 Kprobes enables you to dynamically break into any kernel routine and collect debugging and performance information non-disruptively. You can trap at almost any kernel code address 1, specifying a handler routine to be invoked when the breakpoint is hit.There are currently two types of probes: kprobes, and kretprobes (also called return probes). A kprobe can be inserted on virtually any instruction in the kernel. A return probe fires when a specified function returns.In the typical case, Kprobes-based instrumentation is packaged as a kernel module. The module’s init function installs (“registers”) one or more probes, and the exit function unregisters them. A registration function such as register_kprobe() specifies where the probe is to be inserted and what handler is to be called when the probe is hit. 简单来说，kprobe 是内核的一个提供的一个 trace 机制，在执行我们所设定特定的内核函数时/后，会按照我们所设定的规则触发我们的回调函数。用官方的话来说，“You can trap at almost any kernel code address” 在我们今天的场景下，不管利用 eBPF 还是 SystemTap 都需要依赖 Kprobe 并选择合适的 hook 点来完成我们内核调用的 trace 那么，在我们今天的场景下，我们应该选择在什么函数上加上对应的 hook 呢？ 首先我们来想一下，ICMP 是一个四层的包，最终封装在一个 IP 报文中分发出去，那么我们来看一下，内核中 IP 报文发送中的关键调用，参见下图 在这里我选择将 ip_finish_output 作为我们的 hook 点。 OK，Hook 点确认后，在开始正式编码前，我们来大概介绍下 ip_finish_output ip_finish_output首先来看下这个函数 123456789101112131415static int ip_finish_output(struct net *net, struct sock *sk, struct sk_buff *skb){ int ret; ret = BPF_CGROUP_RUN_PROG_INET_EGRESS(sk, skb); switch (ret) { case NET_XMIT_SUCCESS: return __ip_finish_output(net, sk, skb); case NET_XMIT_CN: return __ip_finish_output(net, sk, skb) ? : ret; default: kfree_skb(skb); return ret; }} 具体细节先不在这里展开（因为实在是太多了Orz），在系统调用 ip_finish_output 时，会触发我们设定的 kprobe 的钩子，在我们所设定的 hook 函数中会收到 net, sk, skb 三个参数（这三个参数也是调用 ip_finish_output 时的值。 在这三个参数中，我们主要来将视线放在 struct sk_buff *skb 上。 熟悉 Linux Kernel 协议栈实现的同学肯定对 sk_buff 这个数据结构非常非常熟悉了。这个数据结构是 Linux Kernel\b 中网络相关的核心数据结构。通过不断的偏移指针，这个数据结构能够很方便帮助我们确认我们待发送/已接收的数据在内存中所存放的位置。 空口直说好像有点抽象，我们来看个图 以发送一个 TCP 包为例，我们能看到这个图中，sk_buff 经历了六个阶段 a. 根据 TCP 中的一些选项如 MSS 等，分配一个 bufferb. 根据 MAX_TCP_HEADER 在我们申请好的内存 buffer 中预留一段足够容纳所有网络层的 header 的空间（TCP/IP/Link等）c. 填入 TCP 的 payloadd. 填入 TCP headere. 填入 IP headerd. 填入 link header 可以参照一下 TCP 报文结构，这样大家会有一个更直观的理解 大家能看到，通过 sk_buff 的一些指针的操作，我们就能很方便的获取到其中不同 layer 的 header 和具体的 payload OK，现在让我们正式的来开始实现我们所需要的功能 eBPF + KProbe首先简单介绍下 eBPF。BPF 指 Berkeley Packet Filter ，最早期是用来设计在内核中实现一些网络包过滤的功能。但是后续社区对其做了非常多的强化增强，使其不仅能应用于网络目地。这也是名字中 e 的来历（extend） 本质上而言，eBPF 在内核维护了一层 VM，可以加载特定规则生成的代码，让内核变得更具有可编程性（后面我争取写一篇 eBPF 从入门到入土的介绍文章） Tips: Tcpdump 的背后就是 BPF 然后在这次实现中，我们使用了 BCC 来简化我们 eBPF 相关的编写难度 OK，先上代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889from bcc import BPFimport ctypesbpf_text = &quot;&quot;&quot;#include &lt;linux/ptrace.h&gt;#include &lt;linux/sched.h&gt; /* For TASK_COMM_LEN */#include &lt;linux/icmp.h&gt;#include &lt;linux/ip.h&gt;#include &lt;linux/netdevice.h&gt;struct probe_icmp_sample { u32 pid; u32 daddress; u32 saddress;};BPF_PERF_OUTPUT(probe_events);static inline unsigned char *custom_skb_network_header(const struct sk_buff *skb){ return skb-&gt;head + skb-&gt;network_header;}static inline struct iphdr *get_iphdr_in_icmp(const struct sk_buff *skb){ return (struct iphdr *)custom_skb_network_header(skb);}int probe_icmp(struct pt_regs *ctx, struct net *net, struct sock *sk, struct sk_buff *skb){ struct iphdr * ipdata=get_iphdr_in_icmp(skb); if (ipdata-&gt;protocol!=1){ return 1; } u64 __pid_tgid = bpf_get_current_pid_tgid(); u32 __pid = __pid_tgid; struct probe_icmp_sample __data = {0}; __data.pid = __pid; u32 daddress; u32 saddress; bpf_probe_read(&amp;daddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;daddr); bpf_probe_read(&amp;saddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;saddr); __data.daddress=daddress; __data.saddress=saddress; probe_events.perf_submit(ctx, &amp;__data, sizeof(__data)); return 0;}&quot;&quot;&quot;class IcmpSamples(ctypes.Structure): _fields_ = [ (&quot;pid&quot;, ctypes.c_uint32), (&quot;daddress&quot;, ctypes.c_uint32), (&quot;saddress&quot;, ctypes.c_uint32), ]bpf = BPF(text=bpf_text)filters = {}def parse_ip_address(data): results = [0, 0, 0, 0] results[3] = data &amp; 0xFF results[2] = (data &gt;&gt; 8) &amp; 0xFF results[1] = (data &gt;&gt; 16) &amp; 0xFF results[0] = (data &gt;&gt; 24) &amp; 0xFF return &quot;.&quot;.join([str(i) for i in results[::-1]])def print_icmp_event(cpu, data, size): # event = b[&quot;probe_icmp_events&quot;].event(data) event = ctypes.cast(data, ctypes.POINTER(IcmpSamples)).contents daddress = parse_ip_address(event.daddress) print( f&quot;pid:{event.pid}, daddress:{daddress}, saddress:{parse_ip_address(event.saddress)}&quot; )bpf.attach_kprobe(event=&quot;ip_finish_output&quot;, fn_name=&quot;probe_icmp&quot;)bpf[&quot;probe_events&quot;].open_perf_buffer(print_icmp_event)while 1: try: bpf.kprobe_poll() except KeyboardInterrupt: exit() OK，这段代码严格意义上来说是混编的，一部分是 C，一部分是 Python，。Python 部分大家肯定都很熟悉，BCC 帮我们加载我们的 C 代码，并 attch 到 kprobe 上。然后不断输出我们从内核中往外传输的数据 那我们重点来看看 C 部分的代码（实际上这严格来说不算标准 C，算是 BCC 封装的一层 DSL） 首先看一下我们辅助的两个函数 123456789static inline unsigned char *custom_skb_network_header(const struct sk_buff *skb){ return skb-&gt;head + skb-&gt;network_header;}static inline struct iphdr *get_iphdr_in_icmp(const struct sk_buff *skb){ return (struct iphdr *)custom_skb_network_header(skb);} 如前面所说，我们可以根据 sk_buff 中的 head 和 network_header 就能计算出我们 IP 头部在内存中的地址，然后我们将其 cast 成一个 iphdr 结构体指针 我们还得再来看一下 iphdr 123456789101112131415161718192021struct iphdr {#if defined(__LITTLE_ENDIAN_BITFIELD) __u8 ihl:4, version:4;#elif defined (__BIG_ENDIAN_BITFIELD) __u8 version:4, ihl:4;#else#error &quot;Please fix &lt;asm/byteorder.h&gt;&quot;#endif __u8 tos; __be16 tot_len; __be16 id; __be16 frag_off; __u8 ttl; __u8 protocol; __sum16 check; __be32 saddr; __be32 daddr; /*The options start here. */}; 熟悉 IP 报文结构的同学肯定就很眼熟了对吧，其中 saddr 和 daddr 就是我们的源地址和目标地址，protocol 代表着我们 L4 协议的类型，其中为1的时候代表着 ICMP 协议 OK 然后来看一下我们的 trace 函数 123456789101112131415161718int probe_icmp(struct pt_regs *ctx, struct net *net, struct sock *sk, struct sk_buff *skb){ struct iphdr * ipdata=get_iphdr_in_icmp(skb); if (ipdata-&gt;protocol!=1){ return 1; } u64 __pid_tgid = bpf_get_current_pid_tgid(); u32 __pid = __pid_tgid; struct probe_icmp_sample __data = {0}; __data.pid = __pid; u32 daddress; u32 saddress; bpf_probe_read(&amp;daddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;daddr); bpf_probe_read(&amp;saddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;saddr); __data.daddress=daddress; __data.saddress=saddress; probe_events.perf_submit(ctx, &amp;__data, sizeof(__data)); return 0;} 如前面所说，kprobe 触发调用时，会将 ip_finish_output 的三个参数传入到我们的 trace 函数中来，那我们就可以根据传入的数据做很多的事了，现在来介绍下上面的代码中所做的事 将 sk_buff 转换成对应的 iphdr 判断当前报文是否为 ICMP 协议 利用内核 BPF 提供的 helper bpf_get_current_pid_tgid 获取当前调用 ip_finish_output 进程的 pid 获取 saddr 和 daddr。注意我们这里用的 bpf_probe_read 也是 BPF 提供的 helper function，原则上来讲，在 eBPF 中为了保证安全，我们所有从内核中读取数据的行为都应该利用 bpf_probe_read 或 bpf_probe_read_kernel 来实现 通过 perf 将数据提交出去 这样一来，我们就能排查到机器上具体什么进程在发送 ICMP 请求了 来看下效果 OK，我们的需求基本上达到了，不过这里算是留了一个小问题，大家可以思考下，我们怎么样根据 pid 获取启动进程时的 cmdline ? SystemTap + kprobeeBPF 的版本实现了，但是有个问题啊，eBPF 只能在高版本的内核中使用。一般而言，在 xb86_64 上，Linux 3.16 中支持了 eBPF。而我们依赖的 kprobe 对于 eBPF 的支持则是在 Linux 4.1 中实现的。通常而言，我们一般推荐使用 4.9 及以上内核来配合 eBPF 使用 那么问题来了。实际上我们现在有很多 Centos 7 + Linux 3.10 这样的传统的搭配，那么他们怎么办呢？ Linux 3.10 live’s matter! Centos 7 live’s matter! 那没办法，只能换一个技术栈来做了。这个时候，我们就首先考虑由 RedHat 开发，贡献进入社区，低版本可用的 SystemTap 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485%{#include&lt;linux/byteorder/generic.h&gt;#include&lt;linux/if_ether.h&gt;#include&lt;linux/skbuff.h&gt;#include&lt;linux/ip.h&gt;#include&lt;linux/in.h&gt;#include&lt;linux/tcp.h&gt;#include &lt;linux/sched.h&gt;#include &lt;linux/list.h&gt;#include &lt;linux/pid.h&gt;#include &lt;linux/mm.h&gt;%}function isicmp:long (data:long)%{ struct iphdr *ip; struct sk_buff *skb; int tmp = 0; skb = (struct sk_buff *) STAP_ARG_data; if (skb-&gt;protocol == htons(ETH_P_IP)){ ip = (struct iphdr *) skb-&gt;data; tmp = (ip-&gt;protocol == 1); } STAP_RETVALUE = tmp;%}function task_execname_by_pid:string (pid:long) %{ struct task_struct *task; task = pid_task(find_vpid(STAP_ARG_pid), PIDTYPE_PID);// proc_pid_cmdline(p, STAP_RETVALUE); snprintf(STAP_RETVALUE, MAXSTRINGLEN, &quot;%s&quot;, task-&gt;comm); %}function ipsource:long (data:long)%{ struct sk_buff *skb; struct iphdr *ip; __be32 src; skb = (struct sk_buff *) STAP_ARG_data; ip = (struct iphdr *) skb-&gt;data; src = (__be32) ip-&gt;saddr; STAP_RETVALUE = src;%}/* Return ip destination address */function ipdst:long (data:long)%{ struct sk_buff *skb; struct iphdr *ip; __be32 dst; skb = (struct sk_buff *) STAP_ARG_data; ip = (struct iphdr *) skb-&gt;data; dst = (__be32) ip-&gt;daddr; STAP_RETVALUE = dst;%}function parseIp:string (data:long) %{ sprintf(STAP_RETVALUE,&quot;%d.%d,%d.%d&quot;,(int)STAP_ARG_data &amp;0xFF,(int)(STAP_ARG_data&gt;&gt;8)&amp;0xFF,(int)(STAP_ARG_data&gt;&gt;16)&amp;0xFF,(int)(STAP_ARG_data&gt;&gt;24)&amp;0xFF);%}probe kernel.function(&quot;ip_finish_output&quot;).call { if (isicmp($skb)) { pid_data = pid() /* IP */ ipdst = ipdst($skb) ipsrc = ipsource($skb) printf(&quot;pid is:%d,source address is:%s, destination address is %s, command is: '%s'\\n&quot;,pid_data,parseIp(ipsrc),parseIp(ipdst),task_execname_by_pid(pid_data)) } else { next }} 实际上大家可以看到，我们思路还是一样，利用 ip_finish_output 来作为 kprobe 的 hook 点，然后我们获取对应的 iphdr 然后进行操作。 嗯，我们的需求的基础功能差不多就是这样了，大家可以在额外进行一些功能增强，比如获取完整的进程 cmdline 等等 更近一步的想法和实验大家可能对于 ICMP 这样的冷门协议没有太明显的感觉，那么我们换个需求大家可能就更为有感觉了 监控机器上哪些进程在发出 HTTP 1.1 请求 嗯，一如往的，我们先来看一下系统中的关键调用 嗯，这里我们选择 tcp_sendmsg 来作为我们的切入点 12345678910int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size){ int ret; lock_sock(sk); ret = tcp_sendmsg_locked(sk, msg, size); release_sock(sk); return ret;} 嗯，其中 sock 是包含我们一些关键元数据的结构体 12345678910111213141516171819202122232425262728293031323334struct sock { /* * Now struct inet_timewait_sock also uses sock_common, so please just * don't add nothing before this first member (__sk_common) --acme */ struct sock_common __sk_common; ...}struct sock_common { /* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned * address on 64bit arches : cf INET_MATCH() */ union { __addrpair skc_addrpair; struct { __be32 skc_daddr; __be32 skc_rcv_saddr; }; }; union { unsigned int skc_hash; __u16 skc_u16hashes[2]; }; /* skc_dport &amp;&amp; skc_num must be grouped as well */ union { __portpair skc_portpair; struct { __be16 skc_dport; __u16 skc_num; }; }; ...} 大家可以看到，我们能在 sock 中获取到我们端口的五元组数据，然后我们从 msghdr 中能获取到具体的数据 那么，以我们需求中的 HTTP 为例，我们实际上只需要判断，我们获取到的 TCP 包中是否包含 HTTP/1.1 ，便可粗略判断，这个请求是否是 HTTP 1.1 请求（很暴力的做法Hhhhh OK，我们来看下代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102from bcc import BPFimport ctypesimport binasciibpf_text = &quot;&quot;&quot;#include &lt;linux/ptrace.h&gt;#include &lt;linux/ip.h&gt;#include &lt;linux/tcp.h&gt;#include &lt;uapi/linux/ptrace.h&gt;#include &lt;net/sock.h&gt;#include &lt;bcc/proto.h&gt;#include &lt;linux/socket.h&gt;struct ipv4_data_t { u32 pid; u64 ip; u32 saddr; u32 daddr; u16 lport; u16 dport; u64 state; u64 type; u8 data[300]; u16 data_size;};BPF_PERF_OUTPUT(ipv4_events);int trace_event(struct pt_regs *ctx,struct sock *sk, struct msghdr *msg, size_t size){ if (sk == NULL) return 0; u32 pid = bpf_get_current_pid_tgid() &gt;&gt; 32; // pull in details u16 family = sk-&gt;__sk_common.skc_family; u16 lport = sk-&gt;__sk_common.skc_num; u16 dport = sk-&gt;__sk_common.skc_dport; char state = sk-&gt;__sk_common.skc_state; if (family == AF_INET) { struct ipv4_data_t data4 = {}; data4.pid = pid; data4.ip = 4; //data4.type = type; data4.saddr = sk-&gt;__sk_common.skc_rcv_saddr; data4.daddr = sk-&gt;__sk_common.skc_daddr; // lport is host order data4.lport = lport; data4.dport = ntohs(dport); data4.state = state; struct iov_iter temp_iov_iter=msg-&gt;msg_iter; struct iovec *temp_iov=temp_iov_iter.iov; bpf_probe_read_kernel(&amp;data4.data_size, 4, &amp;temp_iov-&gt;iov_len); u8 * temp_ptr; bpf_probe_read_kernel(&amp;temp_ptr, sizeof(temp_ptr), &amp;temp_iov-&gt;iov_base); bpf_probe_read_kernel(&amp;data4.data, sizeof(data4.data), temp_ptr); ipv4_events.perf_submit(ctx, &amp;data4, sizeof(data4)); } return 0;}&quot;&quot;&quot;bpf = BPF(text=bpf_text)filters = {}def parse_ip_address(data): results = [0, 0, 0, 0] results[3] = data &amp; 0xFF results[2] = (data &gt;&gt; 8) &amp; 0xFF results[1] = (data &gt;&gt; 16) &amp; 0xFF results[0] = (data &gt;&gt; 24) &amp; 0xFF return &quot;.&quot;.join([str(i) for i in results[::-1]])def print_http_payload(cpu, data, size): # event = b[&quot;probe_icmp_events&quot;].event(data) # event = ctypes.cast(data, ctypes.POINTER(IcmpSamples)).contents event= bpf[&quot;ipv4_events&quot;].event(data) daddress = parse_ip_address(event.daddr) # data=list(event.data) # temp=binascii.hexlify(data) body = bytearray(event.data).hex() if &quot;48 54 54 50 2f 31 2e 31&quot;.replace(&quot; &quot;, &quot;&quot;) in body: # if &quot;68747470&quot; in temp.decode(): print( f&quot;pid:{event.pid}, daddress:{daddress}, saddress:{parse_ip_address(event.saddr)}, {event.lport}, {event.dport}, {event.data_size}&quot; )bpf.attach_kprobe(event=&quot;tcp_sendmsg&quot;, fn_name=&quot;trace_event&quot;)bpf[&quot;ipv4_events&quot;].open_perf_buffer(print_http_payload)while 1: try: bpf.perf_buffer_poll() except KeyboardInterrupt: exit() OK，我们来看下效果 实际上这个我们还可以再扩展一下。比如针对 Go 这样，所发出的 HTTPS 连接有着固定特征的语言，我们也可以用相对简单的做法去完成机器上的包来源的溯源（大家可以参考下无辄的这篇文章，为什么用 Go 访问某网站始终会 503 Service Unavailable ？) 我自己也做了一个测试，大家可以参考下代码：https://github.com/Zheaoli/linux-traceing-script/blob/main/ebpf/go-https-tracing.py 总结实际上无论是 eBPF 还是 SystemTap ，这类动态 tracing 技术可以 Linux Kernel 变得更具被可编程性。相较于传统的 recompile kernel 这些手段来说，更为方便快捷。而 BCC/BPFTrace 这类的更进一步的封装框架的出现，更进一步的降低了我们去观测内核的难度 很多时候我们很多需求都可以选择旁路的方式去更快捷的实现。但是要注意的一点是，动态 tracing 技术的引入势必增加了内核的不稳定性，而且一定程度上会影响性能。所以我们需要根据具体的场景去做 trade-off 好了，这篇文章差不多就水到这里，后面有时间争取出一个 eBPF 从入门到入土的系列文章（flag++","link":"/posts/2021/04/17/how-to-tracing-package-in-the-linux-kernel/"},{"title":"听说我有女朋友","text":"听说我有女朋友我很喜欢写听说系列文章，从入行开始写的第一篇《听说你会 Python》。所以半年了，我干脆也来写一篇《听说你有女朋友》。为啥要今天写？因为明天她送的新键盘要到了，正好老键盘退役，写点啥纪念下 怎么认识的？很多人都会问我这样的问题：”你一个技术男咋和学音乐的认识了？“，我一般这么回答：”知乎认识的“（请知乎给我广告费，明天之前打到我支付宝上 好了，正经说，我们的确算是在知乎上认识的（广告费+1）。当时我们在一个知乎群里，然后后面加上了好友。 很多时候，我这个人有个毛病，加了女孩子之后不怎么聊的。不过幸好，我们最开始有个共同的爱好，养猫。她家的是一只橘猫。我家是母女两。某种意义上讲，这三只猫是我们俩的媒人（回去加罐头） 正式开始聊起来，应该是开学的时候，她当时睡眠不太好。某天晚上，睡前戏称保佑她肯定睡得好之后。第二天她说昨晚居然还真睡得不错（直男如我我现在都不知道我的功力是不是有那么强。问她她也不说，摔） 借着这个契机，就开始聊了起来，包括不限于互相点外卖犒劳对方上课/上班的无奈。出来约了两次火锅（直男撩妹法） 11月初，当时做完18年的 PyCon，她说要送我一份礼物。找个周一兴高采烈的约了井格，拿到了第一份礼物，一条爱马仕的领带和领带夹。花纹是我最喜欢的樱花。然而我现在都还没一个合适的机会用上这条领带。所以请各位大佬行行好。如果有什么可以穿正装的场合。包括不仅限于伴郎，主持，放贷，打架，搞传销等，请务必通知我一声。 说道怎么正式在一起的。说起来也足够的戏剧性和充满了直男的无奈（她现在都还会嫌弃我） 11月6号晚上，当时突然聊到双十一的话题。她说希望这个月底能找到男朋友。我当时没有过大脑，直接说，我奶你没问题！瞬间这话一出口，能明显她感觉不开心了。我当时就在纳闷，不会吧，她不会是喜欢我吧？恩？不可能吧？于是我就正儿八经的向她反复询问这个问题。然后她承认：“没错，我是挺喜欢你的”（后来被她吐槽你咋能反复问女孩子喜不喜欢你呢！）（直男如我）。于是就这么在一起了 看完这一套下来，大家是不是觉得，我这样的人怎么都能找到女朋友呢？如果非要问我为什么能找到女朋友的话，我给大家分享三个原则： 她足够瞎 她足够瞎 她足够瞎 噢，对了，忘了向大家介绍她了，真名就不告诉大家了。她艺名叫荆澈（这也是我的花名），山东人，有很多身份啦，比如李者璈同志的女朋友，大三的钢琴生等等。总之携她向大家问好，请多指教啦！ 恋爱日常恋爱什么感觉呢？荆澈同学（非我）有一段比较好的总结：“和你谈恋爱，感觉就像养了一只大狗，这狗还贼会撒娇” 给大家分享一下我们的恋爱日常 第一次去电影院，当时在无名之辈和无敌破坏王之间纠结。她说去电影院看动画片么？于是我记着这句话了。后续，某次又要去看电影的时候，在海王和蜘蛛侠平行宇宙之间纠结。当时贼想看海王，于是我说蜘蛛侠是动画片！她说好吧，看海王。后续她吐槽，哼，我早就知道你想看海王了，当我不知道是吧。 当时她之前在参加校园卡设计大赛获奖后，某天晚上突然给我说，我奖金，一半给我买了个神仙水，一半给你买了一套海盗船键鼠（然而因为学校的坑爹，我明天才能拿到）好不好？我当时：哇！开心！等等你为啥要鼓励我玩游戏？ 某天去看莎翁的第十二夜，我最开始不习惯，然后差点睡着，荆澈同学一直在旁边捏我。 还是去看莎翁的第十二夜，当时突然想起我们买了一月份的《我，堂吉诃德》的话剧票。然后突然我问她。我们是不是要去看堂诃吉德？她一脸嫌弃，你这个文盲，是堂吉诃德！嫌弃！当时我一脸委屈的马上去京东下单了一套《堂吉诃德》（现在也没看）。后续她感叹，你这个人咋这么容易委屈呢？ 11月裸辞后，一直在家休息，在拿了饿了么的 offer 消息后，给她说，她表示：早知道了。我说，窝草？她说，哼我可是梦见自己中考分数的女人！怎么样！棒不棒。我：棒棒棒，半仙，，半仙 两个人都喜欢吃糖葫芦，草莓的。有次一起去看新房子（租的），然后将草莓糖葫芦放在新屋子里忘了拿！ 某天晚上，你要是不给我吃糖葫芦我就通宵！她：哦，熬得住你就熬吧（这是亲女朋友么 入职之前，荆澈同学问要不要送一束花给我，我说要啊！然后她一脸无辜的问，要是你因为虐狗被开了怎么办！我一脸正气：不允许员工虐狗的公司不是好公司。第二周周一，刚到办公室，收到一束花(事实证明，这束花来的真及时) 荆澈同学回调酒，某天晚上她调了一杯雪球，我给你讲，贼好喝，当时贪嘴，喝的多了点，然后晚上就在床上装死狗。荆澈同学一脸无奈的说：以后你不准喝酒！一滴酒也不能喝！ 2月份，北京下雪，当时我没忍住，晚上出去吃饭的时候去雪地里滚了一圈。然后把手机搞丢了。第二天从派出所捡回手机。荆澈同学一脸无奈：你以后还滚不滚了？ 3月6号，下班回家。回家一看，哇一个大大的蛋糕，然后荆澈同学一脸开心的蹦出来！今天120天啦！一起庆祝一下！ 在荆澈同学回学校之前，每天下班回家都会有个抱抱（出门也有的（回学校就没了（哭 恋爱总结其实很多人都会好奇，你们完全是两个世界的人，怎么会在一起呢？说实话，起初我也担心这个问题。但是其实发现。自己很多的顾虑也是多余。恋爱是一个相互磨合与了解的过程。我们会一起去看剧，一起吐槽，给对方讲我们各自领域的故事，会分享各自的心情与家庭。当然我们也会日常打闹，互黑。 某种意义上来讲，恋爱是一个相互认可，相互成就的过程。有些时候，我笑称她眼瞎才遇上我。她会很认真的给我说，我才不瞎呢。所以很庆幸我自己能遇到这样一个可爱，知性，懂事的女孩子。我也会和她一起好好的经营这一段感情。 最后的最后，喂了这么多狗粮。还是要收尾一下的。祝每一个看这篇文章的人，如果你已经有自己的爱人，那么祝福你和你的爱人一切平安喜乐。如果你还没有遇见那个人，那么请不要着急，请相信，在不远的一天，你终将找到你所爱之人。 （另外，这篇文章的封面，就是我亲爱的荆澈同学啦！（","link":"/posts/2018/12/31/i-have-girlfriend/"},{"title":"写在黎明之前","text":"年少之成绩，或有时而可商，年少之作为，或有时而可讨，为此独立之精神，与自由之思想，纵历百十年整，亦与沱江水长流，共三光而永光 其实突然发现，没写过点什么特殊的文字，来纪念自己这一年。 今天，2月10日，距离离职完毕还有五天，距离离开成都还有11天，距离入职，还有17天。坐在自己屋子里，看着窗外的云和雨雾，慢慢的，用一点东西，来纪念自己过去的一年。 云起 Hey man, think about this world and go fuck it 16年，一开年，便遭受了一个算是影响未来的挫折吧。现在想想，从1年6个月的感情里脱身出来并不是一件很容易的事情。更何况，遇到太多的事情让我更为烦心。 其实在大学尾声的时候，来回顾自己这四年，可能也更多的算是一个 loser 吧，竞赛失败。专业课挂科四分之一，延期毕业。不过心高气傲的我，还是不喜欢自己被同为一群 loser 的人所鄙视。这种感觉不是太好。 在师父的教育下，慢慢的入了 Python 的坑，准备在毕业前找到一个能混口饭吃的工作。所谓初生牛犊不怕虎，在什么都还懵懂的时候，第一场面试的结果就给我了当头一棒。不过到现在，依然很感谢当时面试官给我的灌输的一些东西。可能这样的一些东西，让我明白了真正的程序猿所应该拥有的态度与责任吧。 所幸，第二份工作，平安的通过并入职。恩，这算是一个，还算不错的开端吧。 暗云 当激情与新鲜感退却之后，生活能剩下的只能是坚守。 在入职以后，可能离开了前面一段时间的新鲜期，便跟之前一样，进入了朝 9 晚 2 的生活。可能在自己以为自己会这样过下去的时候。认识了小天，猴子，和一群 Android 老司机（你说我一个写 Python 的，怎么就开始天天和 Android 的人开始搞基了呢？），然后去了掘金，开始以一个四级没过的人的身份，去翻译一点感兴趣的技术文章。然后和一群小伙伴们借着大佬后宫团的名义聚集在了一起。 如果要描述这段时光的话，应该就像现在窗外厚厚的云一样。表面四平八和，实则惊心动魄。看不到阳光的方向，却也只能随着大潮不断的前行。不知道自己的未来是什么样的，心有不甘，却也只能一步步的坚持走下去。不过，始终告诉自己，走下去。无数次的哭过，但也未曾放弃过。 其实很想感谢身边的人，公司的爽姐，涛哥，论道的朋友们，童童后宫团 12 位小伙伴们（童童，艳辉姐，羊哥，鳗鱼，大叔，盖伦哥，波波，田田，老叶，老柯，五月天，雪梨姐），在云层中看不到阳光与未来之时，这些人，用他们的宠爱和优秀，让我至少不会在随波逐流中迷失了自己的方向。 对了，如我所料，学位证延期了。 然后某人的死讯，真的是一条最好的消息。 疑云 为何而生，为何而战，为何而前行。 和优秀的人相处久了，你总是不由自主的想去变得和他们一样的优秀。长久的遗传下来的自卑心却告诉自己，这些东西都离我太过遥远。心中的不安，不甘，自卑，狂妄交织在一起，产生出了一种难以明说的滋味。 其实这样一段时间，体验到了一个词的含义“悲喜参半”，悲的话，其实主要算是可惜吧，那段时间多愁善感，用朋友的话来说，你真成了一贱人的（不过我啥时候不是？）。喜得话应该算是再这样一种不断的挣扎于交集中，真正确定了自己想做什么，该做什么。 对了，最后成功的拿到了学位证。 云开 在黑暗中飞翔的鸟儿，终将会得到主救赎，自由的沐浴在阳光下 拿到学位证之后，想出去看看更大的世界的心变得越发的狂野。写简历，投简历，找内推。一场场面试下来，可以说是体力与精神的双重考验。 不过在面试中，最大的收获，算是对自己的肯定吧。面试的内容和结果都告诉自己，自己貌似，也成为了，一个还算不错的新人？恩，长久被自卑覆盖的心，在这一段时间里体会了阳光的温暖。可能以后，不会在自卑中沉沦了呢？ 恩，最后，很庆幸能遇到一些愿意给我机会的面试官，这样一种特殊的恩赐，算是，命运送给我，最好的新年礼物吧。 最后还有十天就离开成都了，还有很多想说，还有很多不知道怎么说。对未来依旧一无所知，但是会很少再去迷茫。 休对故人思故国，且将新火试新茶。诗酒趁年华。恩，诗酒趁年华！","link":"/posts/2017/02/10/it-s-my-way/"},{"title":"Leetcode BiWeekly Contest 19 题解","text":"例行 Leetcode 周赛，这周双周赛，两场赛打下来，有点酸爽，先写个 BiWeekly 19 Contest 的题解吧 1342. Number of Steps to Reduce a Number to Zer题面： Given a non-negative integer num, return the number of steps to reduce it to zero. If the current number is even, you have to divide it by 2, otherwise, you have to subtract 1 from it. 示例： 123456789Input: num = 14Output: 6Explanation: Step 1) 14 is even; divide by 2 and obtain 7. Step 2) 7 is odd; subtract 1 and obtain 6. Step 3) 6 is even; divide by 2 and obtain 3. Step 4) 3 is odd; subtract 1 and obtain 2. Step 5) 2 is even; divide by 2 and obtain 1. Step 6) 1 is odd; subtract 1 and obtain 0. 这个题题面很简单，一个非负整数，偶数除2，奇数减1，求需要多少步到0 暴力写 12345678910111213class Solution: def numberOfSteps(self, num: int) -&gt; int: count = 0 if num == 0: return count result = num while result &gt; 1: if result % 2 == 0: result = int(result / 2) else: result -= 1 count += 1 return count + 1 Number of Sub-arrays of Size K and Average Greater than or Equal to Threshold题面： Given an array of integers arr and two integers k and threshold.Return the number of sub-arrays of size k and average greater than or equal to threshold. 示例： 123Input: arr = [2,2,2,2,5,5,5,8], k = 3, threshold = 4Output: 3Explanation: Sub-arrays [2,5,5],[5,5,5] and [5,5,8] have averages 4, 5 and 6 respectively. All other sub-arrays of size 3 have averages less than 4 (the threshold) 给定一个数组和一个长度 k，和一个阈值 threshold ，求这个数组中的所有长度为 K 且平均数大于等于阈值的子数组的个数。这个题，暴力写很简单，一个简单的数组的拆分，sum(arr[i:i+k])/k &gt;= threshold 即可，但是这里有个问题，如果实时求和，那么时间复杂度为 O(M*K) M 为数组的长度，这个时候暴力会 T 因此需要做个小技巧的优化。可以考虑这样这样一个做法，假设当前 i 及其后 k 个数的和为 sum[i]，那么有这样一个公式，sum[i]=sum[i-1]-arr[i]+arr[i+k-1]，这样每次计算和都是 O(1) 的复杂度，那么整体就是一个 O(N) 的做法 好了，暴力开写 12345678910111213141516171819202122from typing import Listclass Solution: def numOfSubarrays(self, arr: List[int], k: int, threshold: int) -&gt; int: if not arr: return 0 length = len(arr) sum_threshold = [0.0] * length count = 0 last_index = length for i in range(length - k, -1, -1): if i == length - k: total_sum = sum(arr[i:i + k]) else: total_sum = sum_threshold[i + 1] - arr[last_index] + arr[i] sum_threshold[i] = total_sum if total_sum / k &gt;= threshold: count += 1 last_index -= 1 return count 1344. Angle Between Hands of a Clock题面： Given two numbers, hour and minutes. Return the smaller angle (in sexagesimal units) formed between the hour and the minute hand. 示例： 12Input: hour = 12, minutes = 30Output: 165 求某个时刻，时针与分针的夹角，，，啊，，我的上帝呀，一度梦回小升初。。。一个数学题，首先科普如下知识 普通钟表相当于圆，其时针或分针走一圈均相当于走过360°角； 钟表上的每一个大格（时针的一小时或分针的5分钟）对应的角度是：360°/12=30°； 时针每走过1分钟对应的角度应为：360°/(12*60)=0.5°； 分针每走过1分钟对应的角度应为：360°/60=6°。 好了，那么就暴力做吧 12345class Solution: def angleClock(self, hour: int, minutes: int) -&gt; float: hour %= 12 result = abs((minutes * 6) - (hour * 30 + minutes * 0.5)) return result if result &lt; 360 else 360 - result 1345. Jump Game IV题面： Given an array of integers arr, you are initially positioned at the first index of the array.In one step you can jump from index i to index: i + 1 where: i + 1 &lt; arr.length. i - 1 where: i - 1 &gt;= 0. j where: arr[i] == arr[j] and i != j.Return the minimum number of steps to reach the last index of the array.Notice that you can not jump outside of the array at any time. 示例： 123Input: arr = [100,-23,-23,404,100,23,23,23,3,404]Output: 3Explanation: You need three jumps from index 0 --&gt; 4 --&gt; 3 --&gt; 9. Note that index 9 is the last index of the array. 还是跳格子，给定一个数组，里面会有一些具体的值，现在从 index = 0 的地方起跳，跳跃规则如下： 在 i+1 或 i-1 都在数组的范围内 如果存在 index=j 且 arr[i]==arr[j] 且 i!=j 的时候，可以直接从 i 跳到 j 求从 index=0 跳到 index=arr.length-1 最小的次数 这题我还是没 A，后面琢磨了下，一个搜索的题目 构建一个字典，值为key，index 为 value（相同的值之间可以直接跳） 利用一个 set 来保存跳过的点 从 index = 0 开始进行 BFS ，求每个点在一步之内可以跳到哪个点，然后不断的 BFS 直到到达终点 更新被访问过的点 emmmm，好吧 BFS ，开写吧 123456789101112131415161718192021222324252627282930313233from typing import List, Setimport collectionsclass Solution: def minJumps(self, arr: List[int]) -&gt; int: length = len(arr) if not arr or length == 1: return 0 value_index = collections.defaultdict(list) for index, value in enumerate(arr): value_index[value].append(index) visited: Set[int] = set() traversal_queue = collections.deque([(0, 0)]) result = 0 while traversal_queue: next_step_queue = collections.deque() for _ in range(len(traversal_queue)): cur_index, cur_step = traversal_queue.popleft() cur_value = arr[cur_index] visited.add(cur_index) for next_index in [cur_index + 1, cur_index - 1] + value_index[ cur_value ]: if (length &gt; next_index &gt;= 0) and (next_index not in visited): if next_index == length - 2: return cur_step + 2 if next_index == length - 1: return cur_step + 1 next_step_queue.append((next_index, cur_step + 1)) del value_index[cur_value] traversal_queue = next_step_queue return result 总结这周的题，还是不难，但是需要小心，比如第二题我太大意直接暴力吃了一发T，然后第三题没仔细读题（求最小的度数）吃了一发 WA，不过和第二天周赛比起来，真的是幸福，175 第三题的题面直接让心态崩了，，明天写题解。 好了，滚去睡觉","link":"/posts/2020/02/10/leetcode-biweekly-contest-19/"},{"title":"Leetcode Weekly Contest 174 题解","text":"最近因为生病好久没刷题，今早开始打了一场 Leetcode 的周赛，来写个题解，今早状态还行，，BTW 以后每周都会打周赛，争取写题解 Leetcode 1341. The K Weakest Rows in a Matrix描述： Given a m * n matrix mat of ones (representing soldiers) and zeros (representing civilians), return the indexes of the k weakest rows in the matrix ordered from the weakest to the strongest.A row i is weaker than row j, if the number of soldiers in row i is less than the number of soldiers in row j, or they have the same number of soldiers but i is less than j. Soldiers are always stand in the frontier of a row, that is, always ones may appear first and then zeros. Example 1: 12345678910111213141516Input: mat = [[1,1,0,0,0], [1,1,1,1,0], [1,0,0,0,0], [1,1,0,0,0], [1,1,1,1,1]], k = 3Output: [2,0,3]Explanation: The number of soldiers for each row is: row 0 -&gt; 2 row 1 -&gt; 4 row 2 -&gt; 1 row 3 -&gt; 2 row 4 -&gt; 5 Rows ordered from the weakest to the strongest are [2,0,3,1,4] 题面很简单，其实这道题就是二进制的处理，Python 里面就暴力出奇迹了 123456789101112from typing import Listclass Solution: def kWeakestRows(self, mat: List[List[int]], k: int) -&gt; List[int]: if not mat: return [] number = [] for i in range(len(mat)): number.append((int(&quot;&quot;.join([str(x) for x in mat[i]]), 2), i)) number.sort() return [x for _, x in number[0:k]] 1342. Reduce Array Size to The Half描述： Given an array arr. You can choose a set of integers and remove all the occurrences of these integers in the array.Return the minimum size of the set so that at least half of the integers of the array are removed. 12345Input: arr = [3,3,3,3,5,5,5,2,2,7]Output: 2Explanation: Choosing {3,7} will make the new array [5,5,5,2,2] which has size 5 (i.e equal to half of the size of the old array).Possible sets of size 2 are {3,5},{3,2},{5,2}.Choosing set {2,7} is not possible as it will make the new array [3,3,3,3,5,5,5] which has size greater than half of the size of the old array. 这个题题面也很简单，给定一个数组，选择一组数字移除，被移除后的数组数量小于等于之前的一半，求最少选择多少数字能达到要求 哈希表，O(N) 的做法 12345678910111213141516171819from typing import Listclass Solution: def minSetSize(self, arr: List[int]) -&gt; int: if not arr: return 0 counter = {} for i in arr: counter[i] = counter.setdefault(i, 0) + 1 counter = {k: v for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)} total_count = 0 result_count = 0 for i, count in counter.items(): total_count += count result_count += 1 if total_count &gt;= len(arr) / 2: break return result_count 1343. Maximum Product of Splitted Binary Tree描述： Given a binary tree root. Split the binary tree into two subtrees by removing 1 edge such that the product of the sums of the subtrees are maximized.Since the answer may be too large, return it modulo 10^9 + 7. Example 1: 123Input: root = [1,2,3,4,5,6]Output: 110Explanation: Remove the red edge and get 2 binary trees with sum 11 and 10. Their product is 110 (11*10) 这个题的题面也很简单，给定一个带值的二叉树，移除某个二叉树的边，使之分割成为两个新的二叉树，求两个二叉树和的乘积最大 最开始很多人会被这道题唬到，但是实际上这道题就是一个二叉树的遍历，无论前中后序遍历，先遍历一次二叉树，求出二叉树节点值的总和，以及每个节点的左子树的和 left_sum 以及右子树的总和 right_sum 然后再次遍历，result=max((total_sum-left_sum)*left_sum),(total_sum-right_sum)*right_sum),result) 暴力求解即可 123456789101112131415161718192021222324252627282930313233class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def maxProduct(self, root: TreeNode) -&gt; int: total_sum = self.sum_node(root) result = 0 stack = [] node = root while node or stack: while node: stack.append(node) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) result = max(result, ((total_sum - node.right_sum) * node.right_sum)) node = node.left node = stack.pop() node = node.right if node: result = max(result, ((total_sum - node.right_sum) * node.right_sum)) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) return result % (10 ** 9 + 7) def sum_node(self, root: TreeNode) -&gt; int: if not root: return 0 left_sum = self.sum_node(root.left) right_sum = self.sum_node(root.right) root.left_sum = left_sum root.right_sum = right_sum return left_sum + right_sum + root.val BTW 这段代码的 type hint 使用其实有点问题，我后面比赛完了改了一版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869from typing import Optional, Tuple, Listclass TreeNode: val: int left: Optional[&quot;TreeNode&quot;] right: Optional[&quot;TreeNode&quot;] def __init__(self, x): self.val = x self.left = None self.right = Noneclass TreeNodeWithSum: val: int left: Optional[&quot;TreeNodeWithSum&quot;] right: Optional[&quot;TreeNodeWithSum&quot;] left_sum: int right_sum: int def __init__( self, x: int, left: Optional[&quot;TreeNodeWithSum&quot;], right: Optional[&quot;TreeNodeWithSum&quot;], left_sum: int, right_sum: int, ): self.val = x self.left = left self.right = right self.left_sum = left_sum self.right_sum = right_sumclass Solution: def maxProduct(self, root: TreeNode) -&gt; int: total_sum,new_root = self.sum_node(root) result = 0 stack:List[TreeNodeWithSum] = [] node = new_root while node or stack: while node: stack.append(node) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) result = max(result, ((total_sum - node.right_sum) * node.right_sum)) node = node.left node = stack.pop() node = node.right if node: result = max(result, ((total_sum - node.right_sum) * node.right_sum)) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) return result % (10 ** 9 + 7) def sum_node( self, root: Optional[TreeNode] ) -&gt; Tuple[int, Optional[TreeNodeWithSum]]: if not root: return 0, None left_sum, new_left_node = self.sum_node(root.left) right_sum, new_right_node = self.sum_node(root.right) return ( left_sum + right_sum + root.val, TreeNodeWithSum( root.val, new_left_node, new_right_node, left_sum, right_sum ), ) BTW，这道题因为数据太大，需要对 10^9+7 取模，我智障的忘了取模，WA 了两次，罚时罚哭。。。我真的太菜了。。 1344. Jump Game V描述： Given an array of integers arr and an integer d. In one step you can jump from index i to index:i + x where: i + x &lt; arr.length and 0 &lt; x &lt;= d.i - x where: i - x &gt;= 0 and 0 &lt; x &lt;= d.In addition, you can only jump from index i to index j if arr[i] &gt; arr[j] and arr[i] &gt; arr[k] for all indices k between i and j (More formally min(i, j) &lt; k &lt; max(i, j)).You can choose any index of the array and start jumping. Return the maximum number of indices you can visit.Notice that you can not jump outside of the array at any time. 12345Input: arr = [6,4,14,6,8,13,9,7,10,6,12], d = 2Output: 4Explanation: You can start at index 10. You can jump 10 --&gt; 8 --&gt; 6 --&gt; 7 as shown.Note that if you start at index 6 you can only jump to index 7. You cannot jump to index 5 because 13 &gt; 9. You cannot jump to index 4 because index 5 is between index 4 and 6 and 13 &gt; 9.Similarly You cannot jump from index 3 to index 2 or index 1. 这题的题面是这样，一个数组，里面有若干值，你可以从任意一个位置开始跳跃，一次只能跳一个，跳的时候需要满足规则，假定你从数组 i 位置起跳，每次可跳的范围是 x，那么你需要满足 i+x &lt; arr.length 和 0&lt;x&lt;=d i-x &gt;=0 和 0&lt;x&lt;=d 同时假设你从 i 跳往 j，那么你需要保证 arr[i]&gt;arr[j] 且 i 到 j 中的每个元素都满足 arr[j]&lt;x&lt;arr[i]，求最多能跳多少个元素 最开始觉得这题是一个双头 DP 的题，嫌写起来恶心就懒得写，，但是后面比赛完了发现其实这个题其实单 DP 就能解决的，因为我们只能从高往低跳，于是我们可以先将元素排序后依次遍历，可以得出公式为 dp[i]=max(dp[i]+dp[j]+1) 其中 j 是从 i 起可以到达的索引值，DP 部分的复杂度为 O(DN) 但是因为需要提前排序，因此整体的时间复杂度为 O(logN+DN) 123456789101112131415from typing import Listclass Solution: def maxJumps(self, arr: List[int], d: int) -&gt; int: length = len(arr) dp = [1] * length for a, i in sorted([a, i] for i, a in enumerate(arr)): for di in [-1, 1]: for j in range(i + di, i + d * di + di, di): if not (0 &lt;= j &lt; length and arr[j] &lt; arr[i]): break dp[i] = max(dp[i], dp[j] + 1) return max(dp) 总结很久没刷题了，手还是有点生，在前面几个签到题上花了时间，，而且犯了低级错误，，所以以后一定要坚持刷题了。。BTW 这次的周赛题感觉都很简单，感觉像是被泄题后找的 Backup，好了就先这样吧，我继续卧床养病了。。","link":"/posts/2020/02/02/leetcode-weekly-contest-174/"},{"title":"Leetcode Weekly Contest 176 题解","text":"emmmm，我的拖延症没救了，顺便加上这周沉迷 Kotlin ，这篇本应该周一就写完的题解拖到现在，= =然而这周双周赛，，我又得写两篇题解了。。。 1351. Count Negative Numbers in a Sorted Matrix题面： Given a m * n matrix grid which is sorted in non-increasing order both row-wise and column-wise.Return the number of negative numbers in grid. 示例： 123Input: grid = [[4,3,2,-1],[3,2,1,-1],[1,1,-1,-2],[-1,-1,-2,-3]]Output: 8Explanation: There are 8 negatives number in the matrix. 题面很简单，给定一个矩阵，矩阵横/纵向都是递减的，求这个矩阵中负数的个数，这个题，因为横/纵向的数据规模都是小于100的，那就没啥说的了，，直接暴力，横向遍历，然后遇到负数就停止遍历 123456789101112131415from typing import Listclass Solution: def countNegatives(self, grid: List[List[int]]) -&gt; int: if not grid: return 0 n_length = len(grid[0]) result = 0 for item in grid: for i in range(n_length): if item[i] &lt; 0: result += n_length - i break return result 1352. Product of the Last K Numbers题面: 12345678910Implement the class ProductOfNumbers that supports two methods:1. add(int num)Adds the number num to the back of the current list of numbers.2. getProduct(int k)Returns the product of the last k numbers in the current list.You can assume that always the current list has at least k numbers.At any time, the product of any contiguous sequence of numbers will fit into a single 32-bit integer without overflowing. 示例 12345678910111213141516171819Input[&quot;ProductOfNumbers&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;getProduct&quot;,&quot;getProduct&quot;,&quot;getProduct&quot;,&quot;add&quot;,&quot;getProduct&quot;][[],[3],[0],[2],[5],[4],[2],[3],[4],[8],[2]]Output[null,null,null,null,null,null,20,40,0,null,32]ExplanationProductOfNumbers productOfNumbers = new ProductOfNumbers();productOfNumbers.add(3); // [3]productOfNumbers.add(0); // [3,0]productOfNumbers.add(2); // [3,0,2]productOfNumbers.add(5); // [3,0,2,5]productOfNumbers.add(4); // [3,0,2,5,4]productOfNumbers.getProduct(2); // return 20. The product of the last 2 numbers is 5 * 4 = 20productOfNumbers.getProduct(3); // return 40. The product of the last 3 numbers is 2 * 5 * 4 = 40productOfNumbers.getProduct(4); // return 0. The product of the last 4 numbers is 0 * 2 * 5 * 4 = 0productOfNumbers.add(8); // [3,0,2,5,4,8]productOfNumbers.getProduct(2); // return 32. The product of the last 2 numbers is 4 * 8 = 32 题面很简单，设计一个数据结构，提供一个 add 方法，让用户能够往里面添加速度，提供一个 getProduct 方法，让用户能求倒数K个数的乘积，这题没啥好说的，直接暴力写，中间加个 hashmap 作为缓存 1234567891011121314151617181920212223242526272829303132333435363738394041from typing import List, Dictimport bisectfrom operator import mulfrom functools import reduceclass ProductOfNumbers: _value: List[int] _cache_result: Dict[int, int] _cache_index: List[int] def __init__(self): self._value = [] self._cache_result = {} self._cache_index = [] def add(self, num: int) -&gt; None: self._value.append(num) self._cache_index.clear() self._cache_result.clear() def getProduct(self, k: int) -&gt; int: if k in self._cache_result: return self._cache_result[k] cache_index = bisect.bisect(self._cache_index, k) - 1 if cache_index &gt;= 0: last_k = self._cache_index[cache_index] result = self._cache_result[last_k] for i in range(1, cache_index + 1): temp_last_k = last_k + i if temp_last_k &gt;= len(self._value): break result *= self._value[-last_k] else: temp_value = ( self._value[-1 : -k - 1 : -1] if k &lt;= len(self._value) else self._value ) result = reduce(mul, temp_value, 1) bisect.bisect_left(self._cache_index, k) self._cache_result[k] = result return result 1353. Maximum Number of Events That Can Be Attended题面： Given an array of events where events[i] = [startDayi, endDayi]. Every event i starts at startDayi and ends at endDayi.You can attend an event i at any day d where startTimei &lt;= d &lt;= endTimei. Notice that you can only attend one event at any time d.Return the maximum number of events you can attend. 示例 1234567Input: events = [[1,2],[2,3],[3,4]]Output: 3Explanation: You can attend all the three events.One way to attend them all is as shown.Attend the first event on day 1.Attend the second event on day 2.Attend the third event on day 3. 给定一个数组，数组中每个元素 x 代表一个活动，x[0], x[i] 代表该活动的起始与结束时间，一个用户一天只能参加一个活动，求用户最多能参加多少个活动。经典的一个贪心题目，首先对活动列表以结束时间进行排序，然后依次遍历每个时间，确认具体哪一天可以参加，整体时间复杂度为 O(max(nlogn,n*m)) 12345678910111213141516171819202122from typing import List, Dictclass Solution: def maxEvents(self, events: List[List[int]]) -&gt; int: if not events: return 0 events_size = len(events) if events_size == 1: return 1 events = sorted(events) _day_map: Dict[str, bool] = {} _event_map: Dict[int, bool] = {} count = 0 for i in range(events_size): for j in range(events[i][0], events[i][1]+1): temp = &quot;{}&quot;.format(j) if temp not in _day_map and i not in _event_map: count += 1 _day_map[temp] = True _event_map[i] = True return count 1354. Construct Target Array With Multiple Sums题面 Given an array of integers target. From a starting array, A consisting of all 1’s, you may perform the following procedure : let x be the sum of all elements currently in your array. choose index i, such that 0 &lt;= i &lt; target.size and set the value of A at index i to x. You may repeat this procedure as many times as needed.Return True if it is possible to construct the target array from A otherwise return False. 示例： 1234567Input: target = [9,3,5]Output: trueExplanation: Start with [1, 1, 1] [1, 1, 1], sum = 3 choose index 1[1, 3, 1], sum = 5 choose index 2[1, 3, 5], sum = 9 choose index 0[9, 3, 5] Done 这题算是一个数学题吧，我们首先知道数组中所有的元素的和一定大于数组中每个元素（这不是废话），然后我们假定有这样一个数组 [1,1,9,17,63] ，我们可以往回迭代上一个数组结构是 [1,1,9.17,33] ，然后我们还可以向前迭代一次 [1,1,9,17,5] 然后当前的数字已经不再是数组中最大的数字，于是我们开始寻找下一个数组中最大的数字进行迭代 这里我们也可以发现，数组中最大数字的最原始版本的值是当前数字对其余数字的和的模，于是我们就这样一直迭代就 OK 了 好了，上代码 123456789101112131415161718192021import heapqfrom typing import Listclass Solution: def isPossible(self, target: List[int]) -&gt; bool: if not target: return False total = sum(target) target = sorted([-x for x in target]) heapq.heapify(target) while True: a = -heapq.heappop(target) total -= a if a == 1 or total == 1: return True if a &lt; total or total == 0 or a % total == 0: return False a %= total total += a heapq.heappush(target, -a) 总结这次的题还是周赛的常规水平，然而我刷题实在是太少了QAQ","link":"/posts/2020/02/23/leetcode-weekly-contest-176/"},{"title":"Leetcode Weekly Contest 287 题解","text":"好久没打周赛了，打了一次周赛，简单的写个题解 2224. Minimum Number of Operations to Convert Time题面： 1234567You are given two strings current and correct representing two 24-hour times.24-hour times are formatted as &quot;HH:MM&quot;, where HH is between 00 and 23, and MM is between 00 and 59. The earliest 24-hour time is 00:00, and the latest is 23:59.In one operation you can increase the time current by 1, 5, 15, or 60 minutes. You can perform this operation any number of times.Return the minimum number of operations needed to convert current to correct. 示例： 123456789101112131415161718 Example 1:Input: current = &quot;02:30&quot;, correct = &quot;04:35&quot;Output: 3Explanation:We can convert current to correct in 3 operations as follows:- Add 60 minutes to current. current becomes &quot;03:30&quot;.- Add 60 minutes to current. current becomes &quot;04:30&quot;.- Add 5 minutes to current. current becomes &quot;04:35&quot;.It can be proven that it is not possible to convert current to correct in fewer than 3 operations.Example 2:Input: current = &quot;11:00&quot;, correct = &quot;11:01&quot;Output: 1Explanation: We only have to add one minute to current, so the minimum number of operations needed is 1. 这题没啥好说的吧，直接暴力计算时间写就行 12345678910111213141516class Solution: def convertTime(self, current: str, correct: str) -&gt; int: correct_time = correct.split(':') current_time = current.split(':') minutes = int(correct_time[1]) - int(current_time[1]) hours = int(correct_time[0]) - int(current_time[0]) if correct_time[1] &lt; current_time[1]: minutes += 60 hours -= 1 results = hours flag = [15, 5, 1] for i in flag: if minutes &gt;= i: results += (minutes // i) minutes = minutes % i return results 2225. Find Players With Zero or One Losses题面： 123456789101112You are given an integer array matches where matches[i] = [winneri, loseri] indicates that the player winneri defeated player loseri in a match.Return a list answer of size 2 where:answer[0] is a list of all players that have not lost any matches.answer[1] is a list of all players that have lost exactly one match.The values in the two lists should be returned in increasing order.Note:You should only consider the players that have played at least one match.The testcases will be generated such that no two matches will have the same outcome. 示例： 12345678910111213141516171819Example 1:Input: matches = [[1,3],[2,3],[3,6],[5,6],[5,7],[4,5],[4,8],[4,9],[10,4],[10,9]]Output: [[1,2,10],[4,5,7,8]]Explanation:Players 1, 2, and 10 have not lost any matches.Players 4, 5, 7, and 8 each have lost one match.Players 3, 6, and 9 each have lost two matches.Thus, answer[0] = [1,2,10] and answer[1] = [4,5,7,8].Example 2:Input: matches = [[2,3],[1,3],[5,4],[6,4]]Output: [[1,2,5,6],[]]Explanation:Players 1, 2, 5, and 6 have not lost any matches.Players 3 and 4 each have lost two matches.Thus, answer[0] = [1,2,5,6] and answer[1] = []. 这题实际上就遍历统计就行，时间复杂度 O(N) 空间复杂度 O(N) 1234567891011121314from collections import defaultdictfrom typing import Listclass Solution: def findWinners(self, matches: List[List[int]]) -&gt; List[List[int]]: index = defaultdict(lambda: [0, 0]) for winner, loser in matches: index[winner][0] += 1 index[loser][1] += 1 return [ sorted([k for k, v in index.items() if v[0] &gt; 0 and v[1] == 0]), sorted([k for k, v in index.items() if v[1] == 1]), ] 2226. Maximum Candies Allocated to K Children草，这题题号真有意思，尊。。。。 题面： 123456You are given a 0-indexed integer array candies. Each element in the array denotes a pile of candies of size candies[i]. You can divide each pile into any number of sub piles, but you cannot merge two piles together.You are also given an integer k. You should allocate piles of candies to k children such that each child gets the same number of candies. Each child can take at most one pile of candies and some piles of candies may go unused.Return the maximum number of candies each child can get. 示例： 1234567891011Example 1:Input: candies = [5,8,6], k = 3Output: 5Explanation: We can divide candies[1] into 2 piles of size 5 and 3, and candies[2] into 2 piles of size 5 and 1. We now have five piles of candies of sizes 5, 5, 3, 5, and 1. We can allocate the 3 piles of size 5 to 3 children. It can be proven that each child cannot receive more than 5 candies.Example 2:Input: candies = [2,5], k = 11Output: 0Explanation: There are 11 children but only 7 candies in total, so it is impossible to ensure each child receives at least one candy. Thus, each child gets no candy and the answer is 0. 这题实际上最开始没想清楚，后面仔细想了下，实际上是个二分的题目 首先假设，我们所有的糖的和为 y, 假设被 k 整除后的值是 z（含义是最大的能够整数分割的数），那么我们题目里孩子能获得的最大的糖果的数量的值域一定是 [0,z] 这个区间是具备单调性（单调递增），那么就具备了二分的条件。那么我们二分的题目是什么？假设中间值是 mid ，我们计算每推糖果能够按照 mid 分成几份并求和，如果和小于 k ，那么意味着值比我们目标值大，否则则比目标值小。持续逼近即可 12345678910111213from typing import Listclass Solution: def maximumCandies(self, candies: List[int], k: int) -&gt; int: left, right = 0, sum(candies) // k while left &lt; right: mid = (left + right + 1) // 2 if k &gt; sum(candy // mid for candy in candies): right = mid - 1 else: left = mid return left 2227. Encrypt and Decrypt Strings这题实际上比第三题简单 题面： 12345678910111213141516You are given a character array keys containing unique characters and a string array values containing strings of length 2. You are also given another string array dictionary that contains all permitted original strings after decryption. You should implement a data structure that can encrypt or decrypt a 0-indexed string.A string is encrypted with the following process:For each character c in the string, we find the index i satisfying keys[i] == c in keys.Replace c with values[i] in the string.A string is decrypted with the following process:For each substring s of length 2 occurring at an even index in the string, we find an i such that values[i] == s. If there are multiple valid i, we choose any one of them. This means a string could have multiple possible strings it can decrypt to.Replace s with keys[i] in the string.Implement the Encrypter class:Encrypter(char[] keys, String[] values, String[] dictionary) Initializes the Encrypter class with keys, values, and dictionary.String encrypt(String word1) Encrypts word1 with the encryption process described above and returns the encrypted string.int decrypt(String word2) Returns the number of possible strings word2 could decrypt to that also appear in dictionary. 示例： 1234567891011121314Input[&quot;Encrypter&quot;, &quot;encrypt&quot;, &quot;decrypt&quot;][[['a', 'b', 'c', 'd'], [&quot;ei&quot;, &quot;zf&quot;, &quot;ei&quot;, &quot;am&quot;], [&quot;abcd&quot;, &quot;acbd&quot;, &quot;adbc&quot;, &quot;badc&quot;, &quot;dacb&quot;, &quot;cadb&quot;, &quot;cbda&quot;, &quot;abad&quot;]], [&quot;abcd&quot;], [&quot;eizfeiam&quot;]]Output[null, &quot;eizfeiam&quot;, 2]ExplanationEncrypter encrypter = new Encrypter([['a', 'b', 'c', 'd'], [&quot;ei&quot;, &quot;zf&quot;, &quot;ei&quot;, &quot;am&quot;], [&quot;abcd&quot;, &quot;acbd&quot;, &quot;adbc&quot;, &quot;badc&quot;, &quot;dacb&quot;, &quot;cadb&quot;, &quot;cbda&quot;, &quot;abad&quot;]);encrypter.encrypt(&quot;abcd&quot;); // return &quot;eizfeiam&quot;. // 'a' maps to &quot;ei&quot;, 'b' maps to &quot;zf&quot;, 'c' maps to &quot;ei&quot;, and 'd' maps to &quot;am&quot;.encrypter.decrypt(&quot;eizfeiam&quot;); // return 2. // &quot;ei&quot; can map to 'a' or 'c', &quot;zf&quot; maps to 'b', and &quot;am&quot; maps to 'd'. // Thus, the possible strings after decryption are &quot;abad&quot;, &quot;cbad&quot;, &quot;abcd&quot;, and &quot;cbcd&quot;. // 2 of those strings, &quot;abad&quot; and &quot;abcd&quot;, appear in dictionary, so the answer is 2. 这题加密部分其实直接按照规则写就行了，然后解密部分有个方法就是提前将字典里面的值预计算一次，然后就能 O(1) 计算了 123456789101112131415from collections import Counterfrom typing import Listclass Encrypter: def __init__(self, keys: List[str], values: List[str], dictionary: List[str]): self.index = {k: v for k, v in zip(keys, values)} self.counter = Counter(self.encrypt(item) for item in dictionary) def encrypt(self, word1: str) -&gt; str: return &quot;&quot;.join(self.index.get(letter, &quot; &quot;) for letter in word1) def decrypt(self, word2: str) -&gt; int: return self.counter[word2]","link":"/posts/2022/04/05/leetcode-weekly-contest-287/"},{"title":"简单安利 Rime 输入法","text":"唉，最近因为气胸大过年的住院，春节颓废了好久，今天开始回北京，干脆来安利一个输入法– Rime 碎碎念如同大多数人一样，我之前也是使用搜狗输入法作为自己的主力输入法，但是搜狗输入法的一些缺陷让我放弃了使用搜狗输入法 作为传统艺能，搜狗输入法隐私保护成迷，在 MacOS 上某几个版本的搜狗在寻求获取我的通讯录和日历读取权限 作为传统艺能，搜狗输入法的广告推送实在是一言难尽，特别是在 Windows 上，已经禁了一些组件，但是还是防不胜防 因为和港澳台和国外社区朋友的交流需要，我需要输入法能够比较好的支持繁体，而搜狗输入法的繁体支持也是一言难尽 搜狗输入法的定制能力也着实不满足我的需求。。 因此我在18年开始在寻求一种开源，可控，可定制，对简/繁输入都比较友好的输入法。经过寻找之后，Rime 输入法进入了我的视线，经过一年多的使用，我觉得这个真的是一款非常棒的输入法 Rime 是什么？Rime （又名 中州韻）是一款开源的跨平台的输入法引擎，完全开源，完全可定制，你甚至可以基于 Rime 的源码，来封装一套自己的输入法引擎。同时因为 Rime 极其高的定制性，你可以基于 Rime 制作自己的输入法。 Rime 的优势主要在于通过配置文件的方式，对扩展提供了极好的支持，而且繁体支持非常棒 举个例子 在这里，「才」「纔」不一樣。还有很多的例子，大家可以自行体验。 但是 Rime 成也极高的定制性，败也极高的定制性，对于使用者而言，纯 YAML 配置文件的定制方式，准入门槛太高 让你的 Rime 更好用首先上一下我的 Rime 配置的效果 好了，我们开始来聊聊怎么安装配置 Rime Rime 基础安装没啥好说的，从官网 下载对应平台的安装包安装即可，在 MacOS 下，Rime 的配置在 ~/Library/Rime 下，大家可以用 VSCode 之类的文本编辑器打开对应的目录，进行编辑 官方并不建议直接修改原始的配置文件，因为输入法更新时会重新覆盖默认配置，可能导致某些自定义配置丢失；推荐作法是创建一系列的 patch 配置，通过类似打补丁替换这种方式来实现无感的增加自定义配置； Rime 配色Rime 的配色管理文件是 squirrel.custom.yaml，我自己使用了网友贡献的即刻黄配色 想要切换皮肤配色只需要修改 style/color_scheme 为相应的皮肤配色名称既可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051patch: app_options: &quot;com.runningwithcrayons.Alfred-3&quot;: ascii_mode: true com.google.android.studio: ascii_mode: true com.jetbrains.intellij: ascii_mode: true show_notifications_when: appropriate # 状态通知，适当(appropriate)，开（always）关（never） style: color_scheme: jike preset_color_schemes: apathy: name: &quot;冷漠 / Apathy&quot; author: &quot;LIANG Hai &quot; horizontal: true # 水平排列 inline_preedit: true #单行显示，false双行显示 candidate_format: &quot;%c\\u2005%@\\u2005&quot; # 编号 %c 和候选词 %@ 前后的空间 corner_radius: 5 #候选条圆角 border_height: 0 border_width: 0 back_color: 0xFFFFFF #候选条背景色 font_face: &quot;PingFangSC-Regular,HanaMinB&quot; #候选词字体 font_point: 16 #候选字词大小 text_color: 0x424242 #高亮选中词颜色 label_font_face: &quot;STHeitiSC-Light&quot; #候选词编号字体 label_font_point: 12 #候选编号大小 hilited_candidate_text_color: 0xEE6E00 #候选文字颜色 hilited_candidate_back_color: 0xFFF0E4 #候选文字背景色 comment_text_color: 0x999999 #拼音等提示文字颜色 jike: name: 即刻黄 author: Ryekee back_color: 0x11E4FF corner_radius: 5 #候选条圆角 border_height: 0 border_width: 0 candidate_format: &quot;%c\\u2005%@\\u2005&quot; candidate_text_color: 0x362915 comment_text_color: 0x000000 font_face: &quot;PingFangSC-Regular,HanaMinB&quot; font_point: 16 #候选字词大小 hilited_candidate_back_color: 0xF4B95F hilited_candidate_text_color: 0xFFFFFF horizontal: true inline_preedit: true label_font_face: &quot;STHeitiSC-Light&quot; label_font_point: 12 text_color: 0xFFFFFF Rime 快捷键字符在 Rime 中，可以设置一些快捷键帮助输入一些特殊字符和表情。默认自带了很多， 比如输入 /bg 会给出八卦图案的列表 比如输入 /xl 会给出希腊字符的列表 更多的快捷输入可以参看 symbols.yaml 下的列表，其中一些比较好玩的给大家看看 12345678910111213141516#月份、日期、曜日等 '/yf': [ ㋀, ㋁, ㋂, ㋃, ㋄, ㋅, ㋆, ㋇, ㋈, ㋉, ㋊, ㋋ ] '/rq': [ ㏠, ㏡, ㏢, ㏣, ㏤, ㏥, ㏦, ㏧, ㏨, ㏩, ㏪, ㏫, ㏬, ㏭, ㏮, ㏯, ㏰, ㏱, ㏲, ㏳, ㏴, ㏵, ㏶, ㏷, ㏸, ㏹, ㏺, ㏻, ㏼, ㏽, ㏾ ] '/yr': [ 月, 火, 水, 木, 金, 土, 日, ㊊, ㊋, ㊌, ㊍, ㊎, ㊏, ㊐, ㊗, ㊡, ㈪, ㈫, ㈬, ㈭, ㈮, ㈯, ㈰, ㈷, ㉁, ㉀ ]#時間 '/sj': [ ㍘, ㍙, ㍚, ㍛, ㍜, ㍝, ㍞, ㍟, ㍠, ㍡, ㍢, ㍣, ㍤, ㍥, ㍦, ㍧, ㍨, ㍩, ㍪, ㍫, ㍬, ㍭, ㍮, ㍯, ㍰ ]#天干、地支、干支 '/tg': [ 甲, 乙, 丙, 丁, 戊, 己, 庚, 辛, 壬, 癸 ] '/dz': [ 子, 丑, 寅, 卯, 辰, 巳, 午, 未, 申, 酉, 戌, 亥 ] '/gz': [ 甲子, 乙丑, 丙寅, 丁卯, 戊辰, 己巳, 庚午, 辛未, 壬申, 癸酉, 甲戌, 乙亥, 丙子, 丁丑, 戊寅, 己卯, 庚辰, 辛巳, 壬午, 癸未, 甲申, 乙酉, 丙戌, 丁亥, 戊子, 己丑, 庚寅, 辛卯, 壬辰, 癸巳, 甲午, 乙未, 丙申, 丁酉, 戊戌, 己亥, 庚子, 辛丑, 壬寅, 癸卯, 甲辰, 乙巳, 丙午, 丁未, 戊申, 己酉, 庚戌, 辛亥, 壬子, 癸丑, 甲寅, 乙卯, 丙辰, 丁巳, 戊午, 己未, 庚申, 辛酉, 壬戌, 癸亥 ]#節氣 '/jq': [ 立春, 雨水, 驚蟄, 春分, 清明, 穀雨, 立夏, 小滿, 芒種, 夏至, 小暑, 大暑, 立秋, 處暑, 白露, 秋分, 寒露, 霜降, 立冬, 小雪, 大雪, 冬至, 小寒, 大寒 ]#單位 '/dw': [ Å, ℃, ％, ‰, ‱, °, ℉, ㏃, ㏆, ㎈, ㏄, ㏅, ㎝, ㎠, ㎤, ㏈, ㎗, ㎙, ㎓, ㎬, ㏉, ㏊, ㏋, ㎐, ㏌, ㎄, ㎅, ㎉, ㎏, ㎑, ㏍, ㎘, ㎞, ㏎, ㎢, ㎦, ㎪, ㏏, ㎸, ㎾, ㏀, ㏐, ㏓, ㎧, ㎨, ㎡, ㎥, ㎃, ㏔, ㎆, ㎎, ㎒, ㏕, ㎖, ㎜, ㎟, ㎣, ㏖, ㎫, ㎳, ㎷, ㎹, ㎽, ㎿, ㏁, ㎁, ㎋, ㎚, ㎱, ㎵, ㎻, ㏘, ㎩, ㎀, ㎊, ㏗, ㏙, ㏚, ㎰, ㎴, ㎺, ㎭, ㎮, ㎯, ㏛, ㏜, ㎔, ㏝, ㎂, ㎌, ㎍, ㎕, ㎛, ㎲, ㎶, ㎼ ]#貨幣 '/hb': [ ￥, ¥, ¤, ￠, ＄, $, ￡, £, ৳, ฿, ₠, ₡, ₢, ₣, ₤, ₥, ₦, ₧, ₩, ₪, ₫, €, ₭, ₮, ₯, ₰, ₱, ₲, ₳, ₴, ₵, ₶, ₷, ₸, ₹, ₺, ₨, ﷼ ] 而我参考漠然的配置，在 luna_pinyin_simp.custom.yaml 中添加了一些配置 12345678910111213punctuator: import_preset: symbols symbols: &quot;/fs&quot;: [½,‰,¼,⅓,⅔,¾,⅒] &quot;/dq&quot;: [🌍,🌎,🌏,🌐,🌑,🌒,🌓,🌔,🌕,🌖,🌗,🌘,🌙,🌚,🌛,🌜,🌝,🌞,⭐,🌟,🌠,⛅,⚡,❄,🔥,💧,🌊] &quot;/jt&quot;: [⬆,↗,➡,↘,⬇,↙,⬅,↖,↕,↔,↩,↪,⤴,⤵,🔃,🔄,🔙,🔚,🔛,🔜,🔝] &quot;/sg&quot;: [🍇,🍈,🍉,🍊,🍋,🍌,🍍,🍎,🍏,🍐,🍑,🍒,🍓,🍅,🍆,🌽,🍄,🌰,🍞,🍖,🍗,🍔,🍟,🍕,🍳,🍲,🍱,🍘,🍙,🍚,🍛,🍜,🍝,🍠,🍢,🍣,🍤,🍥,🍡,🍦,🍧,🍨,🍩,🍪,🎂,🍰,🍫,🍬,🍭,🍮,🍯,🍼,🍵,🍶,🍷,🍸,🍹,🍺,🍻,🍴] &quot;/dw&quot;: [🙈,🙉,🙊,🐵,🐒,🐶,🐕,🐩,🐺,🐱,😺,😸,😹,😻,😼,😽,🙀,😿,😾,🐈,🐯,🐅,🐆,🐴,🐎,🐮,🐂,🐃,🐄,🐷,🐖,🐗,🐽,🐏,🐑,🐐,🐪,🐫,🐘,🐭,🐁,🐀,🐹,🐰,🐇,🐻,🐨,🐼,🐾,🐔,🐓,🐣,🐤,🐥,🐦,🐧,🐸,🐊,🐢,🐍,🐲,🐉,🐳,🐋,🐬,🐟,🐠,🐡,🐙,🐚,🐌,🐛,🐜,🐝,🐞,🦋] &quot;/bq&quot;: [😀,😁,😂,😃,😄,😅,😆,😉,😊,😋,😎,😍,😘,😗,😙,😚,😇,😐,😑,😶,😏,😣,😥,😮,😯,😪,😫,😴,😌,😛,😜,😝,😒,😓,😔,😕,😲,😷,😖,😞,😟,😤,😢,😭,😦,😧,😨,😬,😰,😱,😳,😵,😡,😠] &quot;/ss&quot;: [💪,👈,👉,👆,👇,✋,👌,👍,👎,✊,👊,👋,👏,👐] &quot;/dn&quot;: [⌘, ⌥, ⇧, ⌃, ⎋, ⇪, , ⌫, ⌦, ↩︎, ⏎, ↑, ↓, ←, →, ↖, ↘, ⇟, ⇞] &quot;/fh&quot;: [©,®,℗,ⓘ,℠,™,℡,␡,♂,♀,☉,☊,☋,☌,☍,☑︎,☒,☜,☝,☞,☟,✎,✄,♻,⚐,⚑,⚠] &quot;/xh&quot;: [＊,×,✱,★,☆,✩,✧,❋,❊,❉,❈,❅,✿,✲] 设置输入法大家可以在 default.custom.yaml 中设置自己喜欢的输入法，我目前使用的是明月拼音，默认切换输入法的快捷键是 Ctrl+~ 但是因为这个快捷键和 VSCode 快捷键冲突，所以我将其改为 Ctrl+Shift+F12 1234567patch: menu: page_size: 8 schema_list: - schema: luna_pinyin_simp # 朙月拼音 简化字 &quot;switcher/hotkeys&quot;: - &quot;Control+Shift+F12&quot; 调教词库这里引用漠然的讲解： Rime 默认的词库稍为有点弱，我们可以下载一些搜狗词库来进行扩展；不过搜狗词库格式默认是无法解析的，好在有人开发了工具可以方便的将搜狗细胞词库转化为 Rime 的格式(工具点击这里下载)；目前该工具只支持 Windows(也有些别人写的 py 脚本啥的，但是我没用)，所以词库转换这种操作还得需要一个 Windows 虚拟机；转换过程很简单，先从搜狗词库下载一系列的 scel 文件，然后批量选中，接着调整一下输入和输出格式点击转换，最后保存成一个 txt 文本光有这个文本还不够，我们要将它塞到词库的 yaml 配置里，所以新建一个词库配置文件 luna_pinyin.sougou.dict.yaml，然后写上头部说明(注意最后三个点后面加一个换行) 123456789101112# Rime dictionary# encoding: utf-8# 搜狗词库 目前包含如下:# IT计算机 实用IT词汇 亲戚称呼 化学品名 数字时间 数学词汇 淘宝词库 编程语言 软件专业 颜色名称 程序猿词库 开发专用词库 搜狗标准词库# 摄影专业名词 计算机专业词库 计算机词汇大全 保险词汇 最详细的全国地名大全 饮食大全 常见花卉名称 房地产词汇大全 中国传统节日大全 财经金融词汇大全---name: luna_pinyin.sougouversion: &quot;1.0&quot;sort: by_weightuse_preset_vocabulary: true... 接着只需要把生成好的词库 txt 文件内容粘贴到三个点下面既可；但是词库太多的话你会发现这个文本有好几十 M，一般编辑器打开都会卡死，解决这种情况只需要用命令行 cat 一下就行 1cat sougou.txt &gt;&gt; luna_pinyin.sougou.dict.yaml 最后修改 luna_pinyin.extended.dict.yaml 中的 import_tables 字段，加入刚刚新建的词库既可 123456789101112131415---name: luna_pinyin.extendedversion: &quot;2016.06.26&quot;sort: by_weight #字典初始排序，可選original或by_weightuse_preset_vocabulary: true#此處爲明月拼音擴充詞庫（基本）默認鏈接載入的詞庫，有朙月拼音官方詞庫、明月拼音擴充詞庫（漢語大詞典）、明月拼音擴充詞庫（詩詞）、明月拼音擴充詞庫（含西文的詞彙）。如果不需要加載某个詞庫請將其用「#」註釋掉。#雙拼不支持 luna_pinyin.cn_en 詞庫，請用戶手動禁用。import_tables: - luna_pinyin # 加入搜狗词库 - luna_pinyin.sougou - luna_pinyin.poetry - luna_pinyin.cn_en - luna_pinyin.kaomoji 在我的配置中，我加入了来自搜狗的医学，古诗词，军事等词库（逃 快捷键设置这里参考了 Rime 作者的一个 Gist 对快捷键做了一些配置 12345678ascii_composer/good_old_caps_lock: trueascii_composer/switch_key: Caps_Lock: commit_code Control_L: noop Control_R: noop # 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态 Shift_L: commit_code Shift_R: noop 总结经过这一系列折腾下来，我们 Rime 应该就能满足我们日常的使用了，文中的配置都可以直接用我放在 GitHub 上的配置实现开箱即用 RimeConfig 可能有人想问，为什么对于一个输入法都需要这么多的时间进行调教？是这样，我觉得对于一些关系我们日常使用的基础工具，花一定量的时间去寻找合适自己，并且将其按照的自己的需求进行调教，是一件非常有意义的事。在后续的工作生活学习中，这也将极大的提升我们的幸福感与效率 嗯差不多这样吧，新年第一篇文章，祝大家新年快乐！","link":"/posts/2020/01/28/simple-config-for-rime-input/"},{"title":"简单聊聊 SQL 中的 Prepared Statements","text":"好久没写文章了，新年还是得写点技术水文来保证下状态，正好最近遇到一个比较有意思的问题，就来简单聊聊一下关于 MySQL 中 Prepared Statements 吧 开始gorm 是大家在使用 Go 开发时的比较常用的 ORM 了，最近在使用 gORM 的时候遇到一个很有意思的问题。首先我大概描述一下这个问题 在使用 gORM 的 Raw 方法进行 SQL 查询时，构造了如下类似的 SQL 1select * from demo where match(name) AGAINST('+?' IN BOOLEAN MODE) 在随后传入参数的时候，返回 Error : sql: expected 0 arguments, got 1。而其余的诸如如下的查询就正常执行 1select * from demo where name = ? 最开始我以为这是 gORM 中拼接 SQL 模块的问题，但是看了下代码后发现一个很有趣的逻辑。gORM 中并没有拼接 Raw SQL 的相关逻辑，它会直接调用 Golang 中的标准库 database/sql 来进行 SQL 的处理，而 database/sql 将会直接调用对应数据库驱动的实现，我们先来看看在 databse/sql 中关于 Query 的逻辑。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func (db *DB) queryDC(ctx, txctx context.Context, dc *driverConn, releaseConn func(error), query string, args []interface{}) (*Rows, error) { queryerCtx, ok := dc.ci.(driver.QueryerContext) var queryer driver.Queryer if !ok { queryer, ok = dc.ci.(driver.Queryer) } if ok { var nvdargs []driver.NamedValue var rowsi driver.Rows var err error withLock(dc, func() { nvdargs, err = driverArgsConnLocked(dc.ci, nil, args) if err != nil { return } rowsi, err = ctxDriverQuery(ctx, queryerCtx, queryer, query, nvdargs) }) if err != driver.ErrSkip { if err != nil { releaseConn(err) return nil, err } // Note: ownership of dc passes to the *Rows, to be freed // with releaseConn. rows := &amp;Rows{ dc: dc, releaseConn: releaseConn, rowsi: rowsi, } rows.initContextClose(ctx, txctx) return rows, nil } } var si driver.Stmt var err error withLock(dc, func() { // 比较有意思的地方 si, err = ctxDriverPrepare(ctx, dc.ci, query) }) if err != nil { releaseConn(err) return nil, err } ds := &amp;driverStmt{Locker: dc, si: si} rowsi, err := rowsiFromStatement(ctx, dc.ci, ds, args...) if err != nil { ds.Close() releaseConn(err) return nil, err } // Note: ownership of ci passes to the *Rows, to be freed // with releaseConn. rows := &amp;Rows{ dc: dc, releaseConn: releaseConn, rowsi: rowsi, closeStmt: ds, } rows.initContextClose(ctx, txctx) return rows, nil}} 在 database/sql 执行 QueryDC 逻辑时，会调用 ctxDriverPrepare 方法来进行 SQL Query 的预处理，我们来看看这段逻辑 123456789101112131415func ctxDriverPrepare(ctx context.Context, ci driver.Conn, query string) (driver.Stmt, error) { if ciCtx, is := ci.(driver.ConnPrepareContext); is { return ciCtx.PrepareContext(ctx, query) } si, err := ci.Prepare(query) if err == nil { select { default: case &lt;-ctx.Done(): si.Close() return nil, ctx.Err() } } return si, err} 在其中，ctxDriverPrepare 会调用 ci.Prepare(query) 来执行对应 SQL Driver 实现的 Prepare 或者 PrepareContext 方法来对 SQL 预处理，在 go-mysql-driver 中，对应的实现是这样 12345678910111213141516171819func (mc *mysqlConn) PrepareContext(ctx context.Context, query string) (driver.Stmt, error) { if err := mc.watchCancel(ctx); err != nil { return nil, err } stmt, err := mc.Prepare(query) mc.finish() if err != nil { return nil, err } select { default: case &lt;-ctx.Done(): stmt.Close() return nil, ctx.Err() } return stmt, nil} 这一段的逻辑是 go-mysql-driver 会向 MySQL 发起 prepared statement 请求，获取到对应的 Stmt 后将其返回 在 stmt 中包含了对应的参数数量，stmt name 等信息。在这里，SQL 会将 ? 等参数占位符进行解析，并告知客户端需要传入的参数数量 问题也出在这里，我们重新看一下之前的 SQL 1select * from demo where match(name) AGAINST('+?' IN BOOLEAN MODE) 在这里，我使用了 MySQL 5.7 后支持的 Full Text Match ，在这里，我们待匹配的字符串 +? 会被 MySQL 解析成为一个待查询的字符串，而不会作为占位符进行解析，那么返回 stmt 中，需要传入的参数数量为0，而 database/sql 会在后续的逻辑中对我们传入的参数和需要传入的参数数量进行匹配，如果不一致则会抛出 Error 。 好了，问题找到了，那么 Prepared Statement 究竟是什么东西，而我们为什么又需要这个？ Prepared Statement什么是 Prepared Statement？其实大致的内容前面已经聊的比较清楚了，我们来重新复习下：Prepared Statement 是一种 MySQL（其余的诸如 PGSQL 也有类似的东西）的机制，用于预处理 SQL，将 SQL 和查询数据分离，以期保证程序的健壮性。 在 MySQL 官方的介绍中，Prepared Statement 有如下的好处 Less overhead for parsing the statement each time it is executed. Typically, database applications process large volumes of almost-identical statements, with only changes to literal or variable values in clauses such as WHERE for queries and deletes, SET for updates, and VALUES for inserts. Protection against SQL injection attacks. The parameter values can contain unescaped SQL quote and delimiter characters. 简而言之是： 提升性能，避免重复解析 SQL 带来的开销 避免 SQL 注入 MySQL 的 Prepared Statement 有两种使用方式，一种是使用二进制的 Prepared Protocol（这个不在今天的文章的范围内，改天再写篇文章来聊聊 MySQL 中的一些二进制协议） ，一种是使用 SQL 进行处理 在 Prepared Statement 中有着三种命令 PREPARE 用于创建一个 Prepared Statement EXECUTE 用于执行一个 Prepared Statement DEALLOCATE PREPARE 用于销毁一个 Prepared Statement 这里需要注意一点的是，Prepared Statement 存在 Session 限制，一般情况下一个 Prepared Statement 仅存活于它被创建的 Session 。当连接断开，者在其余情况下 Session 失效的时候，Prepared Statement 会自动被销毁。 接下来，我们来动手实验下 怎么使用 Prepared Statement首先我们先创建一个 测试表 1234567create table if not exists `user`( `id` bigint(20) not null auto_increment, `name` varchar(255) not null, primary key (`id`)) engine = InnoDB charset = 'utf8mb4'; 然后插入数据 1insert into user (`name`) values ('abc'); 好了，我们先按照传统的方式进行查询下 123select *from userwhere name = 'abc'; 好了，我们现在来使用 Prepared Statement 首先使用 Prepared 关键字创建一个 statement 123set @s = 'select * from user where name=?';PREPARE demo1 from @s; 然后使用 Execute 关键字来执行 Statement 123set @a = 'abc';EXECUTE demo1 using @a; 嗯，还是很简单的对吧 为什么要使用 Prepared Statement？其中一个很重要的理由是可以避免 SQL Injection Attack （SQL 注入）的情况出现，而问题在于，为什么 Prepared Statement 能够避免 SQL 注入？ 其实很简单，我们将 Query 和 Data 进行了分离 还是以之前的表作为例子 在没有手动处理 SQL 和 参数的情况下，我们往往使用字符串拼接，那么这样会利用 SQL 语法来构造一些非法 SQL，以 Python 为例 12b = &quot;'abc';drop table user&quot;a = f&quot;select * from user where name={b}&quot; 那么这样一段代码将会生成这样的 SQL 1select * from user where name='abc';drop table user 嗯，，，，数据库从入门到删表跑路.pdf 那么，我们来使用 Prepared Statement 来看看 123set @a = '\\'abc\\';drop table user';EXECUTE demo1 using @a; 然后我们最后执行的语句是 1select * from user where name='\\'abc\\';drop table user' 因为我们将 Query 与 Query Params 在结构上进行了区分，这个时候我们无论输入什么，都会将其作为 Query Params 的一部分进行处理，从而避免了注入的风险 Prepared Statement 的优劣好处显而易见 因为数据库会对 Prepared Statement 进行缓存，从而免去了客户端重复处理 SQL 带来的开销 避免 SQL Injection Attack 语义清楚 缺点也有不少 Prepared Statement 的二进制协议存在客户端兼容的问题，有些语言的客户端不一定会对 Prepared Statement 提供二进制的协议支持 因为存在两次与数据库的通信，在密集进行 SQL 查询的情况下，可能会出现 I/O 瓶颈 所以具体还是要根据场景来做 Trade-off 了 碎碎念飞机上写下这篇文章算是作为新年的一个新开始吧，争取多写文章，规范作息，好好照顾女朋友。对了，通过这段时间的一些折腾（比如解析 Binlog 之类的），突然发现 MySQL 是个宝库，后面会写几篇文章来聊聊踩坑 MySQL 中的 Binlog 和 Protocol 中的一些坑和好玩的地方（嗯 Flag ++，千万别催稿（逃 好了，今晚就先这样，飞机要落地了，我先关电脑了（逃","link":"/posts/2020/01/05/simple-introdution-about-sql-prepared/"},{"title":"年轻人第一台 Mac，来自一个开发者的 Macbook Pro 2019 16寸简评","text":"从工作开始，一直就想买个 Mac，但是一直没有买成，虽说有公司配发的 Mac（这也让我从 Macbook Pro 2015 13寸到 Macbook Pro 2017 15寸，到 Macbook Pro 2017 15寸，到 Macbook Pro 2018 13寸，到 Macbook Pro 2018 15寸用了个遍，23333），但是没有自己的 Mac始终是一个比较遗憾的事，所以这次新款 Mac 出来后，就瞬时公司员工优惠（官网95折）+12期免息分期入手了，现在我来从一个开发者的角度来给一个简单的评测吧 正文为什么会考虑 Mac写这个文章之前，我需要介绍下我买 Mac 的背景。目前我的主力机是来自蓝天的准系统，P775TM，配置是 i7 8700+32G+512G SSD+1070。工作系统是 Manjaro（一个 Linux 发型版）。在目前开发的时候觉得非常舒服，但是我也遇到了几个问题 太重了，有些时候临时有事需要出门带着并不方便 Linux 下日常软件的缺少还是会给开发带来一些不便 所以在11月之后，我就在慎重考虑需要换一台电脑，而我自己是偏基础设施的后端开发者，所以对 Unix/类 Unix 比较好的支持是必须的。同时因为有些时候会参与一些大型的开源项目，我也需要电脑有足够的性能来支撑多虚拟机以提供多平台的调试能力。因此当时摆在我面前的有两个选择 买一个硬件友好的机器，装黑苹果和 Linux 双系统 买一个 Macbook Pro 由于我算是有点版权洁癖，黑苹果严格意义上来讲是违规的。因此 Macbook Pro 是我最好的选择了，恰逢新款的 16 寸的 Macbook Pro 上市，我就决定入手 我最后选择的配置是 i9 2.3 Ghz + 64G RAM + 1T SSD + 5500M 4G。选择这样的配置的逻辑是这样的，因为根据目前的评测，这一款 Mac 散热表现不错。因为我日常会在本地编译调试东西，所以果断选择了 i9，1T SSD 则是默认标配。而我不是视频工作者，同时我也不会在 Mac 上玩游戏， 所以显存 4G 版显卡对我来说完全够用。唯一的纠结点在于 RAM，16G 肯定不够用，到底是 32G 还是 64G 这是个问题。考虑了下，咬咬牙上了 64G （后面也说明这是一个正确的决策） 定了之后果断分期下单，18号下单，23号拿到机器（这里要感谢女朋友大力支持，要不是她，我估计最后也舍不得买）。 好了，开始进入开箱，评测环节 开箱23号一早跑去公司拿了快递，然后回家开箱 献祭一只猫后，打开快递箱 苹果的包装一如既往的简洁 下面几张图，是真机原貌 机器到手默认是 MacOS 10.15.1 即 MacOS Catalina 。不得不说，这一代 MacOS 与 iOS 都是 Bug 奇多，堪称 BugOS = =，让人怀疑果家的项目管理是不是彻底失效了 至于机器外观，我选择的是银色版，不得不说，颜值是真高，但是也容易脏qaq，可能也有朋友关系 Macbook Pro 16 寸有多大，下面做了一个图大家可以感受一下 从上往下分别是： 女朋友的 Macbook Air 公司配发的 Macbook Pro 2018 15 寸（后面还会有他的细分） Macbook Pro 16 寸 蓝天 P775TM 从这样一个角度大家能看出来，其实新款的 Macbook Pro 16 寸比 Macbook Pro 15 寸只大出一点点，而在新款出来后，官网也下架了 Macbook Pro 15 寸，所以看起来以后 Macbook Pro 16 寸就是 15 寸的替代品 使用体验其实到手时间还相对较短，比较深的体验暂时没有2333，这一部分可能就相对较短了 屏幕是保持了苹果一如既往的水准，很舒服，不过这一代默认显示分辨率是 1792x1120， 较 15 寸的 1680x1050 更大，很多人可能会觉得字体比较小，需要额外调整（此处推荐 RDM 键盘终于舒服了，蝶式键盘对于我来说有点敲钢板的感觉（不过各有所爱啦） ESC 键的恢复对于 VIM 党来说是重大利好（逃（然而我不是（逃x2，2333333 性能测试首先，我们来看下新款 Mac 的 CPU 测试成绩 不过，说实话，我觉得利用日常的一些场景来做测试可能更有价值，所以我选取了两个项目来做编译测试，一个是 Dubbo （基于 Dubbo 2.7.4.1 进行编译, Maven 3.6.2, JDK 8),另外一个是基于 CPython Master 最新代码进行编译，下面是编译命令 Dubbo: mvn clean package -U -Dmaven.test.skip=true (两台机器都已经搞定依赖) CPython: ./configure --with-pydebug --with-openssl=/usr/local/opt/openssl &amp;&amp; make -j 两台机器分别为 最后的测试结果如下图所示（单位都为秒） 看起来性能提升还是很明显，不过因为编译涉及到频繁的小文件读写，所以导致差距没有理论上的那么大，但是随着代码规模的扩大，i9 的优势会更为明显。后续有时间我会找几个科学计算的例子来进行场景补充 说道编译，大家可能也关心新款的 Mac 的散热怎么样，我大概测试了一下， 结论为，降频无法避免，但是满负载的时候，能在81度左右的温度，将 CPU 频率稳定在 3.5 Ghz。虽然这个结果还是没有其余的高性能本那么显眼，但是对于一个轻薄高性能本来说，我觉得还算不错，毕竟凡事总得做一个 trade-off 对吧 最后，测一下硬盘速度 嗯，还是一如既往的暴力。。。 啊对了，其实有朋友可能会关心，64G 的内存是否浪费，嗯，我看了下，其实对我而言，不算浪费 这是我写这篇文章时候的内存使用率，机器上开了三个 IDEA 项目，两个 Goland 项目，一个 PyCharm 项目，浏览器窗口若干，VSCode 窗口若干，用 Docker 跑了三个 ElasticSearch 节点做 HA 测试，一个 Kibana 节点，如果后续在开几个虚拟机，可能 64G 对我来说就是非常适合的了。不过每个人的场景不一样，这里我也就不对大家的选机做一个建议了（不然选错了可能就要被打QAQ） 总结其实 Macbook Pro 是个水桶机，整体配置相对均衡，适合大多数场景（需要 Office 的除外），而且今年的官方也提供了更为灵活的配置选项让大家来进行组合。所以我建议如果对于大尺寸 Mac 有需求的朋友，其实真的可以考虑入手 千言万语汇成一句话： 最后再次感谢女朋友对我的支持！mua！","link":"/posts/2019/11/25/simple-test-about-new-macbook-pro/"},{"title":"asyncio 笔记","text":"来源 annotated-py-asyncio 阅读补充:1. 基本概念:1.1 协程: “协程 是为非抢占式多任务产生子程序的计算机程序组件，协程允许不同入口点在不同位置暂停或开始执行程序”。 从技术的角度来说，“协程就是你可以暂停执行的函数”。 如果你把它理解成“就像生成器一样”，那么你就想对了。 1.2 事件循环: 事件循环 “是一种等待程序分配事件或消息的编程架构”。 基本上来说事件循环就是，“当A发生时，执行B”。 或许最简单的例子来解释这一概念就是用每个浏览器中都存在的JavaScript事件循环。 当你点击了某个东西（“当A发生时”），这一点击动作会发送给JavaScript的事件循环，并检查是否存在注册过的 onclick 回调来处理这一点击（“执行B”）。 只要有注册过的回调函数就会伴随点击动作的细节信息被执行。 事件循环被认为是一种循环是因为它不停地收集事件并通过循环来发如何应对这些事件。 1.3 Python 的事件循环: 对 Python 来说，用来提供事件循环的 asyncio 被加入标准库中。 asyncio 重点解决网络服务中的问题，事件循环在这里将来自套接字（socket）的 I/O 已经准备好读和/或写作为“当A发生时”（通过selectors模块）。 除了 GUI 和 I/O，事件循环也经常用于在别的线程或子进程中执行代码，并将事件循环作为调节机制（例如，合作式多任务）。 如果你恰好理解 Python 的 GIL，事件循环对于需要释放 GIL 的地方很有用。 事件循环提供一种循环机制，让你可以“在A发生时，执行B”。 基本上来说事件循环就是监听当有什么发生时，同时事件循环也关心这件事并执行相应的代码。 Python 3.4 以后通过标准库 asyncio 获得了事件循环的特性。 1.4 async, await: 将 async/await 看做异步编程的 API 基本上 async 和 await 产生神奇的生成器，我们称之为协程， 同时需要一些额外的支持例如 awaitable 对象以及将普通生成器转化为协程。 所有这些加到一起来支持并发，这样才使得 Python 更好地支持异步编程。 相比类似功能的线程，这是一个更妙也更简单的方法。 在 Python 3.4 中，用于异步编程并被标记为协程的函数看起来是这样的： 12345# This also works in Python 3.5.@asyncio.coroutinedef py34_coro(): yield from stuff() Python 3.5 添加了types.coroutine 修饰器，也可以像 asyncio.coroutine 一样将生成器标记为协程。你可以用 async def 来定义一个协程函数，虽然这个函数不能包含任何形式的 yield 语句；只有 return 和 await 可以从协程中返回值。 12async def py35_coro(): await stuff() 你将发现不仅仅是 async，Python 3.5 还引入 await 表达式（只能用于async def中）。虽然await的使用和yield from很像，但await可以接受的对象却是不同的。await 当然可以接受协程，因为协程的概念是所有这一切的基础。但是当你使用 await 时，其接受的对象必须是awaitable 对象：必须是定义了await()方法且这一方法必须返回一个不是协程的迭代器。协程本身也被认为是 awaitable 对象（这也是collections.abc.Coroutine 继承 collections.abc.Awaitable的原因）。这一定义遵循 Python 将大部分语法结构在底层转化成方法调用的传统，就像 a + b 实际上是a.add(b) 或者 b.radd(a)。 为什么基于async的协程和基于生成器的协程会在对应的暂停表达式上面有所不同？主要原因是出于最优化Python性能的考虑，确保你不会将刚好有同样API的不同对象混为一谈。由于生成器默认实现协程的API，因此很有可能在你希望用协程的时候错用了一个生成器。而由于并不是所有的生成器都可以用在基于协程的控制流中，你需要避免错误地使用生成器。 用async def可以定义得到协程。定义协程的另一种方式是通过types.coroutine修饰器 – 从技术实现的角度来说就是添加了 CO_ITERABLE_COROUTINE标记 – 或者是collections.abc.Coroutine的子类。你只能通过基于生成器的定义来实现协程的暂停。 awaitable 对象要么是一个协程要么是一个定义了await()方法的对象 – 也就是collections.abc.Awaitable – 且await()必须返回一个不是协程的迭代器。 await表达式基本上与 yield from 相同但只能接受awaitable对象（普通迭代器不行）。async定义的函数要么包含return语句 – 包括所有Python函数缺省的return None – 和/或者 await表达式（yield表达式不行）。async函数的限制确保你不会将基于生成器的协程与普通的生成器混合使用，因为对这两种生成器的期望是非常不同的。 1.5 关于 python 协程 和 golang 的对比讨论: From Python to Go and Back Again PPT 关于此PPT 的观点: go 比 pypy 性能高不了多少, 但是复杂度和调试难度增加很高 结尾鼓吹 rust. 异步库参考: hyper curio 将asyncio看作是一个利用async/await API 进行异步编程的框架 David 将 async/await 看作是异步编程的API创建了 curio 项目来实现他自己的事件循环。 允许像 curio 一样的项目不仅可以在较低层面上拥有不同的操作方式 （例如 asyncio 利用 future 对象作为与事件循环交流的 API，而 curio 用的是元组） 2. 源码模块:2.1 (futures.py)[./futures.py]2.1.1 参考: Python 3.5 协程究竟是个啥 译: Python 3.5 协程究竟是个啥 译: github 译: 博客 Python 协程：从 yield/send 到 async/await future 源码剖析 concurrent.futures 源码阅读笔记（Python） concurrent.futures 是一个异步库 concurrent.futures — Asynchronous computation 2.1.2 生成器（Generator）VS 迭代器（iterator）: improve-your-python-yield-and-generators-explained 译文:提高你的Python: 解释‘yield’和‘Generators（生成器）’ 在Python之外，最简单的生成器应该是被称为协程（coroutines）的东西。 generator是用来产生一系列值的 yield则像是generator函数的返回结果 yield唯一所做的另一件事就是保存一个generator函数的状态 generator就是一个特殊类型的迭代器（iterator） 和迭代器相似，我们可以通过使用next()来从generator中获取下一个值 通过隐式地调用next()来忽略一些值 生成器与迭代器的关系 生成器(generator)是一个特殊的迭代器，它的实现更简单优雅。yield 是生成器实现 next() 方法的关键 python黑魔法—迭代器（iterator） 生成器","link":"/posts/2017/06/07/some-note-for-asyncio/"},{"title":"关于 Kubernetes 和容器化的一些随想","text":"这段时间在不少群里争论过关于 Kubernetes 和容器化的一些事，干脆总结下一些碎碎念作为一个概括吧。本文仅代表个人立场，不代表商业观点 容器化目前很主流的一个观点，是能上容器尽可能上容器，说实话这个想法实际上是有一定的合理性的，去 review 这个想法，我们需要去看一下容器这个东西，给我们带来了什么样的改变 容器首先毫无疑问，会给我们带来非常多的好处： 真正意义上让开发与生产环境保持一致是一种非常方便的事，换句话说，开发说的“这个服务在我本地没啥问题”是一句有用的话了 让部署一些服务变的更为方便，无论是分发，还是部署， 能做到一定程度上的资源隔离与分配 那么，看起来我们是不是可以无脑用容器？不，不是，我们需要再来 Review 一下，容器化后我们所要面临的一些弊端： 容器安全性问题，目前最主流的容器实现（此处点名 Docker）本质上而言还是基于 CGroups + NS 来进行资源与进程隔离。那么其安全性将会是一个非常值得考量的问题。毕竟 Docker 越权与逃逸漏洞年年有，年年新。那么这意味着我们是需要去有一个系统的机制去规范我们容器的使用，来保证相关的越权点能被把控在一个可控的范围内。而另一个方向是镜像安全问题，大家都是面向百度/CSDN/Google/Stackoverflow 编(fu)程(zhi)选手，那么势必会出现一个情况，当我们遇到一个问题，搜索一番，直接复制点 Dockerfile 下来，这个时候，将会存在很大的风险点，毕竟谁也不知道 base image 里加了啥料不是？ 容器的网络问题。当我们启动若干个镜像后，那么容器之间的网络互通怎么处理？而大家生产环境，肯定不止一个机器那么少，那么跨主机的情况下，怎么样去进行容器间的通信，同时保证网络的稳定性？ 容器的调度与运维的问题，当我一个机器高负载的时候，怎么样去将该机器上的一些容器调度到其余的机器上？而怎么样去探知一个容器是否存活？如果一个容器 crash 了，怎么样重新拉起？ 容器具体的细节问题，比如镜像怎么样构建与打包？怎么样上传？乃至说怎么样去排查一些 corner case 的问题？ 我们做一个业务决策的时候，我们肯定不会是因为某个技术够先进，够舒服，而是需要去衡量这个业务决策的 ROI，同时在利弊之间做一个 Trade-Off，用容器化这件事来说吧，我们来思考下我们可能迁移容器常见的几个误区： 我们想对利用容器做资源隔离！那么问题来了，用 systemd + cgroup 这样简便的方法做和容器之间有什么区别？是容器的成本更低？ 我们想践行 Devops 所以想上容器化！实际上 Devops 和容器化关联并不算大，它更多的是一种方法论，一个团队之间内部协作的一套方法论。不精确的来讲，是通过自动化，流程改进，SOP 引入等手段，将一套服务的分发与运维更为简便化。换句话说，在我们去践行 Devops 这一套方法论的时候，实际上不是一个技术问题，而是一个制度问题（讲个笑话，Devops 的开发不需要写脚本）。在其中，无论是我们传统的 Ansible 等运维手段，还是一些自动化测试的方法与框架，都可以成为 Devops 的一部分。那么这里还是一个问题，我们为什么要用容器？是因为传统的工具践行 Devops 的 cost 远高于用容器化的？ 从这两个例子大家能看出来，当我们去做容器化这件事的时候，一定要思考的问题是，容器化是真正解决了我们什么痛点，还是只是因为它看起来够先进，够屌，能为我简历背书？ Kubernetes前面聊到容器化的几个问题，促成了以 Kubernetes 为代表的容器编排体系的诞生。大家在想，哇，既然解决了这个问题，那么我们再来聊聊这个问题 首先我已经忽略掉自建 Kubernetes 集群的场景了，因为那不是一般人能 Hold 住的。那么我们来看一下，依托公有云使用的情况吧，以阿里云为例，点开页面，然后我们见到这样张图 好了，提问： VPC 是什么？ Kubernetes 1.16.9 和 1.14.8 有什么区别 Docker 19.03.5 和阿里云安全沙箱 1.1.0 是什么，有什么区别 专有网络是什么？ 虚拟交换机是什么？ 网络插件是什么？Flannel 和 Terway 又是什么？有什么区别？当你翻了翻文档，然后文档告诉你，Terway 是阿里云基于 Calico 魔改的 CNI 插件。那么 CNI 插件是什么？Calico 是什么？ Pod CIDR 是什么怎么设？ Service CIDR 是什么怎么设？ SNAT 是什么怎么设？ 安全组怎么配置？ Kube-Proxy 是什么？iptables 和 IPVS 有什么区别？怎么选？ 是不是和你想象的一键点点点有很大区别？你可能说，我们小公司不管这些，暴力出奇迹，一键全默认。。。。emmmm，那上什么 Kubernetes 啊。。好了，假设你上了后，来，我们继续算账 你得有个镜像仓库吧，不贵，中国区基础版780一个月 你集群内的服务需要暴露出去用吧？行叭，买个最低规格的 SLB，简约型，每个月200 好了，你每个月日志得花钱吧？假设你每个月20G日志，不多吧？行，39.1 你集群监控要不要？好，买，每天50w条日志上报吧？行，不贵，975 一个月 算一下，一个集群吧，(780+200+39.1+975)*12=23292.2 不算集群基础的 ENI，ECS 等费用，美滋滋 而且会衍生很多其余的问题，具体的话，大家可以去 Kubernetes 的 Issue 区看一下盛况 总结写这个文章，并不为吐槽或者喷人，只是想表明一个观点，借用我比较喜欢的一篇文章中台，我信了你的邪 | 深氪 中的一句话 到了去年底，阿里巴巴董事长兼CEO张勇在湖畔大学分享时也说：如果一个企业奔着中台做中台，就是死。 逍遥子是不是说过这句话待考，但我很赞同，同时我认为一个企业奔着技术先进性去搞技术，就是死 ，毕竟技术是需要为业务服务的，而技术的进步很大程度上依赖业务的沉淀与需求 好了，这应该是我写过最水的文章了，先这样吧。继续搬砖了","link":"/posts/2020/06/29/some-tips-about-kubernetes-and-container/"},{"title":"关于我自己被性侵经历的访谈记录","text":"这篇文章是我 2020 年 12 月接受华中师范大学关于儿童性侵的采访所产生采访稿。在这次采访中，我完整的复盘了在我12岁那年发生在我身上的强奸事件。在这次采访中，我完整回顾了当时我和我家庭的一些反应，也表达了我一些关于性侵这件事的看法。我希望每个人都能平安顺利的过完一生，但是如果有不好的事情发生的时候，我希望这篇文章能帮到你。Everything is gonna be OK。 采访稿 时间标签 说话人 逐字稿 备忘录 访谈对象： Hello，你好。 访谈者： Hi，你好啊。好，那在开始之前我再说一下我们的知情同意书吧，就是前面也发给你过。 访谈对象： 哦，对，我看了，啊，签名我忘了签了。 访谈者： 没事，等一下发给我也行。知情同意书主要包括研究目标：就是聚焦于探索儿童期性创伤经历者的创伤应对和表露过程。然后你说的话对于我来说，都很重要，所以需要录音，我们的访谈大概持续是三十到六十分钟。如果说这次访谈中断或者没有完成的话，可以协商下一次。参与这个研究，你随时都有权利选择退出，访谈过程中或遇到一些你不想回答的问题，你都可以选择拒绝回答。 访谈对象： 嗯，没问题没问题。 访谈者： 还有一些小小的风险，就是可能会谈到一些触动你情绪的这件事的时候，可能就会引起你的情绪有些波动。 访谈对象： 嗯，没事儿，快乐水我已经准备好了（笑）。 访谈者： 嗯，好，但是呢我会就是照顾到你的情绪，因为我是有做过心理咨询的经验的。嗯，然后还有就是你的个人隐私处理问题。首先就是这个录音我会给它变进行一个变音，我处理好逐字稿之后就会把它删除。然后还有就是你的一些能够辨认出你个人信息的内容，我会进行一个匿名化的处理。 访谈对象： 之后你生成的就说是paper，我能够拿到一份拷贝吗？ 访谈者： 嗯，你是说论文吗？可以。 访谈对象： 对，是的是的是的，没错，就是你的相关研究结果。因为我其实对这个事情很好奇，因为我我其实一直是在做性侵这方面公益的。但是据我所知的话，其实国内这方面对于专门性侵受害者的研究，其实我觉得好像还是一个很偏门的领域。我其实挺好奇的。对，因为当时你在这个过程中说的时候，我其实也挺好奇的你为啥会选择这个方向。 访谈者： 这个通过我看论文的时候我就发现了，因为国内真的很少，就是比较权威的论文，真的非常少。我参考的文献全部都是英文的，就看论文的时候还是挺痛苦的。 访谈对象： 就是因为我自己亲身经历，还有我去跟就是给别人做。因为我之前在做公益嘛，就是校园暴力，其实校园性侵属于校园暴力一种嘛。 访谈者： 嗯。 访谈对象： 然后的话做的时候，我发现其实国内对于这方面，研究特别少。包括就是说起因、结果的研究，包括怎么样去做进行一个系统性的，就是说是受创伤后的心理干涉的介入。我觉得这方面研究好像都很少，我其实挺好奇的。 访谈者： 嗯，确实很少。 访谈对象： 哎，你们导师也是专门做这一块的？ 访谈者： 没有，我们导师主要是做危机干预和自杀预防的。 访谈对象： 啊，对，自杀干预前几年好像也是比较少。做自杀干预很有必要，我的好朋友八月份就离世了，就因为这抑郁症。来吧，我们现在就开始吧。 访谈者： 嗯，行，好，那开始之前我再做一个自我介绍吧。尽管前面已经介绍过了，我是来自河南信阳，然后今年26岁，是现在是在华中师范心理学院的在读研究生。然后很高兴，也很感谢你能够愿意支持，然后参与这个研究。好，在开始之前你也就是简单做一个自我介绍，包括年龄啊，职业还有居住地和婚姻状况。 访谈对象： 啊，我现在很明显未婚，然后的话成都人，然后年龄的话，我九四年的跟你同岁。然后但是不知道我们月份谁大谁小。然后现在程序员然后在阿里。嗯，对，就是差不多这个情况。我受侵犯的年纪。我想想具体年份，啊，零七年到现在已经是快14年了，已经是十多年了。所以当时应该是我13岁的时候。13岁未满，07年2月，我没记错的话。 访谈者： 还是能够记得很清楚的。 访谈对象： 对，因为我其实我自己是选择把我很多事情公开出来。因为我觉得国内对于同性……因为我其实是比较，呃，在常规的…（遭遇性侵）这样是比较少见的，我是同性的性侵。 访谈者： 嗯。 访谈对象： 对对，然后然后大家可能更focus 在就是说我异性的性侵。但是国内的性侵的话，对于同性性侵其实这一块比异性的研究更少，而且是什么，法律也不完善。所以说我是会刻意的记忆，然后把这个事情分享出来。 访谈者： 这比一般女生更需要勇气。 访谈对象： 啊，其实还好，其实还好。对，我觉得其实男生来讲，就拿我自己的亲身经历来讲，其实男生受创伤它是一个持续的过程，它是一个持续的过程，可能你小时候觉得做出这个事情有点无所谓。但是当你大了之后，潜意识了，因为我之前抑郁过嘛，我之前抑郁过，去做过心理诊断。然后当时就是说是心理咨询师就确定了诱因，有一个其中的诱因可能就是这件事引发了一个长期性的一个p t s d 。然后，对，然后可能说比女性来得更猛一些。但是可能说，但是在社会舆论氛围上相对能够获得就更多的，就是说可能说宽容一些，或者说是我也不知道怎么界定啊。 访谈者： 嗯，好，那现在我们就开始吧，就按照我的访谈提纲。我们就像讲故事那样的方式，以时间发展顺序来讲一下。就是当时遭遇侵犯的背景，然后还有你当时是怎么去应对处理的。在这个过程中你经历了怎么样的一个心路历程。 访谈对象： 哦，o k ，其实我的话当时是在住宿学校，然后的话，因为就是这个事情可能就比较长。就是我在住宿学校，然后我当时也不算太合群的一个人。然后就是可能就说校官就界定了，界定了就是说这个是一个小孩子，而且他可能也是有性需求嘛。然后现在我想起来他部队退役的，然后就这是一个性需求。嗯，那么他会长期的培养我一个服服从。就比如说平时就因为我当时是就是有点不听话的小孩子嘛。然后就比如说训练军姿啊训练或者其他的，那么服从之后呢，然后就在事发当天晚上，然后把我拉到一个卧室去，然后去喝了一些酒，然后这个事情。就是我彻底就失去了反抗的这个意识了，就是说，那么这个事情就发生了。至于发生之后的话，就是说发生之后，我第二天早上回去，然后我爸请我吃香肠，当时刚过年不久嘛，我们家还有香肠，然后我就回家，因为那个时候是晚上是周五晚上，然后第二天周六回家，然后当时就觉得不太对劲，然后就很不爽，然后跟我爸说了这事。然后我家里人就发现，就可能说知道我被性侵了，然后就赶紧去报警，然后做了就是精液采集，然后做了笔录。然后后面的话，在大概在一年之后，他定罪之前，又重新做了一次我的d n a 鉴定。因为体液采集是从我身上采集的嘛，然后做了一次d n a 鉴定。然后你要说心路历程的话，其实，嗯，跟女孩子不同的是，男孩子其实可能最开始在小时候，最开始可能他并不会认识到这是一次强奸。 访谈者： 其实女孩子也不知道。 访谈对象： 呃，女孩子可能最开始知道，因为女孩子可能从小父母就教育她，说你那个地方很羞羞的嘛，然后你就，然后不能让男孩子碰。然后或者碰到这个地方对你不好，可能是有这样一个介绍。那么女孩子最开始会就会觉得这个地方是有个耻辱感，就有个很明确的耻辱感，很明确的被侵犯感。而男孩子可能说，就我自己的亲身经验来讲，你最开始其实反应可能可能并不会太强烈。就可能你最开始都不知道这发生了什么，就觉得很不舒服很不爽。 访谈者： 然后你第二天早上就跟你父亲说了？ 访谈对象： 对，因为当时我觉得就很不爽。对，然后又因为香肠那个诱因嘛，然后这个东西大家都理解，然后就是加上香肠这个诱因。然后的话就我父亲就发现我的异样，然后就去报警。对。 访谈者： 那你父亲还挺细致的一个人。 访谈对象： 对，我父亲，其实这一件事情我一直很感谢我父亲。因为如果说是传统家长，可能就会考虑说，啊，出于名誉考虑遮盖子嘛，对，遮盖子，然后就说或者说：哎，事情发生就发生了，过。然后就报警了，然后最后那个哥们儿是以猥亵儿童罪被判4年，我就记得没错。 访谈者： 4年。 访谈对象： 对，因为因为男生很尴尬的一点，就是说他不是…因為國內的法律的性侵，它强奸是针对于女性的一个性器官的插入说，而对于男生来讲它其实是没有强奸这个说法的，而我当时凑巧的是我年龄小于十四岁，然后所以说他以那个侵犯儿童、猥亵儿童罪，刑法二百三十几条，我记得没错的话，然后他就入刑了。 访谈者： 嗯，就是这样看来的话，家庭关系其实也很重要。你和父亲的交流…… 访谈对象： 非常的重要，非常的重要，我觉得其实很……就是说我自己身边也有被性侵的。其实这个事情的伤害程度很取决于家庭对这个事情的处理。如果你家庭觉得说：这是一个性侵。那么我及时报警，然后寻求公权或者说警察的帮助。那么这个事情呢受伤害或者说再配合再更开明一点，配合及时的心理介入干涉。那么这个伤害会控制到非常小的点。 访谈者： 嗯，就是你父亲知道之后，他当时的一个反应是什么样的，还有包括你的家里人反应？ 访谈对象： 呃，我其实我父亲他在我面前其实没有表现太多的焦虑。嗯，对我父亲懂得，家长嘛，就是可能说再天大的事，可能也不能在孩子面前（焦虑，不淡定）…我母亲当时是在成都，然后我在小地方，我小时候出生在小地方，这个钢铁城市嘛，然后，对，我父亲然后当时其实是很淡定的，当时他做了两件事，我没记错的话，第一个他给他朋友打电话，就是说他怕警察不认这个事情。于是他找他朋友咨询：我能不能去其他的诊所？先将精液或者说其他的体液部分固定下来，然后后面发现这些事情好像并不合规。然后我父亲就当时把我带到了那个住宿学校的辖区的派出所去做了那个就是说笔录，包括第一波的体液采集，对。 访谈者： 你父亲真的很厉害。 访谈对象： 嗯，对。然后就是后面的话，然后出于其他考虑，然后他就将我带来成都了嘛。对，然后的话就说是和那个环境做隔离。因为这个事情整个公开报道出去其实还是会对你的成长造成一个影响，因为这个特别是教育系统，就是基本上是教育系统，你一出去，一转学，大家一打听就知道这孩子不太对劲（笑）。 访谈者： 嗯。就是那你对于你父亲当时那个处理，你当时心里面就是有一个什么样的状态？ 访谈对象： 嗯，其实我当时还是懵懂的状态，其实还是懵懂的状态。就是说，包括体液采集，做笔录的时候，我当时其实并没有意识到这个事情是一个强奸。我当时其实心里没有很明确的这个词，这个是对于男生的一个强奸。然后我其实就觉得这可能是个不好的事情。嗯，然后呢那么我就配合警察，把这个事情完全记录、说出来。然后做笔录，其实还是小孩子对于，就是说当时去做这一套，我父亲替我拍了板之后，我当时做这一套核心的一个动机，更可能还是说，呃，小孩子对于长辈，对于就是说是或者说其他的一个服从，就是我当时并没有意识到这个东西发生了什么。 访谈者： 嗯，那你后来什么时候开始意识到这件事情它的一个性质是什么的？ 访谈对象： 唔，其实初步意识到应该是在高中，然后完整的去复盘我自己的这件事儿的话，应该是在大学。应该是在快毕业的时候，当时其实，呃，我完整的复盘其实诱因应该是国内的me too 运动。 访谈者： 嗯嗯。 访谈对象： 对，实际上是应该是me too 运动。然后诱因…，我其实之前有个粗浅的复盘，但是真正的去重新的就是从头到尾的去审视这件事是就应该是me too，对，然后的话对，差不多就是这样一个情况。 访谈者： 就是这件事情对你有什么影响？就是从它发生一直到现在的话，对你有什么影响？ 访谈对象： 呃，其实我觉得你要说真正的影响很难量化。因为就是说很多东西就包括我去做心理咨询之后，其实医生的看法就是说这个事情是潜移默化的，就是说潜移默化的一个伤害，就是说是长期性的一个p t s d。然后当时我去医院去确诊，然后就因为我当时是确诊抑郁症嘛，然后重度抑郁，然后是伴自杀焦虑，然后的话就是自杀倾向。然后的话，呃，当时其实医生当时听我把这个事情清理完整描述过之后，医生的评价就是说是你这个抑郁的有一部分其实就是来源于你这件事儿。嗯，就说，对，对，因为就是什么，因为我是抑郁需要吃药并伴做那个心理干涉嘛，然后，对，但是就是说你要说这件事情有什么明显的，就是说后遗（症），一个就是要很量化的，就是说你觉得这个事情对你影响有多大，我觉得这没量化。我觉得这个东西是一个潜移默化的过程。因为我当时是在受侵害的，最开始他是没有做到一个系统的心理干涉，我是没有做到的。对，可能对于同性的时候，可能说最开始你觉得没什么，但是越想越想觉得有事儿。 访谈者： 那你诊断为抑郁症的时候，大概是多大？ 访谈对象： 啊，应该是在17年。 访谈者： 嗯，在三年前。就是在此之前就没有表现出来一些具体的症状？ 访谈对象： 有，其实有我一直神经衰弱，但是我其实在大学的时候应该是有，但是我是没有去确诊的，因为就是说没有确诊。对，但是正式确认是在17年。对，然后的话，所以说我觉得这个事情可能更可能来讲还是一个长期的一个潜移默化的。就是说它并不会对你就是说你一下子觉得：哎，你自己不是一个干净的人，你一下子觉得你想跳楼了。我觉得这个不存在，但是可能就是说潜移默化的过程。 访谈者： 那你觉得这种潜移默化主要是来自于哪里呢？ 访谈对象： 我觉得可能最开始就是打破你自己的，就说是一个完整感。就是说你自己觉得自己是一个不完整，或者说是你觉得你自己是…用这种传统的话来讲呢，就是你自己是个不干净的，懂我意思吧？ 访谈者： 嗯。 访谈对象： 对，就是我自己猜的话，潜意识可能会给自己加一个这样的，其实我觉得可能说。呃，然后你就会觉得你自己说话会比别人低一头或者是其他。对，我觉得这个可能还是一个社会氛围所造成的吧。 访谈者： 嗯，就是在高中之后慢慢的开始有这种意识。那其实对是小一点的话，你其实还是不太知道这件事情是什么性质？ 访谈对象： 对，对，对，男生其实很麻烦的一件事就在这个地方，最开始如果你是被性侵的那个男生，男生你在小时候你可能会觉得这是一个游戏，你可能会去更觉得这是一个游戏。但是你在大了之后，你会发现慢慢觉得这不是…（一个词，听不清）欸？怎么这么不太对劲啊？ 访谈者： 嗯，就是慢慢的意识到的。 访谈对象： 对，是的，没错。 访谈者： 你就是最开始就已经选择，也不是说你主动告诉你父亲，就是你父亲他自己觉察到这件事情。那再后来你有没有再表露过这件事情？嗯，再一次向别人说这件事情的时候是什么时候，然后是什么原因？ 访谈对象： 我其实应该是还是在大学吧。然后我其实之前其实只跟几个好朋友说过，然后我正式选择公开这件事情，应该还是在大学或者说快毕业，me too事件之后，因为我觉得可能说我的经（历），因为我当时其实有个背景是我也是在做公益，我之前是救援队的。然后的话我之前大学在救援队服务，然后是以私人身份去做一些，然后就包括啊校园校园暴力和校园性侵，资助我也在做。呃，我可能觉得，更觉得这件事的话，可能就是说，我需要让这件事变得有意义。然后于是我选择就是说公开的把我这个经历分享出来。因为我觉得可能说，呃，一个事情如果只能变成你自己的痛苦，或者说你自己的建议，那么你自己是否痛苦其实是没有太大意义的。对，而且我这个事情又是相对来说对别人更容易产生帮助的，因为其实你知道儿童期间的性侵其实是很常见的一个事情。 访谈者： 嗯嗯。 访谈对象： 对，其实是，然后家庭，就是说父母的干涉又是非常重要的。我选择的在知乎，你看到我的那个答案，包括我选择的一些社群里面把这个事情，呃，就是说我自己的当时的经历，以及我家庭所做的一些手段，包括我家庭当时做的不足的地方，就因为认知的关系嘛，没有给我寻求及时的心理介入，这些事情分享出来我就是希望说，我自己能够把这些，啊，把我自己的经历能够变得更为有意义一点，然后不仅限于我自己所受到的伤害。 访谈者： 做的真的很好。 访谈对象： 唉，（深深叹一口气）对，这也是我参加你的这个研究的原因吧。 访谈者： 嗯，我也看到，你说这就是你分享的意义。 访谈对象： 嗯嗯，对，对，我觉得其实就是说你自己每个人受伤害嘛，然后要怎么样去把自己受到的创伤，受到的伤害变得让它具备除了伤害以外更多的意义。我觉得这是一个很好玩的话题。 访谈者： 嗯，很有意义，很有价值的。 访谈对象： 嗯，对，这就是我自己选择的，我自己披露出来的一个啊，心理活动。 访谈者： 好，还有就是你刚才提到也告诉过好朋友，那你是在什么时候告诉好朋友，然后他们又是怎么回应你，他们当时的这个态度和反应是什么样的？ 访谈对象： 他们其实就觉得就是说：不可思议。就是这样子，然后就觉得就觉得说，出于保护我，就告诉我这件事情还是不要跟别人说。出于好意，就是让我这件事情不要跟别人说。然后这个事情不要跟别人说，其实后面相处也是没有什么太大的变化，因为好朋友嘛，对。 访谈者： 那也是因为你非常的信任他们。 访谈对象： 对，因为其实是在闲聊的时候嘛，就是闲聊的时候就说了这事。这个没有什么可以防备的，就说了这事儿，虽然我父母我一直在告诉我说：这个事情你千万不要给别人说啊。但是我从小都不是个好小孩。 访谈者： 嗯，就是在朋友那里，其实也是得到了一定的支持。 访谈对象： 嗯，是的，没错。 访谈者： 还有很多小孩就是选择告诉父母的话，他还会造成二次伤害。所以就是说你父亲的这种处理真的特别可贵。 访谈对象： 对，对，是的，我觉得其实是被性侵伤害的（人），从我自己打过交道来看，其实是很大一部分伤害是来自原生家庭。这个我们毫不避讳的讲的是其实很多包括就觉得说可能说女孩子就觉得说：啊，你嫁不出去了，就直接原生家庭给的压力，就“你嫁不出去了，你怎么这样？”就受害者有罪推定嘛，是吧，受害者有罪推定。“哎，当时谁叫你穿那么少干嘛，然后叫你再穿这么少？”其实我父母也有，我母亲也有一点这种就说：哎，叫你当时不合群，你要是合群，你教官就不会挑上你了嘛！”我觉得这种东西其实是没有必要的那个谴（责），苛责。 访谈者： 对对对，明明就是加害者的错，反过来还要去揪你的毛病。对，所以面对母亲的那种那种责怪，你其实感觉怎么样？。 访谈对象： 我母亲其实也不是恶意的嘛。她其实就是说教育我要去合群嘛。 访谈者： 嗯。 访谈对象： 对，其实我小时候我一直不太想去为了合群而合群，但是他们长辈嘛，她为了教育我去合群，就是说可能说举了一个不恰当的例子。但是不恰当的例子其实呢也是说，目前我跟你说性侵的伤害，就是说遇到了一个现状，很多时候的压力和伤害其实是更多的是，就是说比性侵这件事情伤害更大的，可能说是来自于原生家庭的一个苛责。 访谈者： 嗯，对，确实是。 访谈对象： 好，你觉得有什么是促进你来表述这件事情，有什么是阻碍你来表述这件事情的？ 访谈者： 哦，其实阻碍我去表述这件事情的因素并不多。 访谈对象： 嗯，我也发现了。 访谈者： 对，因为我觉得你们说“关我吊事儿”，就是用术语来说就是就是“关我什么事儿”。然后我说我的，你们看的惯就看，看不惯就不看，你们又不给我发钱，发工资是不是？我这个人的性格是比较，就是比较那个就怎么说，就反常规的吧，所以说我觉得还好。再一个就是说促进我表露的话，可能就还是说，呃，国内就是说是目前就是说是对于性侵受害者态度的一个变化。嗯，就说是一个……就是因为之前，可能说在零几年或者说前几年或者更早的时候，觉得性侵它是一个非常shame的事情，就是说你不光彩。就是说“how shame you “就是说你怎么能这样，就是就刚才我举的例子吧，你被性侵了，你一定是穿的太少了，你一定是穿的太骚了，或者说你一定是穿的太浪了。啊，这种就是受害者有罪论。这几年国内其实是对性侵受害者态度其实是逐渐变好了。而且就是我刚才说了嘛，另外一个契机可能就是说是me too。然后我觉得可能说，啊，把这个事情公布出来，然后让别人，能够帮到别人，我觉得是能够去抚慰自己被性侵。就特别说如果有一些人告诉你说：我孩子，呃，但是我不希望有这样一天，啊，但是就是如果说有一天跟你说，有个当父亲或当母亲跟你说：我孩子被性侵性了，我按照你说的做了。其实我觉得如果说是有这样一件事情，它其实是能够治愈很多东西的。但是我不希望有一天会出现这种情况。 访谈对象： 嗯。 访谈者： 对，然后我觉得这可能说是，呃，一个是算是一个自我的拯救吧。第二个就是说可能说还是希望就是说能够帮…就说是用自己的一些东西去回馈一些什么。 访谈对象： 嗯。当你回忆了整个的这样的一个经历之后，你现在是有什么感受？用一个词来概括一下。 访谈者： 啊！还好。 访谈对象： 还好？用一个词来概括的话？ 访谈者： 啊，还好，就是跟产品经理撕完逼之后的，怎么说，啊，轻松。 访谈对象： 轻松？ 访谈者： 对，轻松吧，我觉得其实，我觉得怎么说呢？我觉得其实这样面对面聊这个问题，其实聊完之后其实还是会有些轻松感的，对，就像跟产品经理撕完批之后，然后你会觉得：哇，爽死了。 访谈对象： 程序员的快乐。 访谈者： 啊，是的。那能再具体一点说一下，为什么是这种感觉呢？ 访谈对象： 因为我觉得其实你在网上以一个虚拟的身份去说，可能说大家都知道迈克萨卡的i d 是我。因为我可能说我觉得我在这个社区比较活跃，大家可能觉得迈克萨卡的i d 是我，我也会在微信群里面说，大家看到啊，这个啊微信i d是迈克萨卡的人是我。但是我觉得可能说跟你就是一个真人的去one on one的去说，就是相对来说是一个陌生人。我跟我女朋友其实是很深入地聊过这件事儿的。就是说，呃，跟你一个陌生人去one on one的去聊这件事儿，其实也是在治愈自己那个心里面的一个东西，对，其实是因为，因为潜意识来讲可能最开始我觉得这还是一件shame的事情。所以我想的是在网上以一个虚拟的身份去做。那我去给你做访谈，就其实说我有一些枷锁，其实还是（非常大的？）（没听清楚）。 访谈者： 嗯嗯，你刚才我联系你时你说去抽根烟，我就觉得，嗯，可能就是就觉得又要去面对这个事情，情绪上需要准备一下。 访谈对象： 哦，其实你是第一个就是说以一个相对陌生人的身份去忘one on one的去聊这件事儿。 访谈者： 那个单词是怎么你能说慢点吗？one什么？ 访谈对象： 就是one on one，就是一对一，就是一对一，面对面。 访谈者： 哦哦。 访谈对象： 对，我们这边应该就是叫one on one，然后就是跟老板one on one的聊天。 访谈者： 哦哦，在我打断你之前，你刚才要说什么，你接着说吧。 访谈对象： 哦，没有没有，其实我觉得，对，就是说你自己以一个虚拟的身份去说这个事情，就是说是有温度的去说这个事情，我觉得其实是两种体验吧。 访谈者： 嗯，确实是。你刚才提到你女朋友，因为你女朋友毕竟是你除了爸妈之外关系最亲密的人嘛，你告诉了她这件事情，那你再具体说一下，就是你告诉她的这个经过吧，还有你做了什么思想工作？ 访谈对象： 就是在晚上休息之前，我给她就聊到了小时候这件事情，其实也没什么，因为我的事情她都知道，她其实没有什么，我就只是在去聊一些东西。她其实也没什么反应，就是觉得很心疼心疼。 访谈者： 嗯，心疼。 访谈对象： 对，对，对，然后其实我觉得跟亲密的人聊这个事情，其实是没有什么。当然前提是你亲密的人不会再苛责你：你当时为什么穿的那么少，那么浪？对，就是……（没听清楚）其实我觉得去聊这种事情，我觉得是，嗯，也是能够促进感情的一种比较好的方式吧。就是说是互相剖析嘛。 访谈者： 嗯，就是那种完全信任的那种状态。 访谈对象： 是的，没错。 访谈者： 你对于儿童性侵犯的一个现状有什么样的认识？ 访谈对象： 很严重，非常严重，因为性侵其实它是一个很大的概念。我可能下面的话会说的比较露骨一点，不要介意啊。就是说性侵，它可能并不仅仅限于就是说，呃，一个强奸的一个行为。就是插入了，它可能是边缘性的一个性侵，可说搂搂抱抱，对于小女孩或小男孩的一个搂搂抱抱，乃至于说是对于性器官的一个接触，对。然后而且这个，其实现在是越发多样了。可能往常的人来说，我觉得就是说，往常的人来说，就觉得说啊，男生对于女的是一个性侵。后面可能就是说，大家发现男的对于男的其实是一个性侵。但是可能现在这些情况可能会更加多样，就可能说，呃，我并不会去做一些实质性的侵入的工作。我可能就只是说，哎，我见到一个萝莉，我去搂搂抱抱，或者说我心生邪念了，我去做一些性器官的接触。或者说我，或者说是…因为我其实见到，如果你有关注，其实见到很多案例，就可能说，呃，我叫一个小女孩用棒棒糖让她来帮我口一次，也就是说是口交一次，对吧，嗯，其实见过这种案例的，对，然后或者说是，呃，或者说是叫一个小男孩来给口一次，或者说是其他，而且这个他身份可能是会有变化的。可能后面会多样的，就是说我一个二十多岁的女生，或者说就是我自己的生活比较开放。嗯，对，然后我去我去勾搭一个小孩，然后我去跟他发生性关系。其实在我看来这种其实也是一种性侵。因为在小男孩性观念没有成熟之前，你这个操作是会对他造成很大影响的，他的性取向或者性操作会有很大影响吧。包括现在其实同性恋也放到台面上来讲了，就是说我同性恋去强行掰弯一个人，或者说我去性侵一个小男孩，其实也是一个，就是说性侵它其实是现在很严重，儿童性侵其实是很严重的一件事。而且现在是多样化。 访谈者： 嗯，但是相关的法律确实还没有跟得上。 访谈对象： 是的，没错。对，就比如说，呃，我一个，呃就比如说我一个二十多岁三十多岁的女生，我去性侵一个男生，一个十多岁的男生，如果说我是有其他身份的，我是一个老师，或者说是一个在其他国家，他是能够得到一个，就可能说是更重的处罚。而国内的话，其实这一块的法律行为是没有完善的。就可能说如果这个男生小于14岁，你可能会以侵犯儿童，猥亵儿童罪的名义去入刑。然后就比如说小于14岁是强奸这种，其实仅限于女性，没有男性，因为国内强奸它采取的还是一个插入的说法，性器官男生对于女生的一个侵犯。所以说我觉得其实我国法律有其实还有很大的空间要走。包括就是说是在被性侵之后的及时的心理介入。 访谈者： 嗯，对对对，真的是急需完善的一个状态。 访谈对象： 对，其实是国内，其实现在性侵其实是多样化的。比如说是儿童性侵，其实多样化。因为其实生活好了嘛，大家其实会想怎么回事？而且其实比如说同性同龄之间的性侵，其实也是非常严重的。就比如说校园暴力的凌辱的行为，就由凌辱转化为性侵，这种事情其实都没有被重视的。 访谈者： 还有很长的路要要走。 访谈对象： 是的，没错，是的，没错。 访谈者： 然后你觉得对于遭遇过儿童期性侵犯的受害者来说，他需要哪些力量的介入和帮助？ 访谈对象： 哦，我觉得最强力的是公权力。 访谈者： 什么？公权力？ 访谈对象： 嗯，公权力的介入，就是政府部门的介入。就我当时其实这个事情实际上还没有太大，是因为派出所的人给我留下很深的印象。对，当时因为没来得及吃晚饭嘛，然后我当时我还记得就是说那个派出所当时应该和我父亲那边还在联系……（信号原因，没听清）因为一般像这种小地方的警察，可能觉得这种事情就很不耐烦：没事没事，你们先回家自己处理嘛。和稀泥嘛，基层的传统玩法，和稀泥嘛。然后那个警察叔叔就很尽责，然后我记得当时我还没吃早饭，然后录笔录录到下午两点多，还没采体液。小孩子嘛，饿嘛，然后我记得我还在当时他办公室吃了一袋苹果片，然后我现在还记得，然后，对，我觉得这种东西，公权力的及时的介入，其实是能够在第一时间对小孩子就说是一个很贴切的保障。因为大家都从小都得到一个认识嘛：警察叔叔是好人。你受到欺负，警察叔叔会来帮你，对吧？然后，对，这个东西公权力的介入一定是第一优先的，就是说家庭和公权力的及时介入第一是优先。第二个其实我觉得很重要的就说是在性侵的一个月以内，就说是我觉得不管他当时有没有表现出异样啊，一定是要寻求及时的心理干涉。对，就是说因为性侵其实跟其他一样，就是你学心理的肯定比我清楚，就是说性侵跟其他的一样，就是说受伤害的这个东西，他或多或少的一定会有p t sd 相关的东西。如果说是你没有去寻求心理的介入，那么你这个东西可能就是说你心里始终是有个梗儿在这儿。就比如说台湾有个作家叫张苑，就是台湾那个作家自杀的那一位我忘了。 访谈者： 林奕含。 访谈对象： 啊，对，林奕含，然后她其实她其实就是这种，呃，就可能说像我这样皮实的人，可能说会选择自我开导，但是选择像她那样的人，可能就是一个p t s d，然后再加上其他各种事情的不理解，抑郁，然后自杀。其实我觉得所以说第一时间的公权力介入，第一时间的心理干涉一定是非常必要的且非常重要的。 访谈者： 嗯嗯，确实是的。 访谈对象： 但是我觉得国内其实好像专门做儿童性侵心理干涉的好像很少。 访谈者： 确实很少。 访谈对象： 对，我觉得就这个东西其实是亟待完善的。因为其实用成人的那套心理干涉的做法去做儿童心理，其实是（一个词，没听清）不同的。 访谈者： 嗯，首先心理学在中国就刚刚发展起来，就是处于起步的那种朝阳状态。 访谈对象： 啊，就是说这些东西还得需要时间去完善吧。 访谈者： 对，确实它需要一个过程。不过也在慢慢的变好。 访谈对象： 其实说实话，我觉得短期之内其实是不一定能看到太好的变好。 访谈者： 是的是的是的。 访谈对象： 唉，任重道远吧。 访谈者： 路漫漫。 访谈对象： 对，其实是我一个（没听清），我觉得对于小孩子来讲，就是说及时的、周边的帮助非常的重要。 访谈者： 对于预防和干预儿童性侵犯你有什么建议？其实这个问题和刚才那个问题是一个性质的。 访谈对象： 预防其实你这个事情没法解，你预防不了，你预防不了。因为你其实性侵它一定是意味着一个事情，就是说是有一方的呃，就是说是，呃，不管是体力还是其他的，他一定是凌驾于另外一方的。对吧？ 访谈者： 是的。 访谈对象： 对。我就举这么一个不太恰当的例子，就是说性侵，它这个事情其实你只能从就是说是一些呃去打一些预防针。就比如说女孩子就是现在常见的：女孩子啊，你不来让男孩子摸啊，然后这种东西。但是如果说，比如说我，就是说我要采取暴力手段了，我举一个不恰当的例子，希望你不要介意。就是比如说我见到一个萝莉了，对，我采取暴力手段，我去搂搂抱抱，来做一个就是说是，呃，过分的事情，你觉得她有抵抗力吗？没有，对吧？ 访谈者： 嗯。 访谈对象： 对，然后你觉得这件事情能预防吗？没有办法。对，所以说我觉得预防其实，其实它一部分其实是个很奢侈的事情，因为它是需要一个去体系化的一个建设，很奢侈的事情。我觉得更重要的可能却需要去告诉孩子说：这个事情不是一个shame的事情。如果你被侵犯了，这不是你的错误，你需要告诉及时告诉爸爸。 访谈者： 嗯嗯，对。 访谈对象： 我觉得这个事情其实是比预防更重要的事情。 访谈者： 很中肯，嗯。 访谈对象： 对对，因为，呃，知乎上你应该也能看到很多匿名的，就觉得说家里人觉得丢脸，很多，特别是女孩子，然后家里人就不告诉她这些。对吧？然后我觉得这个可能说是因为很多事情预防预防不了。因为就比如说走在路上，突然有人心生歹念，就是你怎么预防？防也防不动啊。除非你随身身上带把刀，然后谁敢碰我我割了他。这种事情当然也都是说笑话，然后我觉得这种事情就一定去培养一个观念：就是说这个事情它不是一个shame的事情，不是你的错，你需要（告诉家里人？没听清，不确定）。 访谈者： 嗯，主要是家庭方面影响特别大。 访谈对象： 是的，其实我觉得公权力方面也影响非常大。因为其实很多就是基层嘛，就是其实很多他虽然说他是警察，但是可能他的法律意识没有吧。然后他可能就会下意识的，就是说，因为这种地方是个小地方，可能就会说：啊，为了你女儿着想，这个事情和了吧。咱们就不走中间法吧，你们就都妥一下对吧。我觉得就是说基层，就说是可能就觉得说，呃，反正就觉得，呃，就千万不要和稀泥，就不要受害者有罪推定，就这个事情不是他的错误。我觉得就是建立一个系统的认知，这个事情是非常重要的。 访谈者： 嗯，就是现在来看的话，你回头去看你的那些应对方式，你会有一个什么样的感受？ 访谈对象： 我父亲做很帮，啊，然后但是可能说他可以做的更棒，但是这个东西其实是马后炮的，我也没法去跟07年的时候，那个时候他……啊，对不起，我去看一下吧，可能有人在敲门。反正我觉得他当时做的很棒，那一套行云流水，其实也很感激他。对，但是的话我觉得可能说，如果说我自己从现在一个事后的角度去复盘，我觉得可能还缺了就我刚刚说的心理干涉。 访谈者： 嗯，那包括你自己的应对方式呢，就是你选择把它给说出来。你觉得如果不说的话，你会怎么样？ 访谈对象： 哦，其实我这个东西很多事情没法做假设呀。我也不知道我不说会怎么样。其实这个东西我觉得其实没法做没法。 访谈者： 嗯，那你去看你的这种应对方式的话，你是什么感受？你怎么看待？ 访谈对象： 我其实也没什么特别的感受。我只是觉得我做了一件应该做的事情。没什么特别的感受。 访谈者： 嗯，我还设计了一个问题，就是如果说你可以回到当初那件事情发生的时候，你会做什么？ 访谈对象： 其实说实话，我觉得这个问题其实没有太大意义。因为其实。你像刚才我说的很多时候，他其实是处于弱势的一个地位，校园性侵他其实是对于性一个很懵懂的这个东西，你没法去假设去说你具备一个完全的性知识人。穿越回去附身到他身上，然后说当时会怎么做，我觉得其实这个东西，这个假设其实是没有意义的。我说的比较直白啊。 访谈者： 嗯，因为可能就是男生比较理性嘛，但是就是对我自己而言，我是希望我能够回去的，就是我假想过这样的一个场景，我如果可以回去，那我就可以避免这所有的一切的发生，就是有一个情绪的宣泄口嘛，这就是我问这个问题的原因。 访谈对象： 是，是一个情绪的宣泄口，但是就是可能说，呃，对于更大多数人来讲，其实这个东西其实没有意义。我们只能往前看，就是就像刚才我说的。其实我知道你意思，你其实想说我去总结一套经验出来，就是说避免当时发生。但是其实就像我刚才说的，很多时候儿童性侵的受害者他其实是面对的是成年人，他其实是面对的是成年人，而且是多种多样的成年人。他其实留给他的选择余地不多。我感觉你情绪好像有点波动啊。 访谈者： 没有，是因为我想到我前面访谈的那些女生，加害者全部都不是成年人，啊，有一个是成年人，其余的全都是中学生，十四岁左右的小男生。 访谈对象： 对，唉，其实我觉得这个事情吧，其实这个事情其实说实话，这种东西就是纯粹就是属于飞来横祸的。对，因为这个东西，就比如我走在路上，然后被突然十二楼掉下来一个东西砸死了。然后我也没法说我提前预知到今天三点十分楼上要掉东西，避开它。这个东西没法预测，除非我直接具备超能力。 访谈者： 嗯，好，再追加一个问题呢，就整个访谈下来你有什么感受，还有你对于我这个访谈的感受吧？ 访谈对象： 哦，我觉得其实反正我觉得其实感觉还好吧。我最开始想的就是你的态度会push一些，就稍微有侵略性一些，学院性一点，其实我觉得整个聊下来感觉还不错。然后的话，对，但是我觉得自己更希望就是说我自己希望是能越来越多的人有来做你这方面的研究的。 访谈者： 我也有这样的希望啊。 访谈对象： 对，我觉得可能是我自己更希望说是有更多人来做这个研究。其实我不介意去做访谈，但是我希望能有更多人。其实说实话我也很好奇，为什么你是第一个找我的？为什么没有其他人找我？其实我也很好奇这个问题。 访谈者： 你是觉得在很早之前就应该有人去做这样的一个研究？ 访谈对象： 对，我觉得其实它并不是一个新鲜事儿，它其实并不是一个新鲜事儿。其实我觉得虽然说国内心理学就是说这方面的研究起步晚吧，但是也不至于到了2020年的今天才有人来开始去做这个，马上都要2021年了，才有人来去找当时的事儿。但是也有可能是我不是在圈子内啊，我不知道啊，也有可能我不在圈子内，我不知道。 访谈者： 其实我很早就想写这个，但是我不太敢跟我导师说，怕被否决。他们会说这是一个特别特别敏感的话题，可行性特别低。 访谈对象： o k ，o k ，o k 明白，我其实很好奇，那你最终决定去做这个话题的时候，你导师你老板什么看法？ 访谈者： 我老师就支持我啊。但是他说你要考虑一个问题，就是你可能找不到被试，你找不够，因为这是一个非常敏感的问题，你找研究对象是非常困难的。 访谈对象： 啊，是的。 访谈者： 还有就是我没有提到的，而你想要说的，想要表达的一些东西？ 访谈对象： 哦，其实我当时想聊的，其实基本上你这个题做的还不错。其实我想聊的其实基本上都聊到了。就是说反正我觉得其实我更希望的就是说是国内这方面的研究能够跟上，其实先不提法律层面上了，因为法律必定是滞后于社会发展。对，然后对，我先不提法律上，我就希望心理干涉这一块儿能够及时跟上，就包括大家的观点，就千万不要去面对一个性侵受害者说：你当时穿的太少了，你太骚，你太浪了…这种东西，就被害者有罪论你千万不要再去讲。 访谈者： 对对对。 访谈对象： 就是这些吧，我就是只想说“去你丫的”！反正我觉得其实这个东西，反正我觉得你做这个事情是比较有意义的。就我刚才说，我很好奇为啥为啥会2020年才有人找。当然我不在圈内，可能更早有人做了，我没知道而已，就翻到了我而已。但是我觉得的确应该是非常少见。 访谈者： 嗯，13年、14年也是有人做的，因为我看到一些相关的硕士论文。 访谈对象： 对，反正希望你这边的研究一切顺利吧，反正就是说我自己还是觉得就是其实是反正如果说你对这个，我不知道你对做这个事情有怀疑没有。如果有的话，反正我还明确告诉你：做这个事情非常有意义。 访谈者： 嗯，谢谢，谢谢。 访谈对象： 对，行，看你这边还有啥想问的没？ 访谈者： 目前是没有了。你做的一切也非常非常有意义。 访谈对象： 谢谢。那要不然就先这样？ 访谈者： 嗯，好，那你忙吧，拜拜。 访谈对象： 拜拜。 总结其实想说的话有很多，但是一下不知道怎么说吧。在这里先引用我的采访者在论文中写的一句话作为结尾吧 我始终相信星星之火，可以燎原。 Everything is gonna be OK","link":"/posts/2021/05/07/something-I-want-to-share-about-sex-assault1/"},{"title":"Python concurrent.future 使用教程及源码初剖","text":"垃圾话很久没写博客了，想了想不能再划水，于是给自己定了一个目标，写点 concurrent.future 的内容，于是这篇文章就是来聊聊 Python 3.2 中新增的 concurrent.future 模块。 正文Python 的异步处理有一个 Python 开发工程师小明，在面试过程中，突然接到这样一个需求：去请求几个网站，拿到他们的数据，小明定睛一想，简单啊，噼里啪啦，他写了如下的代码 1234567891011121314import multiprocessingimport timedef request_url(query_url: str): time.sleep(3) # 请求处理逻辑if __name__ == '__main__': url_list = [&quot;abc.com&quot;, &quot;xyz.com&quot;] task_list = [multiprocessing.Process(target=request_url, args=(url,)) for url in url_list] [task.start() for task in task_list] [task.join() for task in task_list] Easy, 好了，现在新的需求来了，我们想获取每一个请求结果，怎么办?小明想了想，又写出如下的代码 123456789101112131415161718import multiprocessingimport timedef request_url(query_url: str, result_dict: dict): time.sleep(3) # 请求处理逻辑 result_dict[query_url] = {} # 返回结果if __name__ == '__main__': process_manager = multiprocessing.Manager() result_dict = process_manager.dict() url_list = [&quot;abc.com&quot;, &quot;xyz.com&quot;] task_list = [multiprocessing.Process(target=request_url, args=(url, result_dict)) for url in url_list] [task.start() for task in task_list] [task.join() for task in task_list] print(result_dict) 好了，面试官说，恩看起来不错，好了，我再改改题目，首先，我们不能阻塞主进程，主进程需要根据任务当前的状态（结束/未结束）来及时的获取对应的结果，怎么改？，小明想了想，要不，我们直接用信号量，让任务完成后，向父进程发送一个信号量？然后直接暴力出奇迹？还有更简单的方法么？貌似没了？最后面试官心理说了一句 naive ，脸上笑而不语，让小明回去慢慢等消息。 从小明的窘境，我们可以看出一个这样的问题，就是我们最常用的 multiprocessing 或者是 threding 两个模块，对于我们想实现异步任务的场景来说，其实略有一点不友好的，我们往往需要做一些额外的工作，才能比较干净的实现一些异步的需求。为了解决这样的窘境，09 年 10 月，Brian Quinlan 先生提出了 PEP 3148 ，在这个提案中，他提出将我们常用的 multiprocessing 和 threding 模块进行进一步封装，达成较好的支持异步操作的目的。最终这个提案在 Python 3.2 中被引入。也就是我们今天要聊聊的 concurrent.future 。 Future 模式在我们正式开始聊新模块之前，我们需要去了解关于 Future 模式的相关姿势 首先 Future 模式，是什么？ Future 其实是生产-消费者模型的一种扩展，在生产-消费者模型中，生产者不关心消费者什么时候处理完数据，也不关心消费者处理的结果。比如我们经常写出如下的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import multiprocessing, Queueimport osimport timefrom multiprocessing import Processfrom time import sleepfrom random import randintclass Producer(multiprocessing.Process): def __init__(self, queue): multiprocessing.Process.__init__(self) self.queue = queue def run(self): while True: self.queue.put('one product') print(multiprocessing.current_process().name + str(os.getpid()) + ' produced one product, the no of queue now is: %d' %self.queue.qsize()) sleep(randint(1, 3)) class Consumer(multiprocessing.Process): def __init__(self, queue): multiprocessing.Process.__init__(self) self.queue = queue def run(self): while True: d = self.queue.get(1) if d != None: print(multiprocessing.current_process().name + str(os.getpid()) + ' consumed %s, the no of queue now is: %d' %(d,self.queue.qsize())) sleep(randint(1, 4)) continue else: break #create queuequeue = multiprocessing.Queue(40) if __name__ == &quot;__main__&quot;: print('Excited!&quot;) #create processes processed = [] for i in range(3): processed.append(Producer(queue)) processed.append(Consumer(queue)) #start processes for i in range(len(processed)): processed[i].start() #join processes for i in range(len(processed)): processed[i].join() 这就是生产-消费者模型的一个简单的实现，我们利用一个 multiprocessing 中的 Queue 来作为通信渠道，我们的生产者负责往队列中传入数据，消费者负责从队列中获取数据并处理。不过就如同上面所说的一样，在这种模式中，生产者并不关心消费者何时处理完数据，也不关心处理的结果。而在 Future 中，我们可以让生产者等待消息处理完成，如果需要的话，我们还可以获取相关的计算结果。 比如，大家可以看看下面这样一段 Java 代码 1234567891011121314package concurrent;import java.util.concurrent.Callable;public class DataProcessThread implements Callable&lt;String&gt; { @Override public String call() throws Exception { // TODO Auto-generated method stub Thread.sleep(10000);//模拟数据处理 return &quot;数据返回&quot;; }} 这是我们负责处理数据的代码。 12345678910111213141516171819202122232425262728293031package concurrent;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.FutureTask;public class MainThread { public static void main(String[] args) throws InterruptedException, ExecutionException { // TODO Auto-generated method stub DataProcessThread dataProcessThread = new DataProcessThread(); FutureTask&lt;String&gt; future = new FutureTask&lt;String&gt;(dataProcessThread); ExecutorService executor = Executors.newFixedThreadPool(1); executor.submit(future); Thread.sleep(10000);//模拟继续处理自身其他业务 while (true) { if (future.isDone()) { System.out.println(future.get()); break; } } executor.shutdown(); }} 这是我们主线程，大家可以看到，我们可以很方便的获取数据处理任务的状态。同时获取相关的结果。 Python 中的 concurrent.futures前面说了，在 Python 3.2 以后，concurrent.futures 是内置的模块，我们可以直接使用 Note: 如果你需要在 Python 2.7 中使用 concurrent.futures , 那么请用 pip 进行安装，pip install futures 好了，准备就绪后，我们来看看怎么使用这个东西呢 12345678910111213141516171819from concurrent.futures import ProcessPoolExecutorimport timedef return_future_result(message): time.sleep(2) return messageif __name__ == '__main__': pool = ProcessPoolExecutor(max_workers=2) # 创建一个最大可容纳2个task的进程池 future1 = pool.submit(return_future_result, (&quot;hello&quot;)) # 往进程池里面加入一个task future2 = pool.submit(return_future_result, (&quot;world&quot;)) # 往进程池里面加入一个task print(future1.done()) # 判断task1是否结束 time.sleep(3) print(future2.done()) # 判断task2是否结束 print(future1.result()) # 查看task1返回的结果 print(future2.result()) # 查看task2返回的结果 首先 from concurrent.futures import ProcessPoolExecutor 从 concurrent.futures 引入 ProcessPoolExecutor 作为我们的进程池，处理我们后面的数据。(在 concurrent.futures 中，为我们提供了两种 Executor ，一种是我们现在用的 ProcessPoolExecutor, 一种是 ThreadPoolExecutor 他们对外暴露的方法一致，大家可以根据自己的实际需求选用。) 紧接着，初始化一个最大容量为 2 的进程池。然后我们调用进程池中的 submit 方法提交一个任务。好了有意思的点来了，我们在调用 submit 方法后，得到了一个特殊的变量，这个变量是 Future 类的实例，代表着一个在未来完成的操作。换句话说，当 submit 返回 Future 实例的时候，我们的任务可能还没有完成，我们可以通过调用 Future 实例中的 done 方法来获取当前任务的运行状态，如果任务结束后，我们可以通过 result 方法来获取返回的结果。如果在执行后续的逻辑时，我们因为一些原因想要取消任务时，我们可以通过调用 cancel 方法来取消当前的任务。 现在新的问题来了，我们如果想要提交很多个任务应该怎么办呢？concurrent.future 为我们提供了 map 方法来方便我们批量添加任务。 123456789101112131415import concurrent.futuresimport requeststask_url = [('http://www.baidu.com', 40), ('http://example.com/', 40), ('https://www.github.com/', 40)]def load_url(params: tuple): return requests.get(params[0], timeout=params[1]).textif __name__ == '__main__': with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor: for url, data in zip(task_url, executor.map(load_url, task_url)): print('%r page is %d bytes' % (url, len(data))) 恩，concurrent.future 中线程/进程池所提供的 map 方法和标准库中的 map 函数使用方法一样。 剖一下 concurrent.futures前面讲了怎么使用 concurrent.futures 后，我们都比较好奇，concurrent.futures 是怎么实现 Future 模式的。里面是怎么将任务和结果进行关联的。我们现在开始从 submit 方法着手来简单看一下 ProcessPoolExecutor 的实现。 首先，在初始化 ProcessPoolExecutor 时，它的 __init__ 方法中进行了一些关键变量的初始化操作。 1234567891011121314151617181920212223242526272829303132333435363738394041class ProcessPoolExecutor(_base.Executor): def __init__(self, max_workers=None): &quot;&quot;&quot;Initializes a new ProcessPoolExecutor instance. Args: max_workers: The maximum number of processes that can be used to execute the given calls. If None or not given then as many worker processes will be created as the machine has processors. &quot;&quot;&quot; _check_system_limits() if max_workers is None: self._max_workers = os.cpu_count() or 1 else: if max_workers &lt;= 0: raise ValueError(&quot;max_workers must be greater than 0&quot;) self._max_workers = max_workers # Make the call queue slightly larger than the number of processes to # prevent the worker processes from idling. But don't make it too big # because futures in the call queue cannot be cancelled. self._call_queue = multiprocessing.Queue(self._max_workers + EXTRA_QUEUED_CALLS) # Killed worker processes can produce spurious &quot;broken pipe&quot; # tracebacks in the queue's own worker thread. But we detect killed # processes anyway, so silence the tracebacks. self._call_queue._ignore_epipe = True self._result_queue = SimpleQueue() self._work_ids = queue.Queue() self._queue_management_thread = None # Map of pids to processes self._processes = {} # Shutdown is a two-step process. self._shutdown_thread = False self._shutdown_lock = threading.Lock() self._broken = False self._queue_count = 0 self._pending_work_items = {} 好了，我们来看看我们今天的入口 submit 方法 1234567891011121314151617def submit(self, fn, *args, **kwargs): with self._shutdown_lock: if self._broken: raise BrokenProcessPool('A child process terminated ' 'abruptly, the process pool is not usable anymore') if self._shutdown_thread: raise RuntimeError('cannot schedule new futures after shutdown') f = _base.Future() w = _WorkItem(f, fn, args, kwargs) self._pending_work_items[self._queue_count] = w self._work_ids.put(self._queue_count) self._queue_count += 1 # Wake up queue management thread self._result_queue.put(None) self._start_queue_management_thread() return f 首先，传入的参数 fn 是我们的处理函数，args 以及 kwargs 是我们要传递 fn 函数的参数。在 submit 函数最开始，首先根据 _broken 和 _shutdown_thread 的值来判断当前进程池中处理进程的状态以及目前进程池的状态。如果处理进程突然被销毁或者进程池已经被关闭，那么将抛出异常表明目前不再接受新的 submit 操作。 如果前面状态没有问题，首先，实例化 Future 类，然后将这个实例和处理函数和相关参数一起，作为参数来实例化 _WorkItem 类，然后将实例 w 作为 value ，_queue_count 作为 key 存入 _pending_work_items 中。然后调用 _start_queue_management_thread 方法开启进程池中的管理线程。现在来看看这部分代码 123456789101112131415161718192021def _start_queue_management_thread(self): # When the executor gets lost, the weakref callback will wake up # the queue management thread. def weakref_cb(_, q=self._result_queue): q.put(None) if self._queue_management_thread is None: # Start the processes so that their sentinels are known. self._adjust_process_count() self._queue_management_thread = threading.Thread( target=_queue_management_worker, args=(weakref.ref(self, weakref_cb), self._processes, self._pending_work_items, self._work_ids, self._call_queue, self._result_queue)) self._queue_management_thread.daemon = True self._queue_management_thread.start() _threads_queues[self._queue_management_thread] = self._result_queue 这一部分很简单，首先运行 _adjust_process_count 方法，然后开启一个守护线程，运行 _queue_management_worker 方法。我们首先来看看 _adjust_process_count 方法。 12345678def _adjust_process_count(self): for _ in range(len(self._processes), self._max_workers): p = multiprocessing.Process( target=_process_worker, args=(self._call_queue, self._result_queue)) p.start() self._processes[p.pid] = p 根据在 __init__ 方法中设定的 _max_workers 来开启对应数量的进程，在进程中运行 _process_worker 函数。 恩，顺藤摸瓜，我们先来看看 _process_worker 函数吧？ 12345678910111213141516171819202122232425262728def _process_worker(call_queue, result_queue): &quot;&quot;&quot;Evaluates calls from call_queue and places the results in result_queue. This worker is run in a separate process. Args: call_queue: A multiprocessing.Queue of _CallItems that will be read and evaluated by the worker. result_queue: A multiprocessing.Queue of _ResultItems that will written to by the worker. shutdown: A multiprocessing.Event that will be set as a signal to the worker that it should exit when call_queue is empty. &quot;&quot;&quot; while True: call_item = call_queue.get(block=True) if call_item is None: # Wake up queue management thread result_queue.put(os.getpid()) return try: r = call_item.fn(*call_item.args, **call_item.kwargs) except BaseException as e: exc = _ExceptionWithTraceback(e, e.__traceback__) result_queue.put(_ResultItem(call_item.work_id, exception=exc)) else: result_queue.put(_ResultItem(call_item.work_id, result=r)) 首先，这里搞了一个死循环，紧接着，我们从 call_queue 队列中获取一个 _WorkItem 实例，然后如果获取的值为 None 的话，那么证明没有新的任务进来了，我们可以把当前进程的 pid 放入结果队列中。然后结束进程。 如果收到了任务，那么执行这个任务。不管是在执行过程中发生异常，亦或者是得到最终的结果，都将其封装为 _ResultItem 实例，并将其放入结果队列中。 好了，我们回到刚刚看了一半的 _start_queue_management_thread 函数， 123456789101112131415161718192021def _start_queue_management_thread(self): # When the executor gets lost, the weakref callback will wake up # the queue management thread. def weakref_cb(_, q=self._result_queue): q.put(None) if self._queue_management_thread is None: # Start the processes so that their sentinels are known. self._adjust_process_count() self._queue_management_thread = threading.Thread( target=_queue_management_worker, args=(weakref.ref(self, weakref_cb), self._processes, self._pending_work_items, self._work_ids, self._call_queue, self._result_queue)) self._queue_management_thread.daemon = True self._queue_management_thread.start() _threads_queues[self._queue_management_thread] = self._result_queue 在执行完 _adjust_process_count 函数后，我们进程池中的 _processes 变量（它是一个 dict ）便关联了一些处理进程。然后我们开启一个后台守护线程，来执行 _queue_management_worker 函数，我们给它传了几个变量，首先 _processes 是我们的进程映射，_pending_work_items 中存放着我们待处理任务，还有 _call_queue 和 _result_queue 。好了还有一个参数大家可能不太理解，就是 weakref.ref(self, weakref_cb) 这货。 首先，Python 是一门具有垃圾回收机制的语言，有着 GC (Garbage Collection) 机制意味着我们在大多数时候，不太需要去关注内存的分配与回收。在 Python 中，什么时候对象会被回收是由其引用计数所决定的。当引用计数为 0 的时候，这个对象会被回收。在有一些情况下，我们对象因为交叉引用或者其余的一些原因，造成引用计数始终不为0，这意味着这个对象无法被回收。造成内存泄露。因此区别于我们普通的引用，Python 中新增了一个引用机制叫做弱引用，弱引用的意义在于，某个变量持有一个对象，却不会增加这个对象的引用计数。因此 weakref.ref(self, weakref_cb) 在大多数而言，等价于 self （至于这里为什么要使用弱引用，我们这里先不讲，会开一个单章来说） 好了，这一部分代码看完，我们来看看，_queue_management_worker 怎么实现的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115def _queue_management_worker(executor_reference, processes, pending_work_items, work_ids_queue, call_queue, result_queue): &quot;&quot;&quot;Manages the communication between this process and the worker processes. This function is run in a local thread. executor_reference: A weakref.ref to the ProcessPoolExecutor that owns Args: process: A list of the multiprocessing.Process instances used as this thread. Used to determine if the ProcessPoolExecutor has been garbage collected and that this function can exit. workers. pending_work_items: A dict mapping work ids to _WorkItems e.g. {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...} work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]). call_queue: A multiprocessing.Queue that will be filled with _CallItems derived from _WorkItems for processing by the process workers. result_queue: A multiprocessing.Queue of _ResultItems generated by the process workers. &quot;&quot;&quot; executor = None def shutting_down(): return _shutdown or executor is None or executor._shutdown_thread def shutdown_worker(): # This is an upper bound nb_children_alive = sum(p.is_alive() for p in processes.values()) for i in range(0, nb_children_alive): call_queue.put_nowait(None) # Release the queue's resources as soon as possible. call_queue.close() # If .join() is not called on the created processes then # some multiprocessing.Queue methods may deadlock on Mac OS X. for p in processes.values(): p.join() reader = result_queue._reader while True: _add_call_item_to_queue(pending_work_items, work_ids_queue, call_queue) sentinels = [p.sentinel for p in processes.values()] assert sentinels ready = wait([reader] + sentinels) if reader in ready: result_item = reader.recv() else: # Mark the process pool broken so that submits fail right now. executor = executor_reference() if executor is not None: executor._broken = True executor._shutdown_thread = True executor = None # All futures in flight must be marked failed for work_id, work_item in pending_work_items.items(): work_item.future.set_exception( BrokenProcessPool( &quot;A process in the process pool was &quot; &quot;terminated abruptly while the future was &quot; &quot;running or pending.&quot; )) # Delete references to object. See issue16284 del work_item pending_work_items.clear() # Terminate remaining workers forcibly: the queues or their # locks may be in a dirty state and block forever. for p in processes.values(): p.terminate() shutdown_worker() return if isinstance(result_item, int): # Clean shutdown of a worker using its PID # (avoids marking the executor broken) assert shutting_down() p = processes.pop(result_item) p.join() if not processes: shutdown_worker() return elif result_item is not None: work_item = pending_work_items.pop(result_item.work_id, None) # work_item can be None if another process terminated (see above) if work_item is not None: if result_item.exception: work_item.future.set_exception(result_item.exception) else: work_item.future.set_result(result_item.result) # Delete references to object. See issue16284 del work_item # Check whether we should start shutting down. executor = executor_reference() # No more work items can be added if: # - The interpreter is shutting down OR # - The executor that owns this worker has been collected OR # - The executor that owns this worker has been shutdown. if shutting_down(): try: # Since no new work items can be added, it is safe to shutdown # this thread if there are no pending work items. if not pending_work_items: shutdown_worker() return except Full: # This is not a problem: we will eventually be woken up (in # result_queue.get()) and be able to send a sentinel again. pass executor = None 熟悉的大循环，循环的第一步，利用 _add_call_item_to_queue 函数来将等待队列中的任务加入到调用队列中去，先来看看这一部分代码 123456789101112131415161718192021222324252627282930313233343536def _add_call_item_to_queue(pending_work_items, work_ids, call_queue): &quot;&quot;&quot;Fills call_queue with _WorkItems from pending_work_items. This function never blocks. Args: pending_work_items: A dict mapping work ids to _WorkItems e.g. {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...} work_ids: A queue.Queue of work ids e.g. Queue([5, 6, ...]). Work ids are consumed and the corresponding _WorkItems from pending_work_items are transformed into _CallItems and put in call_queue. call_queue: A multiprocessing.Queue that will be filled with _CallItems derived from _WorkItems. &quot;&quot;&quot; while True: if call_queue.full(): return try: work_id = work_ids.get(block=False) except queue.Empty: return else: work_item = pending_work_items[work_id] if work_item.future.set_running_or_notify_cancel(): call_queue.put(_CallItem(work_id, work_item.fn, work_item.args, work_item.kwargs), block=True) else: del pending_work_items[work_id] continue 首先，判断调用队列是不是已经满了，如果满了，则放弃这次循环。紧接着从 work_id 队列中取出，然后从等待任务中取出对应的 _WorkItem 实例。紧接着，调用实例中绑定的 Future 实例的 set_running_or_notify_cancel 方法来设置任务的状态，紧接着将其扔入调用队列中。 12345678910111213141516171819202122232425262728293031323334353637383940def set_running_or_notify_cancel(self): &quot;&quot;&quot;Mark the future as running or process any cancel notifications. Should only be used by Executor implementations and unit tests. If the future has been cancelled (cancel() was called and returned True) then any threads waiting on the future completing (though calls to as_completed() or wait()) are notified and False is returned. If the future was not cancelled then it is put in the running state (future calls to running() will return True) and True is returned. This method should be called by Executor implementations before executing the work associated with this future. If this method returns False then the work should not be executed. Returns: False if the Future was cancelled, True otherwise. Raises: RuntimeError: if this method was already called or if set_result() or set_exception() was called. &quot;&quot;&quot; with self._condition: if self._state == CANCELLED: self._state = CANCELLED_AND_NOTIFIED for waiter in self._waiters: waiter.add_cancelled(self) # self._condition.notify_all() is not necessary because # self.cancel() triggers a notification. return False elif self._state == PENDING: self._state = RUNNING return True else: LOGGER.critical('Future %s in unexpected state: %s', id(self), self._state) raise RuntimeError('Future in unexpected state') 这一部分内容很简单，检查当前实例如果处于等待状态，就返回 True ，如果处于被取消的状态，就返回 False , 在 _add_call_item_to_queue 函数中，会将已经处于 cancel 状态的 _WorkItem 从等待任务中移除。 好了，我们继续回到 _queue_management_worker 函数中去， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115def _queue_management_worker(executor_reference, processes, pending_work_items, work_ids_queue, call_queue, result_queue): &quot;&quot;&quot;Manages the communication between this process and the worker processes. This function is run in a local thread. executor_reference: A weakref.ref to the ProcessPoolExecutor that owns Args: process: A list of the multiprocessing.Process instances used as this thread. Used to determine if the ProcessPoolExecutor has been garbage collected and that this function can exit. workers. pending_work_items: A dict mapping work ids to _WorkItems e.g. {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...} work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]). call_queue: A multiprocessing.Queue that will be filled with _CallItems derived from _WorkItems for processing by the process workers. result_queue: A multiprocessing.Queue of _ResultItems generated by the process workers. &quot;&quot;&quot; executor = None def shutting_down(): return _shutdown or executor is None or executor._shutdown_thread def shutdown_worker(): # This is an upper bound nb_children_alive = sum(p.is_alive() for p in processes.values()) for i in range(0, nb_children_alive): call_queue.put_nowait(None) # Release the queue's resources as soon as possible. call_queue.close() # If .join() is not called on the created processes then # some multiprocessing.Queue methods may deadlock on Mac OS X. for p in processes.values(): p.join() reader = result_queue._reader while True: _add_call_item_to_queue(pending_work_items, work_ids_queue, call_queue) sentinels = [p.sentinel for p in processes.values()] assert sentinels ready = wait([reader] + sentinels) if reader in ready: result_item = reader.recv() else: # Mark the process pool broken so that submits fail right now. executor = executor_reference() if executor is not None: executor._broken = True executor._shutdown_thread = True executor = None # All futures in flight must be marked failed for work_id, work_item in pending_work_items.items(): work_item.future.set_exception( BrokenProcessPool( &quot;A process in the process pool was &quot; &quot;terminated abruptly while the future was &quot; &quot;running or pending.&quot; )) # Delete references to object. See issue16284 del work_item pending_work_items.clear() # Terminate remaining workers forcibly: the queues or their # locks may be in a dirty state and block forever. for p in processes.values(): p.terminate() shutdown_worker() return if isinstance(result_item, int): # Clean shutdown of a worker using its PID # (avoids marking the executor broken) assert shutting_down() p = processes.pop(result_item) p.join() if not processes: shutdown_worker() return elif result_item is not None: work_item = pending_work_items.pop(result_item.work_id, None) # work_item can be None if another process terminated (see above) if work_item is not None: if result_item.exception: work_item.future.set_exception(result_item.exception) else: work_item.future.set_result(result_item.result) # Delete references to object. See issue16284 del work_item # Check whether we should start shutting down. executor = executor_reference() # No more work items can be added if: # - The interpreter is shutting down OR # - The executor that owns this worker has been collected OR # - The executor that owns this worker has been shutdown. if shutting_down(): try: # Since no new work items can be added, it is safe to shutdown # this thread if there are no pending work items. if not pending_work_items: shutdown_worker() return except Full: # This is not a problem: we will eventually be woken up (in # result_queue.get()) and be able to send a sentinel again. pass executor = None result_item 变量 我们看看 首先，大家可能在这里有点疑问了 12345sentinels = [p.sentinel for p in processes.values()]assert sentinelsready = wait([reader] + sentinels) 这个 wait 是什么鬼啊，reader 又是什么鬼啊。一步步来。首先，我们看到，前面，reader = result_queue._reader 也会引起大家的疑问，这里我们 result_queue 是 multiprocess 里面的 SimpleQueue 啊，它没有 _reader 方法啊QAQ 1234567891011class SimpleQueue(object): def __init__(self, *, ctx): self._reader, self._writer = connection.Pipe(duplex=False) self._rlock = ctx.Lock() self._poll = self._reader.poll if sys.platform == 'win32': self._wlock = None else: self._wlock = ctx.Lock() 上面这贴出来的，是 SimpleQueue 的部分代码，我们可以很清楚的看到，SimpleQueue 本质是利用一个 Pipe 来进行进程间通信的，然后 _reader 是读取 Pipe 的一个变量。 Note : 大家可以复习下其余几种进程间通信的方法了 好了，这一部分看懂后，我们来看看 wait 方法吧。 1234567891011121314151617181920212223def wait(object_list, timeout=None): ''' Wait till an object in object_list is ready/readable. Returns list of those objects in object_list which are ready/readable. ''' with _WaitSelector() as selector: for obj in object_list: selector.register(obj, selectors.EVENT_READ) if timeout is not None: deadline = time.time() + timeout while True: ready = selector.select(timeout) if ready: return [key.fileobj for (key, events) in ready] else: if timeout is not None: timeout = deadline - time.time() if timeout &lt; 0: return ready 这一部分代码很简单，首先将我们待读取的对象，进行一次注册，然后当 timeout 为 None 的时候，就一直等待到有对象读取数据成功为止 好了，我们继续回到前面的 _queue_management_worker 函数中去，来看看这样一段代码 12345678910111213141516171819202122232425262728ready = wait([reader] + sentinels)if reader in ready: result_item = reader.recv()else: # Mark the process pool broken so that submits fail right now. executor = executor_reference() if executor is not None: executor._broken = True executor._shutdown_thread = True executor = None # All futures in flight must be marked failed for work_id, work_item in pending_work_items.items(): work_item.future.set_exception( BrokenProcessPool( &quot;A process in the process pool was &quot; &quot;terminated abruptly while the future was &quot; &quot;running or pending.&quot; )) # Delete references to object. See issue16284 del work_item pending_work_items.clear() # Terminate remaining workers forcibly: the queues or their # locks may be in a dirty state and block forever. for p in processes.values(): p.terminate() shutdown_worker() return 我们用 wait 函数来读取一系列对象，因为我们没有设置 Timeout ，所以当我们拿到可读取对象的结果时，如果 result_queue._reader 没有在列表中，那么意味着，有处理进程突然异常关闭了，这个时候，我们开始执行后面的语句来执行目前进程池的关闭操作。如果在列表中，我们读取数据，得到 result_item 变量 我们再看看下面的代码 123456789101112131415161718192021if isinstance(result_item, int): # Clean shutdown of a worker using its PID # (avoids marking the executor broken) assert shutting_down() p = processes.pop(result_item) p.join() if not processes: shutdown_worker() returnelif result_item is not None: work_item = pending_work_items.pop(result_item.work_id, None) # work_item can be None if another process terminated (see above) if work_item is not None: if result_item.exception: work_item.future.set_exception(result_item.exception) else: work_item.future.set_result(result_item.result) # Delete references to object. See issue16284 del work_item 首先，如果 result_item 变量是 int 类型的话，不知道大家还记不记得在 _process_worker 函数中有这样一段逻辑 123456call_item = call_queue.get(block=True)if call_item is None: # Wake up queue management thread result_queue.put(os.getpid()) return 当调用队列中没有新的任务时，将进程 pid 放入 result_queue 中。那么我们 result_item 如果值为 int 那么意味着，我们之前任务处理工作已经完毕，于是开始清理，关闭我们的进程池。 如果 result_item 既不为 int 也不为 None , 那么必然是 _ResultItem 的实例，我们根据 work_id 取出 _WorkItem 实例，并将产生的异常或者值和 _WorkItem 实例中的 Future 实例（也就是我们 submit 后返回的那货）进行绑定。 最后，删除这个 work_item ，完事儿，手工 最后洋洋洒洒写了一大篇辣鸡文章，希望大家不要介意，其实我们能看到 concurrent.future 的实现，其实并没有用什么高深的黑魔法，但是其中细节值得我们一一品味，所以这篇文章我们先写到这里。后面有机会的话，我们再去看看 concurrent.future 其余部分代码。也有蛮多值得品味的地方。 Reference1.Python 3 multiprocessing 2.Python 3 weakref 3.并发编程之Future模式 4.Python并发编程之线程池/进程池 5.Future 模式详解（并发使用）","link":"/posts/2017/11/27/something-about-concurrent-future/"},{"title":"怎么样去理解 Python 中的装饰器","text":"怎么样去理解 Python 中的装饰器首先，本垃圾文档工程师又来了。开始日常的水文写作。起因是看到这个问题如何理解Python装饰器？，正好不久前给人讲过这些，本垃圾于是又开始新的一轮辣鸡文章写作行为了。 预备知识首先要理解装饰器，首先要先理解在 Python 中很重要的一个概念就是：“函数是 First Class Member” 。这句话再翻译一下，函数是一种特殊类型的变量，可以和其余变量一样，作为参数传递给函数，也可以作为返回值返回。 12345678def abc(): print(&quot;abc&quot;)def abc1(func): func()abc1(abc) 这段代码的输出就是我们在函数 abc 中输出的 abc 字符串。过程很简单，我们将函数 abc 作为一个参数传递给 abc1 ，然后，在 abc1 中调用传入的函数 再来看一段代码 1234567def abc1(): def abc(): print(&quot;abc&quot;) return abcabc1()() 这段代码输出和之前的一样，这里我们将在 abc1 内部定义的函数 abc 作为一个变量返回，然后我们在调用 abc1 获取到返回值后，继续调用返回的函数。 好了，我们再来做一个思考题，实现一个函数 add ，达到 add(m)(n) 等价于 m+n 的效果。这题如果把之前的 First-Class Member 这一概念理清楚后，我们便能很清楚的写出来了 12345def add(m): def temp(n): return m+n return tempprint(add(1)(2)) 嗯，这里输出就是 3 。 正文看了前面的预备知识后，我们便可以开始今天的主题了 先来看一个需求吧现在我们有一个函数 12345def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 现在我们要给这个函数加上一些代码，来计算这个函数的运行时间。 我们大概一想，写出了这样的代码 1234567import timedef range_loop(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_result 先且不论，这样计算时间是不是准确的，现在我们要给如下很多函数加上一个时间计算的功能 12345678910111213141516171819import timedef range_loop(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_resultdef range_loop1(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_resultdef range_loop2(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_result 我们初略一想，嗯，Ctrl+C,Ctrl+V。emmmm 好了，现在你们不觉得这段代码特别脏么？我们想让他变得干净点怎么办？ 我们想了想，按照之前说的 First-Class Member 的概念。然后写出了如下的代码 12345678910111213141516171819202122import timedef time_count(func,a,b): time_flag=time.time() temp_result=func(a,b) print(time.time()-time_flag) return temp_result def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resulttime_count(range_loop,a,b)time_count(range_loop1,a,b)time_count(range_loop2,a,b) 嗯，看起来像那么回事，好了好了，我们现在新的问题又来了，我们现在是假设，我们所有函数都只有两个参数传入，那么现在如果想支持任意参数的传入怎么办？我们眉头一皱，写下了如下的代码 123456789101112131415161718192021222324import timedef time_count(func,*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resulttime_count(range_loop,a,b)time_count(range_loop1,a,b)time_count(range_loop2,a,b) 好了，现在看起来，有点像模像样了，但是我们再想想，这段代码实际上改变了我们的函数调用方式，比如我们直接运行 range_loop(a,b) 还是没有办法获取到函数执行时间。那么现在我们如果不想改变函数的调用方式，又想获取到函数的运行时间怎么办？ 很简单嘛，替换一下不就好了 12345678910111213141516171819202122232425262728import timedef time_count(func): def wrap(*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result return wrap def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resultrange_loop=time_count(range_loop)range_loop1=time_count(range_loop1)range_loop2=time_count(range_loop2)range_loop(1,2)range_loop1(1,2)range_loop2(1,2) emmmm，这样看起来感觉舒服多了？既没有改变原有的运行方式，也输出了函数运行时间。 但是。。。你们不觉得手动替换太恶心了么？？？喵喵喵？？？还有什么可以简化下的么？？ 好了，Python 知道我们是爱吃糖的孩子，给我们提供了一个新的语法糖，这也是今天的男一号，Decorator 装饰器 说说 Decorator我们前面已经实现了，在不改变函数特性的情况下，给原有的代码新增一点功能，但是我们也觉得这样手动的替换，太恶心了，是的 Python 官方也觉得这样很恶心，所以新的语法糖来了 我们上面的代码可以写成这样了 12345678910111213141516171819202122232425262728import timedef time_count(func): def wrap(*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result return wrap@time_count def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result@time_countdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_result@time_countdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resultrange_loop(1,2)range_loop1(1,2)range_loop2(1,2) 哇，写到这里，你是不是恍然大悟！まさか？？？是的，其实 @ 符号其实是一个语法糖，他将我们之前的手动替换的过程交给了环境执行。好了用人话描述下，@ 的作用是将被包裹的函数作为一个变量传递给装饰函数/类，将装饰函数/类返回的值替换原本的函数。 123@decoratordef abc(): pass 如同前面所讲的一样，实际上是发生了一个特殊的替换过程 abc=decorator(abc) ，好了我们来做几个题来练习下吧？ 1234567def decorator(func): return 1@decoratordef abc(): passabc() 这段代码会发生什么？答：会抛出异常。为啥啊？答：因为装饰的时候发生了替换，abc=decorator(abc) ，替换后 abc 的值为 1 。整数默认不能作为一个函数进行调用。 12345678910111213141516171819202122232425262728def time_count(func): def wrap(*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result return wrapdef decorator(func): def wrap(*args,**kwargs): temp_result=func(*args,**kwargs) return temp_result return wrapdef decorator1(func): def wrap(*args,**kwargs): temp_result=func(*args,**kwargs) return temp_result return wrap@time_count@decorator@decorator1 def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 这段代码怎么替换的？答：time_count(decorator(decorator1(range_loop))) 嗯，现在是不是对装饰器什么的有了基本的了解？ 扩展一下现在，我想修改下前面写的 time_count 函数，让他支持传入一个 flag 参数，当 flag 为 True 的时候，输出函数运行时间，为 False 的时候不输出时间 \b我们一步步来，我们先假设新的函数叫做 time_count_plus 我们想实现的效果是这样的 12345@time_count_plus(flag=True)def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 嗯，我们看了下，首先我们调用了 time_count_plus(flag=True) 一次，将它返回的值作为一个装饰函数来替换 range_loop ，OK 那么我们首先 time_count_plus 要接收一个参数，返回一个函数对吧 1234def time_count_plus(flag=True): def wrap1(func): pass return wrap1 好了，现在返回了一个函数来作为装饰函数，然后我们说了 @ 其实触发了一次替换过程，好那么我们现在的替换是不是 range_loop=time_count_plus(flag=True)(range_loop) 好了，现在大家应该很清楚了，我们在 wrap1 里面是不是还应该有一个函数并返回？ 嗯，最终的代码如下 123456789101112131415161718def time_count_plus(flag=True): def wrap1(func): def wrap2(*args,**kwargs): if flag: time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) else: temp_result=func(*args,**kwargs) return temp_result return wrap2 return wrap1@time_count_plus(flag=True)def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 是不是这样就清楚多啦！ 扩展两下好了，我们现在有新的需求来了 1234567891011121314m=3n=2def add(a,b): return a+bdef sub(a,b): return a-bdef mul(a,b): return a*bdef div(a,b): return a/b 现在我们有字符串 a , a 的值可能为 +、-、*、/ 那么现在，我们想根据 a 的值来调用对应的函数怎么办？ 我们煎蛋一想，嗯，逻辑判断嘛 1234567891011121314151617181920212223m=3n=2def add(a,b): return a+bdef sub(a,b): return a-bdef mul(a,b): return a*bdef div(a,b): return a/ba=input('请输入 + - * / 中的任意一个\\n')if a=='+': print(add(m,n))elif a=='-': print(sub(m-n))elif a=='*': print(mul(m,n))elif a=='/': print(div(m,n)) 但是这段代码，if else 是不是太多了点？我们仔细一想，用一下 First-Class Member 的特性，然后配合 dict 实现操作符和函数之间的关联。 12345678910111213141516m=3n=2def add(a,b): return a+bdef sub(a,b): return a-bdef mul(a,b): return a*bdef div(a,b): return a/bfunc_dict={&quot;+&quot;:add,&quot;-&quot;:sub,&quot;*&quot;:mul,&quot;/&quot;:div}a=input('请输入 + - * / 中的任意一个\\n')func_dict[a](m,n) emmmm，看起来不错啊，但是我们注册的过程能不能再简化一点？ 嗯，这个时候装饰器语法特性就能用上了 1234567891011121314151617181920212223m=3n=2func_dict={}def register(operator): def wrap(func): func_dict[operator]=func return func return wrap@register(operator=&quot;+&quot;)def add(a,b): return a+b@register(operator=&quot;-&quot;)def sub(a,b): return a-b@register(operator=&quot;*&quot;)def mul(a,b): return a*b@register(operator=&quot;/&quot;)def div(a,b): return a/ba=input('请输入 + - * / 中的任意一个\\n')func_dict[a](m,n) 嗯，还记得我们前面说的使用 @ 语法的时候，实际上是触发了一个替换的过程么？这里就是利用这一特性，在装饰器触发的时候，注册函数映射，这样我们直接根据 ‘a’ 的值来获取函数处理数据。另外请注意一点，我们这里没有必要修改原函数，所以我们没有必要写第三层的函数。 如果有熟悉 Flask 同学就知道，在调用 route 方法注册路由的时候，也是使用了这一特性 ，可以参考另外一篇很久前写的垃圾水文 菜鸟阅读 Flask 源码系列（1）：Flask的router初探 总结其实全文下来，大家应该能知道这样一点东西。Python 中的装饰器其实是 First-Class Member 概念的更进一层应用，我们将函数传递给其余函数，包裹上新的功能后再行返回。@ 其实只是将这样一个过程进行了简化而已。在 Python 中，装饰器无处不在，很多官方库中的实现也依赖于装饰器，比如很久之前写过这样一篇垃圾水文 菜鸟阅读 Flask 源码系列（1）：Flask的router初探。 嗯，今天就先写到这里吧！","link":"/posts/2018/02/22/something-about-decorator/"},{"title":"Linux 上关于 inotify 的小笔记","text":"最近还是无心写啥文章，说好的写几篇关于 Raft 的论文也因为一些事 delay 了。但是想了想还是准备写点什么，于是写个小的水文来记录下关于今天碰到的一个 Linux 内核参数的问题， 顺便做个笔记 开始我是一个不太喜欢 Mac 的人，所以我自己在家使用的开发环境是 Manjaro（这里打个广告，非常棒的发行版，堪称开箱即用，广告五毛一条）。然后代码工具就是 Jetbrains 的全家桶和 VSCode 搭配使用。 今天打开 Goland 的时候，发现 IDE 给了这样一个 Warning ，External file changes sync may be slow: The current inotify(7) watch limit is too low. 于是大家知道，我是个看着这些 warning 有强迫症的人，于是我就去查了查 简单聊聊我们平常经常会有需求，去监控一个文件或者一个目录下的变化，比如创建文件，删除文件等。我们常规的做法可能是一个直接暴力轮询的方式来做 但是这样的性能会极差。那么我们有没有什么手段来处理一下这个事么？ 有的！ Linux 提供了对应的 API 来处理这事，这就是我们今天要聊到的 inotify 按照官方的说法，inotify 其实很简单， The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory. 大意就是说 inotify 是用来监控文件系统事件的。可以使用在单个文件或者目录上。被监听的文件目录本身的变化或者内部文件的变化都在监听范围内。 在监听了对应的文件后，inotify 将返回如下事件 IN_ACCESS 文件可读 IN_ATTRIB 元数据变化 IN_CLOSE_WRITE File opened for writing was closed IN_CLOSE_NOWRITE File not opened for writing was closed IN_CREATE 被监听的目录下有文件/目录被创建 IN_DELETE 被监听的目录下有文件/目录被删除 IN_DELETE_SELF 被监听的文件/目录被删除 IN_MODIFY 文件被修改 IN_MOVE_SELF 被监听的文件/目录被移动 IN_MOVED_FROM 有文件/目录从被监听的目录中被移出 IN_MOVED_TO 有文件/目录移动至被监听的目录中 IN_OPEN 文件被打开 总共12类事件，已经能涵盖住我们常见的需求。但是 inotify 也有其自己的弊端。 不支持递归监听。举个例子，我监听 A 目录，我可以捕获到在 A 目录下创建 B 目录这个事件。但是我们没法监听到 B 目录下事件，除非将 B 目录也添加到监听队列中 Python 可用的 inotify 很少 对于第一个缺陷。常见的解决手段，是我们自行实现递归监听。当主目录下存在创建文件/目录事件的时候，我们将对应的文件/目录也添加到监听队列中。 但是这样就带来一个新的问题。如果一个非常大的项目，我们按照这样的方式去做，那么最后对应的内存损耗是很吓人的。所以在 inotify 设计之初，就通过一些内核参数做了一些限制 我们常见的有两个 /proc/sys/fs/inotify/max_queued_events 限制事件队列长度，一旦出现事件堆积，那么新的事件将被废弃 /proc/sys/fs/inotify/max_user_watches 限制每个 User ID 能够创建的 watcher 数，以免监听过多导致内存爆炸 在默认情况下 max_user_watches 的值取决于不同的 Linux 发行版，对于大多数发行版而言，其值相对较小。也就是说一旦达到限制，那么将没法添加新的 watcher。这也是 IDE 为什么会提示External file changes sync may be slow: The current inotify(7) watch limit is too low. 的原因 可以通过修改 /etc/sysctl.conf 来修改对应的参数，最后解决这个问题 最后Linux 果然是个宝库。感觉隔三差五就会遇到自己没涉及到的东西。所以还是记录下来，当作一篇水文，顺便供自己参阅","link":"/posts/2019/07/02/something-about-file-system-watch/"},{"title":"Flask 中的 Context 初探","text":"Flask 中的 Context 初探大家新年好！鉴于今年春晚非常好看，我觉得承受不起，于是来写点辣鸡水文娱乐下大家，这也是之前立的若干 Flag 中的一个 正文做过 Flask 开发的朋友都知道 Flask 中存在着两个概念，一个叫 App Context , 一个叫 Request Context 。 这两个算是 Flask 中很独特的一种机制。 从一个 Flask App 读入配置并启动开始，就进入了 App Context，在其中我们可以访问配置文件、打开资源文件、通过路由规则反向构造 URL。当 WSGI Middleware 调用 Flask App 的时候开始，就进入了 Request Context 。我们可以获取到其中的 HTTP HEADER 等操作，同时也可以进行 SESSION 等操作。 不过作为辣鸡选手而言，经常分不清为什么会存在这两个 Context ，没事，我们慢慢来说一说。 预备知识首先要清楚一点，我们要在同一个进程中隔离不同线程的数据，那么我们会优先选择 threading.local ，来实现数据彼此隔离的需求。但是现在有个问题来了，现在我们并发模型可能并不是只有传统意义上的进程-线程模型。也有可能是 coroutine(协程) 模型。常见的就是 Greenlet/Eventlet 。在这种情况下，threading.local 就没法很好的满足我们的需求。于是 Werkzeug 实现了自己的 Local 即 werkzeug.local.Local 那么 Werkzeug 自己实现的 Local 和标准的 threading.local 相比有什么不同呢？我们记住最大的不同点在于 前者会在 Greenlet 可用的情况下优先使用 Greenlet 的 ID 而不是线程 ID 以支持 Gevent 或 Eventlet 的调度，后者只支持多线程调度； Werkzeug 另外还实现了两种数据结构，一个叫 LocalStack ，一个叫做 LocalProxy LocalStack 是基于 Local 实现的一个栈结构。栈的特性就是后入先出。当我们进入一个 Context 时，将当前的的对象推入栈中。然后我们也可以获取到栈顶元素。从而获取到当前的上下文信息。 LocalProxy 是代理模式的一种实现。在实例化的时候，传入一个 callable 的参数。然后这个参数被调用后将会返回一个 Local 对象。我们后续的所有操作，比如属性调用，数值计算等，都会转发到这个参数返回的 Local 对象上。 现在大家可能不太清楚，我们为什么要用 LocalProxy 来进行操作，我们来给大家看一个例子 1234567891011121314from werkzeug.local import LocalStacktest_stack = LocalStack()test_stack.push({'abc': '123'})test_stack.push({'abc': '1234'})def get_item(): return test_stack.pop()item = get_item()print(item['abc'])print(item['abc']) 你看我们这里的输出的的值，都是统一的 1234 ，但是我们这里想做到的是每次获取的值都是栈顶的最新的元素，\b那么我们这个时候就应该用 proxy 模式了 12345678910111213from werkzeug.local import LocalStack, LocalProxytest_stack = LocalStack()test_stack.push({'abc': '123'})test_stack.push({'abc': '1234'})def get_item(): return test_stack.pop()item = LocalProxy(get_item)print(item['abc'])print(item['abc']) 你看我们这里\b就是 Proxy 的妙用。 Context由于 Flask 基于 Werkzeug 实现，因此 App Context 以及 Request Context 是基于前文中所说的 LocalStack 实现。 从命名上，大家应该可以看出，App Context 是代表应用上下文，可能包含各种配置信息，比如日志配置，数据库配置等。而 Request Context 代表一个请求上下文，我们可以获取到当前请求中的各种信息。比如 body 携带的信息。 这两个上下文的定义是在 flask.ctx 文件中，分别是 AppContext 以及 RequestContext 。而构建上下文的操作则是将其推入在 flask.globals 文件中定义的 _app_ctx_stack 以及 _request_ctx_stack 中。前面说了 LocalStack 是“线程”（这里可能是传统意义上的线程，也有可能是 Greenlet 这种）隔离的。\b\b\b\b\b\b\b同时 Flask 每个线程只处理一个请求，因此可以做到请求隔离。 当 app = Flask(__name__) 构造出一个 Flask App 时，App Context 并不会被自动推入 Stack 中。所以此时 Local Stack 的栈顶是空的，current_app 也是 unbound 状态。 1234567891011121314from flask import Flaskfrom flask.globals import _app_ctx_stack, _request_ctx_stackapp = Flask(__name__)_app_ctx_stack.top_request_ctx_stack.top_app_ctx_stack()# &lt;LocalProxy unbound&gt;from flask import current_appcurrent_app# &lt;LocalProxy unbound&gt; 作为 web 时，当请求进来时，我们开始进行上下文的相关操作。整个流程如下： 好了现在有点问题： 为什么要区分 App Context 以及 Request Context 为什么要用栈结构来实现 Context ？ 很久之前看过的松鼠奥利奥老师的博文Flask 的 Context 机制 解答了这个问题 这两个做法给予我们 多个 Flask App 共存 和 非 Web Runtime 中灵活控制 Context 的可能性。 我们知道对一个 Flask App 调用 app.run() 之后，进程就进入阻塞模式并开始监听请求。此时是不可能再让另一个 Flask App 在主线程运行起来的。那么还有哪些场景需要多个 Flask App 共存呢？前面提到了，一个 Flask App 实例就是一个 WSGI Application，那么 WSGI Middleware 是允许使用组合模式的，比如： 123456789from werkzeug.wsgi import DispatcherMiddlewarefrom biubiu.app import create_appfrom biubiu.admin.app import create_app as create_admin_appapplication = DispatcherMiddleware(create_app(), { '/admin': create_admin_app()}) 奥利奥老师文中举了一个这样一个例子，Werkzeug 内置的 Middleware 将两个 Flask App 组合成一个一个 WSGI Application。这种情况下两个 App 都同时在运行，只是根据 URL 的不同而将请求分发到不同的 App 上处理。 但是现在很多朋友有个问题，就是为什么这里不用 Blueprint ？ Blueprint 是在同一个 App 下运行。其挂在 App Context 上的相关信息都是一致的。但是如果要隔离彼此的信息的话，那么用 App Context 进行隔离，会比我们用变量名什么的隔离更为方便 Middleware 模式是 WSGI 中允许的特性，换句话来讲，我们将 Flask 和另外一个遵循 WSGI 协议的 web Framework （比如 Django）那么也是可行的。 但是 Flask 的两种 Context 分离更大的意义是为了非 web 应用的场合。Flask 官方文档中有这样一段话 The main reason for the application’s context existence is that in the past a bunch of functionality was attached to the request context for lack of a better solution. Since one of the pillars of Flask’s design is that you can have more than one application in the same Python process. 这句话换句话说 App Context 存在的意义是针对一个进程中有多个 Flask App 场景，这样场景最常见的就是我们用 Flask 来做一些离线脚本的代码。 好了，我们来聊聊 Flask 非 Web 应用的场景 比如，我们有个插件叫 Flask-SQLAlchemy然后这里有个使用场景首先我们现在有这样一个代码 123456789101112131415from flask import Flaskfrom flask_sqlalchemy import SQLAlchemydatabase = Flask(__name__)database.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'db = SQLAlchemy(database)class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return '&lt;User %r&gt;' % self.username 这里你应该注意到最开始的几个关键点，第一个，就是 database.config ，是的没错，Flask-SQLAlchemy 就是从当前的 app 中获取到对应的 config 信息来建立数据库链接。那么传递 app 的方式有两种，第一种，就是直接如上图一样，直接 db = SQLAlchemy(database) ，这个很容易理解，第二种，如果我们不传的话，那么 Flask-SQLAlchemy 中通过 current_app 来获取当前的 app 然后获取对应的 config 建立链接。那么问题来了，为什么会存在第二种这种方法呢 给个场景吧，现在我两个数据库配置不同的 app 共用一个 Model 那么应该怎么做？其实很简单 首先写 一个 model 文件，比如就叫 data/user_model.py 吧 1234567891011from flask_sqlalchemy import SQLAlchemydb = SQLAlchemy()class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return '&lt;User %r&gt;' % self.username 好了，那么在我们的应用文件中，我们便可以这样写 1234567891011121314151617181920from data.user_model import Userdatabase = Flask(__name__)database.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'with database.app_context(): db.init_app(current_app) db.create_all() admin = User(username='admin', email='admin@example.com') db.session.add(admin) db.session.commit() print(User.query.filter_by(username=&quot;admin&quot;).first())database1 = Flask(__name__)database1.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test1.db'with database1.app_context(): db.init_app(current_app) db.create_all() admin = User(username='admin_test', email='admin@example.com') db.session.add(admin) db.session.commit() print(User.query.filter_by(username=&quot;admin&quot;).first()) 你看这样是不是就好懂了一些，通过 app context ，我们 Flask-SQLAlchemy 可以通过 current_app 来获取当前 app ，继而获取相关的 config 信息 这个例子还不够妥当，我们现在再来换一个例子 12345678910111213141516171819202122from flask import Flask, current_appimport loggingapp = Flask(&quot;app1&quot;)app2 = Flask(&quot;app2&quot;)app.config.logger = logging.getLogger(&quot;app1.logger&quot;)app2.config.logger = logging.getLogger(&quot;app2.logger&quot;)app.logger.addHandler(logging.FileHandler(&quot;app_log.txt&quot;))app2.logger.addHandler(logging.FileHandler(&quot;app2_log.txt&quot;))with app.app_context(): with app2.app_context(): try: raise ValueError(&quot;app2 error&quot;) except Exception as e: current_app.config.logger.exception(e) try: raise ValueError(&quot;app1 error&quot;) except Exception as e: current_app.config.logger.exception(e) 好了，这段代码很清晰了，含义很清晰，就是通过获取当前上下文中的 app 中的 logger 来输出日志。同时这段代码也很清晰的说明了，我们为什么要用栈这样一种数据结构来维护上下文。 首先看一下 app_context() 的源码 123456789101112131415def app_context(self): &quot;&quot;&quot;Binds the application only. For as long as the application is bound to the current context the :data:`flask.current_app` points to that application. An application context is automatically created when a request context is pushed if necessary. Example usage:: with app.app_context(): ... .. versionadded:: 0.9 &quot;&quot;&quot; return AppContext(self) 嗯，很简单，只是构建一个 AppContext 对象返回，然后我们看看相关的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class AppContext(object): &quot;&quot;&quot;The application context binds an application object implicitly to the current thread or greenlet, similar to how the :class:`RequestContext` binds request information. The application context is also implicitly created if a request context is created but the application is not on top of the individual application context. &quot;&quot;&quot; def __init__(self, app): self.app = app self.url_adapter = app.create_url_adapter(None) self.g = app.app_ctx_globals_class() # Like request context, app contexts can be pushed multiple times # but there a basic &quot;refcount&quot; is enough to track them. self._refcnt = 0 def push(self): &quot;&quot;&quot;Binds the app context to the current context.&quot;&quot;&quot; self._refcnt += 1 if hasattr(sys, 'exc_clear'): sys.exc_clear() _app_ctx_stack.push(self) appcontext_pushed.send(self.app) def pop(self, exc=_sentinel): &quot;&quot;&quot;Pops the app context.&quot;&quot;&quot; try: self._refcnt -= 1 if self._refcnt &lt;= 0: if exc is _sentinel: exc = sys.exc_info()[1] self.app.do_teardown_appcontext(exc) finally: rv = _app_ctx_stack.pop() assert rv is self, 'Popped wrong app context. (%r instead of %r)' \\ % (rv, self) appcontext_popped.send(self.app) def __enter__(self): self.push() return self def __exit__(self, exc_type, exc_value, tb): self.pop(exc_value) if BROKEN_PYPY_CTXMGR_EXIT and exc_type is not None: reraise(exc_type, exc_value, tb) emmmm，首先 push 方法就是将自己推入 _app_ctx_stack ，而 pop 方法则是将自己从栈顶推出。然后我们看到两个方法含义就很明确了，在进入上下文管理器的时候，将自己推入栈，然后退出上下文管理器的时候，将自己推出。 我们都知道栈的一个性质就是，后入先出，栈顶的永远是最新插入进去的元素。而看一下我们 current_app 的源码 123456789def _find_app(): top = _app_ctx_stack.top if top is None: raise RuntimeError(_app_ctx_err_msg) return top.app current_app = LocalProxy(_find_app) 嗯，很明了了，就是获取当前栈顶的元素，然后进行相关操作。 嗯，通过这样对于栈的不断操作，就能让 current_app 获取到元素是我们当前上下文中的 app 。 额外的讲解: gg 也是我们常用的几个全局变量之一。在最开始这个变量是挂载在 Request Context 下的。但是在 0.10 以后，g 就是挂载在 App Context 下的。可能有同学不太清楚为什么要这么做。 首先，说一下 g 用来干什么 官方在上下文这一张里有这一段说明 The application context is created and destroyed as necessary. It never moves between threads and it will not be shared between requests. As such it is the perfect place to store database connection information and other things. The internal stack object is called flask._app_ctx_stack. Extensions are free to store additional information on the topmost level, assuming they pick a sufficiently unique name and should put their information there, instead of on the flask.g object which is reserved for user code. 大意就是说，数据库配置和其余的重要配置信息，就挂载 App 对象上。但是如果是一些用户代码，比如你不想一层层函数传数据的话，然后有一些变量需要传递，那么可以挂在 g 上。 同时前面说了，Flask 并不仅仅可以当做一个 Web Framework 使用，同时也可以用于一些非 web 的场合下。在这种情况下，如果 g 是属于 Request Context 的话，那么我们要使用 g 的话，那么就需要手动构建一个请求，这无疑是不合理的。 最后大年三十写这篇文章，现在发出来，我的辣鸡也是无人可救了。Flask 的上下文机制是其最重要的特性之一。通过合理的利用上下文机制，我们可以再更多的场合下去更好的利用 flask 。嗯，本次的辣鸡文章写作活动就到此结束吧。希望大家不会扔我臭鸡蛋！然后新年快乐！","link":"/posts/2018/02/22/something-about-flask-context/"},{"title":"随便聊聊 PEP570","text":"最近沉迷与 MIT 6.824 这门分布式系统的课，无心写文章。不过看到 PEP570 被接受了，决定还是写篇水文随便聊聊 PEP 570 Python 的 argument在聊 PEP570 之前，我们先要来看看 Python 的 argument 变迁 早在 Python 1.0 或更早，Python 的 argument 系统就已经支持我们现在主要使用的两种参数形式了，一种是 positional 一种是 keyword，举几个例子 1234567891011def abc(a, b, c): passabc(1, 2, 3)abc(1, 2, c=3)abc(1, b=2, c=3)abc(*(1, 2, 3))abc(**{&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3}) 这是不是我们常见的集中使用方式？ 在发展了很长一段时间后，虽然期间有一些提案对 Python 的 argument 系统做优化和增强，但是一直都被 Reject，直到 PEP3102 的出现 3102 主要引入了一个概念叫做 Keyword-Only Arguments，给个例子 有这样一个函数定义 12def abc(a, *, b, c): pass 那么这个函数只支持这样几种方式调用 1234567def abc(a, *, b, c): passabc(**{&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3})abc(a=1, b=2, c=3)abc(1, b=2, c=3) OK，大概聊完 Argument 一个迭代的过程，我们来聊聊 570 这个提案 随便聊聊 PEP 570570 做的事情其和 3102 类似，3102 是引入语法糖，让函数支持 keyword-only 的使用方式，那么 570 就是让函数支持 positional-only 的使用方式 假定有这样一个函数定义 12def abc(a, b, /, c): pass 那么 570 使得函数只支持这样的调用方式 123456def abc(a, b, /, c): passabc(1, 2, c=3)abc(1, 2, 3) 如果不这样做会怎么样呢？我们可以来试试，目前 PEP570 有一个实现，参见 bpo-36540: PEP 570 – Implementation，我们来编译测试一下，效果如下 碎碎念很多人其实没想清楚关于 570 的存在意义，PEP 570 上也提到了很多 Motivation 。不过我自己觉得，它和 3102 一样都是在践行一个理念，则 explicit is better than implicit 换句话说，如果要尽可能的保证代码风格的一致性，我们需要一定程度上语法特性的支持。而 570 和 3102 就是解决这样的问题。 所以从我自己的角度来说，我觉得 570 是个蛮重要的提案，也是很有意义的提案（都是 PEP57x ，为啥大家待遇能差这么多呢？（笑 对了，讲个段子， Python 的 Core 之一 Serhiy Storchaka 非常喜欢这个 PEP，然后在 PEP570 的实现没合并到主分支之前，就已经先把内置的一些库给改良了一下，大家可以去围观一下 PR [WIP] Use PEP 570 syntax for positional-only parameters 好了，今天的水文就到此结束了。。我写文章真的是越来越水了。。","link":"/posts/2019/04/27/something-about-pep-570/"},{"title":"关于 pyright","text":"关于 pyrightPEP 484，出来也快四年了。正好今天看到一个新库，写个短文，安利下&amp;吐槽下。 关于 PEP 484PEP 484，14年正式提出，15年正式接纳，成为 Python 3.5 以后的标准的一部分。简而言之是通过额外的语法，来为 Python 引入静态类型检查的例子 举个简单例子 1234def return_callback(flag: bool, callback: typing.Callable[[int, int], int])-&gt; int: if not flag: return None return callback(1, 2) 我们通过这样的类型静态标注，来增加可读性以及静态检查的能力。具体内容，可以参看我去年在 BPUG 上的分享的 slide。我最近也会抽出时间，详细聊聊 Type Hint 的前世今生（flag+1） 静态检查静态检查的意义在于，能及时发现低级错误，及时检查，可以很方便的集成进 CI 或者 Git Hook 中 举个简单例子 目前而言，主流的静态检查工具有两种 Python 官方和 484 配套出的 mypy Google 出的 pytype mypy 目前的问题有： 性能较差 对于新特性接入持保守态度 所以后续 Google 选择了推出自己的静态检查方案 pytype ，其性能相对于 mypy 来讲，性能和易用性也有了比较大的提升， 而目前，“开源急先锋“微软也在今天推出了自己的静态检查工具 pyright。目前在保证了对 Type Hint 周边特性兼容的情况下，宣称性能相较于 mypy 有5倍的提升 而这对于大项目的 CI 来讲是一个极大的利好 不过目前，关于 pyright 的潜在风险点可能还有这样的问题 基于 TypeScript 开发，运行环境基于 node，这可能会带来 CI 集成的难度。 易用性和可靠性还存在不足 可能和 IDE编辑器等配套的插件不足（官方也说，目前 VSCode 的插件还在开发中） 不过，pyright 还是值得大家在私下尝鲜的。后续我也会尝试阅读下 pyright 的实现，看看微软的实现思路（flag+2) 嗯，本文就到这里，这应该是我写过最水的文章了。","link":"/posts/2019/03/24/something-about-pyright/"},{"title":"聊聊 Python 中生成器和协程那点事儿","text":"文章来源：itsCoder 的 WeeklyBolg 项目 itsCoder主页：http://itscoder.com/ 作者：Manjusaka 审阅者：allenwu,Brucezz 写在前面的话本来想这周继续写写 Flask 那点破事儿的，但是想了想决定换换口味，来聊聊很不容易理解但是很重要的 Python 中的生成器和协程。 Generators 科普我猜大家对于生成器肯定并不陌生，但是为了能让我愉快的继续装逼，我们还是用点篇幅讲一下什么是生成器吧。比如在 Python 里，我们想生成一个范围 (1,100000) 的一个 list，于是我们无脑写了如下的代码出来 12345def generateList(start,stop): tempList=[] for i in range(start,stop): tempList.append(i) return tempList 注1：这里有同学提出了为什么我们不直接返回 range(start,stop)，Nice question，这里涉及到一个基础问题，range 的机制究竟是怎样的。这就要分版本而论了，在 Python 2.x 的版本中，range(start,stop) 其实本质上是预先生成一个 list ,而 list 对象是一个 Iterator ，因此可以被 for 语句所使用。然后在 Python 2.x 中还有一个语句叫做 xrange ，其生成的是一个 Generator 对象。在 Python 3 中事情发生了一点变化，可能社区觉得 range 和 xrange 分裂太过蛋疼，于是将其合并，于是现在在 Python 3 中，取消了 xrange 的语法糖，然后 range 的机制也变成生成一个 Generator 而不是 list 但是大家考虑过一个问题么，如果我们想生成数据量非常大，预先生成数据的行为无疑是很不明智的，这样会耗费大量的内存。于是 Python 给我们提供了一种新的姿势，Generator (生成器) 12345678def generateList1(start,stop): for i in range(start,stop): yield iif __name__==&quot;__main__&quot;: c=generateList1(1,100000) for i in c: print(i) 是的，Generator 其中一个特性就是不是一次性生成数据，而是生成一个可迭代的对象，在迭代时，根据我们所写的逻辑来控制其启动时机。 Generator 深入这里可能有一个问题，大家肯定想问 Python 开发者们不可能为了这一种使用场景而去单独创建一个 Generator 机制吧，那么我们 Generator 还有其余的使用场景么。当然，请看标题，对了嘛，Generator 另一个很大作用可以说就是当做协程使用。不过在这之前，我们要去深入的了解下 Generator 才能方便我们后面的讲解。 关于 Generator 中的内建方法关于 Python 中可迭代对象的一点背景知识首先，我们来看看 Python 中的迭代过程。在 Python 中迭代有两个概念，一个是 Iterable ，另一个是 Iterator 。让我们分别来看看第N次首先，Iterable 近似的可以理解成为一个协议，判断一个 Object 是否是 Iterable 的方法就是看其实现了 iter 与否，如果实现了 iter ，那么这便可以认为是一个 Iterable 对象。空谈误国，实干兴邦，让我们直接来看一段代码理解下 12345678910111213141516171819class Counter: def __init__(self, low, high): self.current = low self.high = high def __iter__(self): return self def next(self): # Python 3: def __next__(self) if self.current &gt; self.high: raise StopIteration else: self.current += 1 return self.current - 1if __name__ == '__main__': a=Counter(3,8) for c in a: print(c) 好了，让我们来看看上面这段代码里发生了什么，首先 for 语句的引用首先去判断迭代的是 Iterable 对象还是 Iterator 对象，如果是实现了 __iter__ 方法的对象，那么就是一个 Iterable 对象，for 循环首先调用对象的 __iter__ 方法来获取一个 Iterator 对象。那么什么是 Iterator 对象呢，这里可以近似的理解为是实现了 next() 方法（注：在Python3中是 next 方法)。 OK，让我们继续回到刚刚说到的那里，在上面的代码中 for 语句首先判断是一个 Iterable 对象还是 Iterator 对象，如果是 Iterable 对象那么调用其 iter 方法来获取一个 Iterator 对象，接着 for 循环会调用 Iterator 对象中的 next() （注：Python3 里是 __next__)方法来进行迭代，直到迭代过程结束抛出 StopIteration 异常。 好了，来聊聊 Generator 吧让我们先看看前面那段代码吧： 12345678def generateList1(start,stop): for i in range(start,stop): yield iif __name__==&quot;__main__&quot;: c=generateList1(1,100000) for i in generateList1: print(i) 首先我们要确定一点的是 Generator 其实也是一个 Iterator 对象。OK 让我们来看看上面这段代码，首先 for 确定 generateList1 是一个 Iterator 对象，然后开始调用 next() 方法进行进一步迭代。OK 此时你肯定想问这里面 next() 方法是怎样让 generateList1 进一步往下迭代的呢？答案在于 Generator 的内建 send() 方法。我们还是来看一段代码。 1234567def generateList1(start,stop): for i in range(start,stop): yield iif __name__==&quot;__main__&quot;: a=generateList1(0,5) for i in range(0,5): print(a.send(None)) 这里我们应该输出什么？答案就是 0,1,2,3,4 ，结果上和我们用 for 循环进行运算的结果是不是一样。好了，我们现在可以得出一个结论就是 Generator 迭代的本质就是通过内建的 next() 或 __next__() 方法来调用内建的 send() 方法。 继续吐槽 Generator 的内建方法前面我们提到一个结论 Generator 迭代的本质就是通过内建的 next() 或 __next__() 方法来调用内建的 send() 方法。 现在我们来看个例子： 12345678910111213141516def countdown(n): print &quot;Counting down from&quot;, n while n &gt;= 0: newvalue = (yield n) # If a new value got sent in, reset n with it if newvalue is not None: n = newvalue else: n -= 1if __name__=='__main__': c = countdown(5) for x in c: print x if x == 5: c.send(3) 好了这段代码的输出应该是什么？答案是 [5，2，1，0] ，是不是很迷惑？别急，我们先来看看这段代码的运行流程 简而言之就是，当我们调用 send() 函数的时候，我们 send(x) 的值会发送给 newvalue 向下继续执行直到遇到下一次 yield 的出现，然后返回值作为一个过程的结束。然后我们的 Generator 静静的沉睡在内存中，等待下一次的 send 来唤醒它。 注2：有同志问：“这里没想明白，c.send(3) 是 相当于 yield n 返回了个 3 给 newvalue ?”，好的，nice question，其实这个问题我们看前面之前的代码运行图就知道， c.send(3) 首先，将 3 赋值给 newvalue ，然后程序运行剩下的代码，直到遇到下一个 yield 为止，那么在这里，我们运行剩下完代码，在遇到 yiled n 之前，将 n 的值已经改变为 3 ,接着，yield n 即约等于 return 3。接着 countdown 这个 Generator 将所有变量的状态冻结，然后静静的呆在内存中，等待下一次的 next 或 __next__() 方法或者是 send() 方法的唤醒。 小贴士：我们如果直接调用 send() 的话，第一次请务必 send(None) 只有这样一个 Generator 才算是真正被激活了。我们才能进行下一步操作。 说说关于协程首先关于协程的定义，我们来看一段 wiki Coroutines are computer program components that generalize subroutines for nonpreemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.According to Donald Knuth, the term coroutine was coined by Melvin Conway in 1958, after he applied it to construction of an assembly program.[1] The first published explanation of the coroutine appeared later, in 1963. 简而言之，协程是比线程更为轻量的一种模型，我们可以自行控制启动与停止的时机。在 Python 中其实没有专门针对协程的这个概念，社区一般而言直接将 Generator 作为一种特殊的协程看待，想想，我们可以用 next 或 __next__() 方法或者是 send() 方法唤醒我们的 Generator ，在运行完我们所规定的代码后， Generator 返回并将其所有状态冻结。这是不是很让我们 Excited 呢！！ 关于 Generator 的一点课后作业现在我们要后序遍历二叉树，我知道看这篇文章神犇们都能无脑写出来的，让我们看看代码先： 123456789101112131415161718class Node(object): def __init__(self, val, left, right): self.val = val self.left = left self.right = rightdef visit_post(node): if node.left: return visit_post(node.left) if node.right: return visit_post(node.right) return node.valif __name__ == '__main__': node = Node(-1, None, None) for val in range(100): node = Node(val, None, node) print(list(visit_post(node))) 但是，我们知道递归深度太深的话，我们要么爆栈要么 py 交易失败，OK ，Generator 大法好，把你码农平安保，还是直接看代码： 123456789101112131415161718192021222324252627def visit_post(node): if node.left: yield node.left if node.right: yield node.right yield node.valdef visit(node, visit_method): stack = [visit_method(node)] while stack: last = stack[-1] try: yielded = next(last) except StopIteration: stack.pop() else: if isinstance(yielded, Node): stack.append(visit_method(yielded)) elif isinstance(yielded, int): yield yieldedif __name__ == '__main__': node = Node(-1, None, None) for val in range(100): node = Node(val, None, node) visit_generator = visit(node, visit_method=visit_post) print(list(visit_generator)) 看起来很复杂是不是？没事当做课后作业，大家可以在评论里给我留言，我们一起进行一下 py 交易吧~ 参考链接1.提高你的Python: 解释‘yield’和‘Generators（生成器）’2.yield大法好3.http://my.oschina.net/1123581321/blog/1605604.python的迭代器为什么一定要实现__iter__方法(关于迭代器那离，为了便于理解，我简化了一些东西，具体可以参看这个问题的高票答案)","link":"/posts/2016/09/11/something-about-yield-in-python/"},{"title":"我与 PyCon China 这两年","text":"其实这篇文章最开始动笔是写于9月份，PyCon China 2019 上海场的工作结束后。后续因为还有北京，成都场的工作，所以拖到了现在。正好我自己的三年计划刚刚落下帷幕。下一个三年计划正在开展。我也来聊聊在这三年里面，让我花费精力最多，也是最为重要的部分组成之一吧。 PyCon China，嗯这三年的时间里，我有两年都在与这个熟悉而陌生的名字关联在了一起 我与 PyCon China 的结识说实话，之前 Laike9M 的一句话让我产生了共鸣 Kenneth Reitz 曾经说，他的一年是按 PyCon 计算的。尽管围绕他有很多争议，这句话依然让我有了奇妙的共鸣。对他来说，”PyCon”自然是指 PyCon US，而对我来说，则是 PyCon China 而我18/19年这两年的时间里，有很大一部分时间都在围绕着 PyCon China 要说最早结识 PyCon，应该能追溯到的 2016 年，当时初学 Python 的我，看到 David Beazley 在 PyCon US 2009 上分享的一个名为 A Curious Course on Coroutines and Concurrency 的分享，简直惊为天人 也让我当时立下来几个 Flag 在 PyCon China 做一次分享 组织一次 PyCon China 提一个被接受的 PEP 提案 晋升成为大陆第一位 Python Core Developer（不过在16年11月，来自华为的 angwer 晋升成为大陆第一位 Core 后，这个 Flag 就有所修改了23333) 嗯，后续我也在持续关注 PyCon China 的进展，却发现，历年 PyCon China 口碑在不断的下滑，也感觉十分的痛心 在17年，PyCon China 口碑进入谷底，当时年少轻狂的我，跑去邮件组喷了一圈后，又在考虑，我自己的能力是不是足够支撑一场大型会务筹办，可能需要先试试水。所以决定和小伙伴以『Python 北京开发者社区』的名义，组织了一场『Python 北京开发者活动第一期』。借用 Thoughtworks 的场地在北京自行发起了一场活动。从事后的反馈来看，这个活动还是相对成功的，因此我也坚定了参与进 PyCon China 的意向。BTW，这次活动也收获了很多很棒的朋友，比如18年北京场的志愿者负责人姚前，19年组织者刘玉龙等人，都结识于此。 18年3月，我正式通过邮件的形式向大妈提出参与进 PyCon China 中来，7月，和 PyCon China 幕后的负责人辛庆老师见面后，我正式参与进18年 PyCon China 的筹办中来。然后入坑之后没法逃脱，，我也全程参与了19年的筹备工作 聊聊 PyCon China 这两年说实话，我不止一次向辛老师吐槽过 我觉得只有傻逼才会来参与办这个会 辛老师说： 不用你说，我们所有人都觉得自己是傻逼 办会真的太苦了。。。。我是18年北京场负责人、成都场负责人/讲师，19年北京/上海/成都负责人，成都场讲师。说实话，这两年结束之后，我基本都有半个月缓不过劲来。嗯，就是那种基本要虚脱的感觉 很多人要问，办会真的这么惨么？ 是的，很惨。我大概说一下目前 PyCon China 的运作模式吧 PyCon China 目前由一群志愿者在承担幕后的会务工作，大概从每年3到4月开始，进入今年的 PyCon China 的筹备状态 我们的工作包括不仅限于下面这样一些部分 场地的选取 讲师的征集 今年周边的设计 主题的审核以及讲师预讲服务 赞助 国内外社区沟通 媒体稿的准备 会务现场 基本进入每年的8月开始，到10月结束，基本每个人都处于高度紧张模式，基本每天大家都需要折腾到凌晨1/2点才能完成当天的准备工作（因为大家都是志愿准备工作） 而一旦各个城市开始进入会时，各位组织者也将迎来更大的挑战，无论是体能上还是具体的事务中，举个例子，我上海场/北京场都是两天睡了4个小时不到，而成都场更惨，因为日本讲师因为台风将缺席上午的分享，我需要临时做 PPT 进行补位，因此72小时也就睡了6个小时。 所以说，做会，无论是对于体能，还是自己各方面的能力，都会提出很高的要求。 为什么？其实我也不知道为什么，可能真的是傻逼吧？ 说实话，这两年 PyCon China 在我人生最重要的职业初期的两年中扮演了非常非常重要的角色，在这里一直被大家溺爱，包容着，也教会了我许许多多。所以吧，我感恩，痛苦，然后快乐着 有朋友说：以我片面的视角来看，Manjusaka 正是让 PyCon China 涅槃重生的关键人物。 其实我非常感谢大家的认可，但是不得不说，我只是这两年中微不足道的一环，在幕后，有太多让人感动并产生对这个群体的归属感的事情发生。比如18年垫付了20多万亏损的辛庆老师，做设计做到哭，但是边哭边做还是把设计成功的交出来的错姐，在背后一直默默支持的志愿者大管家倩姨，财务大管家大猫，因为台风取消航班但是依旧选择来到中国的日本小姐姐藤井美娜等等等。正是因为这样一群人的存在，PyCon China 才能不断的进步与成长 所以吧，很荣幸能在人生最重要的几年时间里有这样的一段难以忘怀的经历。嗯，一下也不知道怎么说了 关于未来关于未来的话，职业方面的话，目前刚调动到阿里云做中间件相关的开发，应该还是会继续在技术这条路上走很长的一段时间吧。然后对于 PyCon China 2020，目前的筹备工作已经开展，不过因为我自己的身体原因，还不确定是否会参与，不过不出以外的话，虽然可能没法像18/19年这样支撑多城市（因为我们各城市的志愿者也成长起来啦！），但是大家也会见到我的身影 最后两张图镇楼 最后 PyCon China 2020 已经开始筹备工作，欢迎大家加入！","link":"/posts/2019/11/03/somethng-between-me-pycon-china/"},{"title":"聊聊网络事件中的惊群效应","text":"关于惊群问题，其实我是在去年开始去关注的。然后向 CPython 提了一个关于解决 selector 的惊群问题的补丁 BPO-35517。现在大概来聊聊关于惊群问题那点事吧 惊群问题的过去惊群问题是什么？惊群问题又名惊群效应。简单来说就是多个进程或者线程在等待同一个事件，当事件发生时，所有线程和进程都会被内核唤醒。唤醒后通常只有一个进程获得了该事件并进行处理，其他进程发现获取事件失败后又继续进入了等待状态，在一定程度上降低了系统性能。 可能很多人想问，惊群效应为什么会占用系统资源？降低系统性能？ 多进程/线程的唤醒，涉及到的一个问题是上下文切换问题。频繁的上下文切换带来的一个问题是数据将频繁的在寄存器与运行队列中流转。极端情况下，时间更多的消耗在进程/线程的调度上，而不是执行 接下来我们来聊聊我们网络编程中常见的惊群问题。 常见的惊群问题在 Linux 下，我们常见的惊群效应发生于我们使用 accept 以及我们 select 、poll 或 epoll 等系统提供的 API 来处理我们的网络链接。 accept 惊群首先我们用一个流程图来复习下我们传统的 accept 使用方式 那么在这里存在一种情况，即当一个请求到达时，所有进程/线程都开始 accept ，但是最终只有一个获取成功，我们来写段代码看看 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;arpa/inet.h&gt;#include &lt;assert.h&gt;#include &lt;errno.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#define SERVER_ADDRESS &quot;0.0.0.0&quot;#define SERVER_PORT 10086#define WORKER_COUNT 4int worker_process(int listenfd, int i) { while (1) { printf(&quot;I am work %d, my pid is %d, begin to accept connections \\n&quot;, i, getpid()); struct sockaddr_in client_info; socklen_t client_info_len = sizeof(client_info); int connection = accept(listenfd, (struct sockaddr *)&amp;client_info, &amp;client_info_len); if (connection != -1) { printf(&quot;worker %d accept success\\n&quot;, i); printf(&quot;ip :%s\\t&quot;, inet_ntoa(client_info.sin_addr)); printf(&quot;port: %d \\n&quot;, client_info.sin_port); } else { printf(&quot;worker %d accept failed&quot;, i); } close(connection); } return 0;}int main() { int i = 0; struct sockaddr_in address; bzero(&amp;address, sizeof(address)); address.sin_family = AF_INET; inet_pton(AF_INET, SERVER_ADDRESS, &amp;address.sin_addr); address.sin_port = htons(SERVER_PORT); int listenfd = socket(PF_INET, SOCK_STREAM, 0); int ret = bind(listenfd, (struct sockaddr *)&amp;address, sizeof(address)); ret = listen(listenfd, 5); for (i = 0; i &lt; WORKER_COUNT; i++) { printf(&quot;Create worker %d\\n&quot;, i + 1); pid_t pid = fork(); /*child process */ if (pid == 0) { worker_process(listenfd, i); } if (pid &lt; 0) { printf(&quot;fork error&quot;); } } /*wait child process*/ int status; wait(&amp;status); return 0;} 我们来看看运行的结果 诶？怎么回事？为什么这里没有出现我们想要的现象（一个进程 accept 成功，三个进程 accept 失败）？原因在于在 Linux 2.6 之后，Accept 的惊群问题从内核上被处理了 好，我们接着往下看 select/poll/epoll 惊群我们以 epoll 为例，我们来看看传统的工作模式 好了，我们来看段代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;netdb.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#define SERVER_ADDRESS &quot;0.0.0.0&quot;#define SERVER_PORT 10087#define WORKER_COUNT 4#define MAXEVENTS 64static int create_and_bind_socket() { int fd = socket(PF_INET, SOCK_STREAM, 0); struct sockaddr_in server_address; server_address.sin_family = AF_INET; inet_pton(AF_INET, SERVER_ADDRESS, &amp;server_address.sin_addr); server_address.sin_port = htons(SERVER_PORT); bind(fd, (struct sockaddr *)&amp;server_address, sizeof(server_address)); return fd;}static int make_non_blocking_socket(int sfd) { int flags, s; flags = fcntl(sfd, F_GETFL, 0); if (flags == -1) { perror(&quot;fcntl error&quot;); return -1; } flags |= O_NONBLOCK; s = fcntl(sfd, F_SETFL, flags); if (s == -1) { perror(&quot;fcntl set error&quot;); return -1; } return 0;}int worker_process(int listenfd, int epoll_fd, struct epoll_event *events, int k) { while (1) { int n; n = epoll_wait(epoll_fd, events, MAXEVENTS, -1); printf(&quot;Worker %d pid is %d get value from epoll_wait\\n&quot;, k, getpid()); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (listenfd == events[i].data.fd) { struct sockaddr in_addr; socklen_t in_len; int in_fd; in_len = sizeof(in_addr); in_fd = accept(listenfd, &amp;in_addr, &amp;in_len); if (in_fd == -1) { printf(&quot;worker %d accept failed\\n&quot;, k); break; } printf(&quot;worker %d accept success\\n&quot;, k); close(in_fd); } } } return 0;}int main() { int listen_fd, s; int epoll_fd; struct epoll_event event; struct epoll_event *events; listen_fd = create_and_bind_socket(); if (listen_fd == -1) { abort(); } s = make_non_blocking_socket(listen_fd); if (s == -1) { abort(); } s = listen(listen_fd, SOMAXCONN); if (s == -1) { abort(); } epoll_fd = epoll_create(MAXEVENTS); if (epoll_fd == -1) { abort(); } event.data.fd = listen_fd; event.events = EPOLLIN; s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); for (int i = 0; i &lt; WORKER_COUNT; i++) { printf(&quot;create worker %d\\n&quot;, i); int pid = fork(); if (pid == 0) { worker_process(listen_fd, epoll_fd, events, i); } } int status; wait(&amp;status); free(events); close(listen_fd); return EXIT_SUCCESS;} 然后，我们用 telnet 发送一下 TCP 请求，看看效果，，我们能得到这样的结果 恩，我们能看到当一个请求到达时，我们四个进程都被唤醒了。现在为了更直观的看到这一个过程，我们用 strace 来 profile 一下 我们还是能看到，四个进程都被唤醒，但是只有 Worker 3 成功 accept ，而其余的进程在 accept 的时候，都获取到了 EAGAIN 错误， 而 Linux 文档 对于 EAGAIN 的描述是 The socket is marked nonblocking and no connections are present to be accepted. POSIX.1-2001 and POSIX.1-2008 alloweither error to be returned for this case, and do not require these constants to have the same value, so a portableapplication should check for both possibilities. 现在我们对于 EPOLL 的惊群问题是不是有了直观的了解？那么怎么样去解决惊群问题呢？ 惊群问题的现在从内核解决惊群问题首先如前面所说，Accept 的惊群问题在 Linux Kernel 2.6 之后就被从内核的层面上解决了。但是 EPOLL 怎么办？在 2016 年一月，Linux 之父 Linus 向内核提交了一个补丁 参见 epoll: add EPOLLEXCLUSIVE flag 其中的关键代码是 1234if (epi-&gt;event.events &amp; EPOLLEXCLUSIVE) add_wait_queue_exclusive(whead, &amp;pwq-&gt;wait);else add_wait_queue(whead, &amp;pwq-&gt;wait); 简而言之，通过增加一个 EPOLLEXCLUSIVE 标志位作为辅助。如果用户开启了 EPOLLEXCLUSIVE ，那么在加入内核等待队列时，使用 add_wait_queue_exclusive 否则则使用 add_wait_queue 至于这两个函数的用法，可以参考这篇文章Handing wait queues 其中有这样一段描述 The add_wait_queue( ) function inserts a nonexclusive process in the first position of a wait queue list. The add_wait_queue_exclusive( ) function inserts an exclusive process in the last position of a wait queue list. 好了，我们现在来改一下我们的代码（内核版本要在 Linux Kernel 4.5）之后 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;netdb.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#define SERVER_ADDRESS &quot;0.0.0.0&quot;#define SERVER_PORT 10086#define WORKER_COUNT 4#define MAXEVENTS 64static int create_and_bind_socket() { int fd = socket(PF_INET, SOCK_STREAM, 0); struct sockaddr_in server_address; server_address.sin_family = AF_INET; inet_pton(AF_INET, SERVER_ADDRESS, &amp;server_address.sin_addr); server_address.sin_port = htons(SERVER_PORT); bind(fd, (struct sockaddr *)&amp;server_address, sizeof(server_address)); return fd;}static int make_non_blocking_socket(int sfd) { int flags, s; flags = fcntl(sfd, F_GETFL, 0); if (flags == -1) { perror(&quot;fcntl error&quot;); return -1; } flags |= O_NONBLOCK; s = fcntl(sfd, F_SETFL, flags); if (s == -1) { perror(&quot;fcntl set error&quot;); return -1; } return 0;}int worker_process(int listenfd, int epoll_fd, struct epoll_event *events, int k) { while (1) { int n; n = epoll_wait(epoll_fd, events, MAXEVENTS, -1); printf(&quot;Worker %d pid is %d get value from epoll_wait\\n&quot;, k, getpid()); sleep(0.2); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (listenfd == events[i].data.fd) { struct sockaddr in_addr; socklen_t in_len; int in_fd; in_len = sizeof(in_addr); in_fd = accept(listenfd, &amp;in_addr, &amp;in_len); if (in_fd == -1) { printf(&quot;worker %d accept failed\\n&quot;, k); break; } printf(&quot;worker %d accept success\\n&quot;, k); close(in_fd); } } } return 0;}int main() { int listen_fd, s; int epoll_fd; struct epoll_event event; struct epoll_event *events; listen_fd = create_and_bind_socket(); if (listen_fd == -1) { abort(); } s = make_non_blocking_socket(listen_fd); if (s == -1) { abort(); } s = listen(listen_fd, SOMAXCONN); if (s == -1) { abort(); } epoll_fd = epoll_create(MAXEVENTS); if (epoll_fd == -1) { abort(); } event.data.fd = listen_fd; // add EPOLLEXCLUSIVE support event.events = EPOLLIN | EPOLLEXCLUSIVE; s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); for (int i = 0; i &lt; WORKER_COUNT; i++) { printf(&quot;create worker %d\\n&quot;, i); int pid = fork(); if (pid == 0) { worker_process(listen_fd, epoll_fd, events, i); } } int status; wait(&amp;status); free(events); close(listen_fd); return EXIT_SUCCESS;} 然后我们来看看效果 诶？为什么还是有两个进程被唤醒了？原因在于 EPOLLEXCLUSIVE 只保证唤醒的进程数小于等于我们开启的进程数，而不是直接唤醒所有进程，也不是只保证唤醒一个进程 我们来看看官方的描述 Sets an exclusive wakeup mode for the epoll file descriptor that is being attached to the target file descriptor, fd. When a wakeup event occurs and multiple epoll file descriptors are attached to the same target file using EPOLLEXCLUSIVE, one or more of the epoll file descriptors will receive an event with epoll_wait(2). The default in this scenario (when EPOLLEXCLUSIVE is not set) is for all epoll file descriptors to receive an event. EPOLLEXCLUSIVE is thus useful for avoid‐ ing thundering herd problems in certain scenarios. 恩，换句话说，就目前而言，系统并不能严格保证惊群问题的解决。很多时候我们还是要依靠应用层自身的设计来解决 应用层解决目前而言，应用解决惊群有两种策略 这是可以接受的代价，那么我们暂时不管。这是我们大多数的时候的策略 通过加锁或其余的手段来解决这个问题，最典型的例子是 Nginx 我们来看看 Nginx 怎么解决这样的问题的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758voidngx_process_events_and_timers(ngx_cycle_t *cycle){ ngx_uint_t flags; ngx_msec_t timer, delta; if (ngx_timer_resolution) { timer = NGX_TIMER_INFINITE; flags = 0; } else { timer = ngx_event_find_timer(); flags = NGX_UPDATE_TIME; } if (ngx_use_accept_mutex) { if (ngx_accept_disabled &gt; 0) { ngx_accept_disabled--; } else { if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) { return; } if (ngx_accept_mutex_held) { flags |= NGX_POST_EVENTS; } else { if (timer == NGX_TIMER_INFINITE || timer &gt; ngx_accept_mutex_delay) { timer = ngx_accept_mutex_delay; } } } } delta = ngx_current_msec; (void) ngx_process_events(cycle, timer, flags); delta = ngx_current_msec - delta; ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;timer delta: %M&quot;, delta); ngx_event_process_posted(cycle, &amp;ngx_posted_accept_events); if (ngx_accept_mutex_held) { ngx_shmtx_unlock(&amp;ngx_accept_mutex); } if (delta) { ngx_event_expire_timers(); } ngx_event_process_posted(cycle, &amp;ngx_posted_events);} 我们这里能看到，Nginx 主体的思想是通过锁的形式来处理这样问题。我们每个进程在监听 FD 事件之前，我们先要通过 ngx_trylock_accept_mutex 去获取一个全局的锁。如果拿锁成功，那么则开始通过ngx_process_events 尝试去处理事件。如果拿锁失败，则放弃本次操作。所以从某种意义上来讲，对于某一个 FD ，Nginx 同时只有一个 Worker 来处理 FD 上的事件。从而避免惊群。 总结这篇文章从去年到现在拖了很久了，惊群问题一直是我们日常工作中遇到的问题，我自己觉得，还是有必要写篇详细的笔记，记录下去年到现在的一些学习记录。差不多就这样吧，祝各位看的好。","link":"/posts/2019/03/28/somthing-about-thundering-herd/"},{"title":"排查一个特殊的 No space left on device","text":"好久没写水文了，新年第一篇水文总得写一下，完成下 OKR，正好最近帮群友查了一个特殊的 No space left on device 问题，记录一下。 问题半夜接到群友求助，说自己的测试环境遇到了点问题，正好我还没睡，那就来看一下 问题的情况很简单， 用 docker run -d --env-file .oss_env --mount type=bind,src=/data1,dst=/cache {image} 启动了一个容器，然后发现在启动后业务代码报错，抛出 OSError: [Errno 28] No space left on device 的异常 这个问题其实很典型，但是最终排查出来的结果确实非典型的。不过排查思路其实应该是很典型的线上问题的一步步分析 root casue 的过程。希望能对看官就帮助 排查首先群友提供了第一个关键信息，空间有余量，但是就 OSError: [Errno 28] No space left on device 。那么熟悉 Linux 的同学可能第一步的排查工作就是排查对应的 inode 情况 执行命令 1df -ih 我们能看到 /data1 实际上的 inode 和整机的 inode 数量都是足够的（备注：这里是我自己在我自己的机器上复现问题的截图，第一步由群友完成，然后给我提供了信息） 那么我们继续排查，我们看到了我们使用了 mount bind1 的方式将宿主机的 /data1 挂载到了容器内部的 /cache 目录下, mount bind 可以用下面一张图来表示和 volume 的区别 都在不同版本的内核上，mount bind 的行为有一些特殊的情况，所以我们需要确认下 mount bind 的情况是否正确，我们用 fallocate2 来创建一个 1G 的文件，然后在容器内部查看文件的大小 1fallocate -l 10G /cache/test 文件创建没有问题，实际上我们就可以排除掉 mount bind 的缺陷了 接着，群友提供了这个盘是云厂商的云盘（经过扩容），我让群友确认下是具体的 ESSD 还是 NAS 这种走 NFS 挂载的 Block Device（这块也有坑）。确认是标准的 ESSD 后进入下一步（驱动的问题可以先排除） 接着，我们需要考虑 mount –bind 在跨文件系统情况下的问题。虽然前面一步我们成功创建了文件。但是为了保险起见，我们执行 fdisk -l 和 tune2fs -l 两个命令，来确认分区和文件系统的正确性，确认文件系统的类型都是 ext4，那么没有问题。具体两个命令的使用方式参见 fdisk3 和 tune2fs4 然后再回顾我们之前直接在 /cache 下创建问题没有问题，那么这个时候我们心里应该大概有底，这个应该不是代码问题，也不是权限问题（这一步我额外排除镜像的构建里没有额外的用户操作），那么我们需要排除一下扩容的问题。我们将 /data1 unmount 之后，重新 mount 后，再执行容器，发现问题依旧存在，那么我们就可以去排除扩容的问题了。 现在一些常见的问题已经基本排除，那么我们来考虑文件系统本身的问题。我登录到机器上，执行了以下两个操作 在出问题的目录 /cache/xxx/ 下，我用 fallocate -l 创建一个报错的文件（长文件名），失败 在出问题的目录 /cache/xxx/ 下，我用 fallocate -l 创建一个短文件名），成功 OK，我们现在排查路径就往文件系统异常的方向上靠了，执行命令 dmesg5 查看内核日志，发现了如下错误 12[13155344.231942] EXT4-fs warning (device sdd): ext4_dx_add_entry:2461: Directory (ino: 3145729) index full, reach max htree level :2[13155344.231944] EXT4-fs warning (device sdd): ext4_dx_add_entry:2465: Large directory feature is not enabled on this filesystem OK，我们期待的异常信息找到了。原因是，ext4 基于的 BTree 索引，默认情况下只允许树的层高为2，实际上就大概限制了目录下的文件数量大概在 2k-3kw 以内。经过确认，这个问题目录下的确有大量小文件。我们再用 tune2fs -l 确认下是否是如我们猜想，得到结果 123Filesystem revision #: 1 (dynamic)Filesystem features: has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file dir_nlink extra_isize metadata_csumFilesystem flags: signed_directory_hash bingo，的确没有开启 large_dir 的选项。那么我们执行 tune2fs -O large_dir /dev/sdd 开启这个选项，然后再次执行 tune2fs -l 确认下，发现已经开启了。然后我们再次执行容器，发现问题已经解决。 验证上面的问题排查看似告一段落。但是实际上并没有闭环。一个问题的闭环有两个特征 定位到具体的异常代码 有最小可复现版本确认我们找到 root cause 是符合预期的。 从上面 dmesg 的信息我们能定位到内核中的函数，其实现如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171static int ext4_dx_add_entry(handle_t *handle, struct ext4_filename *fname, struct inode *dir, struct inode *inode){ struct dx_frame frames[EXT4_HTREE_LEVEL], *frame; struct dx_entry *entries, *at; struct buffer_head *bh; struct super_block *sb = dir-&gt;i_sb; struct ext4_dir_entry_2 *de; int restart; int err;again: restart = 0; frame = dx_probe(fname, dir, NULL, frames); if (IS_ERR(frame)) return PTR_ERR(frame); entries = frame-&gt;entries; at = frame-&gt;at; bh = ext4_read_dirblock(dir, dx_get_block(frame-&gt;at), DIRENT_HTREE); if (IS_ERR(bh)) { err = PTR_ERR(bh); bh = NULL; goto cleanup; } BUFFER_TRACE(bh, &quot;get_write_access&quot;); err = ext4_journal_get_write_access(handle, sb, bh, EXT4_JTR_NONE); if (err) goto journal_error; err = add_dirent_to_buf(handle, fname, dir, inode, NULL, bh); if (err != -ENOSPC) goto cleanup; err = 0; /* Block full, should compress but for now just split */ dxtrace(printk(KERN_DEBUG &quot;using %u of %u node entries\\n&quot;, dx_get_count(entries), dx_get_limit(entries))); /* Need to split index? */ if (dx_get_count(entries) == dx_get_limit(entries)) { ext4_lblk_t newblock; int levels = frame - frames + 1; unsigned int icount; int add_level = 1; struct dx_entry *entries2; struct dx_node *node2; struct buffer_head *bh2; while (frame &gt; frames) { if (dx_get_count((frame - 1)-&gt;entries) &lt; dx_get_limit((frame - 1)-&gt;entries)) { add_level = 0; break; } frame--; /* split higher index block */ at = frame-&gt;at; entries = frame-&gt;entries; restart = 1; } if (add_level &amp;&amp; levels == ext4_dir_htree_level(sb)) { ext4_warning(sb, &quot;Directory (ino: %lu) index full, &quot; &quot;reach max htree level :%d&quot;, dir-&gt;i_ino, levels); if (ext4_dir_htree_level(sb) &lt; EXT4_HTREE_LEVEL) { ext4_warning(sb, &quot;Large directory feature is &quot; &quot;not enabled on this &quot; &quot;filesystem&quot;); } err = -ENOSPC; goto cleanup; } icount = dx_get_count(entries); bh2 = ext4_append(handle, dir, &amp;newblock); if (IS_ERR(bh2)) { err = PTR_ERR(bh2); goto cleanup; } node2 = (struct dx_node *)(bh2-&gt;b_data); entries2 = node2-&gt;entries; memset(&amp;node2-&gt;fake, 0, sizeof(struct fake_dirent)); node2-&gt;fake.rec_len = ext4_rec_len_to_disk(sb-&gt;s_blocksize, sb-&gt;s_blocksize); BUFFER_TRACE(frame-&gt;bh, &quot;get_write_access&quot;); err = ext4_journal_get_write_access(handle, sb, frame-&gt;bh, EXT4_JTR_NONE); if (err) goto journal_error; if (!add_level) { unsigned icount1 = icount/2, icount2 = icount - icount1; unsigned hash2 = dx_get_hash(entries + icount1); dxtrace(printk(KERN_DEBUG &quot;Split index %i/%i\\n&quot;, icount1, icount2)); BUFFER_TRACE(frame-&gt;bh, &quot;get_write_access&quot;); /* index root */ err = ext4_journal_get_write_access(handle, sb, (frame - 1)-&gt;bh, EXT4_JTR_NONE); if (err) goto journal_error; memcpy((char *) entries2, (char *) (entries + icount1), icount2 * sizeof(struct dx_entry)); dx_set_count(entries, icount1); dx_set_count(entries2, icount2); dx_set_limit(entries2, dx_node_limit(dir)); /* Which index block gets the new entry? */ if (at - entries &gt;= icount1) { frame-&gt;at = at - entries - icount1 + entries2; frame-&gt;entries = entries = entries2; swap(frame-&gt;bh, bh2); } dx_insert_block((frame - 1), hash2, newblock); dxtrace(dx_show_index(&quot;node&quot;, frame-&gt;entries)); dxtrace(dx_show_index(&quot;node&quot;, ((struct dx_node *) bh2-&gt;b_data)-&gt;entries)); err = ext4_handle_dirty_dx_node(handle, dir, bh2); if (err) goto journal_error; brelse (bh2); err = ext4_handle_dirty_dx_node(handle, dir, (frame - 1)-&gt;bh); if (err) goto journal_error; err = ext4_handle_dirty_dx_node(handle, dir, frame-&gt;bh); if (restart || err) goto journal_error; } else { struct dx_root *dxroot; memcpy((char *) entries2, (char *) entries, icount * sizeof(struct dx_entry)); dx_set_limit(entries2, dx_node_limit(dir)); /* Set up root */ dx_set_count(entries, 1); dx_set_block(entries + 0, newblock); dxroot = (struct dx_root *)frames[0].bh-&gt;b_data; dxroot-&gt;info.indirect_levels += 1; dxtrace(printk(KERN_DEBUG &quot;Creating %d level index...\\n&quot;, dxroot-&gt;info.indirect_levels)); err = ext4_handle_dirty_dx_node(handle, dir, frame-&gt;bh); if (err) goto journal_error; err = ext4_handle_dirty_dx_node(handle, dir, bh2); brelse(bh2); restart = 1; goto journal_error; } } de = do_split(handle, dir, &amp;bh, frame, &amp;fname-&gt;hinfo); if (IS_ERR(de)) { err = PTR_ERR(de); goto cleanup; } err = add_dirent_to_buf(handle, fname, dir, inode, de, bh); goto cleanup;journal_error: ext4_std_error(dir-&gt;i_sb, err); /* this is a no-op if err == 0 */cleanup: brelse(bh); dx_release(frames); /* @restart is true means htree-path has been changed, we need to * repeat dx_probe() to find out valid htree-path */ if (restart &amp;&amp; err == 0) goto again; return err;} ext4_dx_add_entry 函数的主要功能是将新的目录项添加到目录索引中，我们能看到这段函数在 add_level &amp;&amp; levels == ext4_dir_htree_level(sb) 这里检查对应的特性是否打开，以及当前 BTree 层高，如果超出限制，则返回 ENOSPC 即 ERROR 28 好了，在复现异常之前，我们来获取下这个函数的被调用路径。这里我用 eBPF 的 trace 来获取 stacktrace，因为与主体无关，我在这里就不放代码了 123456789101112ext4_dx_add_entryext4_add_nondirext4_createpath_openatdo_filp_opendo_sys_openat2do_sys_open__x64_sys_openatdo_syscall_64entry_SYSCALL_64_after_hwframe[unknown][unknown] 那么我们怎么验证这个是我们的异常呢 首先我们利用 eBPF + kretproble 来获取 ext4_dx_add_entry 的返回值，如果返回值是 ENOSPC，则我们就可以确定这个是我们的异常 代码如下（不要问我这里为啥不用 Python 写，要写 C 了（ 12345678910111213141516171819202122232425262728293031323334353637383940414243from bcc import BPFbpf_text = &quot;&quot;&quot;#include &lt;uapi/linux/ptrace.h&gt;BPF_RINGBUF_OUTPUT(events, 65536);struct event_data_t { u32 pid;};int trace_ext4_dx_add_entry_return(struct pt_regs *ctx) { int ret = PT_REGS_RC(ctx); if (ret == 0) { return 0; } u32 pid=bpf_get_current_pid_tgid()&gt;&gt;32; struct event_data_t *event_data = events.ringbuf_reserve(sizeof(struct event_data_t)); if (!event_data) { return 0; } event_data-&gt;pid = pid; events.ringbuf_submit(event_data, sizeof(event_data)); return 0;}&quot;&quot;&quot;bpf = BPF(text=bpf_text)bpf.attach_kretprobe(event=&quot;ext4_dx_add_entry&quot;, fn_name=&quot;trace_ext4_dx_add_entry_return&quot;)def process_event_data(cpu, data, size): event = bpf[&quot;events&quot;].event(data) print(f&quot;Process {event.pid} ext4 failed&quot;)bpf[&quot;events&quot;].open_ring_buffer(process_event_data)while True: try: bpf.ring_buffer_consume() except KeyboardInterrupt: exit() 然后我们写段很短的 Python 脚本 12345678910import uuidimport osfor i in range(200000000): if i % 100000 == 0: print(f&quot;we have created {i} files&quot;) filename=str(uuid.uuid4()) file_name=f&quot;/data1/cache/{filename}+{filename}.txt&quot; with open(file_name, &quot;w+&quot;) as f: f.write(&quot;&quot;) 然后我们看到执行结果 符合预期，那么我们可以说这个问题的排查路径的因果关系链完整了。那么我们也可以正式宣告解决了这个问题了 那么锦上添花的一点，对于这种上游的问题，我们如果能找到具体在什么时间点进行了修复，那就更好了。就这个 case 而言，ext4 的 large_dir 在 Linux 4.13 中得到引入，具体可以参见 88a399955a97fe58ddb2a46ca5d988caedac731b6 这个 commit。 OK 这个问题就告一段落 总结其实这个问题比较冷门，但是排查方式其实是挺典型的线上问题的排查方法。对于问题，不要预设结果，一步步的根据现象去逼近最终的结论。以及 eBPF 真的好东西，能帮助做很多内核的事。最后我的 Linux 文件系统方面的底子还是太薄弱了，希望后面能重点加强一下 差不多就这样 Reference [1]. https://docs.docker.com/storage/bind-mounts/ ] [2]. https://man7.org/linux/man-pages/man2/fallocate.2.html [3]. https://man7.org/linux/man-pages/man8/fdisk.8.html [4]. https://linux.die.net/man/8/tune2fs [5]. https://man7.org/linux/man-pages/man1/dmesg.1.html [6]. https://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4.git/commit/?h=dev&amp;id=88a399955a97fe58ddb2a46ca5d988caedac731b","link":"/posts/2023/01/07/special-case-no-space-left-on-device/"},{"title":"Stay Simple, Stay Naive","text":"本来想在农历年前交年终总结的，不过想了想，去年的年终总结就因为太懒导致我鸽了。所以这个“双年”怎么也得总结下2022了。不过说起来怎么还没去字节，就开始 ego 了起来。 开篇实际上每年都在觉得这一年很魔幻，但是下一年总会跳出来说“这一年更魔幻”。不过这也是人生的乐趣吧。 看了下20年总结的标题叫做”但行好事，莫问前程“，去年一下想不起标题，群内求助了下，发现”Stay Simple，Stay Naive“这个标题还不错，挺适合作为去年的总结与展望的。不过在写下这点文字的时候发现当年 +1S 的对象也已经仙去了。怎么说心里也还是有点很奇怪的感觉在里面。 不过，一日膜法，终身膜法，所以就还是 Naive 的 +1S 吧 生活去年从年初开始，我从太极图形离职后，就开始进入了我数字游民的生活。作为一个 FreeLancer，可能最大的好处就在于说免去了通勤的时间后，我可以有更多的时间做自己的事（睡大觉（不是 在离职之后，和女朋友一起换了一个新的房子，有着很大的落地窗的露台，采光很好，所以让我在这里有时间安心做一些自己的事情 所以去年在有自己的时间的情况下，我开始看之前没有怎么涉猎的杂书，印象比较深的有这样几本 人造美人 置身事内：中国政府与经济发展 唐史并不如烟 最伟大的交易 病人家属，请来一下 昨日的世界 成为一颗星：宇航学员日记 整体的阅读量在20本左右吧。然后发现，你去慢慢找书，然后发现某个作者的风格很符合你的 XP 是件非常幸福的事。 在看书之余，我也开始看番了，这个行动一度占据了我 Q3/Q4 很多业余时间（导致我这段时间不去干其余事了（你们这个群害人不浅啊。当然，补剧，看纪录片，也都是这一年的一部分，过这对于我来说实际上也是全新的体验了。技术和睡觉之外的世界也是格外的大啊 然后家里新入职了两只猫咪，现在家里整整有六只猫，这对于我来说完全是幸福的烦恼。撸猫一时爽，一直撸猫一直爽。（当然铲屎和猫咪集体生病的时候就很不爽了。 当然好事说了这么多，当然要说点坏事了，去年的减肥计划执行的很不彻底，以及去年的运动计划也没有执行，呜呜呜呜呜。 从去年开始再次勇敢面对抑郁症的现实后，在药物和整体相对自由的环境的情况下，我自己的精神状态控制的也还不错。不过可能因为这一年是我被性侵到现在第十五个年头的缘故吧，去年的噩梦有点多，希望时间能继续治愈一切吧。（不过说起来，我讨厌药物副作用（真的让人很不爽 说回来，去年有了自己时间后，家里也添置了不少能极大提升自己生活幸福度的物件 一个 8*16T 的 NAS，妈妈再也不担心我的动漫没地方放了！ 一个 4090 + i9-13900K 的台式机，工作娱乐都很爽 Apple TV 4K 配合局域网 NAS，我和妹子一起看了不少的电影（ 一个尺寸合格的 Android 手机，追网络小说（ 一个顶配的 iPad Pro 12.9 寸，轻办公和看微信读书都很好用（我真的没用来盖泡面看爱奇艺！ 另外一提的是，去年公益我也在继续坚持坐着，我自己累计捐款10k+，然后公益群的小伙伴一起凑了点钱给一个村小捐款6K+，另外一点非常开心的是，我也带动了身边的人，去捐助学生。教育是最好的公益.jpg。不知道还能坚持多久，但是还是做一些力所能及的事吧。 差不多是这样，2022 整体的生活也还算是有滋有味。不过心里还是会隐隐约约有点担心，在整体局势下行的情况下，我这样小确幸的生活又能持续多久呢？ 感情感情步入了第四个年头，去年因为北京疫情的原因，和荆澈同学一起朝夕相处（这是真的朝夕相处）了一年了。用我很喜欢的《士兵突击》里的一句台词 常相守是个考验，随时随地，一生。 两个人朝夕相处，因为各种细节上的差异，一定会有一些小的争执与摩擦。这个时候就需要两个人相互包容。相互理解。说道这里我就很庆幸荆澈同学对我的包容与监督了。她经常碎碎念的督促我起床，督促我运动，督促我继续改掉我很多不好的习惯Hhhhhh（mua.jpg 很多时候，我半夜噩梦醒来，总会下意识的去抱着荆澈同学，她即便迷迷糊糊搞不清情况，也会转过来给我抱抱。某种意义上，荆澈同学的陪伴，是让我不断走下去的勇气的源泉 说回来，朝夕相处也未必是个坏事，去年和荆澈同学一起去公园散步，一起逛吃逛吃的时间多了很多。也一起去泡了温泉，一起去了环球影城（Remote 万岁！）。 希望 2023 年也和荆澈同学也能一起顺顺利利的走下去，完成对荆澈同学的承诺（我要有八块腹肌.jpg（以及去旅游，去做更多的手工艺品！ 反正我一如既往的 感激并享受着荆澈同学的爱 。 工作与技术首先聊聊我自己的变更，如前面所说，在22年初，因为自己的规划和身体的原因，我正式离开了太极（说实话挺舍不得这群同事的），正式开始了我数字游民的生活。目前来说，我依靠给一些客户做 SRE 方面的能力输出为生。这对于我来讲其实是个蛮大的挑战。因为我之前的定位其实更多的还是偏向于一个 Infra Developer，将 SRE 作为我正式工作方向，其实对于我来讲，也是开天辟地头一回了（感谢客户爸爸的信任 去年其实工作内容也发生了很大的转变，也让我更多的意识到了自己的不足。如果说自己之前是一个纯粹的 IC 的角色，那么去年我的工作内容的边界实际上有了不少的扩展。我需要去更多的考虑协调的有效性，体系化的建设。很多时候我都在笑称我自己这周写的文档可能比我写的代码还多了（XD 不过这对于我来说也是一个好事，思路的转变我相信会让我提升很多。 2022另外一个比较重大的改变就是从2022二月开始，在 Xuanwo 几位好友的启发下，我开始正式的以公开的方式，记录自己每周的生活与技术学习（我老板说会看我周报（摊手。这一点其实对于我自己来说，也是比较好的一个手段吧。用一个锚点，去约束自己的生活（面向周报有内容式学习（不是），去记录自己的一些感悟与心得（输出了不少稳定性与可观测性的东西。希望23年能继续坚持。 技术方面的话，去年的成长我自己觉得也还是比较明显的。一方面是在开源社区这块。年初因为寻找 Docker 替代品，机缘巧合之下开始为 nerdctl 做输出，6月被 Promote 成为 Reviewer，12月被 Promote 成为 Committer。这也是我比较深度的参与开源社区了。同时我自己也会和身边的好友去交流关于开源社区的东西，比如和 Xuanwo 一起聊聊他的 OpenDAL，和 GaoCe GG 一起聊聊/吐槽他的创业项目 envd （他时常因为我比他还看好这个项目而惊讶（这个项目真的是好项目啊！。我自己觉得这一年去给不同社区贡献代码，参与讨论，对于我自己的提升是全方位的，更明确的意识到自己的 naive，也接受来自不同人的帮助与指导。如同我之前在一篇文章中的感悟一样 从互联网诞生之初到现在，开源这一极具理想主义气质的行为事实上的改变了这个世界。世界各地的人都在开源的旗帜下，自由的挥发着自己的创意，尽情的一点点的改变着这个世界。有些时候想到我会有机会去参与到这样一个伟大的活动中，我会不由自主的颤栗。我很庆幸在我最初的职业生涯里就加入到了这个伟大的事业，我也希望我身边会有越来越多的人参与进来，一起挥洒着汗水，一起在这个操蛋但是又美好的世界里，找到自己心灵的应许之地。 另外一方面的话，去年在技术深度这块做的也还算 OK，继续在之前自己积累的可观测性和稳定性方面精进，系统性的提升自己的一些体系化的思考（抽象成方法论），也继续在内核和 eBPF 这块做一些有意思的工作（比如帮助人去做一些小的工具）。希望23年也能继续勇猛精进 说起来，去年有一个很大的收获不知道算不算技术这块的，姑且算吧。之前组建的刷题群在去年格外的活跃，大家一起刷题，一起捐款，一起推荐番祸害群主（不是），一起做开源（去年群内诞生了两位开源项目的 maintainer），我很多时候遇到各种事情的时候都会在群里和群友们一起吐槽和发泄。很多时候我自己在感叹，在这个人心浮躁的时代，能遇到这样一群热情又纯粹的人，实则人生幸事，当浮一大白（不过我肝不好，就以零度代酒干了这杯） 差不多就这些，去年也还零零碎碎的做了很多其余的工作，开始翻译人生第一本书,保持了每日一题，读了十多篇论文，组织了好几次群内分享，写自己的 toy，很多很多。很多人觉得程序员是个很枯燥的行业。但是说实话，这一行真的让人迷醉 总结对比了下年初的目标，然后自评了一下差不多能给自己个3.5的绩效吧优点和缺点都比较明显，聊聊缺点吧 是自己在开源这块有技术深度的产出还是不够 自律性还是差了一些，花了不少时间在睡懒觉，玩游戏，看番上了 专注度上差了一些，一些拟好的目标经常走神 自己的运动目标没达成 缺点和改进方向还是比较明确的，希望明年继续努力。我自己目前列好的一些 OKR 差不多是这样 O1：代码能力 学习前端，成为一个前端开源项目的 maintainer 继续保持 Leetcode 每日一题，确保百分之70以上的 daily 随机 medium 及以上题目能不看题解写出来 O2: 读书 重读 TCP 卷一 读完 CSAPP，并保证每章作业完成率不低于百分之八十 剩下的书待定 O3：分享 参考 xdp-tutorial 写一个 netfilter-tutorial 全年群内分享5次以上 六篇有效博客 O4: 努力生活 通过运动的方式，将体重降到 145（和某位群友赌了两千块的公益捐款） 和女朋友去三个以上地方玩 争取每周超过3点睡觉不超过两天（现在就emmm 最后2022 实际上真的挺魔幻的，不过套用狄更斯的一句老掉牙的话 这是最坏的一年，这也是最好的一年 说实话我也不知道23年会怎么样，未来几年会怎么样。不过无论怎么样，爱与希望总是会支撑我们走过一年年。嗯，Everything is gonna be OK. 说起来，今年有人问过我我想成为一个怎么样的人，我想了下，这么回答到 我希望身边的人在很多年后，和老头老太太聊天或者给自己孙子提到我的时候会这么说”我之前认识一个叫 saka 的人，是个还不错的人“，那么我心满意足了 Stay Simple，Stay Naive，永远谦逊，敬畏生活，勇敢前行 再见 2022，你好 2023","link":"/posts/2023/01/01/stay-simple-stay-naive/"},{"title":"简单聊聊 MySQL 全文索引","text":"最近踩 MYSQL 中文本搜索的坑踩了挺多，来写个具体的文章总结下 MYSQL 中文本搜索的一些知识点吧 模糊搜索在我们是使用 MYSQL 的过程中，总会有一些模糊搜索的需求，比如我们现在有这样一张表 1234567891011create table if not exists `user`( `id` bigint(20) not null auto_increment, `name` varchar(255) not null, `age` int not null, `update_time` timestamp not null, `create_time` timestamp not null, index (`name`), primary key (`id`)) engine = InnoDB charset = 'utf8mb4'; 现在我们需要对于 name 做一些模糊匹配的需求，比如我们需要去匹配 name 中包含 草 字，于是大家仔细一想，OK，写出了如下的 SQL 1select * from user where name like '%草%' 好了，当你行高采烈的将这段代码上线后，你发现，线上炸了，为啥？因为 MYSQL 的坑. MYSQL 的 like 查询存在这样两个限制 只有前缀匹配 ‘草%’ 和后缀匹配 ‘%草’ 才会走索引，而任意匹配则不会 当无法走索引的时候，MYSQL 会遍历全表来查询数据 当你一个表的数据规模很大的时候，那么暴力扫表必然会带来极大的开销 但是我们实际工作中这样的任意匹配的需求肯定很多，那么我们应该怎么做？或许可以尝试下全文搜索 全文搜索简单聊聊全文搜索全文搜索大家已经不太陌生了，简而言之用一种不太精确的说法就是，用一组关键词在一堆文本数据中寻找匹配项。在目前业界比较主流的全文搜索方案有： 支持全文搜索的关系行数据库 Apache Lucene 基于 Apache Lucene 的 ElasticSearch Apache Solr 后两种是目前业界主要的方案，可能很多全文搜索的需求都会考虑用 ES 或者 Solr 实现。但是这样一种方法并不是无代价的。有这样几个比较现实的问题 ES/Solr 在数据量比较大的情况下的运维问题，怎么样保证集群的 HA 将是一个很考研团队功底的问题 怎么样将 MYSQL 或其余数据源中的数据实时/离线 ETL 至 Search Engine 中 新增的学习与 Codebase 的维护成本。 新增一个依赖之后，对于系统整体的 HA 的保证 在技术决策中，我们往往需要去衡量一个选项的 ROI 来辅助决策。如果我们面对一个比较简单的搜索场景，那么选用 ES/Solr 所带来的开销将会使其 ROI 变得相对较低。因此在一些简单的场景，我们可能会更希望利用数据库本身的能力来完成我们的需求 所幸，在 MySQL 5.5 之后，其支持了一定的全文搜索的能力 MySQL 全文搜索MYSQL 全文搜索的前提是需要在表中建立一个 Full Text Index 12alter table `user` ADD FULLTEXT INDEX name_index (`name`); 注意全文索引，仅对类型为 CHAR/VARCHAR/TEXT 的字段生效。 然后，我们插入两条数据 12345insert into `user` (name, age, createTime, updateTime)values ('Jeff.S.Wang', 18, current_timestamp, current_timestamp);insert into `user` (name, age, createTime, updateTime)values ('Jeff.Li', 18, current_timestamp, current_timestamp); 好了，我们可以来看看 MYSQL 怎么进行全文查询了 首先，按照官方的定义， 1MATCH (col1,col2,...) AGAINST (expr [search_modifier]) 而 search_modifier 是所选取的匹配模式，在MYSQL中共有四种 IN NATURAL LANGUAGE MODE 自然语言模式 IN NATURAL LANGUAGE MODE WITH QUERY EXPANSION 自然语言带扩展模式 IN BOOLEAN MODE 逻辑模式 WITH QUERY EXPANSION 扩展模式 我们常用的是 自然语言模式 和 逻辑模式。 首先来聊聊 自然语言模式，很简单，顾名思义，MYSQL 会直接计算待匹配关键字，然后返回对应的值，这里引用一段官网的解释： By default or with the IN NATURAL LANGUAGE MODE modifier, the MATCH() function performs a natural language search for a string against a text collection. A collection is a set of one or more columns included in a FULLTEXT index. The search string is given as the argument to AGAINST(). For each row in the table, MATCH() returns a relevance value; that is, a similarity measure between the search string and the text in that row in the columns named in the MATCH() list. 我们来写一段 SQL 123select * from `user` where MATCH(name) AGAINST('Jeff' IN NATURAL LANGUAGE MODE) 然后我们发现能得到如下的结果 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 然后，我们来尝试匹配下用户的 LastName，比如我们想找一位姓 Wang 的用户 然后我们写出了如下的 SQL 123select * from `user` where MATCH(name) AGAINST('Jeff' IN NATURAL LANGUAGE MODE) 得到如下结果 id name age updateTime createTime 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 然后我们开始尝试，去搜索一位姓 Li 的用户，然后我们写下了，如下的 SQL 123select * from `user` where MATCH(name) AGAINST('Li' IN NATURAL LANGUAGE MODE) 然后我们发现，什么结果都没有？？？？？WTF？Why？ 原因在于分词粒度，在我们进行录入新数据的时候，MySQL 会将我们的索引字段中的数据按照一定的分词基准长度进行分词，然后存储以待查询，其有四个参数控制分词的长度 innodb_ft_min_token_size innodb_ft_max_token_size ft_min_word_len 作用同上，不过是针对 MyISAM 引擎 ft_max_word_len 以 InnoDB 为例，其默认的 innodb_ft_min_token_size 的值是 3，换句话说在我们之前的录入的数据中，我们数据中存储的分词后的单元是 Jeff Wang 所以我们第二次搜索没有结果，现在我们将 MySQL 的参数修改一下后，重新执行一下？ 123select * from `user` where MATCH(name) AGAINST('Li' IN NATURAL LANGUAGE MODE) 还还还是不行？？？？ 查了下官方文档后，我们发现有这样的描述 Some variable changes require that you rebuild the FULLTEXT indexes in your tables. Instructions for doing so are given later in this section. 而索引分词粒度也包含在其中，，所以我们需要删除/rebuild索引，，然后重新执行（有点坑。。） 123select * from `user` where MATCH(name) AGAINST('Li' IN NATURAL LANGUAGE MODE) 好了，现在正常的返回结果了 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 现在让我们来聊聊另一种匹配模式，BOOLEAN MODE 逻辑模式允许我们用一些操作符来检索一些数据，我们举一些常见的例子，剩下大家可以去看看 MYSQL 官方文档 AGAINST(‘Jeff Li’ IN BOOLEAN MODE) 表示，要么存在 Jeff 要么存在 Li AGAINST(‘+Jeff’ IN BOOLEAN MODE) 表示，必须存在 Jeff AGAINST(‘+Jeff -Li’ IN BOOLEAN MODE) 表示 必须存在 Jeff 且 Li 必须不存在 我们来执行下这几个 SQL 123select * from `user` where MATCH(name) AGAINST('Jeff Li' IN BOOLEAN MODE) 结果 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 123select *from `user` where MATCH(name) AGAINST('+Jeff' IN BOOLEAN MODE) 结果 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 123select * from `user` where MATCH(name) AGAINST('+Jeff -Li' IN BOOLEAN MODE) 结果 id name age updateTime createTime 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 好，现在我们有一些中文搜索的需求，我们先来插入数据 12insert into `user` (name, age, createTime, updateTime)values ('奥特曼', 18, current_timestamp, current_timestamp); 现在我们来搜索姓奥的用户，我们按照之前的 Guide 写出了如下的 SQL 123select *from `user`where MATCH(name) AGAINST('+奥' IN BOOLEAN MODE) 然后我们惊喜的发现，又又又没有结果？？？Why？？？ 其实还是之前提到过的一个问题，分词，MySQL 的默认的分词引擎，只支持英文的分词，而不支持中文分词，那么没有分词，没有搜索？怎么办？ 在 MySQL 5.7 之后，MySQL 提供了 ngram 这个组件来帮助我们进行中文分词，使用很简单 12alter table `user` add fulltext index name_index (`name`) with parser ngram; 这里有几点要注意： ngram 不仅适用于中文，按照官方文档，韩文，日文也都支持 一个字段上只能有一个全文索引，所以需要删除原有全文索引 同时，如同默认的分词一样，ngram 也受分词粒度的限制，不过 ngram 的设置参数是 ngram_token_size 我们按照需要设置即可 总结全文搜索对于日常开发来讲，是一个很常见的需求，在我们的 infra 没法让我们去安心的使用外部组件的时候，利用数据库提供的能力也许是个不错的选项。不过还是有很多的坑要踩，有很多的参数要优化。。BTW 阿里云的 RDS 设置真的难用（小声吐槽 好了。。我的拖延症实在没救了。。而且这两天牙疼真的无奈，呜呜呜呜呜","link":"/posts/2020/03/01/talk-about-full-text-search-in-mysql/"},{"title":"「最简单」的 Core Data 上手指南","text":"原文地址：The Easiest Core Data 原文作者：Alberto De Bortoli 译文出自：掘金翻译计划 译者：Zheaoli 校对者：Kulbear, cbangchen 在过去的几个月里，我花费了大量的时间在研究 Core Data 之上，我得去处理一个使用了很多陈旧的代码，糟糕的 Core Data 以及违反了多线程安全的项目。讲真，Core Data 学习起来非常的困难，在学习 Core Data 的时候，你肯定会感到迷惑和一种深深的挫败感。正是因为这些原因，我决定给出一种超级简单的解决方案。这个方案的特点就是简洁，线程安全，非常易于使用，这个方案能满足你大部分对于 Core Data 的需求。在经过若干次的迭代后，我所设计的方案最终成为一个成熟的方案。 OK，女士们，先生们，现在请允许我隆重向您介绍 Skiathos 和 Skopelos。其中 Skiathos 是基于 Objective-C 所开发的，而 Skopelos 则基于 Swift 所开发的。这两个框架的名字来源于希腊的两个岛，在这里，我渡过了2016年的夏天，同时，在这里完成了两个框架的编写工作。 写在前面的话整个项目的目的就是能够让您以及其简便的方式在您的 App 中引入 Core Data。 我们将从如下几个方面来进行一个介绍: CoreDataStack AppStateReactor DALService (Data Access Layer) CoreDataStack如果你有过使用 Core Data 的经验，那么你应该知道创建一个堆栈是一个充满陷阱的过程。这个组件是用于创建堆栈（用于管理 Obejct Context ），具体的设计说明可以参看 Marcus Zarra 所写的这篇文章。 其中一个和 Magical Record 或者其余第三方插件不同的是，整个存储过程都是在一个方向上发起的，可能是从某个子节点向下或者向上传递来进行持久化储存。其余的组件允许你创建以 private context 作为父节点的子节点，这将会导致 main context 不能被更新，同时只能通过通知的方式来进行合并更新。main context 是相对固定的并与 UI 进行了绑定：这样较为简单的方式可以帮助开发者更好的去完成一个 APP 的开发。 AppStateReactor唔，其实你可以忽略这一段。这个组件属于 CoreDataStack ，在 App 切换至后台，失去节点，或者即将退出时，它负责监视相对应的修改，并把其保存。 DALService (Data Access Layer) / (Skiathos/Skopelos)如果你拥有使用 Core Data 的经验，那么你也应该知道，我们大部分操作都是重复的，我们经常在一个 context 中调用 performBlock:/performBlockAndWait: 函数，而这个 Context 提供了一个最终会调用 save: 作为最终语句的 block 。数据库的所有操作都是基于 API 中所提供的 read: 和 write: ：这两个协议提供了 CQRS （命令和查询分离） 的实现。用于读取的代码块将在主体中进行运行（因为这被认为是一个已确定的单个资源）。用于写入的代码块将会在一个子线程中运行，这样可以保证实时的进行数据储存，变化的数据将会在不会阻塞主线程的情况下通过异步的方式进行储存。write:completion: 方法将会程序运行完后来对数据的更改进行持久化储存。 换句话说，写入的数据在 main managed object context 和最后持久化过程中都会保证其一致性。在 主要管理对象的 context 中，相应的数据也能保证其可用性。 Skiathos/Skopelos 是 DALService 的子类, 这样可以给这个组件一个比较好听的名字。 使用介绍在使用这一系列组件之前，你首先需要创建一个类型为 Skiathos 的属性，然后以下面这种方式去初始化它： 123self.skiathos = [Skiathos setupInMemoryStackWithDataModelFileName:@&quot;&lt;#datamodelfilename&gt;&quot;];// orself.skiathos = [Skiathos setupSqliteStackWithDataModelFileName:@&quot;&lt;#datamodelfilename&gt;&quot;]; 在使用 Skopelos 时，代码如下所示： 123self.skopelos = SkopelosClient(inMemoryStack: &quot;&lt;#datamodelfilename&gt;&quot;)// orself.skopelos = SkopelosClient(sqliteStack: &quot;&lt;#datamodelfilename&gt;&quot;) 你可以通过使用依赖注入的方式来在应用的其余地方使用这些对象。不得不说，为 Core Data 栈上的不同对象创建单例是一种很不错的做法。当然，不断的创建实例的开销是十分巨大的。通常来讲，我们不是很推荐使用单例模式。单例模式的测试性不强，在使用过程中，使用者无法有效的控制其声明周期，这样可能会违背一些最佳实践的编程原则。正是因为如此，在这个库里，我们不推荐使用单例。 由于下面几个原因，你在使用时需要从 Skiathos/Skopelos 进行继承： 创建一个全局可共享的实例。 重载 handleError(error: NSError) 方法，以便在你的程序里出现一些错误时，这个方法能够正常的被调用。 为了创建单例，你应该如下面的示例一样去从 Skiathos/Skopelos 进行继承： 单例12345678910111213141516171819@interface SkiathosClient : Skiathos+ (SkiathosClient *)sharedInstance;@endstatic SkiathosClient *sharedInstance = nil;@implementation SkiathosClient+ (SkiathosClient *)sharedInstance{ static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ sharedInstance = [self setupSqliteStackWithDataModelFileName:@&quot;&lt;#datamodelfilename&gt;&quot;];&lt;/#datamodelfilename&gt; }); return sharedInstance;}- (void)handleError:(NSError *)error{ // clients should do the right thing here NSLog(@&quot;%@&quot;, error.description);}@end 或者是 1234567class SkopelosClient: Skopelos { static let sharedInstance = Skopelos(sqliteStack: &quot;DataModel&quot;) override func handleError(error: NSError) { // clients should do the right thing here print(error.description) }} 读写操作写到这里，让我们同时看看在一个标准 Core Data 的操作方式和我们组件所提供的方式吧。 标准的读取姿势: 1234567891011__block NSArray *results = nil;NSManagedObjectContext *context = ...;[context performBlockAndWait:^{ NSFetchRequest *request = [[NSFetchRequest alloc] init]; NSEntityDescription *entityDescription = [NSEntityDescription entityForName:NSStringFromClass(User) inManagedObjectContext:context]; [request setEntity:entityDescription]; NSError *error; results = [context executeFetchRequest:request error:&amp;error];}];return results; 标准的写入姿势: 12345678910111213NSManagedObjectContext *context = ...;[context performBlockAndWait:^{ User *user = [NSEntityDescription insertNewObjectForEntityForName:NSStringFromClass(User) inManagedObjectContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;; NSError *error; [context save:&amp;error]; if (!error) { // continue to save back to the store }}]; Skiathos 中的读取姿势： 1234[[SkiathosClient sharedInstance] read:^(NSManagedObjectContext *context) { NSArray *allUsers = [User allInContext:context]; NSLog(@&quot;All users: %@&quot;, allUsers);}]; Skiathos 中的写入姿势： 1234567891011121314151617181920212223242526// Sync[[SkiathosClient sharedInstance] writeSync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;}];[[SkiathosClient sharedInstance] writeSync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;} completion:^(NSError *error) { // changes are saved to the persistent store}];// Async[[SkiathosClient sharedInstance] writeAsync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;}];[[SkiathosClient sharedInstance] writeAsync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;} completion:^(NSError *error) { // changes are saved to the persistent store}]; Skiathos 当然也支持链式调用： 1234567891011__block User *user = nil;[SkiathosClient sharedInstance].write(^(NSManagedObjectContext *context) { user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;}).write(^(NSManagedObjectContext *context) { User *userInContext = [user inContext:context]; [userInContext deleteInContext:context];}).read(^(NSManagedObjectContext *context) { NSArray *users = [User allInContext:context];}); 如果是在 Swift中，代码将会变成下面这个样子 读取： 1234SkopelosClient.sharedInstance.read { context in let users = User.SK_all(context) print(users)} 写入： 1234567891011121314151617181920212223242526// SyncSkopelosClient.sharedInstance.writeSync { context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}SkopelosClient.sharedInstance.writeSync({ context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot; }, completion: { (error: NSError?) in // changes are saved to the persistent store})// AsyncSkopelosClient.sharedInstance.writeAsync { context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}SkopelosClient.sharedInstance.writeAsync({ context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}, completion: { (error: NSError?) in // changes are saved to the persistent store}) 链式调用： 123456789101112SkopelosClient.sharedInstance.write { context in user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}.write { context in if let userInContext = user.SK_inContext(context) { userInContext.SK_remove(context) }}.read { context in let users = User.SK_all(context) print(users)} NSManagedObject 类所提供了非常清楚的 CRUD 方法。在作为读/写代码块的参数传递之时，对象应该被作为一个整体进行处理。你应该优先使用这些内建的方法。主要的方法有下面这些： 1234567+ (instancetype)SK_createInContext:(NSManagedObjectContext *)context;+ (NSUInteger)SK_numberOfEntitiesInContext:(NSManagedObjectContext *)context;- (void)SK_deleteInContext:(NSManagedObjectContext *)context;+ (void)SK_deleteAllInContext:(NSManagedObjectContext *)context;+ (NSArray *)SK_allInContext:(NSManagedObjectContext *)context;+ (NSArray *)SK_allWithPredicate:(NSPredicate *)pred inContext:(NSManagedObjectContext *)context;+ (instancetype)SK_firstInContext:(NSManagedObjectContext *)context; 1234567static func SK_create(context: NSManagedObjectContext) -&gt; Selfstatic func SK_numberOfEntities(context: NSManagedObjectContext) -&gt; Intfunc SK_remove(context: NSManagedObjectContext) -&gt; Voidstatic func SK_removeAll(context: NSManagedObjectContext) -&gt; Voidstatic func SK_all(context: NSManagedObjectContext) -&gt; [Self]static func SK_all(predicate: NSPredicate, context:NSManagedObjectContext) -&gt; [Self]static func SK_first(context: NSManagedObjectContext) -&gt; Self? 注意，在使用 SK_inContext(context: NSManagerObjectContext) 时，不同的读写代码块可能会得到同一个对象。 线程安全所有 DALService 所产生的实例都可以认为是线程安全的。 我们特别建议你在项目中进行这样的设置 -com.apple.CoreData.ConcurrencyDebug 1 ，这可以确保你不会在多线程和并发的情况下滥用 Core Data。 这个组件不是为了通过隐藏 ManagedObjectContext: 的概念来达到接口引入的目的：它将会在客户端中引入更多的线程问题，因为开发者有责任去检查所调用线程的类型（而那将会是在忽视 Core Data 所带给我们的好处）。","link":"/posts/2016/09/01/the-easiest-core-data/"},{"title":"重回 Windows 之路","text":"最近会回到 Windows 下办公了两周多，简单记录下自己的使用体验 背景最近刚配置了一台高配的主机（i9-13900k+4090），之前的 MBP 沦为开会本，开始尝试把 PC 作为主力机使用，由于驱动太新没搞上合适的 Linux 发行版，Ubuntu 系动不动又在“Ubuntu 内部错误”，所以我开始使用 Windows 作为自己的工作生活主力机。两周使用下来发现 Windows 的使用体验还是超出我的预期了。下面大概聊聊 优点时隔多年主力使用 Windows，感觉进步还是蛮大的，主要体现在 系统交互上 开发者体验上 系统交互上，Windows 吸收了不少其余的系统和软件的精髓，感觉进步不小，写几个我印象里比较深的吧 开始菜单栏进行了改进，比之前感觉更好用了 File Explorer 的改进，比如支持 Tab 什么的，还是很舒服的 非常优秀的分屏功能，将窗口拖到屏幕上方就能选择不同的分屏比例和应用，同时在切换任务的时候，有 Task Group 的概念。这不比 macOS 高了不知道10个 Linux？ 全局深色主题支持比之前好很多了 整体使用下来，体验比前面的 Windows 好了不少，给人带来不少惊喜 然后在开发者体验方面，巨硬这几年拥抱开源的战略在 Windows 的开发者体验的优化体现的很不错 WSL2 目前实测比 WSL1 时期好用了太多，IDE 以及其余的工具都能很好的支持 WSL2 的兼容性也出乎我的意料，我搞内核依赖的 BCC/eBPF 等工具都能在 WSL2 上正常运行，很不错 Visual Studio 对于 CMake 的支持超出了我的意料，非常不错 果然巨硬现在是开发者的好朋友啊（ 缺点虽然 Windows 体验上有了很大的提升，但是还是有一些缺点的 BUG 不少，比如在任务管理器里面重新排序任务之类的会卡界面（也有可能和我用的 dev channel 的 insider preview 有关 WSL2 的小问题不少，比如内存泄露，比如导出备份失败之类的 Windows 下面的权限管理始终是个问题，我迄今都没想明白，原神作为一个游戏要 UAC 干什么？检查我电脑上装莉莉丝的游戏了吗？ 虽然有了 scope 这样的工具，Windows 上的软件包的管理还是有点蛋疼 好像没看到有类似 MacOS 上 setapp 这样的集中的软件分销体系，不知道是不是我没找到。反正买软件太分散了也挺蛋疼的","link":"/posts/2022/11/12/the-road-back-to-windows/"},{"title":"从一个重构项目中能学到什么东西","text":"本来这篇文章是要在 2022 最后一个工作日前写完的，但是拖延癌发作，到现在才写完。不过还是发出来，希望里面的内容能帮到大家 背景介绍这个重构项目如果从我第一个超大型重构 PR 算起（22年12月11日），到现在已经历史一个半月了。目前重构进度已经超过了 80%，超过6+位贡献者集体贡献。这绝对是个不小的工程了 那问题来了，我为什么要发起这个重构项目呢？ 在重构项目之前，nerdctl 项目存在一个很大的问题，即 command 的入口处，flag 的处理和逻辑耦合的问题，比如用 nerdctl apparmor 系列的代码来举一个例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package mainimport ( &quot;bytes&quot; &quot;errors&quot; &quot;fmt&quot; &quot;text/tabwriter&quot; &quot;text/template&quot; &quot;github.com/containerd/nerdctl/pkg/apparmorutil&quot; &quot;github.com/spf13/cobra&quot;)func newApparmorLsCommand() *cobra.Command { cmd := &amp;cobra.Command{ Use: &quot;ls&quot;, Aliases: []string{&quot;list&quot;}, Short: &quot;List the loaded AppArmor profiles&quot;, Args: cobra.NoArgs, RunE: apparmorLsAction, SilenceUsage: true, SilenceErrors: true, } cmd.Flags().BoolP(&quot;quiet&quot;, &quot;q&quot;, false, &quot;Only display profile names&quot;) // Alias &quot;-f&quot; is reserved for &quot;--filter&quot; cmd.Flags().String(&quot;format&quot;, &quot;&quot;, &quot;Format the output using the given go template&quot;) cmd.RegisterFlagCompletionFunc(&quot;format&quot;, func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) { return []string{&quot;json&quot;, &quot;table&quot;, &quot;wide&quot;}, cobra.ShellCompDirectiveNoFileComp }) return cmd}func apparmorLsAction(cmd *cobra.Command, args []string) error { quiet, err := cmd.Flags().GetBool(&quot;quiet&quot;) if err != nil { return err } w := cmd.OutOrStdout() var tmpl *template.Template format, err := cmd.Flags().GetString(&quot;format&quot;) if err != nil { return err } switch format { case &quot;&quot;, &quot;table&quot;, &quot;wide&quot;: w = tabwriter.NewWriter(cmd.OutOrStdout(), 4, 8, 4, ' ', 0) if !quiet { fmt.Fprintln(w, &quot;NAME\\tMODE&quot;) } case &quot;raw&quot;: return errors.New(&quot;unsupported format: \\&quot;raw\\&quot;&quot;) default: if quiet { return errors.New(&quot;format and quiet must not be specified together&quot;) } var err error tmpl, err = parseTemplate(format) if err != nil { return err } } profiles, err := apparmorutil.Profiles() if err != nil { return err } for _, f := range profiles { if tmpl != nil { var b bytes.Buffer if err := tmpl.Execute(&amp;b, f); err != nil { return err } if _, err = fmt.Fprintf(w, b.String()+&quot;\\n&quot;); err != nil { return err } } else if quiet { fmt.Fprintln(w, f.Name) } else { fmt.Fprintf(w, &quot;%s\\t%s\\n&quot;, f.Name, f.Mode) } } if f, ok := w.(Flusher); ok { return f.Flush() } return nil} 你能看到在函数 apparmorLsAction 的逻辑中包含了两个部分的东西 flag 的处理（大道至简的 err 处理（XDDDDD command logic 的处理 这样的设计存在很明显的问题 代码可读性与可维护性的问题，比如我需要添加一个 flag 的时候，那么需要在多处添加。而且满天飞的 flagging process 会导致提升新人进入项目的门槛 logic 的处理与 flag 的处理耦合在一起，这样会额外导致如果社区在试图基于 nerdctl 封装一套自定义的 CLI 脚手架的时候，那么会出现非常难处理的情况。 同时 nercdctl 还存在另外一个问题。在 cmd 的入口处，因为同归属于一个 sub package，于是之前的开发过程中为了省事，文件之间为了省事，交叉引用了彼此的 internal helper function 在 nerdctl 项目最开始只作为 containerd CLI 的一个替代品的时候。之前的设计缺陷实际上暴露的并不明显。但是 nerdctl 完整提供了一套基于 containerd 的容器生命周期及网络管理（base on CNI）及其余进阶特性（比如 cosign，IPFS 等），开始作为 containerd 实质上的一个入口标准的时候。社区无疑会提出更高的需求。比如 Move *.go files for subcommand out main package nerdctl#1631 就是一个很典型的例子。 在这种情况下，对于 nerdctl 的入口进行一个合理的但是大范围的重构，就是一个必须且迫在眉睫的事了。 又到了白色相薄重构的季节 — 蛮久抚子（Nadeshiko Manju） 重构过程分析好了，社区有需要，saka 哦不，蛮久抚子（Nadeshiko Manju）我就得站出来了，重构嘛，很简单嘛，Goland 搞一搞就完事了嘛。好说好说。于是我有了一个超大的 PR ：Refactor the package structure in cmd/nerdctl nerdctl#1639。规模 +5000 -4000 不过，因为这个 PR 太过于惊世骇俗，在我 COVID-19 Positive 后，Suda 开始帮我 carry 这个 PR。但是最后 Suda 也高呼不可 carry（Suda の惊く：ばか saka！） どうしてこうなるんだろう…初めて、リファクタリングしたいという欲求があり、リファクタリングの必要性がありました。嬉しいことが二つ重なって。その二つの嬉しさが、また、たくさんの嬉しさを連れてきてくれて。夢のように幸せな時間を手に入れたはずなのに…なのに、どうして、こうなっちゃうんだろう…为什么会变成这样呢，第一次有了想重构的欲望，又有了重构的必要。两件快乐事情重合在一起。而这两份快乐，又给我带来更多的快乐。得到的，本该是像梦境一般幸福的时间……但是，为什么，会变成这样呢…… —- 《nerdctl 相薄》 实际上原因很简单 冬马小三 ，哦不是，是我小三，哦，不是，是我脑子被门夹了 言归正传，其实这个 PR 是个教科书式的反面例子 在启动大型项目之前没有达成社区的共识 违背了 One PR for One Thing 的基本原则 重构时的无关的改动太多，导致 review 难度过大 所以在吸取了 Refactor the package structure in cmd/nerdctl nerdctl#1639 的教训后，我正式在社区提出了一个重构 Proposal Let’s refactor the nerdctl CLI package nerdctl#1680 ，在这个 Proposal 中我做了几个事情 完整阐述了重构的必要性，方便社区成员后续回溯 定义了重构的几个 step 约定好了多人协作重构时所共同遵守的约定 社区其余几位 maintainer 在这个 Proposal 下额外讨论了一些细节，并达成了一些共识 将最终的重构范围缩小为仅处理 flagging process 优化了一些文件结构的设计 截止到现在，nerdctl 的重构才算开始正式进入了一个快车道的状态。毕竟重构不是乱写，要是写错了，要向社区谢罪的。 这里面其实还有个插曲，最开始我在 Issue 中创建 TODO Task 之后，为了方便 track project 的进度，我将这些 TODO Task 直接全部转成了 Issue（然后就相当于给 subscribe 了这个 repo 的老哥们来了一个邮箱 DDOS）。这里不得不吐槽一句，GitHub 的项目管理工具真的很弱诶（XDDDDD 花开两朵，各表一只，在 Proposal 正式通过了之后，整体的重构就开始进入了快车道了，这里列一些有意思的讨论，大家有兴趣可以去看看 Refactor the apparmor flagging process nerdctl#1774，Proposal 接收后的一个模板 PR，在这个 PR 下，继续细化了一些在 Proposal 中讨论没有完善的细节 [Refactor] Refactor the build subcommand flagging process nerdctl#1792，Proposal 接收后第一个比较大命令的重构，某种意义上也是一个模板 PR 了，里面就讨论了不少参数设计风格的问题 refactor: consolidate main logic of volume.List into volume.Volumes, 不属于 Proposal 原本涵盖的范围内，但是里面关于函数语义设计的讨论值得关注一下 pkg/cmd: inconsistent arguments ordering nerdctl#1889，关于函数设计风格的问题。 当然还有很多 PR 中的讨论也是非常有意思的，这里就不完整列出来了。欢迎大家去直接看原始的 PR（当然欢迎加入讨论） 总结差不多就这样吧，大概复盘了一下到现在为止重构过程中的得失。希望大家能喜欢","link":"/posts/2023/01/26/what-I-can-learn-from-a-refactor-project/"},{"title":"你所不知道的 Flask Part1:Route 初探","text":"前言我自己都记不清楚上一次写博客是什么时候了（笑），上一次挖的坑现在还没填完，干脆，开个新坑吧，你不知道的 Flask ，记录下自己用 Flask 过程中一些很好玩的东西，当然很大可能我又会中途弃坑 开篇引子之前遇到一个很奇怪的需求，需要在flask中支持正则表达式比如，@app.route('/api/(.*?)') 这样，在视图函数被调用的时候，能传入 URL 中正则匹配的值。不过 Flask 路由中默认不支持这样的方法，那么我们该怎么办？我们先思考五分钟吧？ 好了，我先给出解决方案吧 12345678910from flask import Flaskfrom werkzeug.routing import BaseConverterclass RegexConverter(BaseConverter): def __init__(self, map, *args): self.map = map self.regex = args[0]app = Flask(__name__)app.url_map.converters['regex'] = RegexConverter 在经过这样的设置后我们便可以按照我们刚才的需求写代码了 12345@app.route('/docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt;')def hello(url=None): print(url) 在这里，我们函数中传入的url变量，就是我们代码中所匹配到的值 但是为什么这样就OK了呢？ 详解首先，我们要弄清楚一个东西，Flask 是 基于 Werkzurg 的一个框架，Flask 的 Route 机制基于 Werkzurg 上更进一步封装所得到的，OK，我们上面所以实现的 Converter 便是利用了 Werkzurg 中的 Route 的特性 好了，我先给出官方文档 custom-converters 然后我们来仔细讲讲， 首先，Werkzurg 中存在着一种机制叫做 Converter ，简而言之就是通过一定的特殊语法，将 URL 中的特定部分，转化成特定的 Python 变量，其语法格式为 /url/&lt;converter_name(&quot;表达式&quot;):变量名&gt; 看起来有点复杂对吧，OK 用我们之前的例子来讲一下吧，你看，我们之前定义了一个 '/docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt;' 的 URL ，其中后面部分就是利用了我们提到的 Converter 语法。具体的含义是，这个部分的 url 交给 regex 这个 Converter 来处理，最终生成的变量名为 url。 好了，我们来说说自定义 Converter 参数中的注意事项，在构建一个自己的 Converter 过程中，我们将按照如下的方式编写代码 1234class RegexConverter(BaseConverter): def __init__(self, map, regex,*args): self.map = map self.regex = regex map 是指 werkzurg.routing 中的 Map 对象，而 regex 则是指你所写的表达式。其中 map 的作用我们将放在下一章进行讲解，（又立flag了，笑）。 好了这里差不多完成了，我们来看看 Flask 喔，不，werkzurg 中怎么实现的这样的方法吧 简明代码剖析最前面，你首先得有一点 flask 装饰器路由的知识，详情可以参考这篇文章，菜鸟阅读 Flask 源码系列（1）：Flask的router初探 首先在 werkzurg 框架的 routing 文件中，存在着这样一段代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374_rule_re = re.compile(r''' (?P&lt;static&gt;[^&lt;]*) # static rule data &lt; (?: (?P&lt;converter&gt;[a-zA-Z_][a-zA-Z0-9_]*) # converter name (?:\\((?P&lt;args&gt;.*?)\\))? # converter arguments \\: # variable delimiter )? (?P&lt;variable&gt;[a-zA-Z_][a-zA-Z0-9_]*) # variable name &gt;''', re.VERBOSE)_simple_rule_re = re.compile(r'&lt;([^&gt;]+)&gt;')_converter_args_re = re.compile(r''' ((?P&lt;name&gt;\\w+)\\s*=\\s*)? (?P&lt;value&gt; True|False| \\d+.\\d+| \\d+.| \\d+| \\w+| [urUR]?(?P&lt;stringval&gt;&quot;[^&quot;]*?&quot;|'[^']*') )\\s*,''', re.VERBOSE | re.UNICODE)def parse_converter_args(argstr): argstr += ',' args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group('stringval') if value is None: value = item.group('value') value = _pythonize(value) if not item.group('name'): args.append(value) else: name = item.group('name') kwargs[name] = value return tuple(args), kwargsdef parse_rule(rule): &quot;&quot;&quot;Parse a rule and return it as generator. Each iteration yields tuples in the form ``(converter, arguments, variable)``. If the converter is `None` it's a static url part, otherwise it's a dynamic one. :internal: &quot;&quot;&quot; pos = 0 end = len(rule) do_match = _rule_re.match used_names = set() while pos &lt; end: m = do_match(rule, pos) if m is None: break data = m.groupdict() if data['static']: yield None, None, data['static'] variable = data['variable'] converter = data['converter'] or 'default' if variable in used_names: raise ValueError('variable name %r used twice.' % variable) used_names.add(variable) yield converter, data['args'] or None, variable pos = m.end() if pos &lt; end: remaining = rule[pos:] if '&gt;' in remaining or '&lt;' in remaining: raise ValueError('malformed url rule: %r' % rule) yield None, None, remaining 首先，_rule_re 以及 _converter_args_re 两段是很骚的正则表达式，不过作者已经给出了足够的注释，大家可以对照着正则表达式的语法进行学习一个，然后 parse_converter_args 以及 parse_rule 则是利用正则表达式对其进行解析操作。 OK，我们紧接着往下查看 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def compile(self): &quot;&quot;&quot;Compiles the regular expression and stores it.&quot;&quot;&quot; assert self.map is not None, 'rule not bound' if self.map.host_matching: domain_rule = self.host or '' else: domain_rule = self.subdomain or '' self._trace = [] self._converters = {} self._weights = [] regex_parts = [] def _build_regex(rule): for converter, arguments, variable in parse_rule(rule): if converter is None: regex_parts.append(re.escape(variable)) self._trace.append((False, variable)) for part in variable.split('/'): if part: self._weights.append((0, -len(part))) else: if arguments: c_args, c_kwargs = parse_converter_args(arguments) else: c_args = () c_kwargs = {} convobj = self.get_converter( variable, converter, c_args, c_kwargs) regex_parts.append('(?P&lt;%s&gt;%s)' % (variable, convobj.regex)) self._converters[variable] = convobj self._trace.append((True, variable)) self._weights.append((1, convobj.weight)) self.arguments.add(str(variable)) _build_regex(domain_rule) regex_parts.append('\\\\|') self._trace.append((False, '|')) _build_regex(self.is_leaf and self.rule or self.rule.rstrip('/')) if not self.is_leaf: self._trace.append((False, '/')) if self.build_only: return regex = r'^%s%s$' % ( u''.join(regex_parts), (not self.is_leaf or not self.strict_slashes) and '(?&lt;!/)(?P&lt;__suffix__&gt;/?)' or '' ) self._regex = re.compile(regex, re.UNICODE) 这是 werkzurg 框架的 routing 文件中 Rule 类种的一部分的源码，其中在 def _build_regex(rule): 之前的是一些准备代码，然后我们接着往下看，for converter, arguments, variable in parse_rule(rule): 这一段代码，就是 URL 解析，通过调用 parse_rule 函数来实现对我们之前提到的 converter 语法进行解析，紧接着，如果 URL 里不存在我们 Converter 的语法，则 converter 为空，我们执行处理其余 URL 的逻辑，如果 converter 存在，进行下面的流程，首先，如果我们在 Converter 语法中设定了解析表达式，那么我们利用 parse_converter_args 函数来处理我们的表达式，方便后续的操作，处理完成后，我们利用 get_converter 方法来初始化我们的 Converter , 代码如下： 123456789def get_converter(self, variable_name, converter_name, args, kwargs): &quot;&quot;&quot;Looks up the converter for the given parameter. .. versionadded:: 0.9 &quot;&quot;&quot; if converter_name not in self.map.converters: raise LookupError('the converter %r does not exist' % converter_name) return self.map.converters[converter_name](self.map, *args, **kwargs) 以我们之前的 demo 为例， 12345678910from flask import Flaskfrom werkzeug.routing import BaseConverterclass RegexConverter(BaseConverter): def __init__(self, map, *args): self.map = map self.regex = args[0]app = Flask(__name__)app.url_map.converters['regex'] = RegexConverter 我们已经添加了一个名为 regex 的 Converter 对象，在 get_converter 方法中我们传入了值为 regex 的 converter_name 变量，紧接着，我们初始化了一个 RegexConverter 对象的实例，然后返回这个实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 def compile(self): &quot;&quot;&quot;Compiles the regular expression and stores it.&quot;&quot;&quot; assert self.map is not None, 'rule not bound' if self.map.host_matching: domain_rule = self.host or '' else: domain_rule = self.subdomain or '' self._trace = [] self._converters = {} self._weights = [] regex_parts = [] def _build_regex(rule): for converter, arguments, variable in parse_rule(rule): if converter is None: regex_parts.append(re.escape(variable)) self._trace.append((False, variable)) for part in variable.split('/'): if part: self._weights.append((0, -len(part))) else: if arguments: c_args, c_kwargs = parse_converter_args(arguments) else: c_args = () c_kwargs = {} convobj = self.get_converter( variable, converter, c_args, c_kwargs)############################################################# 无耻分割线 regex_parts.append('(?P&lt;%s&gt;%s)' % (variable, convobj.regex)) self._converters[variable] = convobj self._trace.append((True, variable)) self._weights.append((1, convobj.weight)) self.arguments.add(str(variable)) _build_regex(domain_rule) regex_parts.append('\\\\|') self._trace.append((False, '|')) _build_regex(self.is_leaf and self.rule or self.rule.rstrip('/')) if not self.is_leaf: self._trace.append((False, '/')) if self.build_only: return regex = r'^%s%s$' % ( u''.join(regex_parts), (not self.is_leaf or not self.strict_slashes) and '(?&lt;!/)(?P&lt;__suffix__&gt;/?)' or '' ) self._regex = re.compile(regex, re.UNICODE) 在分割线后面的代码中，我们对处理后的 url 进行一些收尾的操作，以我们之前的 demo 为例，我们设定的 /docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt; URL 最终转化成 /docs/model_utils/(?P&lt;url&gt;.*) ，编译成 re 对象后赋值给 Rule 实例中的 _regex 变量 好了，我们知道处理的部分后，我们大致来看一下怎么匹配并生成值的吧 12345678910111213141516171819202122232425262728293031323334353637383940414243def match(self, path, method=None): &quot;&quot;&quot;Check if the rule matches a given path. Path is a string in the form ``&quot;subdomain|/path&quot;`` and is assembled by the map. If the map is doing host matching the subdomain part will be the host instead. If the rule matches a dict with the converted values is returned, otherwise the return value is `None`. :internal: &quot;&quot;&quot; if not self.build_only: m = self._regex.search(path) if m is not None: groups = m.groupdict() # we have a folder like part of the url without a trailing # slash and strict slashes enabled. raise an exception that # tells the map to redirect to the same url but with a # trailing slash if self.strict_slashes and not self.is_leaf and \\ not groups.pop('__suffix__') and \\ (method is None or self.methods is None or method in self.methods): raise RequestSlash() # if we are not in strict slashes mode we have to remove # a __suffix__ elif not self.strict_slashes: del groups['__suffix__'] result = {} for name, value in iteritems(groups): try: value = self._converters[name].to_python(value) except ValidationError: return result[str(name)] = value if self.defaults: result.update(self.defaults) if self.alias and self.map.redirect_defaults: raise RequestAliasRedirect(result) return result 这也是 werkzurg 框架的 routing 文件中 Rule 类种的一部分的源码，在这段代码中，首先利用 re 对象中的 search 方法，检测当前传入的 Path 是否匹配，如果匹配的话，进入后续的处理流程，还记得我们之前最终生成的 /docs/model_utils/(?P&lt;url&gt;.*) 么，这里面利用了正则表达式命名组的语法糖，在这里，匹配成功后，Python 的 re 库里给我们提供了一个 groupdict 让我们取出命名组里所代表的值。然后我们调用 conveter 实例里面的 to_python 方法来对我们匹配出来的值进行处理（注：这是 Converter 系列对象中的一个可重载方法，我们可以通过重载这个方法，来对我们匹配到的值进行一些逻辑处理，这个我们还是后面再讲吧，flag++），然后我们把最终的 result 值返回。 最后的最后，Flask 在获取 werkzurg 给出的匹配结果后，将匹配的值，放在 request 实例中的 view_args 变量上，最后通过 dispatch_request 对象传递给我们的视图函数，代码如下 123456789101112131415161718192021def dispatch_request(self): &quot;&quot;&quot;Does the request dispatching. Matches the URL and returns the return value of the view or error handler. This does not have to be a response object. In order to convert the return value to a proper response object, call :func:`make_response`. .. versionchanged:: 0.7 This no longer does the exception handling, this code was moved to the new :meth:`full_dispatch_request`. &quot;&quot;&quot; req = _request_ctx_stack.top.request if req.routing_exception is not None: self.raise_routing_exception(req) rule = req.url_rule # if we provide automatic options for this URL and the # request came with the OPTIONS method, reply automatically if getattr(rule, 'provide_automatic_options', False) \\ and req.method == 'OPTIONS': return self.make_default_options_response() # otherwise dispatch to the handler for that endpoint return self.view_functions[rule.endpoint](**req.view_args) 好了，我们的代码剖析就到此结束 最后想说几句Flask + Werkzurg 是一套设计实现的非常精妙的组合，不过我们在日常的使用中常常忽略了里面的美丽的风景，所以这也是我想写这样剖析代码笔记的文章的原因 好了，给老铁们留几个思考题，欢迎评论区讨论 Flask 为什么不默认支持正则表达式的输入 诸如 PathConverter 这样 Werkzurg 内置的 Converter 为什么在写表达式的时候可以这样 /&lt;path:wikipage&gt;/edit 写，而忽略其中的表达式 前面提到的 parse_converter_args 方法的代码详解 好了，就先这样吧2333 对了，保佑我文章里立的 Flag 都能实现（笑）","link":"/posts/2017/08/13/what-the-fuck-about-flask-part1/"},{"title":"Supervisor 的一个隐藏坑","text":"本垃圾 API 搬运工程师又来了啊，= =今天因为 Supervisor 一个隐藏的参数配置，造成了一个重要项目的线上崩溃。= =我觉得还是有必要分享一波，所以写了一篇垃圾水文。 起因写着写着代码，突然接到一堆报警邮件，让我直接觉得世界不那么可爱 然后定睛一看异常信息？卧槽？新建连接就马上传说中的 [Errno 24] Too many open files ？？这搞你xxx啊，开始搞呗。 查 bug首先，众所周知，Linux 中万物皆文件= =，于是我们操作网络链接的过程，其实也就是操作 File Descriptor 的问题= =，诶，既然 Too many open files 那就优先考虑，是不是系统设置的阀值太小了，于是 ulimit -a 一把梭？？ 诶？open files 一栏数字不小啊？足够啊？那这特么是什么鬼啊？ 行吧，查一下网络连接吧， 一把梭，netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 统计下，当时处于各个状态的连接数量吧 诶？有点意思，TIME_WAIT 数量太多了吧？诶？有意思，那就祭出老夫的内核网络参数的半吊子功夫，魔改一下呗？ 123456789101112#参数优化#表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间net.ipv4.tcp_fin_timeout = 30#表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒，net.ipv4.tcp_keepalive_time = 300#表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭net.ipv4.tcp_syncookies = 1#表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间net.ipv4.tcp_tw_reuse = 1#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭，当前TIME-WAIT 过多，所以开启快速回收net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 5000 65000 在 /etc/sysctl.conf 中新增如上一些配置项。然后 sysctl -p 生效一波。开始看效果吧 诶！报错数量是在减小，TIME_WAIT 数量也逐渐正常 诶？？？等等？？？其余机器没 TIME_WAIT 过多的问题啊，那这尼玛是什么鬼？而且快速回收 TIME_WAIT 的连接也会带来其余的副作用（后面单章说） = =好吧，现在怀疑，是不是 Supervisor 的问题，好的，文档翻阅大赛，开始 恩，，翻了半天，查到原因了，跟一个叫做 minfds 的参数相关 描述如下 The minimum number of file descriptors that must be available before supervisord will start successfully. A call to setrlimit will be made to attempt to raise the soft and hard limits of the supervisord process to satisfy minfds. The hard limit may only be raised if supervisord is run as root. supervisord uses file descriptors liberally, and will enter a failure mode when one cannot be obtained from the OS, so it’s useful to be able to specify a minimum value to ensure it doesn’t run out of them during execution. These limits will be inherited by the managed subprocesses. This option is particularly useful on Solaris, which has a low per-process fd limit by default. 大意为，Supervisor 启动时，将根据 minfds 的值来确定系统中是否有足够的空余 fd 供其使用。同时因为我们跑在 Supervisor 中的服务，都是由 Supervisord fork 而来，因为父子关系，同时保证安全，单个进程开启的描述符最多不允许超过 minfds 设置的值，默认为 1024。然后，它补了一个刀，如果你用 root 用户运行的话，我们默认给你搞到系统最大的哦！ 卧槽。。。原来是这啊，你谁没事用 root 跑服务啊= =简直药丸。。。 行吧，改参数，改参数 后续最后这事就这样的结束了，趟一个雷，顺便复习了下内核的网络参数，虽然感觉美滋滋，不过感觉，贵 Supervisor 吃枣药丸！","link":"/posts/2017/12/28/what-the-fuck-supvisor/"},{"title":"Sanic 的若干吐槽","text":"Sanic 的若干吐槽刚刚和红姐，在 哪些 Python 库让你相见恨晚？ 这个答案下面讨论了一下 Sanic 的优劣。 突然想起，我司算是国内应该比较少见的把 Sanic 用在正式生产线上的公司了，作为一个主力推（da）动（shui）者（bi），我这个辣鸡文档工程师觉得有必要来说一下我们在使用 Sanic 过程中所采用的一系列深坑。 正文首先 Sanic 官方 的口号是一个 Flask Like 的 web framework 。这回让很多人有一种错觉，就是 Sanic 内部的实现和 Flask 近乎一致，但是事实真的是这样么？ 我们首先来看一下一组 Hello World 123456789101112# Flaskfrom flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 1234567891011121314# Sanicfrom sanic import Sanicapp = Sanic()@app.route(&quot;/&quot;)async def hello_world(request): return &quot;Hello World!&quot;if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, port=8000) 大家有没有发现什么不同之处？嗯？是不是 Sanic 的 View 函数多了一个参数是为什么呢？ Flask 众所周知的一个最典型的 Feature 就是，它有个 Global Variable 的概念，比如全局的 g 变量，以及 request 变量，这个是借助 werkzurg 里面独立实现的一套类似于 Thread.Local 的机制。在一个请求周期内，在我们业务逻辑，我们可以通过 from flask import request 来获取当前的 request 变量。我们也可以通过这样的机制，在上面挂一些数据来实现数据的全局使用。 但是 Sanic 则没有这个 Global Variable 这个概念，也就是说，我们需要在业务逻辑中使用 request 变量的话，就需要不断的传递一个 request 变量，直到一个请求周期的终结。 这样方式处理，有好，也有坏，不过我们的吐槽刚刚开始 坑点一：扩展极为不方便比如，我们现在有个需求，我们需要写一个插件，提供给其余部门的同事使用，在插件中，我们需要给原本的 Request 类以及 Response 类新增一些功能，在 Flask 中我们可以这么做 1234567891011from flask import Request,Responsefrom flask import Flaskclass APIRequest(Request): passclass APIResponse(Response): passclass NewFlask(Flask): request_class = APIRequest response_class = APIResponse Flask 中可以通过设置 Flask 类中的两个属性 request_class 以及 response_class 来替换原本的 Request 类，以及 Response 类。 就如同上面这段代码一样，我们很轻松的就可以为 Request 以及 Response 添加一些额外的功能。 但是在 Sanic 中呢？很蛋疼 12345678910111213141516171819202122232425262728293031323334353637class Sanic: def __init__(self, name=None, router=None, error_handler=None, load_env=True, request_class=None, strict_slashes=False, log_config=None, configure_logging=True): # Get name from previous stack frame if name is None: frame_records = stack()[1] name = getmodulename(frame_records[1]) # logging if configure_logging: logging.config.dictConfig(log_config or LOGGING_CONFIG_DEFAULTS) self.name = name self.router = router or Router() self.request_class = request_class self.error_handler = error_handler or ErrorHandler() self.config = Config(load_env=load_env) self.request_middleware = deque() self.response_middleware = deque() self.blueprints = {} self._blueprint_order = [] self.configure_logging = configure_logging self.debug = None self.sock = None self.strict_slashes = strict_slashes self.listeners = defaultdict(list) self.is_running = False self.is_request_stream = False self.websocket_enabled = False self.websocket_tasks = set() # Register alternative method names self.go_fast = self.run 这是 Sanic 中 Sanic 类的初始化代码，首先在 Sanic 中，我们没办法很轻松的替换 Response ,其次，我们通过查看其 __init__ 方法，我们就可以知道，如果要替换默认的 Request 我们需要给其初始化的时候传递一个参数 request_class。这就是让人感觉很迷的地方，这个东西，怎么可以让用传入呢？ 诚然我们可以通过重载 Sanic 类的 __init__ 方法，修改其默认的参数来解决这个问题。 但是新的问题也来了，我一直觉得写组件要默认一个假设，就是所有用你东西的人，智商emmmm都不太高。 好了，因为我们是提供的是插件，如果用户在使用的时候重新继承了我们的定制的 Sanic 类，同时没有使用 super 调用我们魔改后的 __init__ 方法。那么这个时候，就会出一些很有趣的乱子。 同时，Sanic 内部耦合严重，也会造成我们构建插件的时候的困难。 坑点二: 内部耦合严重现在，我们写插件，想在生成 Response 的时候进行一些额外的处理，在 Flask 中，我们可以这样做 123456from flask import Flaskclass NewFlask(Flask): def make_response(self): pass 我们直接可以重载 Flask 类中的 make_response 方法来完成我们 Response 生成的时候新增的一些额外操作。 这个看似简单的操作，在 Sanic 中就变得很恶心 Sanic 中没有像 Flask 这样，一个请求周期内的不同阶段的数据流的处理有着各自独立的方法，比如 dispatch_request,after_request , teardown_request 等等，Request 的处理和 Response 的处理也有着很清晰的界限，我们按需重载就好 Sanic 将一个请求周期类的 Request 数据和 Response 数据的处理，都统一包裹在一个大的 handle_request 方法内 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class Sanic: #..... async def handle_request(self, request, write_callback, stream_callback): &quot;&quot;&quot;Take a request from the HTTP Server and return a response object to be sent back The HTTP Server only expects a response object, so exception handling must be done here :param request: HTTP Request object :param write_callback: Synchronous response function to be called with the response as the only argument :param stream_callback: Coroutine that handles streaming a StreamingHTTPResponse if produced by the handler. :return: Nothing &quot;&quot;&quot; try: # -------------------------------------------- # # Request Middleware # -------------------------------------------- # request.app = self response = await self._run_request_middleware(request) # No middleware results if not response: # -------------------------------------------- # # Execute Handler # -------------------------------------------- # # Fetch handler from router handler, args, kwargs, uri = self.router.get(request) request.uri_template = uri if handler is None: raise ServerError( (&quot;'None' was returned while requesting a &quot; &quot;handler from the router&quot;)) # Run response handler response = handler(request, *args, **kwargs) if isawaitable(response): response = await response except Exception as e: # -------------------------------------------- # # Response Generation Failed # -------------------------------------------- # try: response = self.error_handler.response(request, e) if isawaitable(response): response = await response except Exception as e: if self.debug: response = HTTPResponse( &quot;Error while handling error: {}\\nStack: {}&quot;.format( e, format_exc())) else: response = HTTPResponse( &quot;An error occurred while handling an error&quot;) finally: # -------------------------------------------- # # Response Middleware # -------------------------------------------- # try: response = await self._run_response_middleware(request, response) except BaseException: error_logger.exception( 'Exception occurred in one of response middleware handlers' ) # pass the response to the correct callback if isinstance(response, StreamingHTTPResponse): await stream_callback(response) else: write_callback(response) 这就造成了一个现象，我们只需要对于某一个阶段数据进行额外的操作的时候，我们势必要重载 handle_request 这个大方法。就比如前面说的，我们只需要在 Response 生成的时候，进行一些额外操作，在 Flask 中我们只需要重载对应的 make_response 方法即可，而在 Sanic 中我们需要重载整个 handle_request 。可谓牵一发动全身。 同时，Sanic 不像 Flask 一样，做到了 WSGI 层的请求处理和 Framework 层的逻辑相互分离。这样一种分离，有时会给我们带来很多方便。 比如我之前写过这样一篇辣鸡文章你所不知道的 Flask Part1:Route 初探，里面提到了这样一个场景。 之前遇到一个很奇怪的需求，需要在flask中支持正则表达式比如，@app.route('/api/(.*?)') 这样，在视图函数被调用的时候，能传入 URL 中正则匹配的值。不过 Flask 路由中默认不支持这样的方法，那么我们该怎么办？ 解决方案很简单 123456789101112from flask import Flaskfrom werkzeug.routing import BaseConverterclass RegexConverter(BaseConverter): def __init__(self, map, *args): self.map = map self.regex = args[0]app = Flask(__name__)app.url_map.converters['regex'] = RegexConverter 在经过这样的设置后我们便可以按照我们刚才的需求写代码了 1234@app.route('/docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt;')def hello(url=None): print(url) 大家可以看到，由于 Flask 的 WSGI 层的处理是基于 Werkzurg 来做的，也就是说，我们有些时候对于 URL 或者其余涉及到 WSGI 层的东西的时候，我们只需要重载/使用 Werkzurg 给我们提供的相关的类或者函数就可以了。同时 app.url_map.converters['regex'] = RegexConverter 这个操作，看了源码的同学就知道，url_map 这个是 werkzurg.routing 类中的 Map 类的一个子类，我们对它的操作，其实本质上也是对于 Werkzurg 的操作，而与 Flask 的框架逻辑无关。 但是在 Sanic 中，并没有这样的分离机制，比如就上面这个场景而言 12345678910class Sanic: def __init__(self, name=None, router=None, error_handler=None, load_env=True, request_class=None, strict_slashes=False, log_config=None, configure_logging=True): #.... self.router = router or Router() #.... Sanic 中对 URL 的解析是由 Router() 实例来触发的，我们如果需要定制我们自己的 URL 解析，我们需要替换 self.router ，这实际上是对 Sanic 本身进行了修改，感觉略有不妥。 同时这里的 Router 类中，如果我们需要定制自己的解析，需要重载 Router 中的 1234567class Router: routes_static = None routes_dynamic = None routes_always_check = None parameter_pattern = re.compile(r'&lt;(.+?)&gt;') parameter_pattern 属性及其余几个解析方法。这里的 Router 并没有像 Werkzurg 中的 Router 一样，实现 Route 和 Parser 以及 Forammter（就是 Converter) 彼此相互分离的特性，我们只需要按需重构添加即可，如同文中所举的例子。 整个这一部分，其实就在吐槽，Sanic 内部耦合严重，如果想实现一些额外的操作，可以说牵一发动全身。 坑点三：细节以及其余的坑这一部分大概有几方面要说。 第一，Sanic 依赖的库，其实，emmmmmm，不太稳定，比如 10 月份的时候，触发了一个 bug ，其所依赖的 ujson 在序列化一些特定数据的时候，会抛出异常，这个问题，14年就已经爆出来了，不过到目前没修，2333333，同时当时的版本，如果要使用内置的函数的话，是不可以让用户选择具体的 parser 的，具体可以参考当时我提的 PR 第二，Sanic 一些东西实现的并不严谨，比如这篇文章有吐槽过日常辣鸡水文:一个关于 Sanic 的小问题的思考 第三，Sanic 现在不支持 UWSGI ，同时和 Gunicorn 配合部署的话，是自己实现了一套 Gunicorn Worker ，在我们生产环境下，会有一些诸如未知原因 504 这样的玄学 BUG，不过我们还在追查（另外有消息声称，Sanic 的 Server 部分并不严格遵守 PEP333即 WSGI 协议，= =我改天核查一下） 总结Sanic 的性能的确很棒，当时技术验证时，测试的时候，不同业务逻辑下，基本都能保证其性能在 Flask 的 1.5 倍以上。但是就目前的使用经验来说 Sanic 距离真正生产可用，还有相当长一段路要走。无论是内部的架构，还是周边的生态，亦或者是其他。大家可以没事拿来玩玩，但是如果要上生产线，请做好被坑的准备。 最后祝大家新年快乐，Live Long And Prosper!","link":"/posts/2018/02/22/why-i-dont-recommend-sanic/"},{"title":"去 async&#x2F;await 之路","text":"去 async/await 之路看到彭总写的文章这破 Python，感慨颇多，我也来灌水吧。 首先，我司算是在国内比较敢于尝试新东西的公司吧，最直接的提现就在于我们会及时跟进社区相关基础服务的迭代，并且敢于去尝试新的东西。嗯，从去年6月到现在，我司在线上推行了很长一段时间的 async/await ，并且引入新的注入 Sanic 这样全新的框架，但是不得不说，我们现在要对 async/await 暂时的说再见了。 我们为什么选用 async/await ？和我们组具体场景有关，我们组有相当一部分场景，是根据不同的 URL 去不同的子服务请求数据，组合之后，再进行下一步的统一处理。那么这个时候，传统的同步的方式在数据源越来越杂的情况下就显得很无奈。 我们当时有这样几个选择： 维护进程/线程池，利用通用的进程/线程来处理请求 利用 Gevent 这样第三方的 coroutine+EventLoop 方案 使用 async/await + asyncio 这一套 首先，1被我们排除了，原因很简单，太重了。2最开始也被我们暂时性的排除，当时我们对于 monkey-patch 这样看起来不太清真的方式心存畏惧 于是我们就很欢喜鼓舞的选择了3，利用 async/await + asyncio 这一套方案 事实上最开始的效果还是很美妙的。然而，在后面会发现这一套操作其实是在吃屎QwQ 去 async/await我们为什么放弃 async/await?其实几个老生重谈的问题 1. 代码层面的传染性Python 官方的 coroutine 实现，其实是基于 yield/yield from 这一套生成器的魔改的，那么这也意味着你需要入口处开始，往下逐渐的遵循 async/await 的方式进行使用。那么在同一个项目里，充斥着同步/异步的代码，相信我，维护起来，某种意义上来讲算是一种灾难。 2. 生态与兼容性async/await 目前的兼容性真的让人很头大，目前 async/await 的生效范围仅限于 Pure Python Code。这里有个问题，我们很多在项目中使用的诸如 mysqlclient 这样的 C Extension ，async/await 并不能覆盖。 同时，目前而言，async/await 的周边真的堪称一个非常非常大的问题，可以说处于一个 Bug 随处见，发现没人修的状态。比如 aiohttp 的对于 https 链接所存在的链接泄漏的问题。再比如 Sanic 的一团乱麻的设计结构。 我们在为生产项目调研一门新的技术的时候，我们往往会着重去考察一个新的东西，它对于现有的技术是否能覆盖我们的服务，它的周边是否能满足我们日常的需求？目前而言 async/await 周边一套并不能满足 3. 性能问题目前而言，PEP 3156 提出的 asyncio 是 async/await 官方推荐的事件循环的搭配。但是目前而言官方的实现欠缺很多，比如之前 aiohttp 针对于 https 的链接泄漏的问题，底层其实可以追溯至 asyncio 的 SSL 相关的实现。所以我们在使用的时候，往往会选用第三方的 loop 进行搭配。而目前而言第三方的 Loop 而言目前主流的实现方式均是基于 libuv/libev 进行魔改。而这样一来，其性能和 Gevent 不相上下，甚至更低（毕竟 Greenlet 避免了维护 PyFrameObject 的开销） 所以，为了我们的头发着想，目前我们将选择逐渐的将 async/await 从我们的线上代码中退役，最迟今年年底前，完成我们的去 async/await 的操作。 我们替代品是什么？目前而言，我们准备使用 Gevent 作为替代品（嗯，真香） 原因很简单： 目前发展成熟，无明显大的 Bug 周边发展成熟，对于 Pure Python Code，可以 Monkey-Patch 一把梭迁移存量代码，对于 C Extension 有豆瓣内部生产验证过的 Greenify 来作为解决方案 底层的 Greenlet 提供了对应的 API ，在必要的时候可以方便的对协程的切换做上下文的 trace。 关于 async/await 其他一些想说的东西首先而言，async/await 是个好东西，但是现在不实用。这一点其实要看社区去进一步摸索相关的使用方法。 说到这里，很多人又想问我，你对于 ASGI 和 Django Channel 这样的东西怎么看？ 首先我们要明确一点 ASGI 其实并不是为了 async/await 所设计，其最初的设计思路，是为了解决 PEP333/PEP3333 WSGI 协议在面对越来越复杂的网络协议模型力不从心的问题。而 Django Channel 也是为了解决这个问题，从而对于 ASGI 进行实现的产物（最开始是解决 Websocket？）。这一套的确解决了很多问题，比如 Django Channel 2.0 中可以很方便的实现 WebSocket Boardcast，但是他们和 async/await 其实关联并不大。 今年 PyCon 2018 上，Django 组的 Core 来介绍说，Channel 2.0 增加了对 async/await 的支持。未来 Django 也可能会增加对应的支持。但是问题在于，一旦到了使用 async/await 的时候，目前整体的生态，依旧是让人最为担心的，也是最为薄弱的点 。 所以，你好 async/await，再见 async/await！","link":"/posts/2018/10/04/why-i-dont-use-async/"},{"title":"我所热爱的开源社区","text":"今天是个不错的日子，最开始由我带进 nerdctl 社区的 @yuchanns 因为其很活跃的表现被项目的主要维护者 @AkihiroSuda 推荐成为了项目的 maintainer，参见 nerdctl#PR1540。而我也在这个项目中被提名成为 committer，参见 nerdctl#1539。加上今天的公益群有太多关于开源的讨论，所以我想写篇文章记录下我自己的经历，希望能帮助更多的人热爱开源，拥抱开源。 为什么我会参加开源我参与的第一个开源项目，应该是能追溯到16年，我还没有本科毕业的时候，当时的我参加了 稀土掘金翻译计划（slogan 里说的最好的英文技术资讯翻译项目，我觉得毫不夸张），在这个项目里我第一次接触到了 Git Workflow，也完整接触到了 GitHub 这个世界最大的同性交友社区（大雾（不过我相交至今对我帮助巨大的几位密友真的是通过这个项目结识的）。而我第一个参与的代码项目，应该可以追溯到17年3月，我给 Sanic 这个项目新增了一个 Code Example，参见 Sanic#PR558。 在往后，我就一直在不断的参与开源社区，到现在为止，我贡献过不少的开源项目，CPython，Docker/Moby，Taichi，Logseq，Kubernetes，Dubbo，TiDB，nerdctl 等等。我也在不断的学习开源社区的工作方式，我也在不断的学习开源社区的文化，我也在不断的学习开源社区的技术。（最后面这句由 GitHub Copilot 自动完成）（XD 那么回到这一章的标题，我为什么会参与开源社区？或者更功利的说，开源社区给我带来了什么样的利益？ 无他，对于我自己全方位的成长。 首先，参与开源社区对于我来讲，对于我自己是一个非常非常棒的提升的过程。你可以在这里面学到很多的东西 怎么样去有效的说服别人 怎么样去写好 UT 怎么样去打磨 code style 怎么样去帮助同为新人的其余人 更早的链接先不谈，大家可以看我 2022 年在 nerdctl 项目上的贡献 nerdctl#ZheaoLi，大家可以很明显的看到，我的 PR 从最开始到后面，无论是质量，还是风格都有不少的提升。这实际上就是开源社区所带给我的最直观的成长。我很庆幸有很棒的 Community Mentor 对我的 PR 从不放水，Review 非常严格，促使我不断的成长。 同时，让我也有机会去表达自己的想法，去发起 Proposal（比如 nerdctl#Issue1387），去学会做一个 Owner，去帮助更多的新人参与进来。 某种意义上，这是日常的工作所给予不了我的特殊的体验，开源社区相对较少的利益纠葛，会让互利互惠的行为变得更纯粹，更加的自然。也会让人收益更大。这里引用 @yuchanns 今晚的一段发言 我想大家刚学编程的时候都会有这种困境：学完不知道干啥、感觉好像没学，所以就想寻找各种实战教程来加深体会。这种现象会在实际从事工作后迅速消除，因为有了实际应用场景。但是当你对一些其他领域的东西产生兴趣，又会有这种困惑；而这是工作中不太有机会接触到的东西。除非你换了个工作、不然没法再通过工作经验来摆脱困境。这时候参与到一个开放式的社区就很好了。其他人的工作中产生的需求给你提供了实战机（（你不需要自己一一涉足到具体的工作中，只要解决他们延伸出来的需要，就可以有机会运用学到的东西（（ 当然，从功利的角度来说，积极的参与开源社区，你能认识很多有意思的人，让你职业生涯更为顺利也是能给你带来的好处就是了（ 那么怎么样去参与开源社区参与开源社区无外乎有两种途径， 自己创立一个项目的开源社区 加入一个已经存在的开源社区 我主要会讨论下后者 很多人会给出开源三问 “我想参与开源社区，但是我不知道怎么做”，“我想参与开源社区，但是我不知道怎么找到一个项目”，“我想参与开源社区，但是我太菜了怎么办啊” 实际上这些问题解决起来都是没有你想象的那么困难，可能只是需要一点行动能力加一点好奇心。 实际上发展到现在，开源社区已经极其的庞大了，无论你的技术栈是什么，你都能找到合适的项目去参与。而且，开源社区的参与门槛也越来越低了，你不需要去了解整个项目的代码，你只需要去了解项目的 Issue，然后去解决这些 Issue，就可以参与到开源社区中来了。那么 How to find a project to contribute to ？ 我自己的途径有两个 通过 GitHub 的 Explore 页面，找一些新的项目，看这个项目是否戳中了我的痛点 社交媒体上大家的宣传 nerdctl 这个项目实际上的来源就是当时好友 @Junnplus 在推上的推广 然后我去看了下这个项目的定位，发现这个项目实际上戳中了我的痛点，于是我就开始在自己的环境中使用这个项目。 实际上去找到你感兴趣的项目实际上不是一件难事，可能只是需要一点点好奇心 那么，我找到一个项目后，我应该怎么样去参与进去？ 实际上这里就需要一点行动力了，我自己大概方法是这样 扫 Issue 区，以及订阅项目，一个项目的 Issue 能让我一定程度上的去了解这个项目的发展方向 我会不断的去使用这个项目，将我在使用中的问题转化成 Issue，进而转化成 PR 我会用我已有的知识进行迁移，尝试是否有可能发现新的潜在的问题 以 nerdctl 为例，Issue 区时不时的会有 Good First Issue 的出现，这个时候你可以主动的去认领对应的 Issue 进行贡献（从我的视角来看，项目的维护者对于 Good First Issue 的上心程度将会决定了一个项目的长远发展），@yuchanns 第一个 PR nerdctl#PR1331 实际上就来源于我提的一个 Good First Issue nerdctl#1330。当然对于一个已经有一定规模的项目来说，坐着等 Good First Issue 可能需要点运气，那么怎么办，答案就是第二，第三点 我在 nerdctl 第一个贡献的 PR nerdctl#PR790 来自于我提出的 Issue nerdctl#Issue775 ，这个 Issue 是我在使用过程中发现的 Bug，简而言之就是在私有镜像仓库下鉴权的一些问题。然后将 Issue 转化成对应的 PR 了。我在这个项目中其余的一些贡献也是修我自己遇到的一些问题 另外一个方法是，我会用我已有的知识去进行迁移，尝试是否能发现有潜在的问题。我在 Affine(一个非常棒的笔记项目)提的 PR Affine#PR403 是我在本地构建 Affine 的时候，顺手读了一下他们的 Dockerfile（我是 SRE，对这个比较敏感（不然前端项目我去读 Dockerfile 干嘛），发现他们没有高效的利用缓存，然后我就提了 PR，进行了构建加速。这是实际上就是跨领域的去看一个项目能给你带来不一样的视角，进而促进你对项目的贡献。 那么，开源三问最后一问，”我想参与开源社区，但是我太菜了怎么办啊“ 首先要说一点，开源社区的精髓就在于边做边学边成长，比如 @yuchanns 在写 nerdctl#PR1407 的时候（这个 PR 主要是给容器新增一个可以绑定 MacAddress 的选项），他当时对于 CNI 这块也不是很熟悉，然后边做边学，我和他也在群里讨论过几次方案。最终 PR 合并的非常顺利。这某种意义上也是开源社区的一种乐趣与魅力。 那如果你说你现在就是背景知识不够，你想等再学学再写代码，那还能贡献吗？可以啊，用 @tison 的经典言论”一个社区的活绝对是很多样的“。你看，我给 bytebase 提 Bug 的时候，发现他们的 Ticket 模板太难用了，然后我交了Bytebase#PR3050 重构了他们的 Issue Template，后面他们基于我的基础上又完善了一波。所以，无论是 Issue，文档完善，帮助完善用例等，都是很棒的参与开源社区的方式。 当然可能新进来的同学还有个顾虑就是如果被拒绝了怎么办？那其实很常见，你看我拍脑袋给 lima 提的 lima#Issue1087 被拒的很惨。但是被拒绝也是一种学习，能让我自己从这个讨论的过程里去回顾到我思考不完善的地方。 所以看到这，你会发现，参与开源社区，真的没有那么难。需要的真的只是一点点行动力，以及一点点的好奇心而已 总结从互联网诞生之初到现在，开源这一极具理想主义气质的行为事实上的改变了这个世界。世界各地的人都在开源的旗帜下，自由的挥发着自己的创意，尽情的一点点的改变着这个世界。有些时候想到我会有机会去参与到这样一个伟大的活动中，我会不由自主的颤栗。我很庆幸在我最初的职业生涯里就加入到了这个伟大的事业，我也希望我身边会有越来越多的人参与进来，一起挥洒着汗水，一起在这个操蛋但是又美好的世界里，找到自己心灵的应许之地。 Long Live the Open Source！","link":"/posts/2022/11/22/why-i-love-the-open-source-community/"},{"title":"为什么有些时候 Python 中乘法比位运算更快","text":"我本来以为我不再会写水文了，但是突然发现自己现在也只能勉强写写水文才能维持生活这样子。那就继续写水文吧 某天，一个技术群里老哥提出了这样一个问题，为什么在一些情况下，Python 中的简单乘/除法比位运算要慢 首先秉持着实事求是的精神，我们先来验证一下 1234567891011In [33]: %timeit 1073741825*2 7.47 ns ± 0.0843 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)In [34]: %timeit 1073741825&lt;&lt;1 7.43 ns ± 0.0451 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)In [35]: %timeit 1073741823&lt;&lt;1 7.48 ns ± 0.0621 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)In [37]: %timeit 1073741823*2 7.47 ns ± 0.0564 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each) 我们发现几个很有趣的现象 在值 x&lt;=2^30 时，乘法比直接位运算要快 在值 x&gt;2^32 时，乘法显著慢于位运算 这个现象很有趣，那么这个现象的 root cause 是什么？实际上这和 Python 底层的实现有关 简单聊聊PyLongObject 的实现在 Python 2.x 时期，Python 中将整型分为两类，一类是 long, 一类是 int 。在 Python3 中这两者进行了合并。目前在 Python3 中这两者做了合并，仅剩一个 long 首先来看看 long 这样一个数据结构底层的实现 1234struct _longobject { PyObject_VAR_HEAD digit ob_digit[1];}; 在这里不用关心，PyObject_VAR_HEAD 的含义，我们只需要关心 ob_digit 即可。 在这里，ob_digit 是使用了 C99 中的“柔性数组”来实现任意长度的整数的存储。这里我们可以看一下官方代码中的文档 Long integer representation.The absolute value of a number is equal to SUM(for i=0 through abs(ob_size)-1) ob_digit[i] * 2*(SHIFTi)Negative numbers are represented with ob_size &lt; 0; zero is represented by ob_size == 0.In a normalized number, ob_digit[abs(ob_size)-1] (the most significant digit) is never zero. Also, in all cases, for all valid i,0 &lt;= ob_digit[i] &lt;= MASK.The allocation function takes care of allocating extra memory so that ob_digit[0] … ob_digit[abs(ob_size)-1] are actually available.CAUTION: Generic code manipulating subtypes of PyVarObject has to aware that ints abuse ob_size’s sign bit. 简而言之，Python 是将一个十进制数转为 2^(SHIFT) 进制数来进行存储。这里可能不太好了理解。我来举个例子，在我的电脑上，SHIFT 为 30 ，假设现在有整数 1152921506754330628 ，那么将起转为 2^30 进制表示则为: 4*(2^30)^0+2*(2^30)^1+1*(2^30)^2 。那么此时 ob_digit 是一个含有三个元素的数组，其值为 [4,2,1] OK，在明白了这样一些基础知识后，我们回过头去看看 Python 中的乘法运算 Python 中的乘法运算Python 中的乘法运算，分为两部分，其中关于大数的乘法，Python 使用了 Karatsuba 算法1，具体实现如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167static PyLongObject *k_mul(PyLongObject *a, PyLongObject *b){ Py_ssize_t asize = Py_ABS(Py_SIZE(a)); Py_ssize_t bsize = Py_ABS(Py_SIZE(b)); PyLongObject *ah = NULL; PyLongObject *al = NULL; PyLongObject *bh = NULL; PyLongObject *bl = NULL; PyLongObject *ret = NULL; PyLongObject *t1, *t2, *t3; Py_ssize_t shift; /* the number of digits we split off */ Py_ssize_t i; /* (ah*X+al)(bh*X+bl) = ah*bh*X*X + (ah*bl + al*bh)*X + al*bl * Let k = (ah+al)*(bh+bl) = ah*bl + al*bh + ah*bh + al*bl * Then the original product is * ah*bh*X*X + (k - ah*bh - al*bl)*X + al*bl * By picking X to be a power of 2, &quot;*X&quot; is just shifting, and it's * been reduced to 3 multiplies on numbers half the size. */ /* We want to split based on the larger number; fiddle so that b * is largest. */ if (asize &gt; bsize) { t1 = a; a = b; b = t1; i = asize; asize = bsize; bsize = i; } /* Use gradeschool math when either number is too small. */ i = a == b ? KARATSUBA_SQUARE_CUTOFF : KARATSUBA_CUTOFF; if (asize &lt;= i) { if (asize == 0) return (PyLongObject *)PyLong_FromLong(0); else return x_mul(a, b); } /* If a is small compared to b, splitting on b gives a degenerate * case with ah==0, and Karatsuba may be (even much) less efficient * than &quot;grade school&quot; then. However, we can still win, by viewing * b as a string of &quot;big digits&quot;, each of width a-&gt;ob_size. That * leads to a sequence of balanced calls to k_mul. */ if (2 * asize &lt;= bsize) return k_lopsided_mul(a, b); /* Split a &amp; b into hi &amp; lo pieces. */ shift = bsize &gt;&gt; 1; if (kmul_split(a, shift, &amp;ah, &amp;al) &lt; 0) goto fail; assert(Py_SIZE(ah) &gt; 0); /* the split isn't degenerate */ if (a == b) { bh = ah; bl = al; Py_INCREF(bh); Py_INCREF(bl); } else if (kmul_split(b, shift, &amp;bh, &amp;bl) &lt; 0) goto fail; /* The plan: * 1. Allocate result space (asize + bsize digits: that's always * enough). * 2. Compute ah*bh, and copy into result at 2*shift. * 3. Compute al*bl, and copy into result at 0. Note that this * can't overlap with #2. * 4. Subtract al*bl from the result, starting at shift. This may * underflow (borrow out of the high digit), but we don't care: * we're effectively doing unsigned arithmetic mod * BASE**(sizea + sizeb), and so long as the *final* result fits, * borrows and carries out of the high digit can be ignored. * 5. Subtract ah*bh from the result, starting at shift. * 6. Compute (ah+al)*(bh+bl), and add it into the result starting * at shift. */ /* 1. Allocate result space. */ ret = _PyLong_New(asize + bsize); if (ret == NULL) goto fail;#ifdef Py_DEBUG /* Fill with trash, to catch reference to uninitialized digits. */ memset(ret-&gt;ob_digit, 0xDF, Py_SIZE(ret) * sizeof(digit));#endif /* 2. t1 &lt;- ah*bh, and copy into high digits of result. */ if ((t1 = k_mul(ah, bh)) == NULL) goto fail; assert(Py_SIZE(t1) &gt;= 0); assert(2*shift + Py_SIZE(t1) &lt;= Py_SIZE(ret)); memcpy(ret-&gt;ob_digit + 2*shift, t1-&gt;ob_digit, Py_SIZE(t1) * sizeof(digit)); /* Zero-out the digits higher than the ah*bh copy. */ i = Py_SIZE(ret) - 2*shift - Py_SIZE(t1); if (i) memset(ret-&gt;ob_digit + 2*shift + Py_SIZE(t1), 0, i * sizeof(digit)); /* 3. t2 &lt;- al*bl, and copy into the low digits. */ if ((t2 = k_mul(al, bl)) == NULL) { Py_DECREF(t1); goto fail; } assert(Py_SIZE(t2) &gt;= 0); assert(Py_SIZE(t2) &lt;= 2*shift); /* no overlap with high digits */ memcpy(ret-&gt;ob_digit, t2-&gt;ob_digit, Py_SIZE(t2) * sizeof(digit)); /* Zero out remaining digits. */ i = 2*shift - Py_SIZE(t2); /* number of uninitialized digits */ if (i) memset(ret-&gt;ob_digit + Py_SIZE(t2), 0, i * sizeof(digit)); /* 4 &amp; 5. Subtract ah*bh (t1) and al*bl (t2). We do al*bl first * because it's fresher in cache. */ i = Py_SIZE(ret) - shift; /* # digits after shift */ (void)v_isub(ret-&gt;ob_digit + shift, i, t2-&gt;ob_digit, Py_SIZE(t2)); Py_DECREF(t2); (void)v_isub(ret-&gt;ob_digit + shift, i, t1-&gt;ob_digit, Py_SIZE(t1)); Py_DECREF(t1); /* 6. t3 &lt;- (ah+al)(bh+bl), and add into result. */ if ((t1 = x_add(ah, al)) == NULL) goto fail; Py_DECREF(ah); Py_DECREF(al); ah = al = NULL; if (a == b) { t2 = t1; Py_INCREF(t2); } else if ((t2 = x_add(bh, bl)) == NULL) { Py_DECREF(t1); goto fail; } Py_DECREF(bh); Py_DECREF(bl); bh = bl = NULL; t3 = k_mul(t1, t2); Py_DECREF(t1); Py_DECREF(t2); if (t3 == NULL) goto fail; assert(Py_SIZE(t3) &gt;= 0); /* Add t3. It's not obvious why we can't run out of room here. * See the (*) comment after this function. */ (void)v_iadd(ret-&gt;ob_digit + shift, i, t3-&gt;ob_digit, Py_SIZE(t3)); Py_DECREF(t3); return long_normalize(ret); fail: Py_XDECREF(ret); Py_XDECREF(ah); Py_XDECREF(al); Py_XDECREF(bh); Py_XDECREF(bl); return NULL;} 这里不对 Karatsuba 算法1 的实现做单独解释，有兴趣的朋友可以参考文末的 reference 去了解具体的详情。 在普通情况下，普通乘法的时间复杂度位 n^2 (n 为位数），而 K 算法的时间复杂度为 3n^(log3) ≈ 3n^1.585 ，看起来 K 算法的性能要优于普通乘法，那么为什么 Python 不全部使用 K 算法呢？ 很简单，K 算法的优势实际上要在当 n 足够大的时候，才会对普通乘法形成优势。同时考虑到内存访问等因素，当 n 不够大时，实际上采用 K 算法的性能将差于直接进行乘法。 所以我们来看看 Python 中乘法的实现 12345678910111213141516171819202122static PyObject *long_mul(PyLongObject *a, PyLongObject *b){ PyLongObject *z; CHECK_BINOP(a, b); /* fast path for single-digit multiplication */ if (Py_ABS(Py_SIZE(a)) &lt;= 1 &amp;&amp; Py_ABS(Py_SIZE(b)) &lt;= 1) { stwodigits v = (stwodigits)(MEDIUM_VALUE(a)) * MEDIUM_VALUE(b); return PyLong_FromLongLong((long long)v); } z = k_mul(a, b); /* Negate if exactly one of the inputs is negative. */ if (((Py_SIZE(a) ^ Py_SIZE(b)) &lt; 0) &amp;&amp; z) { _PyLong_Negate(&amp;z); if (z == NULL) return NULL; } return (PyObject *)z;} 在这里我们看到，当两个数皆小于 2^30-1 时，Python 将直接使用普通乘法并返回，否则将使用 K 算法进行计算 这个时候，我们来看一下位运算的实现，以右移为例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960static PyObject *long_rshift(PyObject *a, PyObject *b){ Py_ssize_t wordshift; digit remshift; CHECK_BINOP(a, b); if (Py_SIZE(b) &lt; 0) { PyErr_SetString(PyExc_ValueError, &quot;negative shift count&quot;); return NULL; } if (Py_SIZE(a) == 0) { return PyLong_FromLong(0); } if (divmod_shift(b, &amp;wordshift, &amp;remshift) &lt; 0) return NULL; return long_rshift1((PyLongObject *)a, wordshift, remshift);}static PyObject *long_rshift1(PyLongObject *a, Py_ssize_t wordshift, digit remshift){ PyLongObject *z = NULL; Py_ssize_t newsize, hishift, i, j; digit lomask, himask; if (Py_SIZE(a) &lt; 0) { /* Right shifting negative numbers is harder */ PyLongObject *a1, *a2; a1 = (PyLongObject *) long_invert(a); if (a1 == NULL) return NULL; a2 = (PyLongObject *) long_rshift1(a1, wordshift, remshift); Py_DECREF(a1); if (a2 == NULL) return NULL; z = (PyLongObject *) long_invert(a2); Py_DECREF(a2); } else { newsize = Py_SIZE(a) - wordshift; if (newsize &lt;= 0) return PyLong_FromLong(0); hishift = PyLong_SHIFT - remshift; lomask = ((digit)1 &lt;&lt; hishift) - 1; himask = PyLong_MASK ^ lomask; z = _PyLong_New(newsize); if (z == NULL) return NULL; for (i = 0, j = wordshift; i &lt; newsize; i++, j++) { z-&gt;ob_digit[i] = (a-&gt;ob_digit[j] &gt;&gt; remshift) &amp; lomask; if (i+1 &lt; newsize) z-&gt;ob_digit[i] |= (a-&gt;ob_digit[j+1] &lt;&lt; hishift) &amp; himask; } z = maybe_small_long(long_normalize(z)); } return (PyObject *)z;} 在这里我们能看到，在两侧都是小数的情况下，位移动算法将比普通乘法，存在更多的内存分配等操作。这样也会回答了我们文初所提到的一个问题，“为什么一些时候乘法比位运算更快”。 总结本文差不多就到这里了，实际上通过这次分析我们能得到一些很有趣但是也很冷门的知识。实际上我们目前看到这样一个结果，是 Python 对于我们常见且高频的操作所做的一个特定的设计。而这也提醒我们，Python 实际上对于很多操作都存在自己内建的设计哲学，在日常使用的时候，其余语言的经验，可能无法复用 差不多就这样吧，只能勉强写水文苟活了（逃 Reference [1]. Karatsuba 算法","link":"/posts/2020/11/06/why-mul-faster-than-bit-shift-sometimes-in-python/"},{"title":"关于PostCSS的一点小科普","text":"原文链接 : PostCSS – What It Is And What It Can Do 原文作者 : Jake Bresnehan 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: aidistan, JolsonZhu PostCSS起源于2013年9月，发展到现在，已经有很多开发者在工作中使用它。如果你尚未接触过PostCSS，这篇文章正适合你。 PostCSS是一个使用JavaScript插件来转换CSS的工具。 PostCSS本身很小，其只包含CSS解析器，操作CSS节点树的API，资源生成器（译者注1：原文是source map），以及一个节点树字符串化工具。所有的黑魔法都是通过利用插件实现的。 截止目前，PostCSS的生态圈内已经拥有超过100种插件。这些插件可以做太多的事情，比如lint（译者注2：一种用来检测CSS代码的工具），添加vendor prefixes（译者注3：添加浏览器内核前缀，可以使用浏览器的一些独有特性），允许使用最新的CSS特性，在你的CSS里提供统计数据，或者是允许你使用Sass，Less或是Stylus等CSS预处理器。 让我们看看以下十种插件Autoprefixer 根据用户的使用场景来解析CSS和添加vendor prefixes（前文注2）。 PostCSS Focus 一种利用键盘操作为每个**:hover添加:focus选择器的PostCSS**插件。 PreCSS 一个允许你在代码中使用类似Sass标记的插件。 Stylelint 一种强大的，先进的可以使你在CSS样式中保持一致性，避免错误的CSS linter工具。 PostCSS CSS Variables 一种将用户自定义CSS变量（CSS variables）转化为静态样式的插件。 PostCSS Flexbugs Fixes 一种用于修复flexbug的bug的插件。 PostCSS CSSnext 一种可以让你使用CSS最新特性的插件。它通过将最新的CSS特性转变为现阶段浏览器所兼容的特性，这样你不用再等待浏览器对某一特定新特性的支持。 PostCSS CSS Stats 一种支持cssstats的插件。这个插件将会返回一个cssstatus对象，这样你可以使用它来进行CSS分析。 PostCSS SVGO 优化在PostCSS中内联SVG。 PostCSS Style Guide 一种可以自动生成风格指导的插件。将会在Markdown中生成CSS注释，并在生成的HTML文档中显示。 如果你想编写自己的插件，并希望将其贡献给社区的话，请确保你是先看过guidelines这篇文档还有PostCSS Plugin Boilerplate这篇官方文档。 在你的工作中使用PostCSSPostCSS是用JavaScript所编写的，这使得我们在Grunt，Gulp或Webpack等常用的前端构建工具中使用它变得非常方便。 下面是我们使用Autoprefixer插件的示例。 npm install autoprefixer --save-dev Gulp如果你使用Gulp，那么你需要安装gulp-postcss。 npm install --save-dev gulp-postcss gulp.task('autoprefixer', function () { var postcss = require('gulp-postcss'); var autoprefixer = require('autoprefixer'); return gulp.src('./src/*.css') .pipe(postcss([ autoprefixer({ browsers: ['last 2 versions'] }) ])) .pipe(gulp.dest('./dest')); }); Grunt如果你使用Grunt，那么你需要安装grunt-postcss。 npm install grunt-postcss --save-dev module.exports = function(grunt) { grunt.loadNpmTasks('grunt-postcss'); grunt.initConfig({ postcss: { options: { map: true, processors: [ require('autoprefixer')({ browsers: ['last 2 versions'] }) ] }, dist: { src: 'css/*.css' } } }); grunt.registerTask('default', ['postcss:dist']); }; Webpack如果你使用Webpack，那么你需要安装postcss-loader。 npm install postcss-loader --save-dev var autoprefixer = require('autoprefixer'); module.exports = { module: { loaders: [ { test: /\\.css$/, loader: &quot;style-loader!css-loader!postcss-loader&quot; } ] }, postcss: function () { return [autoprefixer]; } } 关于怎么整合PostCSS，你可以从这里PostCSS repo获取到帮助。 最后最后的诚心安利~在有些时候，在新技术，新工具，新框架发布的时候，去使用并观察其发展趋势无疑是一种明智的行为。现在，PostCSS已经发展到一个相当成熟的阶段，我强烈建议你在你的工作中使用它。因为它现在已经在工程中被广泛的使用，同时在未来一段时间内它不会发生太大的变化。","link":"/posts/2016/07/22/%E5%85%B3%E4%BA%8EPostCSS%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E7%A7%91%E6%99%AE/"},{"title":"在Swift中实现撤销功能","text":"原文链接 : Undo History in Swift 原文作者 : chriseidhof 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: xcc3641, Jaeger 在过去的一段时间里，有很多的Blog推出了关于他们想在Swift中所添加的动态特性的文章。事实上Swift 已经成为了一门具有相当多动态特性的语言：它拥有泛型，协议， 头等函数（译者注1：first-class function指函数可以向类一样作为参数传递），和包含很多可以的动态操作的函数的标准库，比如map和filter等（这意味着我们可以利用更安全更灵活的函数来代替 KVC 来使用 字符串）（译者注2：KVC指Key-Value-Coding一个非正式的 Protocol，提供一种机制来间接访问对象的属性）。对于大多数人而言，特别希望介绍反射这一特性，这意味着他们可以在程序运行时进行观察和修改。 在Swift中，反射机制受到很多的限制，但是你仍然你可以在代码运行的时候动态的生成和插入一些东西。 比如这里是怎样为NSCoding或者是JSON动态生成字典的实例。 今天在这里，我们将一起看一下在Swift中怎样去实现撤销功能。 其中一种方法是通过利用Objective-C中基于的反射机制所提供的NSUndoManager。通过利用struct，我们可以利用不同的方式在我们的APP中实现撤销这一功能。 在教程开始之前，请务必确保你自己已经理解了Swift中struct的工作机制(最重要的是理解他们都是独立的拷贝)。首先要声明的一点是，这篇文章并不是想告诉大家我们不需要对runtime进行操作，或者我们提供的是一种NSUndoManager的替代品。这篇文章只是告诉了大家一种不同的思考方式而已。 我们首先创建一个叫做UndoHistory的struct。 通常而言，创建UndoHistory时会伴随一个警告，提示只有当A是一个struct的时才会生效。为了保存所有状态信息，我们需要将其存放入一个数组之中。当我们修改了什么时，我们只需要将其push进数组中，当我们希望进行撤回时，我们将其从数组中pop出去。我们通常希望有一个初试状态，所以我们需要建立一个初始化方法： 1234567struct UndoHistory&lt;A&gt; { private let initialValue: A private var history: [A] = [] init(initialValue: A) { self.initialValue = initialValue }} 举个例子，如果我们想在一个tableViewController中通过数组的方式提供撤销操作，我们可以创建这样一个struct： 1var history = UndoHistory(initialValue: [1, 2, 3]) 对于不同情境下的撤销操作，我们可以创建不同的struct来实现: 1234struct Person { var name: String var age: Int} 1var personHistory = UndoHistory(initialValue: Person(name: &quot;Chris&quot;, age: 31)) 当然，我们希望获得当前的状态，同时设置当前状态。(换句话说：我们希望实时地操作我们的历史记录）。我们可以从history数组中的最后一项值来获取我们的状态，同时如果数组为空的话，我们便返回我们的初始值。 我们可以通过将当前状态添加至history数组来改变我们的操作状态。 12345678910extension UndoHistory { var currentItem: A { get { return history.last ?? initialValue } set { history.append(newValue) } }} 比如，如果我们想修改个人年龄（译者注3：指前面作者编写的Person结构体中的age属性）， 我们可以通过重新计算属性来很轻松的做到这一点： 12personHistory.currentItem.age += 1personHistory.currentItem.age // Prints 32 当然，undo 方法的编写并未完成。对于从数组中移出最后一个元素来讲是非常简单的。 根据你自己的声明，你可以在数组为空的时候抛出一个异常，不过，我没有选择这样一种做法。 123456extension UndoHistory { mutating func undo() { guard !history.isEmpty else { return } history.removeLast() }} 很简单的使用它（译者注4：这里指作者前面所编写的undo相关代码） 123456789101112131415161718192021222324252627282930 personHistory.undo() personHistory.currentItem.age // Prints 31 again~~~~当然，我们到现在的**UndoHistory**操作只是基于一个很简单的**Person**类。比如，如果我们想利用**Array**来实现一个**tableviewcontroller**的**undo**操作，我们可以利用**属性**来获取从数组中得到的元素：~~~ Swift final class MyTableViewController&lt;item&gt;: UITableViewController { var data: UndoHistory&lt;[item]&gt; init(value: [Item]) { data = UndoHistory(initialValue: value) super.init(style: .Plain) } override func tableView(tableView: UITableView, numberOfRowsInSection section: Int) -&gt; Int { return data.currentItem.count } override func tableView(tableView: UITableView, cellForRowAtIndexPath indexPath: NSIndexPath) -&gt; UITableViewCell { let cell = tableView.dequeueReusableCellWithIdentifier(&quot;Identifier&quot;, forIndexPath: indexPath) let item = data.currentItem[indexPath.row] // configure `cell` with `item` return cell } override func tableView(tableView: UITableView, commitEditingStyle editingStyle: UITableViewCellEditingStyle, forRowAtIndexPath indexPath: NSIndexPath) { guard editingStyle == .Delete else { return } data.currentItem.removeAtIndex(indexPath.row) } } 在struct中另一个非常爽的特性是：我们可以自由的使用监听者模式。 比如,我们可以修改data的值： 12345var data: UndoHistory&lt;[item]&gt; { didSet { tableView.reloadData() }} 我们即使是修改数组内很深的值（比如：data.currentItem[17].name = “John”**），我们通过didSet也能很方便地定位到修改的地方。当然,我们可能希望做一些例如reloadData**这样方便的事情。比如， 我们可以利用Changeset 库来计算变化，然后来根据插入/删除/移动/等不同的操作来添加动画。 很明显的是, 这种方法有着它自身的缺点。例如，它保存了整个状态的历史操作，不是每次状态变化之间的不同点。 这种方法只使用了struct来实现undo操作 （更为准确的讲：是只使用了struct中值的一些特性）。这意味着，你并不需要去阅读 runtime编程指导这本书， 你只需要对struct和generics（译者注5：generics指泛型）有足够的了解。 为data.currentItem提供了一个可计算的属性 items 来进行获取和设置操作，是一个不错的想法。这使得data-source和delegate等方法的实现变得更为容易。 如果你想更进一步优化，这里有一些非常有意思的想法：添加恢复功能，或者是编辑功能。你可以在tableView中去实现, 如果你真的很天真的按照这个去做了，那么你会发现在你的undo历史中会存在重复记录。","link":"/posts/2016/07/22/%E5%9C%A8Swift%E4%B8%AD%E5%AE%9E%E7%8E%B0%E6%92%A4%E9%94%80%E5%8A%9F%E8%83%BD/"},{"title":"好与坏，Swift面面观 Part2","text":"原文链接 : Good Swift, Bad Swift — Part 2 原文作者 : Kristian Andersen 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: owenlyn, yifili09 不久之前，在我写的好与坏，Swift面面观 Part1一文中，我介绍了一些关于在 Swift 里怎样去写出优秀代码的小技巧。在 Swift 发布到现在的两年里，我花费了很长时间去牢牢掌握最佳的实践方法。欲知详情，请看这篇文章：好与坏，Swift面面观 Part1. 在这个系列的文章中，我将尝试提炼出我认为的 Swift 语言中好与不好的部分。唔，我也希望在未来有优秀的 Swift 来帮助我征服 Swift （唔，小伙子，别看了，中央已经决定是你了，快念两句诗吧）。如果你有什么想法，或者想告诉我一点作为开发者的人生经验什么的话，请在 Twitter 上联系我，我的账号是 ksmandersen。 好了废话不多说，让我们开始今天的课程吧。 guard 大法好，入 guard 保平安在 Swift 2.0 中， Swift 新增了一组让开发者有点陌生的的特性。Guard 语句在进行防御性编程的时候将会起到不小的作用。（译者注1：防御性编程（Defensive programming）是防御式设计的一种具体体现，它是为了保证，对程序的不可预见的使用，不会造成程序功能上的损坏。它可以被看作是为了减少或消除墨菲定律效力的想法。防御式编程主要用于可能被滥用，恶作剧或无意地造成灾难性影响的程序上。来源自wiki百科）。每个 Objective-C 开发者可能对防御性编程都不陌生。通过使用这种技术，你可以预先确定你的代码在处理不可预期的输入数据时，不会发生异常。 Guard 语句允许你为接下来的代码设定一些条件和规则，当然你也必须钦定当这些条件（或规则）不被满足时要怎么处理。另外，guard 语句必须要返回一个值。在早期的 Swift 编程中，你可能会使用 if-else 语句来对这些情况进行预先处理。但是如果你使用 guard 语句的话，编译器会在你没有考虑到某些情况下时帮你对异常数据进行处理。 接下来的例子有点长，但是这是一个非常好的关于 guard 作用的实例。 didPressLogIn 函数在屏幕上的 button 被点击时被调用。我们期望这个函数被调用时，如果程序产生了额外的请求时，不会产生额外的日志。因此，我们需要提前对代码进行一些处理。然后我们需要对日志进行验证。如果这个日志不是我们所需要的，那么我们不在需要发送这段日志。但是更为重要的是，我们需要返回一段可执行语句来确保我们不会发送这段日志。guard 将会在我们忘记返回的时候抛出异常。 1234567891011121314@objc func didPressLogIn(sender: AnyObject?) { guard !isPerformingLogIn else { return } isPerformingLogIn = true let email = contentView.formView.emailField.text let password = contentView.formView.passwordField.text guard validateAndShowError(email, password: password) else { isPerformingLogIn = false return } sendLogInRequest(ail, password: password)} 当 let 和 guard 配合使用的时候将会有奇效。下面这个例子中，我们将把请求的结果绑定到一个变量 user ，之后通过 finishSignUp 方法函数使用(这个变量)。如果 result.okValue 为空，那么 guard 将会产生作用，如果不为空的话，那么这个值将对 user 进行赋值。我们通过利用 where 来对 guard 进行限制。 123456789currentRequest?.getValue { [weak self] result in guard let user = result.okValue where result.errorValue == nil else { self?.showRequestError(result.errorValue) self?.isPerformingSignUp = false return } self?.finishSignUp(user)} 讲道理 guard 非常的强大。唔，如果你还没有使用的话，那么你真应该慎重考虑下了。 在使用 subviews 的时候，将声明和配置同时进行。如前面一系列文章中所提到的，开发 viwe 的时候，我比较习惯于用代码生成。因为对 view 的配置套路很熟悉，所以在出现布局问题或者配置不当等问题时，我总是能很快的定位出错的地方。 在开发过程中，我发现将不同的配置过程放在一起非常的重要。在我早期的 Swift 编程经历中，我通常会声明一个 configureView 函数，然后在初始化时将配置过程放在这里。但是在 Swift 中我们可以利用 属性声明代码块 来配置 view （其实我也不知道这玩意儿怎么称呼啦（逃）。 唔，下面这个例子里，有一个包含两个 subviews 、 bestTitleLabel 、 和 otherTitleLabel 的 AwesomeView 视图。两个 subviews 都在一个地方进行配置。我们将配置过程都整合在 configureView 方法中。因此，如果我想去改变一个 label 的 textColor 属性，我很清楚的知道到哪里去进行修改。 1234567891011121314151617181920cclass AwesomeView: GenericView { let bestTitleLabel = UILabel().then { $0.textAlignment = .Center $0.textColor = .purpleColor()tww } let otherTitleLabel = UILabel().then { $0.textAlignment = . $0.textColor = .greenColor() } override func configureView() { super.configureView() addSubview(bestTitleLabel) addSubview(otherTitleLabel) // Configure constraints }} 对于上面的代码，我很不喜欢的就是在声明 label 时所带的类型标签，然后在代码块里进行初始化并返回值。通过使用 Then这个库 ，我们可以进行一点微小的改进。你可以利用这个小函数去在你的项目里将代码块与对象的声明进行关联。这样可以减少重复声明。 1234567891011121314151617181920class AwesomeView: GenericView { let bestTitleLabel = UILabel().then { $0.textAlignment = .Center $0.textColor = .purpleColor()tww } let otherTitleLabel = UILabel().then { $0.textAlignment = . $0.textColor = .greenColor() } override func configureView() { super.configureView() addSubview(bestTitleLabel) addSubview(otherTitleLabel) // Configure constraints }} 通过不同访问级别来对类成员进行分类。唔，对我来讲，最近发生的一件比较重要的事儿就是，我利用一种比较特殊的方法来将类和结构体的成员结合在一起。这是我之前在利用 Objective-C 进行开发的时候养成的习惯。我通常将私有方法放置在最下面，然后公共及初始化方法放在中间。然后将属性按照公共属性到私有属性的顺序放置在代码上层。唔，你可以按照下面的结构在组织你的代码。 公共属性 内联属性 私有属性 初始化容器 公共方法 内联方法 私有方法 你也可以按照静态/类属性/固定值的方式进行排序。可能不同的人会在此基础上补充一些不同的东西。不过对于我来讲，我无时不刻都在按照上面的方法进行编程。 好了，本期节目就到此结束。如果你有什么好的想法，或者什么想说的话，欢迎通过屏幕下方的联系方式联系我。当然欢迎通过这样的方式丢硬币丢香蕉打赏并订阅我的文章（大雾）。 下期预告：将继续讲诉 Swift 里的点点滴滴，不要走开，下期更精彩 。","link":"/posts/2016/07/22/%E5%A5%BD%E4%B8%8E%E5%9D%8F%EF%BC%8CSwift%E9%9D%A2%E9%9D%A2%E8%A7%82-Part2/"},{"title":"如何检测iPhone处于低电量模式","text":"原文链接 : Detecting low power mode 原文作者 : useyourloaf 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者 : LoneyIsError, wild-flame 这个星期，我阅读了一篇关于Uber怎样检测手机处于省电模式的文章。（注：文章连接是Uber found people more likely to pay） 在人们手机快要关机时，使用Uber可能会面临更高的价格。 这家公司（注：指Uber）宣称他们不会利用手机是否处于节能模式这一数据来进行定价， 但是这里我想知道 我们怎么知道用户的iPhone处于低电量模式 低电量模式在iOS 9中，苹果为iPhone手机新添加了 低电量模式 功能。在你能充电之前，低电量模式通过关闭诸如邮件收发，Siri，后台消息推送能耗电功能来延长你的电池使用时间。 在这里面，很重要的一点是，是否进入低电量模式是由用户自行决定的。 你需要进入电池设置中去开启低电量模式。当你进入低电量模式的时候，状态栏上的电池图标会变成黄色。 当你充电至80%以上时，系统会自动关闭低电量模式。 低电量模式检测事实证明，在iOS 9中获取低电量模式信息是很容易的一件事。 你可以通过NSProcessInfo这个类来判断用户是否进入了低电量模式： 1234if NSProcessInfo.processInfo().lowPowerModeEnabled { // stop battery intensive actions} 如果你想用Objective-C来实现这个功能: 1234if ([[NSProcessInfo processInfo] isLowPowerModeEnabled]) { // stop battery intensive actions} 如果你监听了NSProcessInfoPowerStateDidChangeNotification通知，在用户切换进入低电量模式的时候你将接收到一个消息。比如，在视图控制器中的viewDidLoad方法中: 1234NSNotificationCenter.defaultCenter().addObserver(self, selector: #selector(didChangePowerMode(_:)), name: NSProcessInfoPowerStateDidChangeNotification, object: nil) 1234[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(didChangePowerMode:) name:NSProcessInfoPowerStateDidChangeNotification object:nil]; 在我第一次发布这篇文章后，很多人提醒我：对于只对iOS 9.X适配的开发者而言，没有必要在 ViewController 消失时去移除 Observer 。 接着在这个方法会监视电池模式并在切换的时候给予一个响应。 1234567func didChangePowerMode(notification: NSNotification) { if NSProcessInfo.processInfo().lowPowerModeEnabled { // low power mode on } else { // low power mode off }} 1234567- (void)didChangePowerMode:(NSNotification *)notification { if ([[NSProcessInfo processInfo] isLowPowerModeEnabled]) { // low power mode on } else { // low power mode off }} 小贴士: 这个通知方法和NSProcessInfo里的属性是在iOS 9系统中新提供的方法。如果你想让你的APP兼容iOS8或者更早版本的系统，你需要去这个网站 test for availability测试你的代码是否能正常运行。 低电量模式是iPhone独有的特性，如果你在iPad上测试前面的代码，会一直返回false。 只有在你的 App 能够采取一些节能措施来延长电池寿命的情况下，检测用户开启了低电量模式才是有用的。这里，苹果给了一些建议： 停止更新位置 减少用户交互动画 关闭数据流量这样的后台操作 关闭特效","link":"/posts/2016/07/22/%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8BiPhone%E5%A4%84%E4%BA%8E%E4%BD%8E%E7%94%B5%E9%87%8F%E6%A8%A1%E5%BC%8F/"},{"title":"详解Swift的类型检查器","text":"原文链接: Exponential time complexity in the Swift type checker 原文作者: Matt Gallagher 译文出自: 掘金翻译计划 译者: Zheaoli 校对者: geeeeeeeeek, Graning 这篇文章将围绕曾不断使我重写代码的一些 Swift 编译器的报错信息展开： 错误：你的表达式太过于复杂，请将其分解为一些更为简单的表达式。（译者注：原文是 error: expression was too complex to be solved in reasonable time; consider breaking up the expression into distinct sub-expressions） 我会看那个触发错误的例子，谈谈以后由相同底层问题引起以外的编译错误的负面影响。我将会带领你看看在编译过程中发生了什么，然后告诉你，怎样在短时间内去解决这些报错。 我将为编译器设计一种时间复杂度为线性算法来代替原本的指数算法来彻底的解决这个问题，而不需要采用其余更复杂的方法。 正确代码的编译错误如果你尝试在 Swift 3 中编译这段代码，那么将会产生报错信息： 1let a: Double = -(1 + 2) + -(3 + 4) + 5 这段代码无论从哪方面来讲都是合法且正确的代码，从理论上讲，在编译过程中，这段代码将会被优化成一个固定的值。 但是这段代码在编译过程中没有办法通过 Swift 的类型检查。编译器会告诉你这段代码太复杂了。但是，等等，这段代码看起来一点都不复杂不是么。里面包含 5 个变量， 4 次加法操作， 2 次取负值操作和一次强制转换为 Double 类型的操作。 但是，编译器你怎么能说这段仅包含 12 个元素的语句相当复杂呢？ 这里有非常多的表达式在编译的时候会出现同样的问题。大多数表达式包含一些变量，基础的数据操作，可能还有一些重载之类的操作。接下来的表达式在编译时会面对同样的错误信息： 1234567891011let b = String(1) + String(2) + String(3) + String(4)let c = 1 * sqrt(2.0) * 3 * 4 * 5 * 6 * 7let d = [&quot;1&quot; + &quot;2&quot;].reduce(&quot;3&quot;) { &quot;4&quot; + String($0) + String($1) }let e: [(Double) -&gt; String] = [ { v in String(v + v) + &quot;1&quot; }, { v in String(-v) } + &quot;2&quot;, { v in String(Int(v)) + &quot;3&quot; }] 上面的代码都是符合 Swift 语法及编程规则的，但是在编译过程中，它们都没有办法通过类型检查。 需要较长的编译时间编译报错只是 Swift 类型检查器缺陷带来的副作用之一，比如，你可以试试下面这个例子： 1let x = { String(&quot;\\($0)&quot; + &quot;&quot;) + String(&quot;\\($0)&quot; + &quot;&quot;) }(0) 这段代码编译时不会报错，但是在我的电脑上，使用 Swift 2.3 将花费 4s 的时间，如果是使用 Swift 3 将会花费 15s 时间。编译过程中，将会花费大量的时间在类型检查上。 现在，你可能不会遇到太多需要耗费这么多时间的问题，但是一个大型的 Swift 项目中，你将会遇到很多 expression was too complex to be solved in reasonable time 这样的报错信息。 不可预知的操作接下来，我将讲一点 Swift 类型检查器的特性：类型检查器选择尽可能的解决非泛型重载的问题。 编译器中处理这种特定行为的路径下的代码注释对此给出了解释，这是一种避免性能问题的优化手段，用于优化造成 expression was too complex 报错的性能问题。 接下来是一些具体的例子： 1let x = -(1) 这段代码将会编译失败，我们会得到一个 Ambiguous use of operator ‘-‘ 的报错信息。 这段代码并不算很模糊，编译器将会明白我们想要使用一个整数类型的变量，它将会把 1 作为一个 Int 进行处理，同时从标准库中选择如下的重载方式： 1prefix public func -&lt;T : SignedNumber&gt;(x: T) -&gt; T 然而，Swift 只能进行非泛型重载。在这个例子中，Float 、 Double 、 Float80 类型的实现并不完善，编译器无法根据上下文选择使用哪种实现，从而导致了这个报错信息。 某些特定的优化可以对操作符进行优化，但是可能导致如下的一些问题： 12345678910func f(_ x: Float) -&gt; Float { return x }func f&lt;I: Integer&gt;(_ x: I) -&gt; I { return x }let x = f(1)prefix operator %% {}prefix func %%(_ x: Float) -&gt; Float { return x }prefix func %%&lt;I: Integer&gt;(_ x: I) -&gt; I { return x }let y = %%1 在这段代码里，我们定义了两个函数（ f 和一个自定的操作 prefix %% ）。每个函数都进行了两次重载，一个参数为 (Float) -&gt; Float ，另一个是 &lt;I: Integer&gt;(I) -&gt; I。 当调用 f(1) 的时候，将会选择使用 &lt;I: Integer&gt;(I) -&gt; If(1) 的实现，然后 x 将会作为 Int 类型进行处理。这应该是你所期待的方式。 当调用 %%1 时，将会使用 (Float) -&gt; Float 的实现，同时会将 y 作为 Float 类型处理，这和我们所期望的恰恰相反。在编译过程中，编译器选择将 1 作为 Float 处理，而不是作为 Int 处理，虽然作为 Int 处理也同样能正常工作。造成这样情况的原因是，编译器在对方法的进行泛型重载之前就已经先行确定变量的类型。这不是基于前后文一致性的做法，这是编译器对于避免类似于 expression was too complex to be solved 等报错信息以及性能优化上的一种妥协。 在代码中解决上诉问题通常来讲，Swift 里的显示代码太过复杂的缺陷并不是一个太大的问题，当然前提是你不会在单个表达式里使用两个或两个以上的下面列出的特性： 方法重载（包括操作符重载） 常量 不明确类型的闭包 会引导 Swift 进行错误类型转换的表达式 一般而言，如果你不使用如上面所述的特性，那么你一般不会遇到类似于 expression was too complex 的报错信息。然而，如果选择是用了上面所诉的特性，那么你可能会面临一些让你感到困惑的问题。通常，在编写一个足够大小的方法和其余常规代码的时候，将会很容易用到上面这些特性，这意味着有些时候我们可能要仔细考虑怎样避免大量使用上面这些特性。 你肯定是想只通过一点细微的修改来通过编译，而不是大量修改你的代码。接下来的一点小建议可能帮得上一些忙。 当上面所诉的编译报错信息出现时，编译器可能建议你将原表达式分割成不同的子表达式： 123let x_1: Double = -(1 + 2)let x_2: Double = -(3 + 4)let x: Double = x_1 + x_2 + 5 好了，从结果上来看，这样的修改是有效的，但是却让人有点蛋疼，特别是在分解成子表达式的时候会明显破坏代码可读性的时候。 另一个建议是通过显示类型转换，减少编译器在编译过程中对方法和操作符重载的选取次数。 1let x: Double = -(1 + 2) as Double + -(3 + 4) as Double + 5 上面这种做法避免了在使用 (Float) -&gt; Float 或者是 (Float80) -&gt; Float80 编译器需要去查找相对应的负号重载。这样的做法很有效的将编译过程中编译器的6次查找相对应的方法重载过程降至4次。 在上面的处理方式中有一个点要注意一下：不同于其余语言，在 Swift 中 Double(x) 并不等同于 x as Double。构造函数通常会如同普通方法一样，当有不同参数的重载需求时，编译器还是会将构造函数的各种重载加入到搜索空间中（尽管这些重载可能在代码中的不同的位置）。在前面所举的例子里，通过在括号前用 Double 进行显示类型转换会解决一部分问题（这种方法有利于编译器进行类型检查），同时在一些情况下，采用这种方法会导致出现一些其余的问题（请参见本文开始所举的关于 String 的例子）。最终， 使用as 操作符是在不增加复杂度的情况下解决这类的问题的最好方式。幸运的是，as 操作符的优先级比大多数二元运算符更高，这样我们可以在大多数的情况下使用它。 另一种方法是使用一个独立命名的自定义函数： 1let x: Double = myCustomDoubleNegation(1 + 2) + myCustomDoubleNegation(3 + 4) + 5 这种方法可以解决之前方法重载所带来的一系列问题。然而，在一系列轻量级的代码里使用这种方式会让我们的代码显得格外的丑陋。 好了，让我们来说说最后的方法，在很多情况下，你可以根据情况自行替换方法和操作符： 1let x: Double = (1 + 2).negated() + (3 + 4).negated() + 5 因为在使用对应方法时，和使用常见算数运算符相比，会有效的减少重载次数，同时使用 . 操作符时其效率相较于直接调用方法更高，因此，这种方法能有效解决我们前面所提到的问题。 Swift类型约束系统简析编译时出现的 expression was too complex 错误是由 Swift 编译器的语义分析系统所抛出的。语义分析系统的意义在于解决整个代码里的类型问题，从而确保输入表达式的类型是正确且安全的。 最重要的是，整个报错信息是由the constraints system solver (CSSolver.cpp)里所编写的语义分析系统所定义的。类型约束系统将从 Swift 的表达式里构建一个由类型和方法组成的图，并根据节点之间的关系来对代码进行约束。约束系统将对每个节点进行推算直至每个节点都已获得明确的类型约束。 讲真，上面的东西可能太抽象了，让我们看点具体的例子吧。 1let a = 1 + 2 类型约束系统将表达式解析成下面这个样子： 每个节点的名字都以 T 开头（意味着需要待确定明确的类型），然后它们用来代表需要解决的类型约束或者方法重载。在这个图里，这些节点被如下的规则所约束： T1 是 ExpressibleByIntegerLiteral 类型 T2 是 ExpressibleByIntegerLiteral 类型 T0 是一个传入 (T1,T2) 返回 T3 的方法 T0 是 infix + ，其在 Swift 里有28种实现 T3 与 T4 之间可以进行交换 小贴士: 在 Swift 2.X 中，ExpressibleByIntegerLiteral 的替代者是 IntegerLiteralConvertible 在这个系统中，类型约束系统遵循着 最小分离 原则。分割出来的单元被这样一个规则所约束着，即，每个单元都是一个拥有一套独立值的个体。在上面的这个例子里，实际上只有一个最小单元：在上述的约束 4 里，T0 发生了重载。在重载之时，编译器选择了 infix + 实现列表里第一种实现：即签名是 (Int, Int) -&gt; Int 的实现。 通过上述这个最小的单元，类型约束系统开始对元素进行类型约束：根据约束 3 T1、 T2 、 T3 被确定为 Int 类型，根据约束 4 ， T4 同样被确认为 Int 类型。 在 T1 、 T2 被确定为 Int 之后（最开始它们被认为是 ExpressibleByIntegerLiteral）， infix + 的重载方式便已经确定，这个时候编译器便不需要再考虑其余可行性，并把其当做最终的解决方案。我们在确定每个节点对应的类型后，我们便可以选择我们所需要的重载方法了。 让我们看点复杂的例子吧！到目前为止，并没有什么超出我们意料之中的异常出现，你可能想象不到当表达式开始变得复杂之时， Swift 的编译系统将会开始不断的出现错误信息。来让我们修改下上面的例子：第一·将 2 放在括号里，第二·添加负号操作符，第三·规定返回值为 Double 类型。 1let a: Double = 1 + -(2) 整个节点结构如下图所述： 节点约束如下： T1 是 ExpressibleByIntegerLiteral 类型 T3 是 ExpressibleByIntegerLiteral 类型 T2 是一个传入 T3 返回 T4 的方法 T2 是 prefix -，其在 Swift 里有6种实现 T0 是一个传入 T1、T4，返回 T5 的方法 T0 是 infix + ，其在 Swift 里有28种实现 T5 是 Double 类型 相较于上面的例子，这里多了两个约束，让我们看看类型约束系统会怎样处理这个例子。 第一步：选择最小分离单元。这次是约束 4 ：“ T2 是 prefix -，在 Swift 里有6种实现”。最后系统选择了签名为 (Float) -&gt; Float 的实现。 第二步：和第一步一样，选择最小分离单元，这次是约束 6 ：“T0 是 infix + ，其在 Swift 里有28种实现”。系统选择了签名为 (Int, Int) -&gt; Int 的实现。 最后一步是：利用上述的类型约束确定所有节点的类型。 然而，这里出现了点问题：在第一步里我们选择的签名为 (Float) -&gt; Float 的 prefix - 实现和第二步里我们选择的签名为 (Int, Int) -&gt; Int 的 infix + 实现和我们的约束 5 （T0 是一个传入 T1、T4，返回 T5 的方法）发生了冲突。解决方法是放弃当前的选择，然后重新回滚至第二步，为 T0 最终，系统将遍历所有的 infix + 实现，然后发现没有一种实现同时满足约束 5 和约束 7 （T5 是 Double 类型）。 所以，类型约束系统将回滚至第一步，为 T2 选取了签名为 (Double, Double) -&gt; Double 的实现。最后，这种实现也满足了 T0 的约束。 然而，在发现 Double 类型和 ExpressibleByIntegerLiteral 相互不匹配后，类型约束系统将继续回滚，寻找合适的重载方法。 T2 总共有6种实现，但是最后3种实现不能被优化(因为它们是通用的实现，因此优先级高于显示声明参数为 Double 的实现）。 在类型约束系统里，这种特殊优化是我曾经在Unexpected behaviors一文中提到的快速重载的一些特性。 拜这种特殊的“优化”所赐，类型约束系统需要76次查询才能找到一个合理的解决方案。如果我们添加了其余的一些新的重载，那么这个数字会变得超出我们的想象。例如，我们我们在例子里添加另外一个 infix + 操作符，比如： let a: Double = 0 + 1 + -(2) ，那么将需要1190次查询才能找到合理的解决方案。 查询解决方案的这个过程是一个典型的具有指数时间复杂度的操作。在分离单元里进行搜索的范围称为“笛卡尔积”，然后，对于图中的 n 个分离单元，算法将会在 n 维笛卡尔乘积的范围内进行查找（这是一个空间复杂度同样为指数的操作）。 根据我的测试，单语句内拥有6个分离单元，便足以触发 Swift 中的 expression was too complex 的错误。 线性化的类型约束系统针对本文所反复提到的这个问题，最好的解决方法就是在编译器中进行修复。 类型约束系统之所以采用时间复杂度为指数算法来解决方法重载的问题，是因为 Swift 需要对方法重载所生成的 n 维“笛卡尔乘积”空间里的元素进行遍历并搜索从而确定一个合适的选项（在没有更好方案之前，这应该是最好的方案）。 为了避免生成 n 维笛卡尔乘积空间，我们需要设计一个方法来实现相关逻辑实现的独立性，而不让它们彼此依赖。 在开始之前我必须给你们一个很重要的提醒： 友情提醒，这些东西仅代表我的个人观点：接下来的一些讨论，都是我从理论的角度上来分析怎样在 Swift 的类型约束系统中怎样去解决函数重载的问题。我并没有写一些东西来证明我提出的解决方案，这可能意味着我会忽略某些非常重要的东西。 前提我们想实现如下两个目标： 限制一个节点不应该与其余节点相互依赖或引用 从前一个方法分析出来的分离单元应该与后一个方法分离出来的存在着交集，并进一步简化分离单元的两个约束条件。 第一个目标，可以通过限制节点的约束路径实现。在 Swift 中，每个节点的约束是双向的，每个节点的约束都从表达式的每一个分支开始，然后依照着遍历主干-&gt;线性遍历子节点的方式不断传播。在这个过程中，我们可以有选择性的简单地合并相同的约束逻辑来组合这些约束，而不是从其余节点引用相对应的类型约束。 第二个目标里，支持前面通过减少类型约束的传播复杂度来进一步简化相关约束条件。每个重载方法的分离单元之间最重要的交叉点是一个重载函数的输出，可能会作为另一个重载函数的输入。这个操作应该根据参数相互交叉的两个重载方法所产生的2维笛卡尔积来进行计算。对于其余的可能存在的交叉点来说，给出一个真正意义上的数学上的严格交叉证明是非常困难的，同时这样的证明是没有必要的，我们只需要复制 Swift 里在复杂情况下的对于类型选择的时所采用的贪婪策略即可。 让我们重新看看之前的例子让我们看看如果我们实现了前文所讲的两个目标后，类型约束系统将会变成什么样子。首先让我们复习下之前所生成的节点图： 1let a: Double = 1 + -(2) 然后让我们也复习下以下节点约束： T1 是 ExpressibleByIntegerLiteral 类型 T3 是 ExpressibleByIntegerLiteral 类型 T2 是一个传入 T3 返回 T4 的方法 T2 是 prefix -，其在 Swift 里有6种实现 T0 是一个传入 T1、T4，返回 T5 的方法 T0 是 infix + ，其在 Swift 里有28种实现 T5 是 Double 类型 将节点约束从右至左传递我们从右至左进行遍历（从叶子节点向主干遍历）。 在节点约束从 T3 向 T2 传播时，添加了这样一个新的约束：“ T2 节点的输入值必须是一个由 ExpressibleByIntegerLiteral 转化而来的值”。现在在新的约束规则和原有规则同时发生作用后，一旦我们确认所有拥有 T2 的节点都被新规则约束成功之后，或者是与“特定操作重载优先于通用操作重载（比如在 prefix - 中 Double、 Float 或者是 Float80 会被优先重载）”这条规则冲突之时，便可以丢弃我们新建立的节点约束规则。在节点约束从 T2 向 T4 中传播的过程中，添加新约束为：“ T4 必须是 prefix - 所返回的6中类型的值之一，其中 Double、Float 或 Float80 优先被考虑）。在节点约束从 T4 朝 T0 传播的过程中，添加新约束为：“ T0 的第二个参数必须是从 prefix - 返回的6种参数里的任意一种演变而来，其中 Double、 Float 或 Float80 类型优先）。在结合 T0 已有的节点约束后，T0 的节点约束变为：“ T0 是 infix + 的6种实现之一，同时从右侧传入的参数是来自 prefix - 返回参数中的任意一种，在这个过程中类型是 Double、 Float 或者 Float80 的参数优先被考虑）。在节点约束从 T1 朝 T0 传递之时，没有新的约束条件需要添加（在这里，T0 已经被我们所增加的约束条件严格约束了，同时，原本所使用的 ExpressibleByIntegerLiteral 类型已经被 Double、 Float 或者 Float80 中的任意一种类型所替代了）。在节点约束从 T0 向 T5 传播时，需新增加约束为：“ T5 是 infix + 的6种返回值中的一种，且 infix + 的第二个参数是来自 prefix - 的返回值，在这个过程中，Double、 Float 或者 Float80 类型优先被考虑）。在上述约束的共同作用下，我们可以最终确认 T5 的类型为 Double。 经过上述过程的变动之后，整个节点约束集迭代成下面这个样子： T1 是 ExpressibleByIntegerLiteral 类型 T3 是 ExpressibleByIntegerLiteral 类型 T2 是一个传入 T3 返回 T4 的方法 T2 是 prefix - 的6种实现之一，同时为了满足在 Swift 中特殊操作重载优先级高于通用运算重载的原则，类型为 Double、 Float 或者 Float80 的 prefix - 重载优先被考虑。 T4 是 prefix - 的六种返回值之一，同样为了满足在 Swift 中特殊操作重载优先级高于通用运算重载的原则，类型为 Double、 Float 或者 Float80 的 prefix - 重载优先被考虑。 T0 是一个传入 T1、T4，返回 T5 的方法 T0 是 infix + 的6种实现之一，同时从右侧传入的参数是来自 prefix - 返回参数中的任意一种，在这个过程中为了满足在 Swift 中特殊操作重载优先级高于通用运算重载的原则，类型是 Double、 Float 或者 Float80 的参数优先被考虑 T5 是 Double 类型 将节点约束从左至右传递现在我们开始从左至右进行遍历（先遍历主干，后遍历叶子节点）。 首先从 T5 开始遍历，约束 5 是：“ T5 是 Double 类型的节点”。这时我们为 T0 添加新的约束：“ T0 的返回值类型一定要是 Double 类型的”。在这个约束生效后，我们就可以排除除 (Double, Double) -&gt; Double 之外的 infix + 的重载了。节点约束继续从 T0 朝 T1 传递，根据 infix + 的(Double, Double) -&gt; Double 重载的参数要求，我们为 T1 创建一个新的约束： T1 一定是 Double 类型的。在多种约束的作用下，之前所提到的“T1 是 ExpressibleByIntegerLiteral 类型”变为“T1 是 Double 类型”。在节点约束从 T0 朝 T4 ，根据 infix + 的第二个参数的要求，我们确定 T4 的类型为 Double。节点约束从 T4 朝 T2 传播的过程中，我们新增加一个约束：“ T2 的返回值一定为 Double 类型”。在以上规则共同作用下，我们可以确定 T2 为 prefix - 的参数类型为 (Double) -&gt; Double 重载。最后根据以上的约束，我们可以得知 ‘T3’ 的类型为 ‘Double’。 最后整个类型约束系统编程下面这个样子： T1 为 Double 类型。 T3 为 Double 类型。` T2 是 prefix - 的参数为 (Double) -&gt; Double 类型的重载 T0 是 infix + 的参数为 (Double, Double) -&gt; Double 类型的重载 T5 为 Double 类型。 好了，现在整个类型约束操作便已经告一段落了。 唔，我提出这算法的目的是改善方法重载的相关操作，因此，我将方法重载的次数用 n 表示。然后我将平均每个函数重载次数用 m 表示。 如我前面所述，在 Swift 中，编译器是通过在一个 n 维的笛卡尔积空间内进行搜索来确定最终的结果。它的时间复杂度是 O(m^n) 。 而我所提出的算法，是在一个2维的空间内去搜索 n-1 个分离单元来实现的。其执行时间是 *m^2n**.因为 m 是和 n 相关联的，我们可以得到其最终的时间复杂度为 O(n) 。 通常来讲，在 n 为很大的时候，线性复杂度的算法比指数时间复杂度的算法更能适应当前的状况，不过我们得搞清楚什么样的情况才能被称之为 n 为很大的数。在这个例子中，3 已经是一个非常 “大” 的数了。正如我前面所提到的一样，Swift 自带的类型约束系统将进行1190次搜索来确认最后的结果。而我设计的算法只需要336次搜索。这可以说很明显的降低了最后的耗时。 我做了一个很有趣的实验：在之前所提到的 let a: Double = 1 + -(2) 这个例子里，不管是 Swift 里的类型约束系统，还是我所设计的算法，它们都是在一个2维的笛卡尔积空间内进行搜索，里面都包含了168中可能性。 Swift 里现在所采用的类型约束算法选取了在 prefix - 和 infix + 重载生成的2维笛卡尔积空间内的168种可能性的76种。但是这样做的话，整个过程里会产生567次对 ConstraintSystem::matchTypes的调用，其中546次是用于搜索相适应的重载函数。 我所设计的算法，搜索了全部168种可能性，但是根据我的分析，其最后只产生了22次对 ConstraintSystem::matchTypes 的调用。 去确定一个非公开的算法，需要进行很多次的猜测，所以知道某一种算法的的具体细节是一件非常困难的事儿。但是我想，我的算法在任意数量级的情况下，其表现优于或与现在已有的算法持平并不是一件不可能的事儿。 Swfit 很快会改进他的类型系统么？虽然我很想说：“我一个人就把所有工作做完了，看看这些代码运行的多么完美啊”，但是这也只能是想想罢了。一个整个系统由成千上万个逻辑和单元组成，并不能单独抽出某一个节点来进行讨论。 你觉得 Swift 开发团队是不是在尝试把类型约束系统进行线性化处理呢？我对此持否定看法。 在这篇文章里“[swift-dev] A type-checking performance case study”表明官方开发者认为类型约束系统采用时间复杂度为指数的算法是一件很正常的事儿。与其将时间放在优化算法上，还不如去重构标准库，使其更为合理。 一点吐槽： 现在看来本文的前面两章简直就是在做无用功，我应该静静的将其删除。 我觉得我想法是正确的，类型约束系统应该进行大幅度改进，这样我们次啊不会被上面所提到的问题所困扰。 友情提醒: 理论上将类型约束系统并不是整个语言最主要的一部分，因此如果其进行了改进，应该是在一个小版本迭代中进行发布，而不是一个大版本更新。 结论在我使用 Swift 的经历里, expression was too complex to be solved in reasonable time 是一个经常出线的错误，而且我并不认为这是一个简单的错误。如果你在单个例子中是用了大量的方法或者是数学操作的时候，你应该定期看看这篇文章。 Swift 里所采用的时间复杂度为指数的算法也可能导致编译时间较长的问题。 尽管我没有确切的统计整个编译里的时间分配，但是不出意外的话，系统应该将大部分时间放在了类型约束器的相关计算上。 这个问题可以再我们编写的代码的时候予以避免，但是讲真，没有必要这么做。如果编译器能采用我所提出的线性时间的算法的话，我敢肯定，这些问题都不在是问题。 在编译器做出具体的改变之前，本文所提到的问题会一直困扰着我们，与编译器的斗争还要持续下去。","link":"/posts/2016/08/02/%E8%AF%A6%E8%A7%A3Swift%E7%9A%84%E7%B1%BB%E5%9E%8B%E6%A3%80%E6%9F%A5%E5%99%A8/"},{"title":"蓝莲花公益小组简报","text":"愿每个人心里，都盛开着永不凋零的蓝莲花 从2021年11月第一次发起刷题公益计划，到现在也一年多时间了。起初是为了让大家有一些特殊的动力去刷题，所以有了这样的基础规则 1题一元人民币，在打卡后向公益基金捐款。 基金池最开始由群主承担，后续有超过25位+群友集体捐款 再后来，这个群就发展成了基于技术的各种闲聊群，推荐番毒害群友群。 到目前也差不多一年多时间了，写个简报回顾一下 一、刷题公益计划截至目前，从2021年11月开始，到2022年6月作为一个阶段的结束。 共计捐款 1625 元人民币 在2022年6月，经过群友同意，再经过一轮扩资后，蓝莲花小组向一个村小项目捐款 6000 元人民币 前不久得到反馈，这笔钱已经用在应该用的地方了。开心 二. 技术分享从2022年6月开始，群友决定在群内以一周两次的频率进行分享，截至目前举行了八次分享 SRE 二三事 当前端在讨论字体时，我们在讨论什么 编译原理入门到出家 OLAP 入门出家 简单聊聊家庭网络 Homelab 101 稳定性建设101 物联网简介 三. 开源项目截至目前，群友的足迹包括不仅限于 Vue SWR containerd envd 2022 年群内也新诞生了两位开源项目的 maintainer 总结时间过的很快，转眼这个群就一年多了。很荣幸能在这个浮躁的时代里认识一些很纯粹的人。2023 一起加油 最后，愿每个人心中都能盛开着永不凋零的蓝莲花","link":"/posts/2023/01/19/simple-brief-about-blue-lotus-group/"},{"title":"菜鸟阅读 Flask 源码系列（1）：Flask的router初探","text":"文章来源：itsCoder 的 WeeklyBolg 项目 itsCoder主页：http://itscoder.com/ 作者：写代码的香港记者 审阅者：Brucezz 前言没有一个完整的开源项目的的阅读经验的程序猿是一个不合格的程序猿，虽然曾经阅读过部分诸如 Redis 等项目的源码，但是还没有过一个完整的开源项目的阅读经验，因此在经过某个前辈的不断安利后，我决定用 Flask 来作为阅读开源源码计划的开始。而这一个系列的文章，将作为我自己的阅读笔记，来巩固自己曾经所没有重视的 Python 的很多细节。 关于 Flask关于 Flask 的背景知识，就不需要太多的描述了，网上已经有很多的资料了。在使用 Flask 的时候，我们经常用如下的方式来设置我们的自定义的路由： 12345678910111213141516171819202122232425262728293031323334353637##Flask官方Example中flaskr项目部分代码app = Flask(__name__)@app.route('/')def show_entries(): db = get_db() cur = db.execute('select title, text from entries order by id desc') entries = cur.fetchall() return render_template('show_entries.html', entries=entries)@app.route('/add', methods=['POST'])def add_entry(): if not session.get('logged_in'): abort(401) db = get_db() db.execute('insert into entries (title, text) values (?, ?)', [request.form['title'], request.form['text']]) db.commit() flash('New entry was successfully posted') return redirect(url_for('show_entries'))@app.route('/login', methods=['GET', 'POST'])def login(): error = None if request.method == 'POST': if request.form['username'] != app.config['USERNAME']: error = 'Invalid username' elif request.form['password'] != app.config['PASSWORD']: error = 'Invalid password' else: session['logged_in'] = True flash('You were logged in') return redirect(url_for('show_entries')) return render_template('login.html', error=error) 那么问题来了，上面的例子中，我们知道 app.route('xxxx',methods=['xxx']) 将会设置我们对应的方法与对应 url 的关联，那么这样一种做法是怎样生效的呢？ Flask 源码阅读让我们看看最开始的 router 是什么样子的首先让我们从 Flask 这里获取 flask 源码，然后我们将版本号切换至最初的 0.1 版（git tag为8605cc310d260c3b08160881b09da26c2cc95f8d） 小tips：阅读开源项目时，如果当前版本太过于复杂，可以切换至项目最初发布时的版本，然后根据每次项目版本发布的 Release Note 来进行跟进。 在 flask.py 文件里，我们能看到如下的的结构 讲真这个时候我们就可以看到 Flask 里的 route 的核心代码了 1234567def route(self, rule, **options): def decorator(f): self.add_url_rule(rule, f.__name__, **options) self.view_functions[f.__name__] = f return f return decorator 我们能很清楚的看到之前代码中的 app.route('/') 本质上是调用了一个装饰器来对我们对应的方法进行请求与方法之间进行关联。在 route 方法被触发后，进一步来调用 add_url_rule 来注册我们所设定的url。 1234def add_url_rule(self, rule, endpoint, **options): options['endpoint'] = endpoint options.setdefault('methods', ('GET',)) self.url_map.add(Rule(rule, **options)) 在最初版本的 Flask 中， Router 的实现就这么简单暴力 两个关于装饰器的知识点第一个：很多人肯定想问，在之前的代码里，我们没有调用相关的方法，例如 index() ，那么装饰器为什么会被触发呢？ 答：首先，请大声告诉我，装饰器的作用是什么？很明显嘛，在不修改原有代码的基础上，对函数进行一次封装，然后实现为原有方法增加一些功能的特殊实现。是不是感觉很抽象？来我们看个例子 12345678910111213def testDe1(func): def de(a, b, c): func(a, b, c) print('1') print('2') return de@testDe1def test2(a, b, c): print(a+b+c)if __name__ == '__main__': test2(1,2,3) 来，告诉我，这段代码的输出应该是什么？答案是 2,6,1，看到这里，你是不是感觉似乎明白了些什么？是的没错，上面的例子其实等价于 12345678910111213def testDe1(func): def de(a, b, c): func(a, b, c) print('1') print('2') return dedef test2(a, b, c): print(a+b+c)if __name__ == '__main__': testDe1(test2)(1,2,3) 那么我们换个例子 123456789101112def testDe1(func): def de(a, b, c): func(a, b, c) print('1') print('2') return de@testDe1def test2(a, b, c): print(a+b+c)if __name__ == '__main__': pass 这段代码的输出会是什么？是的没错，这段代码的输出是 2 。看到这里你是不是感觉更明白些什么？恩，在 Python 中，使用函数装饰器的时候，等于先行调用了装饰函数一次，具体来讲在使用装饰器后，装饰器会用装饰后的函数来进行一个替换，即在什么也不做的情况下，会产生这样一个调用 test2=testDe1(test2)，接着如果在 __main__ 中添加一段代码 test2(1,2,3),是不是就等价于 testDe1(test2)(1,2,3) 。看到这里是不是彻底明白了？恩，来，我们再来复习下前面的例子 1234567@app.route('/')def show_entries(): db = get_db() cur = db.execute('select title, text from entries order by id desc') entries = cur.fetchall() return render_template('show_entries.html', entries=entries) 上面这段代码里发生了什么？是不是有一个调用为 show_entries=app.route('/')(show_entries)？看到这里是不是很清楚了呢？ 第二个，在第一个小 tip 的基础之上，我们来讲一个关于装饰器传参的问题可能很多人不清楚装饰器传参的使用情景，首先如前面所说装饰器的最根本的作用在于 在不修改原有代码的基础上，对函数进行一次封装，然后实现为原有方法增加一些功能的特殊实现 现在假设我们需要对函数的运行时间进行输出，这个时候我们该怎么办 123456789101112def testTime(func): def dec(*args,**kwargs): flag=time.time() func(*args,**kwargs) print(time.time()-flag) return decdef func(): passif __name__=='__main__': func() 如前所述，前面这段代码等价于 test(func)(),那么这个时候我们想给我们时间输出以一定的单位进行格式化怎办，修改上面装饰器代码如下 12345678910111213def testtime(time=None): def dec1(func): def dec2(*args,**kwargs): flag=time.time() func(*args,**kwagrs) flag2=time.time() if time is not None: print((flag2-flag)/time) else: print(flag2-flag) return dec2 return dec1 写到这里，大家是不是明白了带参数的装饰器的使用情景呢？ 后记Flask 的路由系统相对简单，其本质是利用带参数的装饰器来进行相应的路由记录，同时利用装饰器的包装特性，将我们的对应的处理函数进行包装，同时加入路由表中，一旦触发我们所注册的路由，便可调用我们所对应的处理函数。","link":"/posts/2016/08/09/reading-the-fucking-flask-source-code-Part1/"},{"title":"日常辣鸡水文:关于 logging 的进程安全问题","text":"日常辣鸡水文:关于 logging 的进程安全问题团队聚餐喝了点酒，作为一个垃圾文档工程师来写一篇日常水文 正文现在团队的日志搜集方式从原本的 TCP 直传 logstash 的方式改进为写入一个单文件后，改用 FileBeat 来作为日志搜集的前端。但是这样时常带来一个问题，即日志丢失 嗯，我们线上服务是 Gunicorn 启用多个 Worker 来处理的。这就有个问题了，我们都知道，logging 模块是 Thread Safe 的，在标准的 Log Handler 内部加了一系列锁来确保线程安全，但是 logging 直写文件是不是进程安全的呢？ 分析我们写文件的方式是用的是 logging 模块中自带的 FileHandler ，首先看看它源码吧 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class FileHandler(StreamHandler): &quot;&quot;&quot; A handler class which writes formatted logging records to disk files. &quot;&quot;&quot; def __init__(self, filename, mode='a', encoding=None, delay=False): &quot;&quot;&quot; Open the specified file and use it as the stream for logging. &quot;&quot;&quot; # Issue #27493: add support for Path objects to be passed in filename = os.fspath(filename) #keep the absolute path, otherwise derived classes which use this #may come a cropper when the current directory changes self.baseFilename = os.path.abspath(filename) self.mode = mode self.encoding = encoding self.delay = delay if delay: #We don't open the stream, but we still need to call the #Handler constructor to set level, formatter, lock etc. Handler.__init__(self) self.stream = None else: StreamHandler.__init__(self, self._open()) def close(self): &quot;&quot;&quot; Closes the stream. &quot;&quot;&quot; self.acquire() try: try: if self.stream: try: self.flush() finally: stream = self.stream self.stream = None if hasattr(stream, &quot;close&quot;): stream.close() finally: # Issue #19523: call unconditionally to # prevent a handler leak when delay is set StreamHandler.close(self) finally: self.release() def _open(self): &quot;&quot;&quot; Open the current base file with the (original) mode and encoding. Return the resulting stream. &quot;&quot;&quot; return open(self.baseFilename, self.mode, encoding=self.encoding) def emit(self, record): &quot;&quot;&quot; Emit a record. If the stream was not opened because 'delay' was specified in the constructor, open it before calling the superclass's emit. &quot;&quot;&quot; if self.stream is None: self.stream = self._open() StreamHandler.emit(self, record) def __repr__(self): level = getLevelName(self.level) return '&lt;%s %s (%s)&gt;' % (self.__class__.__name__, self.baseFilename, level) 嗯，其中关注的点是 _open 方法，以及 emit 方法，首先科普一个背景知识，在我们用 logging 输出日志的时候，logging 模块会调用对应 Handler 中的 handle 方法，在 handle 方法中，会调用 emit 方法输出最后的日志。于是我们如果使用 FileHandler 的话，那么就是先触发 handle 方法的调用，然后触发 emit 方法，在调用 _open 方法获取一个 file point 后，调用父类（更准确的描述书 MRO 上一级）StreamHandler 中 emit 方法。 来看看 StreamHandler 中的 emit 方法吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class StreamHandler(Handler): &quot;&quot;&quot; A handler class which writes logging records, appropriately formatted, to a stream. Note that this class does not close the stream, as sys.stdout or sys.stderr may be used. &quot;&quot;&quot; terminator = '\\n' def __init__(self, stream=None): &quot;&quot;&quot; Initialize the handler. If stream is not specified, sys.stderr is used. &quot;&quot;&quot; Handler.__init__(self) if stream is None: stream = sys.stderr self.stream = stream def flush(self): &quot;&quot;&quot; Flushes the stream. &quot;&quot;&quot; self.acquire() try: if self.stream and hasattr(self.stream, &quot;flush&quot;): self.stream.flush() finally: self.release() def emit(self, record): &quot;&quot;&quot; Emit a record. If a formatter is specified, it is used to format the record. The record is then written to the stream with a trailing newline. If exception information is present, it is formatted using traceback.print_exception and appended to the stream. If the stream has an 'encoding' attribute, it is used to determine how to do the output to the stream. &quot;&quot;&quot; try: msg = self.format(record) stream = self.stream stream.write(msg) stream.write(self.terminator) self.flush() except Exception: self.handleError(record) def __repr__(self): level = getLevelName(self.level) name = getattr(self.stream, 'name', '') if name: name += ' ' return '&lt;%s %s(%s)&gt;' % (self.__class__.__name__, name, level) 嗯很简单，就是调用我们之前获取的 file point 往文件中写入数据 问题就在这里，在 FileHandler 中，_open 函数中调用 open 函数时，所选择的 mode 是 'a' ，也就是通常而言的 O_APPEND 模式。我们知道，通常而言 O_APPEND 可以视作进程安全的，因为 O_APPEND 能够保证内容不被别的 O_APPEND 写操作所覆盖。但是这里为什么会出现日志丢失的情况呢？ 原因是在 POSIX 中存在着一种特殊设计，在 《POSIX Programmers Guide》 一书中对此描述如下： A write of fewer than PIPE_BUF bytes is atomic; the data will not be interleaved with data from other processes writing to the same pipe. A write of more than PIPE_BUF may have data interleaved in arbitrary ways. 这段话翻译大概就是，在 POSIX 中存在着一个变量叫做 PIPE_BUF ，这个变量大小为 512 ，在进行写入操作时，如果大小小于 PIPE_BUF 值的写操作，是具有原子性的，即不可被打断，因此不会和其余进程写入的值产生混乱，而如果写入的内容大于 PIPE_BUF ，则操作系统不能保证这一点。 在 Linux 操作系统中，这个值发生了一点变化 POSIX.1 says that write(2)s of less than PIPE_BUF bytes must be atomic: the output data is written to the pipe as a contiguous sequence. Writes of more than PIPE_BUF bytes may be nonatomic: the kernel may interleave the data with data written by other processes. POSIX.1 requires PIPE_BUF to be at least 512 bytes. (On Linux, PIPE_BUF is 4096 bytes.) 即大于 4K 的写入操作都不能保证其原子性，可能会发生数据紊乱。 而发生数据紊乱后，其日志格式不固定，最终造成解析端没法解析，从而最终日志丢失。 这里我们复现一下，首先测试代码 最后这种操作之前从未想过，今天算是打开了新的大门，最后感谢 @依云 前辈的指点= =如果没有前辈的提醒，完全想不到即便是 O_APPEND 模式下，数据也不能保证安全。 Reference文中参考了两处参考资料，链接如下 1.OReilly POSIX Programmers Guide 2.Linux Man: PIPE","link":"/posts/2018/02/22/logging-process-safe/"}],"tags":[{"name":"社会","slug":"社会","link":"/tags/%E7%A4%BE%E4%BC%9A/"},{"name":"PKU","slug":"PKU","link":"/tags/PKU/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"编程","slug":"编程","link":"/tags/%E7%BC%96%E7%A8%8B/"},{"name":"Flask","slug":"Flask","link":"/tags/Flask/"},{"name":"掘金翻译计划","slug":"掘金翻译计划","link":"/tags/%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"容器","slug":"容器","link":"/tags/%E5%AE%B9%E5%99%A8/"},{"name":"iOS","slug":"iOS","link":"/tags/iOS/"},{"name":"Swift","slug":"Swift","link":"/tags/Swift/"},{"name":"人间世","slug":"人间世","link":"/tags/%E4%BA%BA%E9%97%B4%E4%B8%96/"},{"name":"黑魔法","slug":"黑魔法","link":"/tags/%E9%BB%91%E9%AD%94%E6%B3%95/"},{"name":"编程技巧","slug":"编程技巧","link":"/tags/%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7/"},{"name":"协程","slug":"协程","link":"/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"笔记","slug":"笔记","link":"/tags/%E7%AC%94%E8%AE%B0/"},{"name":"水文","slug":"水文","link":"/tags/%E6%B0%B4%E6%96%87/"},{"name":"源码阅读","slug":"源码阅读","link":"/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"SRE","slug":"SRE","link":"/tags/SRE/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"论文","slug":"论文","link":"/tags/%E8%AE%BA%E6%96%87/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"人生","slug":"人生","link":"/tags/%E4%BA%BA%E7%94%9F/"},{"name":"杂记","slug":"杂记","link":"/tags/%E6%9D%82%E8%AE%B0/"},{"name":"随想","slug":"随想","link":"/tags/%E9%9A%8F%E6%83%B3/"},{"name":"云原生","slug":"云原生","link":"/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"eBPF","slug":"eBPF","link":"/tags/eBPF/"},{"name":"SystemTap","slug":"SystemTap","link":"/tags/SystemTap/"},{"name":"秀恩爱","slug":"秀恩爱","link":"/tags/%E7%A7%80%E6%81%A9%E7%88%B1/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"刷题","slug":"刷题","link":"/tags/%E5%88%B7%E9%A2%98/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"输入法","slug":"输入法","link":"/tags/%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"电子产品","slug":"电子产品","link":"/tags/%E7%94%B5%E5%AD%90%E4%BA%A7%E5%93%81/"},{"name":"Apple","slug":"Apple","link":"/tags/Apple/"},{"name":"评测","slug":"评测","link":"/tags/%E8%AF%84%E6%B5%8B/"},{"name":"async","slug":"async","link":"/tags/async/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"源码剖析","slug":"源码剖析","link":"/tags/%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"},{"name":"PEP","slug":"PEP","link":"/tags/PEP/"},{"name":"吐槽","slug":"吐槽","link":"/tags/%E5%90%90%E6%A7%BD/"},{"name":"PEP484","slug":"PEP484","link":"/tags/PEP484/"},{"name":"Unix","slug":"Unix","link":"/tags/Unix/"},{"name":"网络编程","slug":"网络编程","link":"/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"技术","slug":"技术","link":"/tags/%E6%8A%80%E6%9C%AF/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"Objective-C","slug":"Objective-C","link":"/tags/Objective-C/"}],"categories":[{"name":"社会","slug":"社会","link":"/categories/%E7%A4%BE%E4%BC%9A/"},{"name":"编程","slug":"编程","link":"/categories/%E7%BC%96%E7%A8%8B/"},{"name":"PKU","slug":"社会/PKU","link":"/categories/%E7%A4%BE%E4%BC%9A/PKU/"},{"name":"Python","slug":"编程/Python","link":"/categories/%E7%BC%96%E7%A8%8B/Python/"},{"name":"人间世","slug":"人间世","link":"/categories/%E4%BA%BA%E9%97%B4%E4%B8%96/"},{"name":"翻译","slug":"编程/翻译","link":"/categories/%E7%BC%96%E7%A8%8B/%E7%BF%BB%E8%AF%91/"},{"name":"Linux","slug":"编程/Linux","link":"/categories/%E7%BC%96%E7%A8%8B/Linux/"},{"name":"杂记","slug":"编程/杂记","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%9D%82%E8%AE%B0/"},{"name":"水文","slug":"编程/水文","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%B0%B4%E6%96%87/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"人生","slug":"人生","link":"/categories/%E4%BA%BA%E7%94%9F/"},{"name":"随想","slug":"编程/随想","link":"/categories/%E7%BC%96%E7%A8%8B/%E9%9A%8F%E6%83%B3/"},{"name":"leetcode","slug":"编程/leetcode","link":"/categories/%E7%BC%96%E7%A8%8B/leetcode/"},{"name":"随笔","slug":"编程/Python/随笔","link":"/categories/%E7%BC%96%E7%A8%8B/Python/%E9%9A%8F%E7%AC%94/"},{"name":"论文","slug":"编程/水文/论文","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%B0%B4%E6%96%87/%E8%AE%BA%E6%96%87/"},{"name":"总结","slug":"随笔/总结","link":"/categories/%E9%9A%8F%E7%AC%94/%E6%80%BB%E7%BB%93/"},{"name":"杂记","slug":"人生/杂记","link":"/categories/%E4%BA%BA%E7%94%9F/%E6%9D%82%E8%AE%B0/"},{"name":"Kernel","slug":"编程/Linux/Kernel","link":"/categories/%E7%BC%96%E7%A8%8B/Linux/Kernel/"},{"name":"刷题","slug":"编程/leetcode/刷题","link":"/categories/%E7%BC%96%E7%A8%8B/leetcode/%E5%88%B7%E9%A2%98/"},{"name":"秀恩爱","slug":"随笔/总结/秀恩爱","link":"/categories/%E9%9A%8F%E7%AC%94/%E6%80%BB%E7%BB%93/%E7%A7%80%E6%81%A9%E7%88%B1/"},{"name":"刷题","slug":"编程/刷题","link":"/categories/%E7%BC%96%E7%A8%8B/%E5%88%B7%E9%A2%98/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"MySQL","slug":"编程/Python/MySQL","link":"/categories/%E7%BC%96%E7%A8%8B/Python/MySQL/"},{"name":"生活","slug":"生活","link":"/categories/%E7%94%9F%E6%B4%BB/"},{"name":"输入法","slug":"工具/输入法","link":"/categories/%E5%B7%A5%E5%85%B7/%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"电子产品","slug":"生活/电子产品","link":"/categories/%E7%94%9F%E6%B4%BB/%E7%94%B5%E5%AD%90%E4%BA%A7%E5%93%81/"},{"name":"PEP484","slug":"编程/Python/PEP484","link":"/categories/%E7%BC%96%E7%A8%8B/Python/PEP484/"},{"name":"Rime","slug":"工具/输入法/Rime","link":"/categories/%E5%B7%A5%E5%85%B7/%E8%BE%93%E5%85%A5%E6%B3%95/Rime/"},{"name":"网络编程","slug":"编程/网络编程","link":"/categories/%E7%BC%96%E7%A8%8B/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"},{"name":"杂记","slug":"技术/杂记","link":"/categories/%E6%8A%80%E6%9C%AF/%E6%9D%82%E8%AE%B0/"}],"pages":[{"title":"","text":"","link":"/404.html"},{"title":"About","text":"关于我自己一个喜欢编程的香港记者，热爱 Python ,讨厌 Java ,重度拖延症晚期，想学很多东西，但是总觉得智商不够。渴望被这个世界温柔的拥抱着，也学会了去温柔的拥抱这个世界。 之前在某外卖厂，师从松鼠奥利奥中微服务中间件相关的开发 然后 transfer 到某云，从事公有云的中间件开发，欢迎各位大佬提工单 现在在太极图形，欢迎有兴趣的同学一起来做一些有意思的事情 和几个朋友搞了一个播客，『捕蛇者说』 Jincher 家的大狗","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"","text":"Manjusaka 个人简历联系方式 Email：me@manjusaka.me 个人信息 本科/西华大学/信息工程（2012 ~ 2016） 工作年限：6 年 技术博客：https://manjusaka.itscoder.com 期望职位：中间件开发 期望城市：北京 About Me 自驱力较强，能主动去学习新技术并且推动落地实现，并回馈社区。 PyCon China 2018/2019/2020/2021 组织者，讲师。微软 Python 方向 MVP。 多年 Python 经验，具备良好的编程规范 。熟练掌握 Python 相关的技术开发工作。了解 Go/Java，阅读过部分 Go/CPython 代码。 熟练使用 Flask/Gunicorn/Gevent/Django/Celery 等 Python Web 开发过程中所使用的工具与技术栈，并阅读过 Flask/Greenlet 等项目源码 了解 Kubernetes 及其周边生态。能利用 Kubernetes 进行 PaaS 平台开发。阅读过部分 Kubernetes 源码 了解 TCP/HTTP1.x ，并阅读过开源社区部分实现 了解 Linux Kernel，并阅读过部分实现。同时对 Linux 中的一些新技术如 eBPF 有所涉猎并有落地经验 能力较为全面，能同时承担技术社区 PR/布道/运营等工作 工作经历2021.09-2022.06 Senior Engineer@Taichi Graphics工作内容： 内部云服务平台负责人 SRE 与稳定性负责人 技术社区运营 2019.11-2021.08 阿里云（内部调转）工作内容： 对开源网关项目（Zuul，Spring Cloud Gateway，Kong CE）进行云上托管 对开源监控项目（Prometheus）进行云上托管 对开源项目进行功能增强（网关配置热更新，无损升级，协议转换等） 作为 SRE 一号位，负责产品稳定性建设 工作成果： 在任职期间，通过制定线上操作 SOP 来避免人为故障。建立完整监控体系，保证故障及时感知。同时进行周期性故障演练。任职期间所负责产品无故障，无投诉，单月线上服务冒烟数收敛百分之50。 结合阿里云已有公有云产品，基于开源产品实现功能增强（如 Pod 无损升级等） 基于 eBPF 初步实现完整的协议栈级别的链路监控 所负责的产品，完成从0到1的商业化。 参与产品运营，包含客户支持，技术推广等。数次直播 PV 为 BU 内 Top5 2019.02-2019.11 饿了么负责饿了么内部服务治理中心 Huskar 相关开发工作 工作内容： 基于 Zookeeper 构建饿了么内部服务治理中心 维护 Python Zookeeper SDK 并回馈上游 构建周边稳定性服务，维护 Huskar 稳定性 完成上云迁移 工作成果： 在向集团上云迁移过程中，无事故发生 基于 Go 完成保护网关，在特定网络波动导致大规模重连（单机房规模不低于 100k）时，保证后端服务正常运行 2017.02-2018.11 北京闪银科技有限公司负责风控系统基础设施开发及构建。SRE 一号位 工作内容： 负责通用数据请求网关的搭建 负责基础开发框架的封装 构建全链路追踪系统 风控系统稳定性建设 工作成果： 在接手通用数据请求网关的建设后，通过推动升级 runtime ，fix 开源组件等手段，将线上服务异常率从日千分之五降至日十五万分之一，请求时间 PT99 提升50%，内存占用下降30% 通过封装基础脚手架，建立完整 CI/CD 流程，将由人为/低级错误导致的线上故障数从月均一到两次收敛至零 基于 Zipkin 构建 Tracing 系统，封装基础框架，实现调用链路的全覆盖。支持研发人员排查某次异常调用所有周边依赖","link":"/resume/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"","text":"Manjusaka 的每周周报 2022 2023","link":"/weekly/index.html"},{"title":"","text":"2022年2月第一周简报向 @xuanwo 哥哥学习，开始记录每周自己的日常生活 生活本周过年，女朋友不在家，又是自己Hhhhhh 每天按时吃药（换了来士普后爽多了） 焦虑和抑郁控制的不错！ 开始捡起吃灰的 PS5 玩，蜘蛛侠真香 BBC 纪录片《绿色星球》强烈推荐，很多镜头真的很震撼！ 自己做了一顿丰盛的年夜饭！ 复习了 《波西米亚狂想曲》，皇后真的伟大 看了《这个杀手不太冷静》，我能给4.5分 换了一个新的 4K 显示器，96w 闪冲，真的美滋滋！ 读了《文字破解的奥迷》，被人类的智慧所折服 简而言之，这周因为房价的关系，看得东西和生活都比上班时有滋有味许多（ 技术本周我的技术上的精力还是和之前没啥太大区别。不过这周因为闲暇时间的关系，开源和看书进展都不错 开源方面，这周主要做的是一些开发者体验提升相关的工作 给 Logseq 和 taichi 重构了一套全新的 Bug Report Issue Template，利用 GitHub Issue Form Template ，相较于原本的模版，能较好的提升 Bug Report 的体验。 因为不爽 Docker 的一些微操，开始把自己的容器技术栈切换为 lima + nerdctl 的组合，目前的体验其实还好，不过小问题不断，这点稍后会提及 去看了下 containerd remote registry 相关的代码，从我的角度来说，感觉有一些过度抽象的感觉。如果不是要查问题，不建议阅读。当然阅读体验比 moby 好多了 lima + nerdctl 的搭档下，发现了不少的问题，其中一个是 nerdctl 对于 non-https private registry 的支持有点问题（虽然这个也是因为我懒），提了 PR 修。另外一个是发现 lima 的网络在 easyvpn 的情况下有些异常。初步怀疑是 NAT 的问题，不过具体的 root cause 要等下周了。突然感叹下 虽然大家看 docker 不顺眼，但是出了 Linux 这个平台，能替代docker desktop 的还是少之又少 开始继续迭代之前给妹子写的自动回复机器人，被迫开始看一点简单的自然语言的内容（分词，关键词提取） 在阅读方面这周主要是读一本书+三篇论文（含复习） The Art of UNIX Programming 的 Chap5 到 Chap8 ，其中很多观点很符合我胃口，比如关于过度抽象的23333 Ray: A Distributed Framework for Emerging AI Applications OSDI 18的论文，自己封装的一套 Distributed Framework ，虽然不是做 AI 的，但是这篇论文里的一些工程上的思路还是值得学习的。感谢 @gaoce 哥哥的分享 Firecracker: Lightweight Virtualization for Serverless Applications ，偶然想起，复习了下这篇去年看过的 NSDI 20 的论文（当初怎么找到这篇论文的我忘了）。AWS 的工作。自己基于 QEMU 裁剪了一套东西。在 Serverless 越来越流行的今天，对想要做一套私有 runtime 的同学来说有一定参考价值。不过我觉得这篇论文还是以秀肌肉的成份居多。 DBOS: A DBMS-oriented Operating System ，来自 @茄子 哥哥的推荐。我觉得很惊艳的一篇论文。在数据库利用各种 kernel bypass 手段实现高性能服务的今天，我觉得探讨下将完整应用托管在 DB 上还是有很大的意义的。 写作方面，博客一篇，总结了自己20年曾经做的一些当时算比较先进的工作。后面争取有机会开源出来。 简单聊聊在 Linux 内核中的网络质量监控 简单总结一下这周因为过年放假，整体有更多的时间做自己的事情，所以感觉状态比之前好了很多。大概能算没有虚度这个假期？ 不过需要改进的地方也不少，比如专注度，比如写题效率（定期复习）等等。这种开始利用 Trello 来整理自己的一些零零散散的的 idea 并提升自己的时间利用率。 差不多就这样，我们下周见！","link":"/weekly/2022/2022-02-week1.html"},{"title":"","text":"2022年2月第二周简报嘤嘤嘤，这周开始上班了，自己的时间比放假时少了好多，嘤嘤嘤 生活这周开始进入了工作时间，感觉自己的空余时间比之前少了很多，不过还是有一些很有趣的点滴。不过妹子回来陪我了！ 复习了一下之前不甚看好的《罗布奥特曼》，发现实际上这部奥特曼除了变身时间长一点，在设定和剧情上其实比现在许多的新生代好了很多。希望未来不会有我觉得《特利迦奥特曼》比较好看的时间到来。 关注徐州八孩母亲事件，并实名向监察委提交了徐州市政府可能涉嫌渎职的举报，这是关系到我们每一个人的事，每个人都不能选择沉默 开始玩手游使命召唤，M4 用起来真爽啊。 主持了捕蛇者说新春特别活动《lightning talk night》，和大家吹水了三个小时，真的开心 追新的《绿色星球》，大卫爱登堡爵士永远的神。 开始重拾我每天的背单词和英语新闻阅读，英语学习能力还是很重要的！ 我的 2021 年年终总结又只有一点点进度呜呜呜呜 这周没时间做菜，也没怎么出去玩，伐开心。下周争取要出去玩！顺便多读一些技术以外的书籍。 技术这周和春节假期不太一样，业余时间少了不少，顺便还加了一个超长的班。不过还是有一些不小的收获 首先聊聊和工作相关的技术方面我印象里比较深吧 因为工作上的需求，去阅读了一下 WebRTC 上 STUN 的 RFC：RFC5389。 我发现 UDP Media 的场景，在主流的 Kuberntes 的网络模型下支持的都一般。都还是需要一些比较 Trick 的方案（比如 hostNetwork 啊，比如走 CNI 去打通局域网和 Pod 啊）去实现比较好的支持（也有可能是我太菜了）。虽然有一些已有的开源设施在试图去解决这个问题，比如 agones。但是有一说一，agones 我大概看了下源码，实现的非常拉跨。从一个 SRE 的视角来看，如果你要在生产去用这货，基本意味着你需要额外的裹不少东西。贴一个我的爆论在下面 这周我们新增的一个域名出现了 Global 范围内，部分地区的运营商解析失效的问题。后面发现是某云企业版 DNS 给域名默认添加了一个 DNSSEC 相关的 Record 导致部分运营商校验失败。所以系统的看了下 DNSSEC 相关的四个 RFC：RFC2535/RFC4033/RFC4034/RFC4035。这里不得不吐槽下，某云和一些运营商不讲武德！ 然后自己的 side project 这方面： nerdctl 我的 PR776 还没合并进去。因为特殊的 CI 流程，会导致我的一些 test case 会偶发挂掉（damn 在想 nerdctl 的 Issue-759 这个 Windows 挂载路径的问题怎么解能比较优雅。自己写了一版，但是觉得太拉跨了。以及 Windows Style Path suck！ 给女朋友搞的小组自动回复机器人 mock reply 一周后准备上了。不过上之前我可能还得做一次二次分词，以及人工过滤一些不合适的东西。 技术阅读方面，感觉这周进度一般。没有新读什么 Paper。继续啃 The Art of UNIX Programming 这本书。 简单总结一下因为这周刚刚开始工作的原因，所有年前说年后再做的事如潮水般涌来.jpg，所以相对而言比春节做的事少了不少。不过感觉进度可以再后面补回来 差不多就这样，我们下周见！","link":"/weekly/2022/2022-02-week2.html"},{"title":"","text":"2022年2月第三周简报嘤嘤嘤，本周是被嫌弃的一周，挠头（开个玩笑 生活进入工作后我最不爽的一点就是每天单程1个半小时的车程。不过最近在开始有效利用这段时间。感觉可以更有效率一点，不过不知道会不会更累 《罗布奥特曼》补完了，香，比扳机超人香 开始入坑隔壁来打。不比圆谷卖玩具卖的良心？ 这周和两年没见的好友夫妻俩约饭，聊得贼开心 《绿色星球》完结了，不开心。 徐州八链女事件进一步发酵，抽时间补了《古老的罪恶：全国妇女大拐卖纪实》一书。触目惊心。 周末和妹子一起出去买了花，春天的气息会让生活更美好。 给资助的学生转了新学期的资助 这周因为工作上一些事加上各种🇻🇳阴间新闻，导致心情不是很好。不过这日子还是得过.jpg 技术这周还是一个标准的工作周，业余时间不算太多，而且零零散散有点加班。但是也还是有一些奇奇怪怪的收获 首先聊聊和工作相关的技术方面我印象里比较深吧 到目前为止，我的一部分工作的重头还是在和 WebRTC 这周 UDP 流量抗争的过程（我的一生，是与 UDP 抗争的一生（逃）。这周一个不错的进展是，在外挂了一个 coturn 来作为 TURN UDP Proxy 后。对于局域网内的 Local Cluster，不用考虑比较 trick 的方案去解决这个问题了。但是 coturn 本身也有一些问题。比如多 replica 情况下的负载均衡，以及 TURN/STUN 的流量调度按照 RFC 所定义的行为，是个有状态流量调度（UDP+有状态 UDP 调度，啊，我死了）（虽然 coturn 也支持后面挂 DB，但是还是有些坑）。如果在公有云上的方案来说，如果所采购的云服务商有 Pod 和子网二层打通的方案的话，我还说更倾向于尽可能少用 TURN。然后 STUN 来搞一下（好像现在不少云厂商都有类似的方案，比如阿里云的 Terway，AWS 的 VPC CNI amazon-vpc-cni-k8s) 以及千万不要信任 Prometheus Operator 起的 Grafana，要自己做持久化。。说起来就是泪，Scheiße！ 还是聊聊自己的 side project： 给女朋友搞的小组自动回复机器人在 mock reply 一周之后上线了，效果还不错。调整了下分词的粒度，不过互联网的各种敏感词真的多啊。 nerdctl PR776 终于合并进去了，对于 private registry 的兼容提升了很多 nerdctl 最近我在做一个比较有趣的工作，PR824 让 nerdctl 支持一个和 Docker 一样的行为，即用户没有指定 hostPort 的情况下，自动分配一组 hostPort，挺简单的一个功能，但是因为 nerdctl 的 allocate port 动作是在 CLI 做的而不是和 Docker 一样是一个 daemon process ，所以为了提升分配成功的概率，以及跨平台兼容，这里会稍微绕一点。这里感觉有个很好的面试题可以用于社招: 实现一个函数 func allocate(protocol string, count int) ([]int,error)，返回可用的连续的端口号，如果没有则返回 Error。这个题的扩展性很强 年前的一个 Kubernetes 的 PR107531 被催进度了。大概就是之前 K8$ 没有对 labelSelector 的 label value 做格式校验，导致一定情况下在 schedule 的时候会 panic。年前的时候先修这个问题做了个 PR107558 提供了个 nil check 先紧急合并到 v1.20.5 了。但是根治这个问题，还是需要加上额外的 label value 校验。但是这里就会引申出一个新的问题，你需要对之前已经存在的 Object 做兼容，否则一个已经存在带有错误 label value 的 Resource Object 升级之后会导致额外的问题。所以这周加了一些东西。苦力活 这周关注了一下前端包管理器的一个很有意思的工作，来自蚂蚁的 tnpm rapid 工作，写个自己的观点，国外开发者是如何看待蚂蚁 tnpm rapid 包安装加速方案？pnpm 等有可能跟进么？。这里顺便推荐一篇 FAST17 的一篇论文 To FUSE or Not to FUSE: Performance ofUser-Space File Systems。以及最后去给天猪他们的 cnpm 水了 PR 去看了下 netfilter 以及 conntrack 最新的一些实现，发现内核这块的迭代还是挺快的 阅读方面，除了正常阅读 The Art of UNIX Programming 以外，这周的一个小惊喜来自友人推荐的一篇 NDSS22 的论文 KASPER: Scanning for Generalized Transient Execution Gadgets in the Linux Kernel，介绍了现代处理器上 Spectre 漏洞的一些常见利用手法，并提出了攻击建模和提前防范的思路。因为之前不太做这块，所以读的比较吃力，但是我觉得是比较 sexy 的一篇文章。下周读完应该能水一篇博客了。 简单总结一下这周因为各种原因，负能量有点多。也经常会感到不同程度的挫败感。终归可能是我自己还是太菜了。不过但行好事，莫问前程吧。 最后还是好想成为一个和很多人优秀的人一样，温柔，坚定，谦逊而散发出自己光芒的优秀的人啊。。（真的很羡慕","link":"/weekly/2022/2022-02-week3.html"},{"title":"","text":"2022年2月第四周简报Life’s a struggle, 日子还要过。数不清的喜怒哀乐背后，又是数不清的 trouble — 宋岳庭《Life’s a struggle》 生活这周状态有点不太好，焦虑，失眠并存。感觉需要再去看看医生看看需不需要换药了 假面超人比奥特曼香，来打卖玩具还是比奥系列强 和女朋友开始看《甄嬛传》，感觉算是宫斗剧的巅峰。请叫我 Saka 大人！（画风不太对 和网友 F 叔开始约饭，发现互联网圈子真的小。 买了老人环，但是感觉画风有点不太适应。 我妈突然给我打电话问我是不是缺钱了（背景是我过年的时候先给了个小红包，大红包准等我回家的时候给她，然后我妈开始担心了），还是要为爱你的人好好的活着。 收到了 Piglei 大叔的 《Python工匠：案例、技巧与工程实践》，好书，有幸在草稿期就一睹真容，非常推荐。个人认为这应该是这两年里我最眼前一亮的 Python 中文书籍。 收到了 Nebula 的抱枕和 T ，我家猫很喜欢 在女朋友要求下带猫猫去看病，医生说没啥问题，就是胖的啦（小猫咪懂什么嘛 技术一个标准的工作周（我啥时才能放假呜呜呜呜），还是一些奇奇怪怪的收获 老规矩，先聊聊工作里印象比较深的 本周里遇到一个容器里老生常谈的问题 CPU 资源隔离，我们有一个场景需要进行 H.264 视频编码这样的操作。属于是典型 CPU 计算密集型的应用。在 K8S 这种场景里，将 CPU 的资源的 request 转化为 cpu.shares， 将 limit 转化为 cpu.cfs_period_us 与 cpu.cfs_quota_us。 这种分配对于大部分场景来说是 work 的，但是对于一些密集吃 CPU 的场景来说。将因为相关限制，频繁的触发 throttle，代价就是会导致用户的体感会有明显的波动。所以目前来说可能最好的方法还是尽可能用 cpu.shares 来做一些高性能场景的 CPU 策略，再极端一点的场景，可能就得修改 kubelet 的参数，让其支持绑核的操作。这里要夸一下 CMGS 他们做的 eru2 了，在容器资源隔离上面，我觉得做的还是比 K8S 有特色的。 继续聊聊自己的业余时间： 上周做的 nerdctl 自动分配 HostPort 的 PR824 这周有了新的进展。和维护者讨论后决定先不考虑 non-Linux 平台了。然后这周就是在不断的补测试与调测试之间徘徊。也借此机会学习了一下 QEMU 相关的的东西（因为做 integration test 需要这货）。以及写测试真头疼。这个 PR 后续的修复空间还不少，比如我目前走的是 Linux 下的 /proc 文件系统去拿的网络相关的数据，而内核已经明确表示网络相关的 /proc 不再推荐使用。可能要等 PR824 合并进去后找个机会重构到 netlink 的方案。 Kubernetes 的 PR107531 这周犯了点傻逼的错误，要认真阅读 review 意见啊喂，给自己个教训。 这周为了提升自己每天刷题效率，开始定期复习之前刷过的比较经典的题，印象比较深的是 Leetcode-1675 这周吃了网友安利，入了 wezterm 的坑。体验下来，很戳我痛点。Lua 管理配置太爽了。以及提了一个不成功的 PRPR1670（我的第一个 Rust 的 PR），不过作者是对得，路径处理还是比我预估的要头疼一些（跨平台真恶心） 这周开始整理我自己的各种 dot file 的配置，扔进 Repo 统一管理 这周开始我的一个 Side Project: agul-init 的设计与开发了，和朋友讨论后觉得这个项目还是能解决一部分的痛点。主要针对于大部分没有 K8S 等容器编排机制的情况下，在提供一个比 tini 更完备的1号进程管理能力的同时，能提供一部分容器编排系统提供的功能，我会在下面列一个简单的第一期的功能清单。这个项目的取名是我很喜欢的一个奥特曼，中文名是阿古茹（外号逼王）。之前用 Python 写过一个 PoC，不过考虑到容器环境，准备用 Rust 重构一个小版本出来。下面是计划的功能清单： 完整的全功能的1号进程 能正确的实现信号的转发工作 能确保信号转发的可靠性，(比如孙进程跨 ProcessGroup 的信号转发） 在容器环境下，能承担起当前 pid namespace 范围内进程管理的工作 能整合一部分 container 生命周期的管理 prestart 与 poststop 的 hook poststop 能透传对应的 exit code healthcheck 与 readiness check 的能力 阅读方面，这周复习了一下 2019 年一篇经典论文《Cloud Programming Simplified: A Berkeley View on Serverless Computing》以及看了下 Berkeley 2021 年新的观点 《What Serverless Computing Is and Should Become: The Next Phase of Cloud Computing》。 怎么说，Serverless 和传统的 SaaS 这样服务的概念边界并不清晰，感觉一个事物从概念归属上是有同时归属两者的可能性。不过我觉得判定一个事物是否属于 Serverless 范畴的一个核心要素是去判断算力是否是作为核心价值的一部分提供出来。改天水个文章继续聊聊 简单总结一下这周各种不太好的状态奔涌而来，有点周报最开始提到的歌词的味道了。不过，日子还得过 但行好事，莫问前程。","link":"/weekly/2022/2022-02-week4.html"},{"title":"","text":"2022年3月第一周简报三月啦，春暖花开啦，我的状态也要尽快好起来的！（虽然还是要不断的吃药吃药。。 生活每天一个半小时的车程让我属实有点抗不住，不过我也没有荒度！ 看了群友安利的《千万别抬头》，好看，非常推荐 继续和女朋友看 《甄嬛传》，比我想的好看！ 和公司玩的好的两个同事一起吃了火锅！聊的很开心（不过他俩居然说我是很讨人喜欢的社牛（什么鬼啦！ 我爸给我了个惊喜，想捐赠遗体。惊讶之余还有点感动（不是一家人不进一个门（我很早之前就签了协议了 这周和女朋友吃了超好吃的日式烤肉！不过为啥现在各种店都要喊着奇奇怪怪的口号 抽烟没给女朋友报备被抓包，被罚拼一千片的拼图，呜呜呜呜 给猫猫种了新的猫草！ 这周因为老毛病口嗨无度被密友骂了，有这样的朋友挺好。 这周我的公益计划，新增了6位捐款者，开心，能带动身边的人！ 出去逛了逛公园，但是忘了买风筝呜呜呜呜 不过这周发现自己的英语词汇量和阅读能力有点下降。下周准备恢复每日的英语课程 技术一个标准的工作周（清明还有一个月，呜呜呜），还是一些奇奇怪怪的收获 老规矩，先聊聊工作里印象比较深的 半夜 CI 挂了起来修，发现一个很好玩的问题，关于 Docker Inc. 现在在主推一个全新的 Dockerfile 构建工具 buildkit 。首先说说优点，并行化构建，对于大型 Dockerfile 的提速的确很大。不过这周我们踩到了两个问题，一个是在 copy 命令下，如果目标目录有重名文件，老命令的行为是直接覆盖，buildkit 默认会报错（这个我觉得设计上是 make sense 的，但是终归是存在了 broken change ）。一个是 buildkit 对于 insecure-registry 默认支持有问题（准确说需要声明 http:// or https:// ），如果构建的时候依赖了不存在的 plain-http registry 里的镜像，那么在拉取的时候会报错（社区这些奇奇怪怪的问题总是这么多） 这周踩了驱动的一个坑，在 Linux 5.13 下，realtek R8168 系列的网卡依赖内核默认驱动的时候，一些 UDP 的场景下会出现吞吐问题。Linux 升到 516 以上问题解决。或者手动从官方下载 8.049.02 的驱动，自行编译安装 继续聊聊自己的业余时间： 上上周做的 nerdctl 自动分配 HostPort 的 PR824，在经过两轮补测试后，这周算是得到结果了，会随着 v0.18.0 发布 Kubernetes 的 PR107531 ，和 Reviewer 讨论了一番后，还是按照 Reviewer 的意见来处理。不过补全 corner case 的兼容性代码有点蛋疼，这周花了我两个晚上去设计 corner case 的处理路径。 这周有很大一部分精力放在了去推 Linux 514 一个新特性的落地。在 Linux 514 之后，内核 CPU 相关的 CFS 调度新增了一组 burst mode（来自阿里云的工作）。允许用户设定一个值，然后应用在平时空闲期积累一定的 credit，然后在突发性能需求的情况下，可以消耗平时积累的 credit，减少 throttle 的频率，详细介绍可以参见 CFS Bandwidth Control。这一点对于容器以及 K8S Burstable Pod 是个很大的利好。开始看 Intel 的同学提出的 Burst Spec。 后面如果这个 Spec 正式被接收以及进了 runc 的话，我应该会着手将这个特性移植到 containerd 和 nerdctl 中。 这周和一个同事聊了下我关于 Burst Mode 进 K8S 的想法，在具体的语义设计上我俩有点分歧2333，不过先等 containerd 进主分支后，我再正式提一个 KEP 吧。改天可以写个文章聊聊我和同事的一些观点碰撞，还挺有意思的 这周被人出了一个 Hard 的算法题，leetcode-1808 我现在都还有点没想清楚呜呜呜呜（数学我一生之敌）。 这周复习了一下 JVM 调优的相关玩法，给我的 Jetbrains 系 IDE 调整了下参数，切到 zgc 之类的，我和朋友都觉得体验还不错，参见 goland config 这周帮一个朋友排查了 K8$ 上 HPA 不生效的问题。然后发现 Root Cause 是他们的 SRE 在写 helm charts 的时候，将 StatefulSet 的 LabelSelector 和 Cronjobs 的 Pod Label 复用了同一组值。emmmmm，大家要吸取教训。一定要正确使用 Label ，不然线上突然出问题不是说说的 (SREの怨念) 阅读方面，这周一方面因为仔细看 BurstMode 的缘故，系统的看了下 cgroupv1/v2 的文档以及一些关键讨论。补了一些之前忽略的细节。以及整理上周看的 《What Serverless Computing Is and Should Become: The Next Phase of Cloud Computing》 的笔记，整理笔记过程中还有些挺有趣的思考（比如 现代 Serverless 一个很重要的发展方向就是多租户的隔离性与安全性。而现在攻击手段也越来越多元化和底层话，比如上上周介绍的 KASPER: Scanning for Generalized Transient Execution Gadgets in the Linux Kernel 一文中提到的幽灵攻击的手法，估计也应该是个切入点）（说回来我在想要不要把我的一些笔记和碎碎念也一起放在博客里2333） 嗯这周差不多这样，下周开始要看书了（ 简单总结一如既往的彻夜难眠，下周又该去复查了。不过生活会好起来的，是吧！ Everything is gonna be OK","link":"/weekly/2022/2022-03-week1.html"},{"title":"","text":"2022年3月第二周简报疫情反复，大家多保重呀。 生活每天还是在不断的奔波，不过生活感觉彩色元素越来越多 开始看美剧《太空部队》第一季，好看，里面好多梗笑死我了 换了新的发型，被女朋友吐槽之前的头发太长了 还是去吃了超好吃的日式烤肉！ 给我妈换了新的电脑，我妈笑着埋怨我的样子好好玩 上周被女朋友惩罚的拼图我没有拼（逃 女朋友给买了新的衣服！开心！我妹子笑称我从头到脚基本都是她买的了Hhhhhh 药吃没了去复诊拿药，好像状态没再恶化了，好消息！ 重新开始每日背单词了 技术一个标准的工作周（清明还有0.75月，呜呜呜），还是一些奇奇怪怪的收获 老规矩，先聊聊工作里印象比较深的 self-hosted 的 Sentry 一周崩了几次。整体评估下来是之前用的太浪了。每次 CI/CD 的 native symbol 都会上传。而且一些项目存在滥用 Sentry 的情况。系统性的反思了一下可观测性系统的一些治理的思路。整体来说还是要尽可能保证数据的有效性。 这周关注了 CVE-2022-0847，原理参见 The Dirty Pipe Vulnerability。这是个跨版本（Linux &gt; 5.8）的注入漏洞。由于 taichi 项目因为图形学设施的关系，需要 self-hosted runner，所以需要关注一下。不过我们很早之前就已经完成了容器化，以及在我入职的时候做了网络和宿主机加固，所以爆炸半径可控。不过有消息称这个洞存在非特权容器下提权的利用手法，估计要等进一步的 PoC 来做加固 写一些跨 macOS 和 Linux 的 bash 脚本。GNU 和 non-GNU 的差别真的很大，damn 继续聊聊自己的业余时间： 上上上周做的 nerdctl 自动分配 HostPort 的 PR824，经过三周的重构和补测试后，合并进入主分支了。不过这周又新踩了 rootlees container 的坑。简而言之就是为了 rootless container 能够比较好的做好网络隔离。需要一些 trick 的手段来保证网络的隔离性。这导致我实现新特性的时候没法正确的从宿主空间拿到特定的网络信息。虽然 rootless container 的确有其存在的价值，不过目前来说，我还是建议大家保持观望 因为 nerdctl 的 ISSUE-879 的关系，了解到了 Go 1.17 新出的一个 tag format，感觉还挺有趣的 Intel 的同学提出的 Burst Spec 这周拿到了一个 LGTM，下周会继续跟进一下。希望能尽早在 runc 上引入 这周开始给 nerdctl 做一个新的功能，支持 --ip 参数指定容器 IP（和 docker 保持兼容），参见 PR896。这个 PR 写的我肝火有点旺。因为 containerd 严重依赖 CNI ，所以这需要在 CNI 上做手脚。然后发现 CNI 官方的一些插件，比如 host-local 的实现和官方的 SPEC 对不上，以及官方的 go-cni 实现的也很灼急。。。。。查了我一晚上。什么叫云原生啊（战术后仰。不过周末好歹跑通了全部流程，下周开始写测试。 修了 nerdctl 一个历史遗留问题：默认的登录行为对 plain-http registry 有点问题，参见 PR894，又是代码半小时，补测试一整天的经历。顺便学习了一下 nerdctl 的测试手法，收获不少（比如 nerdctl 为了保证和 docker 的兼容性，会有一组 test-compatibility 的测试，来测试自己的 integration test 是有效测试） 这周开了一个新坑，本人是 logseq 的重度用户。目前 logseq 在 Ubuntu 之类发行版下使用不太方便。给官方提了一个 Issue4527 ，尝试增加 snap 的支持。不过看了下目前 logseq 用的是 gulp 打包，稍微有点棘手。下周争取做完。 阅读方面，这周一方面因为填 nerdctl 坑的圆谷，系统的看了下 CNI 的文档以及一些关键讨论。补了一些之前忽略的细节。新开的坑就是看经典书目 《大教堂与集市》。有点向往与仰慕曾经那个众星云集的田园牧歌时代（逃 简单总结现在越来越多的人公开自己每天的开源贡献，分享每天学到的新东西，真好。用 JinaAI CTO 王楠老师在我朋友圈的评论来说就是：开”卷“有益，内卷伤身！","link":"/weekly/2022/2022-03-week2.html"},{"title":"","text":"2022年3月第三周简报这疫情怎么越来越严重了，烦躁。。我好想出去玩啊啊啊啊啊 生活 继续看美剧《太空部队》第一季，真的好看！ 这周开始去看一些宠物视频，狗狗真好！想和女朋友养狗狗！ 看着女朋友打老头环，算了算了，我这种手残果然不适合。还是想组个 3090 台式机打 CS：GO 看了很多 Mac Studio 的评测，糟糕，是心动的感觉，想一步到位 128G 和同事，以及和女朋友吃了南京大排档，这家店得到四省人民一致好评 家里的柠檬树开花了！ 偶然又梦到和过世的老友吃饭，Fuck… 焦虑还在继续，不过 Everything is gonna be OK。 技术离清明还有 0.5 月，大概，聊聊一些新的收获 老规矩，先聊聊工作里印象比较深的 这周合并代码让我吐了都，多人开发。。深感自己 Git 功力不足。滚回去背诵 Pro Git 了 上周提到的 Linux CVE-2022-0847 如同之前拿到的消息一样。研究者利用这个洞复活了 runc 经典逃逸漏洞 CVE-2019-5736， 能实现特定的 payload 完成非特权容器下非 root 用户的提权与逃逸，参见这篇文章。因为 Taichi 的 Repo 的 CI/CD 需要依赖特殊的图形学基础设施，因此依赖 self-hosted runner。因此这个洞对于我们存在一定的危险性。所幸虽然我们依赖的 513 内核在 Upstream 没有修复，不过 Ubuntu Security 团队给出了 5.13.0-35.40 这个修复后的内核。参见这里。不过开源状态下的 self hosted runner ，安全治理一直是个很头疼的问题。这里列一下我之前的一些随笔： 合理的利用 GitHub PR 本身的比如 First Time Contributor 的限制，在一些关键时间（比如第一次给仓库提交代码的用户发起的 PR，需要等待 Approve to Run Test 的信号）加入限制，来增加攻击者潜在的攻击成本 合理的隔离 Self Hosted Runner 和日常办公网之间的一些网络拓扑，控制爆炸面 在宿主机上不要明文存储任何敏感信息 提供给 Self Hosted Runner 的 PAT(Personal Access Token)，尽可能控制权限粒度以及做好 Token 的获取审计。确保 token 遗失后能快速废弃与溯源。 即便有 Secrect 这样的机制，也尽可能不要在公开的 Run 流程中依赖一些 SSH_KEY/SSH_TOKEN 之类的极端敏感的信息。否则攻击者还是有可能进行嗅探与攻击 这周同时在做一些私有 Pypi repo 建设的工作，不得不感叹一下，Python 好用的 Pypi 实现真不多啊（比如原生支持 S3/OSS，原生支持接入云 CDN（没 CDN 拦着能被恶意打爆好嘛！） 继续聊聊自己的业余时间： 上周修的 nerdctl 一个历史遗留问题：默认的登录行为对 plain-http registry 有点问题，参见 PR894，这周合并进去了。当然还是额外补了一些测试。 上周写的我肝活很旺的 nerdctl 的 PR PR896（支持 --ip 参数指定容器 IP（和 docker 保持兼容））。这周花了两天测完，写了 e2e,开始接受 Review。不过受限于 rootless network 的问题，目前这个功能暂时只能支持 non-rootless mode。这个 PR 预计带在 0.18.0 里发布。等这个 PR 合并完后，我就可以开始做 Issue852 了，即实现 docker-compose 里面指定容器 ip 的支持。再往后的计划就是支持 rootless mode 以及支持 ipv6（不过这个得先等我重构一波 nerdctl 的网络部分再说） 再去看了下 go-cni 的实现，叹为观止。云原生，云原生。 因为折腾 rootless mode，去看了下 slirp4netns 一个用户态网络隔离方案的实现。有点意思，有些新的想法，不过以后再拿出来聊 上周开了一个新坑，帮 logseq 实现 Snap 的支持，参见 Issue4527。这周准备了一个小的基于 Gulp.js 的 PoC，不过受限于本人前端苦手的关系，进展不算太大。要加速了啊！ 阅读方面：首先来说，为了加强我的 Git 技能，去仔细阅读了 Pro Git，受益匪浅。以及上周开的坑，《大教堂与集市》，老书新读，读得我酣畅淋漓。不愧是开源圣经。 简单总结本周有点想偷懒，就不太写总结了。反正，一起努力的活着吧！","link":"/weekly/2022/2022-03-week3.html"},{"title":"","text":"2022年3月第四周简报啊，三月终于到月底了，清明还会远吗？ 生活每天继续的奔波，这周不过比上周更好了一些23333 这周各种各样的事情没有继续看《太空部队》（sign 想养狗狗的第 N 天 这周继续手游使命召唤，把 SVD 练出来了，美滋滋 这周的周末和女朋友伙食改善是太二酸菜鱼（这次没图 被女朋友吐槽衣品太单调了（ 去玉渊潭踏青，拍了好多好看的花花！ 做了很多奇奇怪怪的梦（坏消息是梦到了我的死亡，小时候被性侵的经历，父母离异等不好的事，好消息是我死亡的时候女朋友，好友都在身边，我捐献遗体的遗愿也得到了执行），很清晰，很身临其境，不知是好是坏。不过死生之外无大事。 和女朋友吃了好吃的小龙虾！（当然是我给她剥（ 开始在薄荷上阅读英文书籍，这段时间是《当你像鸟飞往你的山》，英语能力退化了不少，要加油了 技术下周要放清明了！开心！还是聊聊这周的进步吧 老规矩，先聊聊工作里的印象深的 这周的时间是和内核奋斗的一周，CUDA 11.4 要求锁的内核 470.57.02 和 Linux 5.13 有一定冲突，遇到了不少玄学问题。蛋疼，Fuck you Nvidia.jpg 这周和同事关注到了一个 infra 细分领域的开源项目，infracost。在内部基于 Terraform 做好 IaC 之后，这个工具能很好的弥补账单估算这样一个痛点，不得不感叹国外软件生态真健康 继续聊聊自己的业余时间： 上上周写的我肝活很旺的 nerdctl 的 PR PR896（支持 --ip 参数指定容器 IP（和 docker 保持兼容）），终于合并进去了，赶上 v0.18.0 的发版了 在 PR PR896 合并进去之后，支持 docker-compose 中指定容器的 IP Address 支持就顺利成章了，这周写了 PR926 ，Suda 大佬非常给力，2h 就合了，赶上了 v0.18.0 的发版了 这周开始在处理 nerdctl 和 docker-compose 的兼容的问题，其中一个是 Issue666 ，社区希望 nerdctl compose 的启动和停止希望能和 docker-compose 的行为保持一致。说实话有点不太好搞。不过这周有了点新的思路。看代码的时候顺便调整了一个 nerdctl compose orphan container 的行为，使其和 docker-compose 保持一致，参见 PR942 Kubernetes 的 PR107531 ，这周处理了一下一些低级错误（我猜应该能很快合并了（ OSDI 2022 论文清单公布了，看了下，下面几篇应该是会比较感兴趣（我对 ML System 实在不感兴趣（： Verifying the DaisyNFS concurrent and crash-safe file system with sequential reasoning. UPGRADVISOR: Early Adopting Dependency Updates Using Production Traces. Transcendent Debugging the OmniTable Way XRP: In-Kernel Storage Functions with eBPF 尝试了朋友新推荐的 starship 好用！ 阅读方面这周因为状态不太好，划水周，进度有限，所以不列出来丢脸了（逃 这周看到了 infracost 这样足够细分的产品出来。突然很感叹国外软件生态和国内已经是完全两个次元了，这个产品，其定位很明确，就是解决 IaC 后内部成本管控的问题，目前已经拿到了 $1M 的天使轮融资。国外基础软件的标准化与各大厂商的标准化，给这些细分领域留了足够的发展空间。想想也挺让人唏嘘。BTW 这周有个彩蛋：我在推特上推广了 Infracost 后，得到了两位创始人的的回复（其中一位好可爱Hhhhh 不过 Infracost 目前在本土化以及和插件化上还有一些缺陷，后续我应该会投入一些业余精力去帮社区共建 简单总结很讨厌状态的波动，这周算是划水周，不过可能劳逸结合才能可持续发展。嗯，继续努力过下去吧","link":"/weekly/2022/2022-03-week4.html"},{"title":"","text":"2022年3月第五周简报这周我也不知道是算是三月第五周还是四月第一周，不过先按照四月第一周来算！清明放假啦！ 生活 这周开的新的（复习）小说坑，大眼珠子的《异常生物见闻录》！ 这周一周没出门，不用通勤的感觉真好 女朋友准备去给我买新的裤子（但是伦家更喜欢迷彩嘛） 本周的伙食改善是巴奴毛肚火锅！啊，乌鱼片是我的爱！ 本周时间多了很多很多，开始看一些杂书，收获很多 家里的植物开花的不错，真好 周末的天气真好，和女朋友出门的时候，阳光暖洋洋的，很舒服 本周开始健身环！被女朋友“嘲笑”了，我太菜了，呜呜呜 看到了我捐助的女学生的近况，真好 买了新的 iPad Pro ！ 这周看了《病人家属，请来一下》，非常感动，突然想起孙思邈《大医精诚》中所言 凡大医治病，必当安神定志，无欲无求，先发大慈恻隐之心，誓愿普救含灵之苦。 在疾病面前，病人，家属，医生是天然的战友。我也更坚定了如果逝去，我一定要将我的遗体进行捐献。要是能培养出一个攻克某个疾病的医生，我也不枉来世上一回！ 安得广厦千万间 大庇天下寒士俱欢颜 技术这周开始修养期，时间多了不少，但是有意识的控制了自己在技术上投入的精力，不过收获也还不少 PR942 这个小的修复进入了主分支 在使用 bytebase 这个新生的数据库管理工具的时候，发现了 nerdctl 的一个问题，不支持 --init 参数，参见 Issue946 ，写了一个 PR PR948 去做这个事，在 Review 过程中，收获不少 之前我在 CI/CD 中依赖外部二进制的时候，并没有 checksum 的习惯，这其实不太妥当 这个 PR 实际上也是从20行代码发展到快两百行的。原因最开始是我认为这算是一个简单的功能迁移，所以只简单的加了一点 e2e 的测试确保能正常的run。但是后续经过作者的讨论后，我发现，实际上从我们自己的角度出发，添加了 --init 参数后，我们实际上是 hook 了 exec 初始进程。我们需要额外的一些 test case 来保证信号转发的功能是正常的 work 。所以我额外构造了一组算是巧妙的 case 完成了这一测试，详情参见 PR 这周在关注容器热迁移的方面的工作，尝试给 nerdctl 添加 checkpoint 的支持，参见 Issue956 ，实际上 PR 已经写完了，但是吧，但是吧，CRIU 这个项目，一直处于不稳定的状态，比如 CRIU 在 Linux 5.13 下，对容器内的 mount 点的 checkpoint 会有问题。参见 Issue860。 我没太大把握能让这个功能稳定的跑完 CI。跑不过 CI 的功能意义也不算大，所以先暂时放弃了 与此对应，Kubernetes 社区的朋友也在做 checkpoint 的支持。参见 PR104907 ，但是吧，外部依赖太不稳定了，我估计这个功能短时间内都无法被大规模的使用 帮一个朋友给他的朋友做了一点渗透测试，提了一些安全加固建议。实际上对于一些在 PoC 期的产品来说，大概做好以下一些常见防护其实就能规避很多问题 对于用户的 API 的操作，需要有着限速，黑名单功能（比如验证码提交接口，如果不加防护，那么可以直接爆破） 对于所依赖的付费资源，需要在业务逻辑上做好限制使用的防护 对于图片，文字之类且带有社交分享功能的产品，需要做好内容安全防护（比如黄图 etc） 最好不要将自己服务器的裸 IP 地址暴露在公网，最好充分利用 CDN 所带来的能力 这周吃 Prometheus 的坑，日经问题之 bucket 设置的不合理导致 histogram 不准，以及日经问题之 Prometheus 集群方案哪家强Hhhhh 这周因为对于 Web3 有点兴趣，现在也是合适的时候去了解一下，所以开了新坑《精通以太坊：开发智能合约和去中心化应用》这本书，写的很不错。非常推荐Hhhhh 这周另外还发生一些事，具体可以看我下面的吐槽 感慨还是很多的。可能我们无法改变这个世界，但是我希望我们自己还是能不忘记 open-source 给我们带来的最纯粹的感动，不要让劣币驱逐良币的现象进一步恶化下去。。 简单总结可能春天来了的关系，情绪始终在波动。不过该做的事好像也都在做了。反正不虚度下去，那就挺好。未来？等未来再说吧","link":"/weekly/2022/2022-04-week1.html"},{"title":"","text":"20224月第二周简报这周继续在家，上海的朋友们多保重 生活 这周把异常生物见闻录看完了，大眼珠子的书真的是欲罢不能 继续一周没咋出门，本来周末想去颐和园逛逛，不过因事取消了 本周偷偷抽烟又被女朋友抓了现行呜呜呜呜 春天来啦，感觉气候越来越舒服了，下周要多出去走走！ 继续健身环的一周，还是太菜了 这周改善伙食是潮汕牛肉火锅。望京那边那家和记味道不行了，大家注意避雷 这周将大部分闲暇时间用在看杂书上，看得很舒服 下周准备重拾我的射箭计划了 这周去复查了，医生觉得我有点双相的躁郁相的情况，给加了新的镇静类药物。 被女朋友要求限制喝零度的数量了呜呜呜呜 刷题公益双月结算 本周的读书清单 法治的细节–罗翔 病人家属，请来一下 元素周期表何以解释一切 量子和粒子物理学何以解释一切 宇宙已知和未知的一切 后面这三本是同一个作者，这个作者奇奇怪怪的冷门的幽默实在太治愈我了 我们无知，所以我们读书，我们读书，所以越承认自己的无知 技术这周在调整期，我基本把我的业余时间放在了非技术的东西上，不过收获也还不错 这周花了一些时间在 Terraform 上，Infra 领域的 IaC 我觉得是非常重要的。不过相较于 Terraform ，我更喜欢 Pulumi 的设计风格与思路。其实原因很简单，Terraform 所依赖的 Hashcorp 自行定义的一套 DSL（他们称作 HCL），诚然，这套 HCL 的基建发展到目前无论是完备性还是稳定程度已经是相对不错。但是单独的 DSL 始终无法像图灵完备的编程语言一样拥有更高的可定制性以及可治理性。所以我更看好 Pulumi 的发展未来。能够让 SRE 团队实现一次 IaC，所有人都能参与进来共建的目标 这周受朋友邀请，在思考一些关于前端可观测性设施建立的实现。有些初步的想法，可以下周来聊聊 容器方面，排查了 nerdctl 的一个 stdin 的问题，参见 Issue973 ，本质上来说，是对于 stdin 的处理出现了偏差，等后续 PR 合并进去就好了 和 @Junnplus 讨论了一下 PR948 里面的一些设计，重新改良了一下语义上的设计，让 API 更易用了一点 继续搂了一眼 CRIU 的实现，容器热迁移任重道远啊 之前搞 Traefik 的时候，踩了 Let’s Encrypt 的坑，这周重看了下 Let’s Encrypt 用 TLS 实现域名所有权验证的 ALPN 的 RFC7301 以及自动 Renew 证书依赖的 ACME 的 RFC8555。 BTW RFC7301 是个很有趣的 TLS Extension，目前主流云厂商做的非备案域名非标端口拦截的特征点之一就是这个 RFC。 帮一个开源项目想了一下他们 CI 的安全设计。实际上对于 self-hosted 的场景来讲，一般主要的思路有这样的一些 合理利用安全组，限制出方向的流量 Token 分发遵循最小权限原则 合理和办公网段进行分割 这周，NSDI2022 开始了，过了一眼，我自己挑了三篇感兴趣的来读 Evolvable Network Telemetry at Facebook Configanator: A Data-driven Approach to Improving CDN Performance. Cocktail: A Multidimensional Optimization for Model Serving in Cloud 这周看了一半 Evolvable Network Telemetry at Facebook 这篇，里面很多实现思路很有趣，也很有参考价值，下周读完写个简评。 以及这周和引证老师讨论了一个很好玩的题 12345678910for (int i = 1; i &lt;= n; ++i) { for (int j = i; j &lt;= n; j &lt;&lt;= 1) { // O(1) computing }}这个时间复杂度是多少A. O(n)B. O(n log n)C. O(n log^2 n)D. O(n^2) 差不多就这样 简单总结虽然又加了一点新的药，不过还是不能讳医，每天吃药看书，还是要让自己的时间更有意义。","link":"/weekly/2022/2022-04-week2.html"},{"title":"","text":"20224月第三周简报这周继续在家，上海这什么爱丁堡剧情 生活 本周小说书慌了，有没有小说推荐啊 继续窝在家一周，太爽了 本周天气太好了，和女朋友去了朝阳公园，拍了超级超级好看的花花 继续健身环一周，膝盖不行，我太菜了 和女朋友尝试攀岩初体验，好爽，但是好累 答应群友的两篇文章，都只写了一半（ 射箭计划搁置了（ 继续杂书时间，真的快乐 本周和女朋友改善伙食是得莫利鱼粉和锅包肉，美滴很 本周过年啦，《新.奥特曼》中文 PV 放出，啊，痞子不愧是资深奥厨。Netflix 的《机动奥特曼》第二季放出，emmmm，打戏很棒，文戏吧，坂本 sir 亲传 这周加了镇静类药物感觉还行，没太大副作用。下下周又要复查了，不开心。 Netflix 的新的纪录片，各地国家公园的美景，好看 LinksTV 的视频真的很治愈我。我心情低落的时候总会看看他的视频。如果有看到这篇文章且认识他的朋友的话，替我给他很真诚的说一声谢谢，祝他和小螃蟹继续美满的生活下去。（真的算是我的赛博抗抑郁药（ 买了新的椅子！ 本周读书清单 法治的细节–罗翔 一个叫欧维的男人决定去死 啊，这周最大的收获就是看了《一个叫欧维的男人决定去死》，亲情，爱情，友情 在这个时代已经是足够的奢侈品。而且这本书又让我想起一句话 但行好事，莫问前程 这周真的是很治愈的一周 放点照片 技术这周还是花了很多时间在非技术的内容上，技术上的进步，算勉强没有荒废吧 这周和一些前辈讨论了 bytebase 这个项目上的后续的一些发展。我自己很看好这个项目。在我的期待中，我期待 bytebase 能够成为 SQL 管控侧的一个事实标准。这样能极大的提升 SRE 的工作体验 看到了一个很有趣的新项目，iasql-engine。实际上现在 Infrastructure as Code 已经成为一个业界共识。IaC 的普及能极大的减少 SRE 压力，提升资源利用率和可管控性。但是具体实现路径的话，有 Terraform 这样自定义 DSL 进行迭代的产品，也有 Pulumi 这样瞄准通用化的实现路径。现在社区也在尝试更进一步，用最终数据状态的模式来管理 Infra，所以有了 Infrastructure as SQL，从我个人的角度出发，我觉得从数据这个维度和视角出发去做 Infra 管控，的确能解决之前 IaC 遗留的一些问题（比如状态管理，数据分析 ETC），能够更好的达成和云厂商解耦的最终目标 这周的一部分经历还是放在了前端可观测性这块，自己也思考了几个问题 Largest Contentful Paint 这一指标我们怎么样去赋予其具体的业务含义。换句话说，如果 LCP 变大了，我们怎么样去衡量它对于业务的影响？无法赋予具体含义的 Metric 在很多时候是没有太大意义的。 我们怎么样去精确测量诸如 DOM Render Time 这样的指标？ 除了对于关键路径的拨测，我们还有没有其余更多的手段去从基础设施的角度去观测一个应用的可用性（如果从业务指标出发，那么当我们有感知的时候，一定意味着业务受损明显了） 容器方面，排查了 nerdctl 的一个 ARM 平台的问题，参见 Issue979 ，用户下错二进制版本了23333。不过 ARMv7 的二进制能够在 ARMV8 的平台上运行还没有 warning，这无疑是不合理的。所以我准备下周搞个 PR 去加强一下这块的提示 PR948 在 @Junnplus 的帮助下合并进去了。这个 PR 合并的时间比我想象的长很多。主要是我自己犯了不少低级的错误导致的。学到的很关键的一点“你需要从项目 maintainer 的角度去自我 review 自己的 PR“。低级错误以后不能再犯了 NSDI2022 的论文 Evolvable Network Telemetry at Facebook 看完了。非常有趣，Meta 这个工作做的很细，对于建立自己的可观测性系统有不小的参考意义。这里简单聊两句 他们构建了一套全链路覆盖的 Telementry 系统来追踪底层系统变动，并评估潜在影响 他们自己抽象了一套数据模型，将数据和具体硬件细节解耦 数据结构和具体的 job 解耦，但是 Job 相关信息也会存在对应的 Event Metadata 中 他们在设计这套数据模型的时候，会构建当前变动所触发的连锁变更，这样形成一个树形结构，更好的去方便去分析故障路径 核心就还是一点，低成本获取更多的数据，数据越多越好 这周除了这篇，也啃了点 Configanator: A Data-driven Approach to Improving CDN Performance. 这篇，也是比较有意思的工作。等我啃完可以聊聊 差不多就这样 简单总结实际上疫情，经济下行，都给大家带来了不少的压力。也有很多人变得更冷漠，更喜欢以旁观者的身份幸灾乐祸，这里分享一段我最喜欢的台词（也是我的座右铭），与大家共勉（出自1973年《艾斯.奥特曼》最终话《明日のエースは君だ！/你是明天的艾斯！》） 優しさを失わないでくれ。弱い者を労り互いに助け合い。どこの国の人とも友達になろうとする気持ちを失わないでくれ。たとえその気持ちが何百回裏切られようとも。それが私の最後の願いだ。 我最喜欢上译意译的版本 热忱之心不可泯灭。要体恤、帮助弱者。与任何国家的人都能成为朋友，别失去这份热心，纵使它已被背叛了千百回。这就是我最后的愿望。 每个人心里都有光的（大雾","link":"/weekly/2022/2022-04-week3.html"},{"title":"","text":"20224月第四周简报这周继续在家，上海无法说了，北京也开始沦陷了 生活 继续是在家的一天，算起来我在家马上整一个月了，目前感觉真爽 本周还是时不时的和女朋友出去散步，真好 女朋友夸我在家之后变得更活力了一些，感觉我生活除了工作和代码多了很多其余的东西Hhhh 健身环继续，感觉还 OK，买了力量器和哑铃准备开始加练 生日要到了，女朋友问我想要什么东西，我不知道（很苦恼 继续杂书时间，Double 的快乐 Netflix 的新的纪录片，各地国家公园的美景，看了几集后发现，Netflix 在镜头，配乐和叙事线上还是和 BBC 有一些距离。不过4K画面真的很好看 这周换了一把键盘用，重拾我的阿米洛樱花，好玩，想玩一段时间客制化了 这周状态好像又在变差，Damn 重新看了《一个叫欧维的男人决定去死》的电影，改编的真的不错。 下周奥特银河格斗三要开始上映了，期待！ 还是小说荒，继续读了N次《希灵帝国》 这周带了猫猫去洗澡 本周读书清单 昨日的世界 风沙星辰 最后分享一点我的键盘存货 感谢女朋友的大力支持（ 技术这周状态属实有点不太好，不过也在坚持保持进步，我自己也不知道算不算荒废了 发现了一个全新的 diff 工具 difftastic。 好用，非常好用，现在用 Rust 写的 CLI 工具越来越多了。Long Live the CLI！ 参与了 taichi 社区一个 Issue Issue4837 的讨论。和 @frostming 的观点一致，我对 __new__ 这种方法以及更进一步的元编程能力的使用抱有谨慎的态度。因为这些 magic way 会很大程度上打破包括 IDE 支持，静态分析支持在内的特性。所以作为社区的引入，我觉得慎之又慎 nerdctl 发了 v0.19.0 ，做了一点微小的工作 本周 container-spec 关于 CPU Burst 的 PR PR1120 没有新的动静，不过周边的 Rust 一些社区开始在实现这个 spec 了。有趣 本周继续帮人做前端可观测性这块的东西。全球范围内拨测的意义可能会比我们想想的重要。而且要注意拨测时最好不要仅限于连通性的测试，也许要将业务可用性的验证纳入在拨测的范围内，这样才能最大程度上及时的发现问题。 prometheus-operator 对于一些灵活的场景真的很头疼。举例如果你有若干台裸 VM，以及一个集群里的 prometheus-operator ，你现在想把整套 Prometheus 全部统一了，这个时候就会很蛋疼。 NSDI2022 的论文 Configanator: A Data-driven Approach to Improving CDN Performance. 看完了，我不是太认同里面的一些实现路径，但是一些大体的设计思路我是认可的，随便记录一点随笔 对于 CDN 需要根据地域和业务载荷来调整参数，通用参数是不 work 的（废话） 常见可能对于 L4 这一层比如 ICW 的优化，以及应用层的 ALPN 等阶段进行一些优化（赞成） 需要去有效记录流量特征（比如 IP Meta，比如 SNI 之类的）（同意，即便不考虑优化，流量特征的识别也有其余很大的作用） 根据流量特征来映射具体的配置模型（有点怀疑，因为特征窗口往往很短，而且载荷是否能抽出有效的数学模型（文中说的什么高斯过程，决策树啊之类的），我持怀疑态度，毕竟还没法大规模验证） 现有的技术环境太 Old School 了，不太好对 L4 这一层做调优。可能 eBPF，kernel bypass 之类的新技术会很好（赞同，大趋势就是这样） 这篇文章的 Appendix 也值得一读 答应煮饭阿姨要写的前端与 Nginx 的文章，进度 +10% 差不多这样，这周状态不太好，所以没有太多经历去读其余的东西。。有点颓废 差不多这样。。 简单总结最近阴间新闻太多，让有些一直在压制的情绪有点超出药物压制了，这样不好。下周该去复查了。继续好好过下去吧“Saka，你也可以变成光的（大雾”（出自《迪迦奥特曼》）。 最后，大家多保重。","link":"/weekly/2022/2022-04-week4.html"},{"title":"","text":"2022年5月第一周简报这周我也不知道是算是四月第五周还是五月第一周，不过先按照五月第一周来算！劳动节放假啦！ 生活 在家整整一个月啦！感觉良好，爽！ 本周北京的云好好看 本周的健身环稍微荒废了下 本周购入哑铃和握力棒，爽 每周的杂书时间，飞一样的感觉 网飞的国家公园纪录片刷到了 Ep5，好看，想出去玩了，以及4K万岁 本周的改善伙食，重八牛府，味道还行，但是有点辣 本周租了新的房子，预计下周搬进去！开心！我有专属的工作间了！ 和女朋友去宜家买了抱抱熊，毯子，粉色的毯子！啊！老夫的少女心！ 女朋友送我一张升降桌当生日礼物！开开开开惢惢惢！ 本周还是小说荒，现在网文的同质化也忒严重了吧。。 过年啦，本周过年啦！奥特银河格斗三，屁大点事开播！看皮套人大战很开心 本周膝盖老伤发作，同时不知道是因为吃药还是其他关系，心情和精力都在低谷期。 本周读书清单： 昨日的世界 茨威格的风格太戳我的爽点了啊，安逸的很。说回来，下周想去复习下《耶路撒冷三千年》这书了。 最后老规矩，分享一点图吧 感谢女朋友的大力支持（ 技术又是状态不太好的一周，还是不知道自己是否算是荒废了时光 Lunarvim 是个好用的 Neovim 的开箱即用的配置。很推荐 本周的吃屎时间，Kubernetes 一个很经典的问题。在 cronjob 更新后，Controller 会做 task 补偿，这样可能会在你意料外的时间节点触发任务执行。我觉得这个地方设计是有问题的，没有留开关给用户配置就离谱。下周去和社区讨论下这个问题。 这周帮人做了一个很好玩的东西，大概场景是一个开源仓库，需要在提交，tag，PR，等 timing 触发一个私有不向社区公开的构建。在依托 GitHub Action 不考虑自己搭 CI，又需要保证安全性的情况下。我的做法是用一个私有仓库作为中转，开源仓库在对应的时间向私有仓库提交包含构建标识的 commit，两个仓库之间的权限通过 GitHub App 所提供的一次性 token 解决。这样就相对满足了我们的需求。 本周帮人查了一个技术问题，大概是 Gunicorn 在重启后，有概率出现 Connection In Use 的问题，且 lsof 找不到对应的端口占用。大概查了一下，挺有趣的一个问题，root cause 是客户端那边有什么原因阻塞了，gunicorn 被重启主动关闭 socket 后，先释放了 fd，然后在 FIN_WAIT_1 的情况下向对端发了 FIN，收到 ACK 后转入 FIN_WAIT2，这个时候对端一直没有 close 对应的 socket，导致内核里对应的 sock 一直处于 FIN_WAIT2 直到 2MSL （或者一个根据 RTO 计算的时间）之后才会被释放，期间重新 bind 的时候就会翻车。具体的最小复现的 case 和验证脚本我扔到了这里：case，eBPF 真是好东西啊！ 这周发现 prometheus-operator 真的不好用，additional-scrape-config 什么的难用啊。所以有没有好用的，开源的，TSDB 推荐一下啊，救救学生吧。 NSDI2022 的论文 Cocktail: A Multidimensional Optimization for Model Serving in Cloud 看一部分，走眼了，不太喜欢这篇，建模这东西我觉得玄学成份太大了。 答应煮饭阿姨要写的前端与 Nginx 的文章，进度 +5% 差不多就这样吧。。 简单总结今晚和朋友讨论最近的一些阴间事情的时候，他问我有时会不会有一种无力感。我沉默了很久，告诉他“我们一路奋战，不是为了改变这个世界，而是为了我们不他妈的被这个世界所改变” 还是那句话吧，热忱之心不可泯灭，我也不知道我这样的中二病还会保持多久。但是在我还能保持热血的年纪，当个傻逼式的中二晚期患者，挺好。 与大家共勉","link":"/weekly/2022/2022-05-week1.html"},{"title":"","text":"2022年5月第二周简报五月第二周！五一假期！不过啊，北京这疫情啥时是个头啊 生活 开始进入新一个月的在家时间！ 本周健身荒废了！ 没有出去吃好吃的，堂食禁止.mp4，我恨疫情 和女朋友搬入了新家（搬家当晚朝阳区停止所有搬家23333） 女朋友给我买了船新的升降桌，打造一个超好看的工作间，美滋滋！ 新家风景和采光贼好，喜欢上在落地窗前晒着太阳，看着书，听着女朋友在旁边上日语课的生活 本周依旧小说谎，重新拾起了《黎明之剑》，大眼珠子的书真好看 本周奥特银河格斗三屁大点事第二话，坂本啊，我球球你别写文戏了，这崩成啥样了？ 关于我的状态，坏消息是没有好转，好消息是没有变坏（XD 本周读书清单： 昨日的世界 喂–出来，星新一小说集 成为一颗星：宇航学员日记 昨日的世界看完了，有点沉重，也有点庆幸，和平真好。以及我突然想起我曾经给自己取的外号航天之子（谁还没有个上天的梦想呢 技术状态依旧一般，但是总结还是得写 这周是属于学习 Rust 的时间，Rust 有很多东西很戳我xp 这周看了一下 mlua 这个 Lua Rust Binding 的实现，我发现我有点爱上 Lua 了，足够轻量，足够好用。我的新的一号进程就准备用这个了 本周深入的学习了一波关于 Vim 的操作，对于自己常用的东西还是要很熟悉比较好 上周帮人排查的问题，让我去重新看了下内核 TCP 这块新版内核的一些状态机的实现，感觉还是挺有意思，以及继续用 eBPF 写了一些自用的排查小玩意儿 本周 aws-cli 翻车了，原因很简单，AWS EKS 使用了 ExecCredential 来作为 kubeconfig 凭证获取的手段。在跟随 v1.24.0 发布的 PR108616 中， ExecCredential 的 APIVersion 正式废除了 v1alpha1，而 AWS Cli 相关的更新 PR6476 21年10月就提交了，但是直到 v1.24.0 发布为止都没有合并，所以导致一堆人的 EKS 的 Kubeconfig 失效（什么叫大企业风范啊（战术后仰。不过好消息是，在这个 bug 爆出来后，这个 PR 加急合了（XXXXXXD 本周的业余时间又是和可观测性战斗的一周。最近在思考一个很有趣的问题，对于前端上报 metric 这种变化频率很高，最好能让业务方自行配置的场景。怎么样能做到尽可能低成本的去更新？良好的 DSL ?抑或是一个可以自定义执行的 Runtime？可能下周我会先基于 TOML 抽一个简单的 DSL 出来做个 PoC 看看效果。 这周周天抽了点时间大概复习了 The benefits and costs of writing a POSIX kernel in a high-level language 这篇 OSDI18 的论文。比较好玩，比什么 AIOps 的论文好玩不知道哪里去了。下周周报可以写个 brief 介绍一下 答应煮饭阿姨写的文章这周没有进步（ 在推特上和人讨论了一个很有意思的话题，详情见下图 简单总结最近心情还是一直低落，不过日子还是得向前走。说起来下周就要28了，希望能在成为一个优秀而温柔的人的路上勇猛前行 世上只有一种英雄主义，就是在认清生活真相之后依然热爱生活 – 罗曼.罗兰","link":"/weekly/2022/2022-05-week2.html"},{"title":"","text":"2022年5月第三周简报五月第三周！现在我已经成了新能源汽车了，每天做一次核酸保证续航 生活 本周的健身因为膝盖还是处于半荒废状态，不过坚持每天出去走走 本周和女朋友去挑了新的琴，不过因为疫情，还没送到 新家的采光太好了，让我睡不着的时候望着月光都能发呆很久，这月色太美太温柔 本周看新的纪录片《重返太空》，马斯克的确是一个毁誉参半的人，但是他和他的 SpaceX 对于人类的进步的贡献也是无可忽视的 本周继续的小说谎，大眼珠子的《黎明之剑》看完了。他的三观真的太戳我 xp 了 本周女朋友买的空气炸锅到了，一起做了豆腐抱蛋，烤虾，烤皮皮虾，我们和小猫咪都喜欢吃 本周奥特银河格斗三屁大点事第三话，心态崩了，这都啥啊 本周看到了盖亚奥特曼中石室指挥官的扮演者渡边裕之自杀的消息，心态崩了，难过了很久，我最喜欢的两个指挥官的扮演者田中实和渡边裕之都是自杀，心态崩了。 关于我的状态，还好吧，和上周一样没有继续变坏 本周的读书清单 成为一颗星：宇航学员日记 意大利第一位女性宇航员萨曼莎•克里斯托弗雷蒂女士写的这本书，让我回想起小时候仰望星空的感动与自己曾经的航天梦。人类因为对星空的好奇而伟大。一个小故事，萨曼莎•克里斯托弗雷蒂女士在2022年五月，搭乘 SpaceX 的龙飞船，开始了她第二次国际空间站的任务（祝福 技术 这周重拾 nerdctl 相关的工作，为了实现 Issue1039 提出的让 nerdctl 支持更多的 log driver，我做了一次小重构 PR1050 ，这里维护者 Suda 提出了个我比较认同的看法 “I think this kind of refactoring should be done when you actually implement another driver. Hard to review whether the design is correct otherwise.”，所以这个 PR 应该会跟随 PR1062 这个实现 journald driver 的 PR 一起合并进去。 关于 nerdctl 的 Issue1039 ，我目前在着手做 fluentd 的集成。而这个 Issue 相关的 PR1062 有一个问题（目前我没太考虑清楚： 在往 journald 写日志的话，涉及到 nerdctl log 这个命令怎么读出已经写入的日志。Docker 的解法是直接用 journald 的 CAPI 走 cgo 裹了一层，或者我们可以走 go-systemd 来读取。但是这里就涉及到另外一个问题，一旦牵扯到 cgo 就会牵扯到动态链接的问题。对于 Docker 这样比较重的服务来说，他可以通过针对不同的 distribution 做不同的 package 处理来保证依赖的完整性。但是 nerdctl 本身定位只是一个 cli ，如果要涉及到 cgo 那么我觉得无疑会比较重 Suda 和我提出可以用 json-logger 来保证 nerdctl log 获取日志的正确性，其余 driver 只读不写，但是 fahedouch 觉得 driver 之间存在依赖关系不是一个比较好的设计。我觉得一定程度上 make sense，下周再讨论下吧，我的 PR 也许要依赖最终的设计 本周还是为可观测性战斗的一周，在思考一个问题，当 Sentry 的各种报错很多的时候（前端/后端都有），我们怎么样去治理比较合适，换句话说，怎么样去合理的使用 Exception 是最佳实践？ 看了下 Kuberntes 一个开源工具的 lens 的源代码，它有一个很具有特色的功能，可以在不依赖 SSH key 之类的情况下，登录集群里的 Node。 实现原理是它会给对应的 Node 跑一个 node-shell 的特殊 Pod，这个 Pod 会开启 privileged 以及和宿主机共享 PID NS，network NS，然后通过 nsenter 挂载 1 号进程关联的所有 NS 来实现权限绕过。路子真的很野。不过我也在思考一个问题，如果有必要吧一些能够执行 Pod 的 Kubeconfig 下发给开发后，怎么样防止研发们绕过我们的安全权限，来搞事情。 这周看了一点推友推荐的一篇论文，很戳我 xp ,10.1145/269005.266660: The performance of μ-kernel-based systems, 99 年的论文，old school, but sexy. 下周读完可以写个简评 答应煮饭阿姨写的文章这周进度+5% 本周把自己的一些祖传脚本全部转为 ansible 了，这样能方便的初始化远程开发环境了 FAST22 文章好像公布了，看了一眼，我觉得比较感兴趣的 CacheSifter: Sifting Cache Files for Boosted Mobile Performance and Lifetime 和推友的一次讨论坚定了我继续写周报的信心（原来我写的周报真的有人看啊（ 简单总结本周28啦，要继续的努力，让自己继续成长为一个优秀而温柔的人！","link":"/weekly/2022/2022-05-week3.html"},{"title":"","text":"2022年5月第四周简报五月第四周！小区明天开始不允许外卖进来了（呜呜呜呜，Fuck 我恨疫情 本周没怎么出去走走，我真的是一个肥宅 女朋友的新的琴到啦！雅马哈的！ 女朋友用新琴给我弹了我最喜欢的曲子之一！电影触不可及的配乐《Una Mattina》！好听！ 本周家里的蓝绣球开花啦啦啦啦！好看！ 本周继续小说慌，开始看大医凌然，要是以后不用为生计所限制了，我一定要去读临床本科！ 本周改善伙食啦！啦啦啦啦啦，外卖的海底捞，用电磁炉煮火锅，美滋滋 本周奥特银河格斗三屁大点事第四话，我对文戏已经死心了，看看皮套人大战吧 这周又开始焦虑了，除了因为自己太菜所带来的不安全感以外，现在每天得吃五种药也是焦虑来源。不喜欢这这种感觉 本周没怎么看闲书，业余时间大部分放在了一部看得我热泪盈眶的纪录片上 Netflix 原创纪录片，《挑战者号：最后的飞行》 美国东部时间1986年1月28日11点38分，挑战者号航天飞机在肯尼迪航天中心39B号发射台升空执行 STS-51-L 任务，73秒后，挑战者号在空中爆炸解体，7名航天员无一生还。Netflix 这部纪录片去完整的剖析了这场人类航天历史上最惨痛的事故的点点滴滴。说实话，看得我热泪盈眶。人类的赞歌是勇气的赞歌 We will never forget them, nor the last time we saw them, this morning, as they prepared for their journey and waved goodbye and “slipped the surly bonds of earth” to “touch the face of God.” — Ronald Wilson Reagan 最后还是在这篇周报里小小的记录下7位献身于蓝天之上的先驱者 迪克·斯科比（Dick Scobee，曾执行STS-41-C以及STS-51-L任务），指令长 迈克尔·约翰·史密斯（Michael J. Smith，曾执行STS-51-L任务），飞行员 朱迪斯·蕾斯尼克（Judith Resnik，曾执行STS-41-D以及STS-51-L任务），任务专家 鬼冢承次（Ellison S. Onizuka，曾执行STS-51-C以及STS-51-L任务），任务专家 罗纳德·麦克内尔（Ronald E. McNair，曾执行STS-41-B以及STS-51-L任务），任务专家 格里高利·贾维斯（Gregory Jarvis，曾执行STS-51-L任务），有效载荷专家 克里斯塔·麦考利芙（Christa McAuliffe，曾执行STS-51-L任务），有效载荷专家 聊完我这周沉重的部分，来看点轻松的 技术 这周继续花了不短的时间在 nerdctl 上。 首先上周，Suda 在我的重构 PR PR1050 中说的 “I think this kind of refactoring should be done when you actually implement another driver. Hard to review whether the design is correct otherwise.”，应验了，fahedouch 在 PR1062 中重新设计了一部分 interface（实际上这是由于一个 Bug Fix 衍生出来的），在考虑到 Bug Fix 的基础上，新的设计比我的设计更合理，所以我的 PR 作废了 上周的一个遗留问题就怎么从 journald 里面读取写入的日志，这周也有了一个答案，就是直接不读取，保证依赖的干净 这周我写完了对于 fluentd 的支持 PR1073， 功能写的比较轻松，e2e 测试写的比较痛苦（调试的也比较痛苦），不过这算写开源的乐趣所在吧2333 下周再把 S3 之类的 docker 已经支持的 LogDriver 支持了。 这周复习了一下 JSON RPC 的 Spec，参见 JSON-RPC 2.0 Specification, 说实话，相较于 gRPC 之类的，我更喜欢 JSON RPC 这种轻量级的契约协议。虽然有着带宽利用率等缺点，但是线上调试起来是真的舒服 这周和 @gaocegege 讨论了一个很有意思的话题。怎么样在一个容器内同时起多个进程？最后讨论下来好像目前没有很友好的解决方案，Supervisor 之类的太重，而且潜在风险不少。可能现阶段用 tini + bash 糊一套上去比较可行。 这周帮人查了一个 Nginx/Openresty 的问题。大概就是在流量高的情况下，这台机器 CPU Usage 会暴涨，导致其余资源无响应。最后查下来是 Nginx 卸载 SSL 证书过于吃资源的问题。这回到我之前在周报里推荐过的一个观点“尽可能的用 CDN/SLB 之类的云厂商的服务来卸 SSL 证书”，如果你实在想让 Nginx 暴露在最前面的话。你可以考虑参考官方的一个关于 SSL 的 Benchmark NGINX SSL Performance。以及如果你需要利用 Nginx 提供 SSL/TLS 的静态资源访问的话，你可以考虑将内核升级到 5.2+ 以及 Nginx 1.21.4+ 来使用 Kernel TLS 的能力，参见 Improving NGINX Performance with Kernel TLS and SSL_sendfile( ) CacheSifter: Sifting Cache Files for Boosted Mobile Performance and Lifetime 看了一些，下周应该能写个简评（ 简单总结和焦虑对抗，要做更好的人！","link":"/weekly/2022/2022-05-week4.html"},{"title":"","text":"2022年5月第五周简报怎么回事，五月怎么有了第五周？？？算了，就算是第五周吧 本周还是没有怎么运动，算了，我就是肥宅 女朋友练琴练的很开心（我听着也很开心 本周继续的小说慌，大医凌然要刷完了，新书没啥吸引我的。。不爽 本周份的改善伙食！姐夫的菜！爽啊！他们家的红烧肉很好吃的！ 这周开始看爱死机，脑洞真的很大 陪着女朋友看《知否知否，应是绿肥红瘦》有点好玩 本周去复查了，好消息是医生觉得我情况相对稳定没有恶化，坏消息是我得药物加量了（幸好不是给我加新品种的药） 本周薄荷阅读开了新的坑，《追风筝的人》 本周份的闲书时间 喂–出来，星新一小说集 星新一先生的脑洞真的是让人欲罢不能，《喂出来》这篇入选过课文，无限 loop 很好玩。这周这本选集里面，有另外两篇我非常喜欢，《友好的吻》和《爱的钥匙》，一篇反转的让人目瞪口呆，一篇温情无比。可谓佳品 技术 本周继续花了一些时间在 nerdctl 上 上周写的 PR1073 完善了一下额外的测试 和社区沟通了一下，感觉不太需要去实现 S3/Cloudwatch 之类的 LogDriver 了，不过 Suda 提出了一个好建议是我们可以在 nerdctl 上额外再裹一层 Plugin ，让开发者可以自行接入想要的 LogDriver。我觉得是个很好的点，但是我一下还没想清楚怎么实现。而且有点我心存疑虑，nerdctl 的 LogDriver 基本上是在 ctd 的 Plugin 上额外裹了一层。我们再裹一层是不是不太优雅。我先参考下 Docker Plugin 的实现再来写 Proposal 吧 最近感觉 nerdctl 的发展进入了一个瓶颈。有一说一，在 Docker 已经成事实标准的基础上，去兼容各种 Docker 奇奇怪怪的行为。我觉得相当难受 本周又在继续思考 K8S 老生常谈的 CPU Throttle 的问题。对于很多 Burstable 的应用来说，CPU Usage 到了70-80这个水位就很难继续上去了，一看监控频繁 Throttle。有一说一这样实际上还是不太利于资源充分利用的。但是你要是强上 BestEffort 绑核吧，又会造成另外一个层面上的资源利用率的问题。下周去 hook 一下 Kubelet 的参数再看看 本周自己的 side project，踩了一个很好玩的坑。大概是在写一个 JSON RPC Proxy 的时候，我需要在拿到 upstream response 后，unpack response 然后输出对应的 metric。这个功能很简单对吧，但是第一次写完往线上一跑的时候，发现疯狂报错，然后拿数据一看，\\x31f 开头，傻逼了，没考虑 Content-Encoding 为 GZIP 的情况233333 本周帮人做的可观测性治理落地了一部分。效果还是很明显的，比如某个应用从日百三的报错量收敛到日万五左右的水平。在这个基础上，后续的比如监控之类的 action 就可以跟进了。这里几点贴士 对于报错/日志系统，一定要定期归纳，定期治理。区分清楚哪些是意料内的错误，那些是意料外的错误。意料内的错误最好的是走日志进行收集。对于 Sentry 这种报错收集系统，一定要将其用在最核心的部分上。 业务端要慎用异常等行为 治理是一个长期性的活，千万不要搞猪突式的突击 CacheSifter: Sifting Cache Files for Boosted Mobile Performance and Lifetime，这篇论文不太喜欢。说实话我一直觉得这类 ML Model 用在工程领域的论文，在眼前阶段，更多还是一种噱头 本周在线上和 Bytebase 的同学聊的很开心Hhhhh（我用 Bytebase 也用的很开心（当然我对我用的不爽的地方也对他们进行了疯狂输出 本周 Python Meetup Shanghai 2021 终于弄完了，所以 RustConf 2021 什么时候开始 简单总结算了，不想总结，以及想挂个傻逼（懂王真多（","link":"/weekly/2022/2022-05-week5.html"},{"title":"","text":"2022年6月第一周简报啊哈哈，六月啦，开心，儿童节和端！ 本周没有运动，所以我成肥宅了，呜呜呜 本周开始每天自己做菜了，招待不周！ 因为伙食开得很好，导致我体重到了177，（妹子：超过170就离婚！（一分钟后上称（177（妹子：你这叫我怎么圆场 家里的蓝雪花继续开新的花了，美滋滋！ 本周把《知否知否，应是绿肥红瘦》看完了（居然还觉得有点好看 本周胃病恢复的差不多了，把胃药停了 本周开始看《间谍过家家》好好看！笑死我了 本周还开了一部纪录片的坑《黑洞》，也是 Netflix 上的纪录片 本周北京的疫情开始好转，而且这两天北京的天气很多变，云很好看，拍了很多有趣的照片！ 蓝莲花刷题行动双月结算，本次共计捐款851元 技术 本周继续花了一些时间在 nerdctl 上 fahedouch 的 PR1062 合并了，我的 PR1073 也开始提上合并日程了 在思考下 rootless 下指定 IP 怎么做的问题 上周困惑的一个问题，nerdctl 的 LogDriver 基本上是在 ctd 的 Plugin 上额外裹了一层。我们再裹一层是不是不太优雅 这周差不多也想清楚了。ctd 的 plugin 终归是和 cli 的 plugin 定位不同。两者没什么太相似的地方，定位也不会存在冲突。构想了一版草案。下周先写个 draft 的 PR 出来。 在维真的指导下，给 tidb 修了一个小问题 Issue35047，参见 PR35089。算是萌新的 DB 初体验。 本周开始关注 buildkit 这个项目，非常有意思，我觉得也非常值得投入时间去啃代码 可能由于没有太多历史遗留包袱的原因，这个项目非常干净清爽，清爽的不像 Docker Inc. 的项目 最近几个大版本新出的 MergeOp &amp; DiffOp 设计的非常有意思，能应用的场景也会比较好玩，具体可以参考 Merge and Diff Ops 这周再去关注了下 Pixie 这个项目，顺便和项目的负责人线上约了一个 meetup 聊了一会儿，收获不错。感觉现在 eBPF 在可观测性的应用也越来越多了 补全了一点自己线上排查问题的内核 tracing 脚本。写的很开心 本周读完的一篇很有趣的论文，阿里出的，但是很不错，Characterizing Microservice Dependency and Performance: Alibaba Trace Analysis，非理论性的论文，主要是对于阿里巴巴内部服务的拓扑，调用关系，性能调优等方面进行的一些统计学上的概括与分析。我觉得很有参考价值。 文中提出了一些构建图结构的一些算法，这块不做过多评价 基于基础的图论理论（入度/出度等）外带各种服务间 metadata 的分析，得出一些结论。有些结论是认知范围内的，比如跨集群服务调用时间更长，CPU 利用率更影响服务 RT 等。有一些也相对反直觉：比如上游服务并行调用下游服务的场景很少等。 各种数据分析都需要基于完备的数据收集，就论文里的展示来看，阿里这块数据的完备度是真的不错（和我工作里的认识也符合） 本周在看的另外一篇论文：Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider，微软 Azure 和研究院出的，类似上面提到的阿里那篇文章。这篇主要是对于已有的 Serverless Workload 进行统计上的归纳和分析。目前看到的部分很有意思。具体的等下周看完来写个简评 简单总结这周状态终于开始好一点了，以及北京疫情告一段落，下周我要出去堂食！！！","link":"/weekly/2022/2022-06-week1.html"},{"title":"","text":"2022年6月第二周简报六月第二周，天气一点点变热了（更不想出门了 本周开始运动了，每天晚上出去跑跑步。中年男人三大爱好：钓鱼，跑步，摄影。 本周因为一些原因，连续两个通宵+一个凌晨三点，导致我状态不太好 家里的植物长得很好，很开心 和妹子出去吃巴奴毛肚火锅改善伙食！ 本周把黑洞纪录片看完了，好看 《间谍过家家》我觉得非常治愈！一家子太萌了！ 本周北京开始进入雨季了，凌晨在家发呆的时候，颇有点听雨高楼的感觉。 给我的博客启用了新域名 https://manjusaka.blog 经过纠结，还是一步到位买了 DS1821+ 作为家里的 NAS，存储方面目前是 16T*6 （16T * 2 做 RAID1 存重要资料，剩下的 RAID5 存媒体资源）的结构 技术 本周继续花了一些时间在 nerdctl 上 完善了一下后，我的 PR1073 合并进入 Master 了，v0.20.1 之后 nerdctl 就默认支持 fluentd 的 logdriver 了 上周提到的 nerdctl logdriver plugin ，我参考了下 Docker LogDriver Plugin Protocol，开始写 Design Proposal 了，下周估计能给社区提交上去 被 Suda 提名成为 nerdctl 项目的 Reviewer，参见 MAINTAINERS: add Zheao Li as a REVIEWER 开心 本周从 tidb 那边认领了一个大的 issue，参见 Issue27762，大概背景是目前 executor package 下面文件太多了，一是影响开发体验，二是会导致 bazel CI 出问题，需要合理拆分目录。周末思考了两天，下周和维真碰一下 Proposal 本周帮安全的哥们实现这样一个需求“拦截特定进程对于特定文件的打开读写操作”，吃屎不少 实现这个需求第一考虑 kprobe 配合 bpf_override_return（毕竟换个思路来说就只是一个 error injection 手法而已） 但是呢，4.17 前后因为一个 commit 会导致 kprobe attch sys_call 的时候出问题（具体我最后会贴图），如果不用 sys_call 呢，bpf_override_return 又是有白名单的，不在白名单范围内的函数搞不了事（这个设计是 make sense 的） 然后因为一些原因，alios 4.19 好像禁了 bpf_override_return，LLVM 编译出来的产物找不到 bpf_override_return。（标准 419 是没问题的），我只能说委座高明 然后因为要向下兼容 310 版本。思前想后，算了，我还是用 ptrace 裹一下吧 PoC 已经跑通，下周 Python 裹一下给朋友 我的一些脚本都可以在 linux-traceing-script 这里找到，不过保证兼容性（ 本周 HTTP3 的 RFC 正式出来了，参见 RFC9114，花了点时间看看，当然没看完（ 上周提到的 Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider，因为各种原因没看完。但是已经看的部分我很喜欢。推荐大家都去看看（逃 又有一个朋友开始写周报了，开心！参见 daily-progress 简单总结最近各种阴间新闻看的我心里发慌。世道越来越不容易了，所有朋友务必保重自己。Saka 在这祝大家平安喜乐了（","link":"/weekly/2022/2022-06-week2.html"},{"title":"","text":"2022年6月第三周简报生活六月第三周，天气更热了，完全不想出门！ 本周继续在运动了，体重下降到可170！好耶！ 身体状态又开始反复了，电话咨询了下医生，可能是药物的问题，可能要换药了！ 本周看了好多好玩的东西 补了 BBC 的纪录片《七个世界一个星球》 间谍过家家 EP11，好温馨啊 我的信仰电视剧2005年的《海猿》终于找到了！看得我热泪盈眶 奥特银河格斗三，我只能说坂本我球球你别拍了 本周极端天气好多（ 朝阳还是不允许堂食，没法去店里改善伙食 本周又开始看杂书了，好耶！美滋滋！ 本周的看书 《人造美人，星新一小说集》 有一说一，星老板的奇怪的幽默感，大开的脑洞太戳我xp了，不愧是溢出道就被认为是天才的人物啊（其实我很怀疑他字里行间透露出来的一种奇怪的冷静感，是他一直在俯视着众生的一种具象化） 技术 本周 nerdctl 有个新的 issue Discussion1128，针对 CNI 的一点活。写完了，不过这周状态不好懒得写测试。所以下周交 PR 吧 本周花了不少精力去实现安全老哥的“拦截特定进程对于特定文件的打开读写操作”的需求，继续吃屎 由于要向下兼容到310，以及某公司的定制版内核对有几个 helper function 支持有限，所以我选择用了 ptrace 去做 为啥不用 SELinux 这种已有的姿势去搞定。原因在于后续注入点可能还会借鉴一些 IO Chaos 里的手法，给已经打开敏感文件的恶意进程在读取数据的时候注入一些随机数据。 这个 trace 可能需要作为一个常态化的进程跑在机器上，确保在符合规则的恶意进程启动的时候，便可以注入防护功能。Netlink Process Monitoring 监听 fork/exec 几个事件，或者 eBPF 上 kprobe 去监听对应 sys_call 就行，不过考虑到要向下兼容 310 我估计还是会用 Netlink 方案。Easy 有个稍头疼的问题，目前 ptrace 常态化 attach 进程的方案存在以下两个问题，所以需要考虑新的方案 性能问题 进程可以感知到自己被 trace，所以可能需要将 attach 的时间尽可能放短 根据上面的问题，目前想到的一种思路是，ptrace 只 attach 一次，给敏感 sys_call 注入一段 jmp 指令，jmp 到我们新指令里。由新指令来校验 paylaod ，然后决定是否放行 这周开始复习一下 Kubernetes 中 Kubelet 相关的代码，要开始做提 CPU Burst 相关 KEP 的准备了。以及和 Burst Spec 的作者交流，看他是不是因为工作关系无法顾及社区 Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider 这篇论文很有意思啊，有些举出来的数据我觉得挺好玩的 HTTP 请求是调用函数的最大来源，其次是计时器（或者叫 Crobjob？） 45%的应用可以有一个小时被调用一次的频率 百分之50以上的应用的单次执行时间在1s以内 百分之90的应用内存消耗不超过 400MB 总结疫情反复，经济下行，大家请务必多保重","link":"/weekly/2022/2022-06-week3.html"},{"title":"","text":"2022年6月第四周简报生活我嘞了个大槽，北京最高温度40度，抗不住，抗不住 本周减肥计划继续，体重继续下降了，下周开始准备看看游泳或者划船机了 本周身体状态一般，主要是药物关系，导致肠胃不太舒坦。。。 本周继续玩了还能多好玩的东西 补完了 BBC 的纪录片《七个世界一个星球》，HDR 版本太爽了，想换个电脑了 间谍过家家 EP12，典型的总集篇，很有新意，笑死我了。以及间谍过家家的 Op/ED 都有点致郁 重新复习完电影海猿四部曲，我当时选择加入救援队以及现在坚持做公益，很大程度上都因为这个系列。复习依旧热泪盈眶。等疫情过了去把 AOW 考了，然后看情况考救援许可 奥特银河格斗，，真的不知道怎么吐槽了 本周的改善伙食依旧是巴奴毛肚火锅！好吃！ 本周家里迎来了第五只小猫咪！名字暂定！ 本周继续的杂书时间！美滋滋！ 本周的看书 《人造美人，星新一小说集》 看完了，我又坚定了一个想法，星新一真的是在俯视着众生。对于众生诸事嬉笑怒骂，入木三分。 技术 本周继续花了一些精力在 nerdctl 上 首先上周的 Discussion1128 ，写了一个 PR PR1150 ，不过这周有了新的需求，参见 Issue1154 ，简单来说，就是在配置里支持启动，构建模版。这个 PR 还是比较简单的。身边的朋友正好有对 nerdctl 感兴趣的。我准备 Mentor 他写这个 PR 这周排查了一个 CNI 方向的 Bug，参见 Issue1149，简单来说就是 compose 模式下，使用 external network 会导致往容器的 /etc/hosts 内写入错误的 record，修起来稍微有点恶心。我还在想具体的方案 这周终于作为 nerdctl 的 maintainer 进入到 containerd org 中，感谢社区 mentor 的包容。 这周 Burst Spec 终于有 follow up 了，感动 这周周末花了一些时间折腾我软路由的 Openwrt 的 IPV6 支持。最后的结果还不错，自行编译了一版支持比较好的干净固件自用，也踩了一些坑 目前不少城市的运营商，还不支持二级/三级路由的 IPV6 NATIVE 获取。目前的解决方案就三种，NAT/Passthough/relay 在使用高版本内核的固件的时候（我用的 5.15）需要关注你日常用的 kmod 是否编译进内核了，不然后续安装会很麻烦 有一个比较疯狂的想法，目前软路由一个很大的缺陷就是内网转发效率不佳，其根源在于全部走 netfilter 效果太差（iptables 本质上也是基于 netfilter），那是不是可以用 eBPF + XDP 写个高性能的内网转发？有空可以试试 因为需要帮人做一个 API 网关，所以这周也花了一些时间去调研目前 K8S 下 Ingress/API Gateway 的生态 APISIX/Kong 之类的传统 Openresty 系网关倒是相对比较成熟了。不过说实话我们需要比较强的扩展场景，Lua 写起来太恶心了。 Linked-Proxy 之类的项目也还是过于冷门了 目前基于 Envoy 的 Gateway 说实话没几家能用的。。虽然我把 Envoy 黑出翔了，不过最后可能方案还是控制面 Istio，数据面 Envoy，扩展走 WASM 本周帮人开始做一些配置治理的工作，生产上硬编码数据库也真是头疼 本周在上周关于“拦截特定进程对于特定文件的打开读写操作”这样一个需求的吃屎经验上，写了一个小玩意儿 ptrace-jail 用 Linux 自带的 Netlink 机制来监控进程的创建与销毁 对于新创建的进程，判断是否符合规则，符合的话，就 ptrace 注入防护规则 相当于实现了一个简单的 SELinux 23333，不过 Python 性能还是灼急，过段时间准备用 Rust 重写一下 总结天气炎热，大家多保重，以及北京的朋友，过段时间一起约饭啊（","link":"/weekly/2022/2022-06-week4.html"},{"title":"","text":"2022年7月第一周简报生活这周雨好多，听雨半夜发呆感觉真好 本周开始用哑铃锻炼了（ 身体状态吧，也就那样吧 本周业余时间还是很丰富 每周都要看奥特银河格斗，每周都要要问候一下版本 复习了一下魔戒三部曲，我现在感觉可以戒掉爱奇艺什么的了，2160P HDR10 看起来不香么？ 间谍过家家居然 Season1 完结了，不开心 看了《四月是你的谎言》大结局少女心发作，半夜哭成狗 和女朋友开了新番《强风吹拂》，每天吃饭的时候刷，看完了，非常推荐，非常热血 家里的小猫咪长大了一圈，也皮了一圈 本周的改善伙食是日式烤肉！美滋滋！ 女朋友考试季过去了！开心！ 本周的杂书： 圆觉经略说 听说看下佛教素材能让我睡的更好。 技术 本周继续花了一些精力在 nerdctl 上 这周有了新的需求，参见 Issue1154 ，简单来说，就是在配置里支持启动，构建模版。这个需求 mentor 身边的朋友开始写了，参见 PR1184 上周 CNI 方向的 Bug，参见 Issue1149，这周开始写 PR 了，有点脏活，准备顺便看下 Podman 的 dnsname 的解决方案， 本周周末花了不少的时间在 WASM 网关的事情上，吃了不少坑，也有不少的收获 Envoy + V8 对于 WASI 的支持比想象的好。 有些依赖最好还是基于 WASI 进行编译，比如 getrandom 目前 cargo 的体验还算不错 Istio 有点奇葩，支持 OCI 分发产物，但是 artifacts 有点奇葩，参见 wasm ，开源的 wasm-to-oci 无法满足需求，估计要基于 oras 自己糊一版 如果不用 Istio 自己的 WASMPlugin 机制的话，估计就要自己抽 CRD 裹一层 EnvoyFilter，不爽 配置治理完成了，pydantic 好东西，后面还能裹一层配置中心 本周继续迭代 ptrace-jail ，有点头疼 这周解决了一个偏移没算对的问题（头疼 有些恶意二进制走 kernel module 对本体做了加固，无法 kill，也无法 ptrace，得考虑走 kernel module 进行对抗 总结最近上海数据泄漏，大家务必多小心电信诈骗","link":"/weekly/2022/2022-07-week1.html"},{"title":"","text":"2022年7月第二周周报生活本周又见证了历史 本周出去透风啦！去延庆泡温泉了！开心 身体状态好像开始恢复了！ 本周周末睡了好多觉（有点小愧疚 本周业余时间还是很丰富 和女朋友一起打卡了《奇异博士2》，旺达好看，剧情鬼畜 继续看《小谢尔顿》好看 《七个世界一个星球》北美和欧洲篇看完了，好想出去玩呜呜呜呜 又看了一遍《强风吹拂》，这年头这样纯粹的感动很少了，不比这两年的傻逼奥特曼强多了 大眼珠子开新书了《深海余烬》，开心啊，美滋滋 本周看了闲书，美滋滋 本周的改善伙食是毛肚火锅和湖北菜（ 小猫打疫苗啦，两周长了1斤多（真能吃啊 本周的闲书时间 《东京贫困女子》 看得很沉重，很压抑，那种书里所采访人的那种没有未来的感觉，太过于沉重，以及东亚三国某种意义上同源了属于是 技术这周纯粹在放羊了，技术上学到的和输出的不多，，还是简单的记一下吧 上周提到的 Issue1154 ，简单来说，就是在配置里支持启动，构建模版。这个需求 mentor 身边的朋友做了一个 PR，参见 PR1184，这周有一波交流，最后达成一个共识，不额外新起配置了，可以复用 docker-compose 的 Spec 。 本周写完一个网关的技术方案，继续投了不少时间在 WASM 上，简单在这碎碎念一下 Istio 的 WASMPlugin 其实目前来说相对来讲成熟了。但是作为一个高阶抽象所要付出的代价，就是相较于配置 EnvoyFilter 你对于整个生命周期的管理能力差了很多。阿里云有类似的配套方案，会更成熟一些，但是有 Vendor LockIn 的问题 WASM 去年支持了 SIMD 128 ，参见 Issue480，在一些高性能的场景下可能比想的更适合 如果要将网关插件全面切换到 WASM 的话，现在还有不少的路要走。因为会涉及到很多和 POSIX API 交互的东西，而目前 WASI 其实成熟度比我想得还会低一些。具体可以看不同的 runtime 支持，比如 Wasmtime 的 Wasi-Common。短期内对于复杂的插件场景，可能 WASI 现在还不支持 POSIX Socket，看了下目前还在 Phase 1 的 Proposal ，参见 WASI-Sockets 要是这个 Proposal 能普及，那又能落地很多场景 看了下 WASM GC 的 Proposal，信息量有点大，还得慢慢消化，参见 WASI-GC 本周在准备在我的一个小群内下周的分享，我和群友们一起组织了7-9月争取每两周一次的技术分享，目前已经有不少前后端的 topic 了，我自己准备了 Terraform/Buildkit/Ansible 相关的分享。大家聚集在一起讨论东西还是好玩的 OSDI 2022 马上开始了，目前我感兴趣的论文有这样一些（我对 MLSys 属实提不起兴趣来）： Verifying the DaisyNFS concurrent and crash-safe file system with sequential reasoning. UPGRADVISOR: Early Adopting Dependency Updates Using Production Traces. Transcendent Debugging the OmniTable Way XRP: In-Kernel Storage Functions with eBPF 自己基于 oras 糊了一个小玩意儿。 MVP 第三年续命成功，我一定再接再厉传播微软福音 总结这周又一次见证历史，颇有种山雨欲来风满楼的感觉，可能乱世降至吧，大家请务必多保重","link":"/weekly/2022/2022-07-week2.html"},{"title":"","text":"2022年7月第三周周报生活最近太热了，热的抗不住，所以我基本也都不出门了 女朋友暑假回家了，呜呜呜，孤身一人独守空房 身体状态相较于上周没有什么太大变化，没有变坏，也没有变好 本周周末还是睡了很多觉，愧疚与爽的一笔反复横跳 本周的业余时间 复习了《决战中途岛》，好看 继续看《小谢尔顿》，SP03 了，简直太欢乐了这家 开始看《特工卡特》，哇，卡特太棒了 《德凯奥特曼》截止目前两话了，开局比扳机超人好了不是一点半点，任务塑造在新生代里算是能排的上好。编剧不搞妖蛾子的话，持平泽塔不是梦（先奶为敬 大眼珠子的新书《深海余烬》，emmmm，现在还看得有点懵 本周的闲书时间继续 本周没有出门改善伙食，不过和女朋友久违的宵夜时间 新到家的小猫结束隔离期了，MD，皮的一笔啊 本周将家里的路由升级到 WiFi 6.0 了，TPLink 对料就是猛 下周要去花卉市场挑一盆红枫养在家里 本周的闲书时间 《东京贫困女子》 看完了，压抑到一度噩梦，算了，人类毁灭吧 以及推荐一篇公众号文章 那对眼里有光又消失了的郑州小夫妻 破防了，泪目，，（我是不是有点太多愁善感了 技术这周还是在继续放羊，最近属于是躺尸体典范了，大家不要学我 本周还是花了一些时间在 nerdctl 上 上上周 CNI 方向的 Bug，参见 Issue1149，这周开始完 PR 了，参见 PR1232，只能算临时打补丁。可能最终还是要从 CNI Plugin 着手彻底解决这个问题。 Issue1039 ，nerdctl 的 Logging Plugin，估计以二进制+ stdout/stderr 的形式走，fahedouch 在开始写这个 PR，我也目前在实现一版，最后在整体的设计吧 增强了下 Logging 这块的用户体验，参见 PR1244 Review PR1214 ，这个 PR 功能挺不错的，而且是社区的新鲜血液贡献的 接到推上朋友的反馈，准备把 Issue54 提高优先级，实际上这也是个 Docker Compatible 的功能，提供完整的 Filter Opts 的实现。 本周遇到一些场景，发现在已有 SaaS 的服务前裹一层 Nginx 或者其余 L7 的代理还是很有必要的 比如如果你调用一些第三方的服务，如果对面有 IP 限制，且你没法更换你的业务的出口 IP，你可以考虑将 L7 Proxy 部署在云厂商的动态 IP 的节点上 比如 Sentry 一些错误服务，如果业务方沙雕导致一次性将 Quota 用尽，那么通过裹一层 L7 Proxy，你可以在让业务尽可能无感知的情况下切换服务 本周在刷题公益群对内进行了一次关于 Terraform/Pulumi 以及 IaC 的分享，效果还 OK，下次轮到我的时候估计聊一下 Buildkit 的东西 帮群友进行了一次 Mock Interview，对我自己也有不少收获 本周的论文时间，新鲜出路的 OSDI 2022 的文章《XRP: In-Kernel Storage Functions with eBPF》，还不错，下周可以在周报里写个短评，2333333 本周也还有一点开心的事情2333 总结最近看了许多的新闻，烂尾楼，村镇银行，突然想起曾经觉得自己很苦的想法就有一点点好笑，也想起了《生活大爆炸》里谢尔顿大哥乔治对谢尔顿的话 I know life is always hard for you, but it doesn’t means it’s easy for rest of us. 长太息以掩涕兮，哀民生之多艰 生活不易，大家请多保重","link":"/weekly/2022/2022-07-week3.html"},{"title":"","text":"2022年7月第四周周报生活莫名其妙的低谷期 本周还是没有女朋友的一周，呜呜呜 本周状态奇差，周一周天晚饭后突然就什么不想干，觉得自己是 Kind of Loser 的一周 本周事情也贼多，四只猫同事生病了，自己生产上搞了事故（技术章中我会完整复盘） 本周的业余时间 德凯奥特曼第三话，保持目前的态势，德凯要成，人物塑造非常的不错 开始复习《实习医生格雷》，医生真的不错 大眼珠子的新书《深海余烬》，目前为止，展开的不错 复习日剧《仁医》，感动得稀里哗啦 本周因为摆烂，没有看闲书 本周没有女朋友，没有动力改善伙食（ 本周出去挑新的植物啦！两盆红枫，两盆茉莉 技术这周是摆烂的一周，愧疚感爆炸的一周，不过还是简单聊聊吧 nerdctl 开始写 Issue54 了，增加一些 Filter，目前在写 Image 的部分，下周估计能 PR 又是与 Envoy 奋战的一周。有一说一，Envoy 我是坚定的黑粉（具体可以看下 CVE-2019-25225 Envoy 早期的心大程度）。不过其很多的设计还是比较有意思的，比如 Envoy 将 Rate Limit 这样一些常见的功能抽象出来，定义 Protocol，让用户可以不用受限于 Plugin 的局限性而是具备完整的 Service 的扩展能力，这样的设计我比较认可。可以参考 Rate Limit Service (RLS) (proto) 又回到了业务系统异常治理的日常，对于一个异常手机系统而言，我觉得应该是具备 Project 粒度的限流能力的，否则如果业务方路子太野导致了异常爆炸，如果不具备限流能力，那么你可能会导致整个业务系统的异常搜集出问题。如果是你的服务商不具备这样的能力，那我觉得 SRE 团队需要具备在上面裹一层流量治理的能力 帮群友 Review 周报的时候，大概复习了一下 HTTP3 的 RFC9114，收获挺大 本周的论文时间 《XRP: In-Kernel Storage Functions with eBPF》 我太喜欢这篇论文了 这篇文章的工作做的相当扎实，比一众 ML System 的论文高到不知道哪里去了（我给作者的问题咨询邮件里也表达了我对这篇论文的超级喜欢） 对于 NVME 这样的新型硬件设备，传统的 VFS/BIO 读取这一套已经无法充分的发挥性能（受限于用户态/内核态迁移，Context Switch），即便 SPDK 这样的 Kernel Bypass 的技术也有不少的局限性 作者在 5.12 内核上 Patch 了一波，在 NVME 驱动层上封装了一层 eBPF 执行点。可以让用户执行 JIT 后的 eBPF Bytecode，这某种意义上算是 Storage 的 XDP。在 NVME Driver Complete Interrupt 回调用户 BPF Function 后，可以由用户自决是否直接 resubmission command 到 NVME 的 SQ 中。尽可能的减少（用户态 syscall-&gt;内核态 VFS -&gt; Kernel Block Layer -&gt; NVME Driver）这样的来回 在此内核 Patch 的基础上，作者将一套简单的 KV，有不错的性能 整体的项目已经开源，参见 xrp-project 排查我的 MBP 为什么在内网下和 NAS 之间带宽不够的问题，结果发现 Apple 会在特定情况下，2.4GHz 更强的情况下将链接切到同一个 SSID 下的 2.4GHz 频段上。即便这个时候 5GHz 频段也很猛，Damn..。 好了，开始本周的一个事故覆盘，以后在允许的情况下，我也会在周报中覆盘我遇到过一些有意思的事。这次之所以覆盘我这周搞出来的小事故，是因为这个事故虽然小，但是比较典型的大意失荆州的例子，所以大概聊聊吧 首先聊聊背景，我这周在落地一个类似网关的东西，需要做流量入口的切换的操作。我之前最开始设计的方案是先新增一个 ALB 关联在新服务上，然后额外新增一个 ALB 对老服务的 ALB 和新服务的 ALB 做流量灰度的操作。当然这就涉及到了对于 CDN 的变更 然后大概的时间线如下 我通过 Terraform 开始变更 CloudFront 的 Origin 相关的配置（不过因为业务方另外一个功能错误使用 CF 的 API，导致帐号被 RateLimit，我的变更实际上是没有原子化成功的，导致 API 访问会出现 CORS 的错误） 三分钟后我之前做的全球拨测报警开始报警（监控系统的重要性） 我开始快速回滚 Terraform 配置，但是由于 Rate Limit ，回滚失败，我直接去网页强行手动修改配置（快速止血） 业务恢复 从事故后的视角去复盘这次事故，我犯的错误实际上都比较典型 实际上在前段时间一次偶然机会下，我就观察到 CloudFront 限流的问题，然而没有推进业务方的治理 实际上在技术方案设计的时候也是比较大意的，有更好的通过 ALB CRD Annotaion 的方式直接灰度流量（虽然写起来比较丑陋），然而我却依靠自己的惯性思路去选择了一条自己最熟悉/最喜欢的路径而不是正确的路径，这实际上也犯了技术方案设计的忌讳 在饿了么时期，我从导师 @tonyseek 那学到的最重要的一句话就是 敬畏生产，遵守 SOP 每当我忘了这句话，或者抱有侥幸心理的时候，事故总会教我做人。所以这句话，也与大家共勉 总结最近负能量太大了，还是想在周报里积极一点，这周看的日剧《仁医》里，坂本龙马去世前，问南方仁这样一个问题 医生你是怎么看待我们这个时代的呢，一定会觉得很笨的吧 突然想起了大眼珠子在《黎明之剑》我最喜欢的一段话 “朋友，希望这封信没有让你太过意外，在我们有限的智识中，我们实在想不到更好的办法，来与一个可能会在我们死后数十年甚至数百年才会降临这个世界的存在建立交流，人类的寿命过于短暂，而灵魂脆弱难以保留，我又不愿将心智交付于亡灵巫术或神明那缥缈的赐福，思来想去，只能给你写一封信。“你可能会认为这种‘交流’并无意义，因为当你收到这封信的时候，我可能已经成为泥土的一部分，但对于我们这些寿命短暂又眷恋世界的生物而言，有许多看上去没有意义的行为都是值得去做的，因为纵使死后，我们也有很多放不下的事情。“你如果能读到这封信，那想必已经如高文卿所言的那样履行了约定，你想必已经降临大地，并在着手振兴这个支离破碎的世界了吧？“圣灵平原现在产的粮食够吃了么？西北边境那边现在应该不会再冻死人了吧？安东尼一直想在东部地区建造一座坚固的大城，那座城建起来了么？“我真的很想看看，你那个时代的世界变成了什么模样，我想象力不足，也勾勒不出未来的风景，但我想那至少会是个比现在更好的时代吧？“我们这个时代……不怎么好。别误会，我仍然热爱这片土地，热爱这片土地上的每一个人以及我们共同建设起的一切，但即便如此，我们仍然生活在一个艰难的世道，粮食总是不够吃，北方的天气比我们启程时所知的还要寒冷。温暖的地方离废土太近，安全的地方万里冰封，我们已经在这片陌生而冷硬的土地上开垦了许多年，但这片土地仍然将我们视为陌生人……“但是到了你那个时代，这应该都已经是过去的记忆了吧？“朋友——我不知道你是否会视我为朋友，但我想这么称呼你。高文卿说你会继承他的记忆，继承他的躯体和姓名，在降临大地的岁月里，你就将是另一个他。尽管对于像你这样的存在而言，这可能只是一个临时取用的躯壳，但他跟我说，你值得信赖，值得把许多东西托付，而我一向是信任他的，所以现在我也信任你。“我相信——如果你连这跨越时光的约定也愿意遵守，连一个在你眼中可能眨眼即逝的族群也愿意关注，那你也一定能实现高文卿和我所畅想过的那些事情。“所以我便将这一切都托付给你了，一个烂摊子，一个烂世道，一个不那么有前途的种族，一个碎成一地的文明，怎样称呼这些都可以——但请务必珍重它们，因为即便再烂，这也是我们一路走至今日所拥有的一切，是我们最为贵重的珍宝。“你素未谋面的友人，C。” 这是最好的时代，也是最坏的时代，愿大家在这样碰撞的时代里，诸事顺意，不负此生","link":"/weekly/2022/2022-07-week4.html"},{"title":"","text":"2022年7月第五周周报生活本周好像可以up一点了 女朋友回家啦！开心心！ 给女朋友过生日，出去玩手工，好玩，以及廿一客的🎂真好吃 本周活儿还是不少，不过产出还 OK 猫猫们都恢复了！ 本周的业余时间 复习《异常生物见闻录》，有机会我一定要出资不计成本的把这部小说动漫化。 《小谢尔顿》S04！好看，这一家子太欢乐了 德凯奥特曼第四话，德凯要成，截止到目前，致敬和创新都做的不错。不过自己也真是老了，当年看奥特曼的时候，里面的女主都是大姐姐，现在看奥特曼里面女主都是小妹妹 和女朋友一起刷了新电影《独行月球》，火星救援+天地大冲撞的集合体，而且全球人类出来给沈腾打光的时候突然想起迪迦奥特曼最终话《致以辉煌的人》，那一年我们都变成了光.jpg。 配乐很赞，《Take Me Home Country Road》改编的很惊艳 本周开始久违的闲书时间了 本周因为核酸关系，没法去买药，断药了两天，情绪发作了（不过我很讨厌药物依赖的感觉 本周的伙食改善：淄博小饼！好吃！德国大猪走子 新到家的茉莉和红枫非常健康！ 本周的闲书时间 星新一的小说集 《人造美人》，印象很深的一章短篇《价值检测仪》，是平台成就了人，还是人成就了平台？ 重读20世纪中国小说，文学评论类的书籍，闲暇时随手翻看几张颇有趣味 技术我最近怎么感觉我摆烂的越来越没有愧疚感了（ 这周花了不少时间在 nerdctl 上，其中不少是 Review 印象比较深的是 PR1260，PR 的大概意思是在 nerdctl ps 的时候，将 label 输出出来，我，作者，Suda 在是否将 nerdctl 需要的一些涉及到 OCI 的 Label 默认输出出来，讨论比较有意思，大家可以看一下。一个很小的变动可能都会牵扯到非常多的用户体验的讨论，这也是开源社区的魅力之一 我的 PR1244 合并了，我自己在代码细节的处理上有些时候还是不太注意。要反思一下 正式开了新坑，先实现 Issue1283，给 nerdctl images 新增 filter 参数，然后后续会依次实现 ps, volumes, networks 等命令的的 filter 支持。应该能保证和 Docker 百分之八十以上的兼容，有兴趣的同学可以一起来写。 这周又在和 WASM 抗争，我感觉我快放弃将 WASM 完整的后端 Plugin 的想法了，WASI 太不成熟了，有些时候你得像 WasmEdge 那样，自己在 runtime 里实现 Host Function 然后绕一圈来实现一些 Posix API，虽然改 Runtime 也不是不能做，但是 ROI 成迷 工作上又开始写网关了，上周说的 Sentry 保护层。最蛋疼的还是怎么样设计配置文件的语义 给家里的 NAS 新增了一组 UPS，然后按照 SOP 进行了一组故障演练，达到预期 上周的论文 《XRP: In-Kernel Storage Functions with eBPF》，这周和作者邮件沟通了一下一些问题，作者非常 Nice： 目前 XRP 项目可能还是会作为 experimental project 存在，短期之内得不到足够的资源的话，很难向 upstream 合并 对于云厂商的 NVME 虚拟化实现，比如 AWS 走 Nitro System 这种专属网络协议 mock nvme 协议的实现，XRP 不确定是否有可能会得到和标准 NVME 硬件一样的性能提升 作者推荐了三篇论文，也是目前学界和工业界对于 eBPF 在 Storage 方向上的应用 Safe and Efficient Remote Application Code Execution on Disaggregated NVM Storage with eBPF Extension Framework for File Systems in User space ExtOS: Data-centric Extensible OS 希望以后能有机会和作者一起合作Hhhhhh 嗯，差不多就这样 总结今天晚上和老爸打电话聊天，他说他觉得自己这一生很失败（父子俩在失败学上这点得到了传承），我告诉他“在我心里，虽然你经常说自己，也说我妈（BTW他俩离婚了）这一生都有些失败。但是从我看来，不是的。我健康的活到了28岁，你们也健康的活到了现在。我现在的事业虽然也才起步吧，但是至少我身边的朋友和网友都觉得我是个三观很正的人，不少信任我的朋友在和我素未谋面的时候愿意几万块给我。你说我这些特质来自于谁？来自于你们俩。这些就够了，至于世俗的钱什么的，I don’t give the shit。所以别想那么多，好好过着，至少我之前在割手腕想要自杀的时候，是你和我妈在我脑海里拦住了我，某种意义上我为你们好好活着，你们也为我好好活着。世道不容易，父子之间不存在失败不失败，咱相互依存，你在儿子心里，绝不失败” 反正世道不容易，大家都好好活着。","link":"/weekly/2022/2022-07-week5.html"},{"title":"","text":"2022年8月第一周周报生活看番差点忘了写周报了 这周和女朋友一起堕落（笑死 本周把乐高钢琴拼完了！开心！ 本周活也不少，产出还行 猫恢复后果然会很皮，小猫精神真好啊 本周番剧时间 《异常生物见闻录》，太好看了，不少章节要落泪了 《小谢尔顿》SP05 Done，到后面越来越沉重了，Life is struggle. 德凯第五话，我觉得很不错，至少到目前的人物塑造很让我惊喜，而且镜头和拍摄手法很惊喜。但是还是有点担心开局世界观太大了，后面能不能收回来。不过至少就前五话的观感来说，我觉得是新生代最 top 的一批的。后续如果没有翻车，我觉得可以和旧平成五五开 这周刷《咒术回战》差点忘了写周报 和蓝莲花小组的朋友们，一起凑了几千块钱，给一个村小捐了过去。明年想一起认领资助学生。教育是最好的公益 恢复吃药后，状态稳定下来了。不过睡眠呼吸暂停的状态越来越严重了，下周去朝阳医院做滴定，买呼吸机吧 本周伙食改善：南京大排档 我的 MVP 礼物到了 技术我摆烂我快乐（ 这周花在 nerdctl 上的时间有点少了 Issue1283 写完了，补完测试就可以发 PR 了，我讨厌写测试。。 之前社区提到的自定义 Logging 插件好像没动静了，准备继续 Follow 一下 这周在优化家里网络的时候，发现 AdGuardHome 的 QueryLog 存 JSON 的方式太过于拉跨了。在跑了一段时间+家里设备很多的情况下，会降低整体的运行速度，我准备切换成 SQLite 试一下，不过这种时序数据存 RDBMS 也是比较蛋疼的 Linux 519 正式 Release 了，尝试把自己定制的 OpenWRT 固件升级到 519 内核未果，编译有点问题。先基于最近的 LTS 515 修了一下用的时候遇到的一点问题，目前自用感觉良好 工作上的 Sentry 保护层网关上线了，不过 Sentry 的协议有点奇葩，要做一些解析，emmm 有点蛋疼 遇到一个很有趣的场景，发现我们私有的一个上报 Metric 的服务存在被人刷数据的情况，目前尽可能临时的解决方案还是考虑给 CDN 上加一层 WAF ，剩下的再去考虑其余的解决方案。BTW 这事拖了一点时间，因为最开始这个接口没有记录 AuditLog，导致没法分析数据。这也提个醒，公网接口一定要对接审计日志 本周份的论文 Extension Framework for File Systems in User space，ATC 19 的文章，比较有趣： 基于 FUSE 的工作，FUSE 的优点很明确，方便调试，易于扩展 缺陷很明显，性能很差（越好的设备影响越大），整体来说 FUSE 会牵扯到多方面的跨态数据传输。比如你一个 write 操作，会牵扯到 getattr 等果然子操作来确保安全性。所以性能会比较影响，这篇论文也引用了 To FUSE or Not to FUSE: Performance ofUser-Space File Systems 中做的一些工作，大家可以去这里面参考具体的场景测试 所以作者考虑基于 eBPF 来做一次优化，通过 eBPF Map 等手段尽可能的减少跨态开销。 和 《XRP: In-Kernel Storage Functions with eBPF》 膜改了一版内核，加了 eBPF Attach Point 目前看起来，大家要实现扩展 eBPF 的能力的话，感觉还是需要自己去膜改 Kernel 然后编译。可能后续社区会出一种通过 Kernel Module 挂载的形式扩展 eBPF 的方案 刷题群的小伙伴的内部分享正式搞起来了，一个月两次，目前排到12月份了，前后端数据库都有Hhhhhh，感觉还是蛮开心的 嗯，差不多就这样 总结本周的事情也真是见证历史，希望世界和平吧","link":"/weekly/2022/2022-08-week1.html"},{"title":"","text":"2022年8月第二周周报生活这周周报延期发布了一天（你们猜是为啥（ 这周在堕落与不堕落之间反复横跳 本周开了新的书的坑 家里的植物状态有点不对劲，有点白霜病，开始植物吃药模式（ 家里大缅因的开始闹肚子，猫猫吃药模式发动！ 本周的番剧时间 《异常生物见闻录》看完第N次，开始看《黎明之剑》第N次 《咒术回站》刷完，听说后面有刀？ 补了下《双星之阴阳师》前期小崩，后期还不错 德凯这周挺更，不爽 本周发现周边一家很好吃的店，兰州热卤面好吃 揭晓谜底，本周忙着跑医院去了， 周天晚上去医院住了一晚上（医院没有零度，也没有好吃的，真烦人），不过睡眠检测结果出来问题不大23333 本周的闲书时间： 谁在掷骰子？不确定的数学，科普向的书，读起来也是交友趣味的 技术有了住院这样的借口，摆烂摆的心安理得 本周花了不少的时间在准备给公益群的小伙伴做分享了，题目是《SRE 二三事》，科普向，差不多包括以下几个方面吧，文字稿我也还在整理，不过后续的讨论也很有意思 SRE 整体的历史和发展现状 目前 SRE 的一些工作内容 我自己对于 SRE 这份工作的一些个人看法 本周开始水了点文章，也是新手向的 容器 CPU 和 Memory 限制行为简述，然后在推特上衍生出一些好玩的讨论，很有趣 OpenWRT 在关闭一些 netfilter 相关的东西后，成功编译到 519 内核了，下周有时间可以跑个虚拟机测一下 家里的 NAS 挂了 NVME 的 SSD 做 cache，不过现在性能提升也还不是很明显，可能要等 warmup 一下 这周又在吃 Celery 的屎 Celery 在 4.x 时期，不支持输出 Prometheus Metrics，第三方的 Exporter 存在比较严重的 mem leak，使用的时候注意 Limit 内存 Celery 5.x 在依赖上存在一些 breaking change，如果你其余有一些库锁了版本，注意检查冲突 Celery 4.x 自己官方的 Flower 也存在比较严重的 mem leak，还是使用的时候注意 Limit 内存 更多讨论参考 这里 总结大家多保重身体","link":"/weekly/2022/2022-08-week2.html"},{"title":"","text":"2022年8月第三周简报生活这周的一开始是出院（也是别样的经历了 这周周末持续躺尸，爽啊 家里的植物状态转好，新的红叶子长出来了，植物生命力真顽强 缅因状态恢复 本周可能是去医院吓了一下，妹子说我晚上睡觉打鼾减轻了很多（难道是变种的安慰剂效应？ 本周的娱乐时间 和妹子去看了《杨戬》，说实话有点失望。古风＋赛博朋克看似新颖的设定，并不能弥补在剧情上的大缺陷，在我看来，这本质上是一部披着神话外壳的超英电影 德凯第六话，男一和男二的人设立起来了，前面几集的一些伏笔开始回收。这一届的编剧不错啊 《黎明之剑》第 N 刷刷完了。。又要小说慌了 《2012》和《黑豹》重新补了下，，感觉不好看，我还是老老实实期待阿凡达2吧 《Top Gun2》看了下，F14 狗斗 SU57 也是牛逼，不过情怀加成（ 本周继续闲书时间 本周的伙食改善时间，还是巴奴毛肚火锅，以及吃了一加很难吃的烤鱼呜呜呜 本周的闲书时间： 谁在掷骰子？不确定的数学 技术 本周花了不少时间在 nerdctl 上， nerdctl images 命令终于支持 –filter 参数了，详情参见 PR1307 开始推进 nerdctl volumes ls 支持 –filter 参数，详情参见 Issue1330， 社区的小伙伴也写了一个 PR 上来，参见 PR1331 和社区的小伙伴讨论怎么收敛一下 nerdctl 中 flag 的使用，现在满天飞的 flag ,改起来颇有糊屎的感觉（逃 nerdctl 在考虑加入 Container Lazy Load 的支持，参见 Issue1329，不过目前方案还没定下来 本周开始糊了下 AdGuardHome 把 DNS QueryLog 的存储引擎换成 SQLite，但是原本代码的设计简直堪称无法形容（ 官方实际上也有考虑在切换，但是考虑 SQLite 会引入 CGO 的依赖，导致一些平台上的二进制分发会有问题，所以一直没有定下来，参见 Issue2290 本周继续在折腾 WASM+网关的一些事情，也有一些收获吧 可以分场景逐步迭代 WASM 的支持，比如 API 签名的一些东西比较好的切换成 WASM 的 Plugin，这样收敛后更方便治理 对于一些有 POSIX 需求的插件场景，可以分两种情况讨论，一种比较简单的文件读取之类的，可以简单搞点 host-function 来解决，比较复杂的还是老老实实走其余的方案吧 监控必须覆盖到位 本周公益群内的分享顺利进行，下一次就是九月分享了，有两场 OLAP 入门到放弃 React Native 无线滚动列表实现 本周开的新的论文坑，Slacker: Fast Distribution with Lazy Docker Containers，FAST16 的文章，AWS 的 Container Lazy Load 的方案，有点意思，下周聊聊 一个新的笔记工具，看设计思路比较有意思，AFFiNE 差不多就这样 总结这周群里的各位突然聊起了人生目标，想了下，我自己的目标好像也没什么很大的愿望 我只是希望很多认识我的人，在很多年后和老朋友聊天或者和小孩子聊天的时候，想起我的时候，能说一句我曾经认识个人，缺点有不少，但是是个不错的人（ 如果能这样，那就真的很满足了（","link":"/weekly/2022/2022-08-week3.html"},{"title":"","text":"2022年8月第四周周报生活这周又是颇为 Hard 的一周 本周状态奇差 家里的红枫挂掉了一棵，不过另外一棵红枫的叶子长得很好看，家里的茉莉花也重新长出了新的花 家里的小猫太太皮了 感觉每晚三点睡有点抗不住了，周六突然难受卧床了一天 本周的娱乐时间 手游使命召唤打到了大师一，真牛逼上战神了 本周的小说《大医凌然》 德凯第七话，扳机超人回归，武居还是牛逼，把斯卖鲁星人的神经质治好了，观感很好 《波西米亚狂想曲》，永远的 Queen，永远热泪盈眶 《铳动彼岸花》/《石蒜》，我也是彼岸花诶（逃，不过有一说一，这番的战术动作绝对是找行家指导过的，C.A.R. 持枪法很难难道正确的应用，不错不错 本周的闲书时间泡汤 本周的伙食改善，两顿芈重山老火锅，美滋滋，不过我快被开除四川籍了 本周的噩梦让我消沉了许久 我和妹子资助的学生走出大山啦！ 技术身体实在不舒服，所以技术继续摆烂了 本周还是花了一些时间在 nerdctl 上 社区小伙伴的 PR1331 经过 Review 之后合并了。现在nerdctl volumes ls 支持 –filter 参数了 开始推进，nerdctl container ls 支持 –filter 参数这事了，参见 Issue1339 这周又开始做一些稳定性上的东西了。算是开了一个新的项目了，算是有些碎碎念吧 做稳定性落地的时候，一定要考虑清楚统计的口径，是以接口报错率，还是以多个接口组合报错率，还是其余指标来作为衡量标准。一个不可量化的稳定性项目不如不做 可观测性的维度一定要多方面的覆盖，比如后端，前端，用户报错率等，单一维度的指标会存在很多的解释性问题 业务的异常治理一定需要长时间跟进，定期收敛，同时也要充分推动业务完成异常细粒度以及详细化处理 本周在折腾一篇之前想写了很久但是一直没有写的文章的代码，即怎么样在非标端口上拦截 https 流量（或者说国内公有云厂商拦截非标端口 https 流量的常见流量特征是什么 推荐一个项目 xdp-tutorial 给对 eBPF 以及 XDP 感兴趣的同学 本周的论文时间 Slacker: Fast Distribution with Lazy Docker Containers，FAST16 的文章，讨论容器的 lazy loading 根据对于主流载荷的一些 benchmark 之后，得到了几个结论 容器启动时间的百分76或者更多是容器处理 package data 的 启动时复制时的数据中只有 6.4% 是启动时所必须的 AWS 自己提出了一套 Slacker 作为 Lazy Loading 的方案 将 AUFS 的 Storage Layer 打平了，统一放在一个多个 Worker 共享的 NFS 池子中 抽象出了一套 snapshot 的概念，会记录在往 Registry 推送的 metadata 中。一次推送之后，其余的 worker 可以直接拿着 snapshot id 在共享的 NFS 池子中查到 block ，然后避免重复拉取 当然也提出了一些改进方向，比如在内核里通过 bitmap 来实现对于打平后的文件的 COW 的支持等 看了下 AWS 的 soci-snapshotter 差不多就这样 总结这周一次午睡的时候，突然梦回07年被性侵/强奸时的那一天，被侵犯时的场景，父亲报警时的话语，在派出所作笔录的过程，历历在目，仿佛昨日重现。15年过去了，我以为我忘了，但是看起来生活还是给我开了个玩笑 不过生活还得继续，Everything is gonna be OK，愿大家一切安好，愿天下所有人都不会遭遇我曾所遭遇过的一切。","link":"/weekly/2022/2022-08-week4.html"},{"title":"","text":"2022年9月第一周简报生活 Life is struggle 本周先重感冒了三天，然后哮喘了两天，最后周末在抑郁翻盘中度过 家里的茉莉开得真好看 本周有不少时间在卧床中度过 本周的娱乐时间 手游使命召唤打到了战神，满足了 德凯第8话，久弥文戏特有的迷幻感，三角形果然是最稳定的几何关系，以及下一话浩二叔回归，泪目 群里的朋友们推荐了一点新番，正在开始补，也欢迎大家在评论区推荐新番，最好是搞笑类的，四月谎言这种最近状态实在是太差，我就谢谢你了（ 这周在重新看炮姐，下周准备看看白箱 本周难得的闲书时间 本周没有去改善伙食，但是附近开了盒马，和女朋友没事去逛逛，里面的排骨超好吃！ 本周状态应该又创了今年最低沉的记录，可能是药物副作用吧。噩梦与现实的半梦半醒之间，一度有了从楼上飞跃下去的想法。可能要换药了。顺带和妹子开玩笑，“要是我状态再这样反复，我就和你分手，你要是不分的话我就出轨”，妹子：“你出轨也得有人要啊” 希望 Everything is gonna be OK 本周的闲书时间 置身事内：中国政府与经济发展 技术总所周知，我现在摆烂摆的是心安理得 本周还是花了一些时间在 nerdctl 上 社区小伙伴的 PR1342 经过 Review 之后合并了。现在nerdctl ps 支持 –filter 参数了 nerdctl 准备支持 syslog 作为 log driver 了，参见 PR1353 在思考 nerdctl 一些模块的重构方案，下周应该能有一波 good first issue 放出来了 重回 K8S PR107531， 这个 PR 真是体力活，争取下周 fix 掉吧。 在 Suda 帮助下，Intel 同学关于 CPU Burst 的 PR PR1120 有了进展，我这周也开始复习下 CRI 相关的东西，准备开始做一些接入的准备工作了 本周搞了一个字面意义上的删库的低级错误，，每当我偷懒的时候，总会有意外教我做人 这周有一部分工作和 socket.io 有关 看了下 socketio 的 spec，在 websocket 上重新抽象了一层状态，挺不错的，参见 socket.io 用 Prometheus 对 socket.io 做监控的话，Node 生态有 socket.io-prometheus-metrics，Python 生态目前好像没看到比较合适的，准备自己造一点轮子了（已有的一些 WSGI Exporter 还是不太适用 本周继续看了下 AWS 的 soci-snapshotter，不过我在纠结接入 nerdctl 的必要性了，下周讨论下 公益群小伙伴本周迎来九月第一次分享，PDD 的老哥带来的 OLAP 的一些分享，听完感觉受益还是蛮多的，下周是关于前端跨端应用的分享23333， 大家都很认真，这样感觉真不错 差不多就这样吧 总结生活总是操蛋，在梦到小时候被强奸的经历后，这两周梦魇也持续围绕着我。不过还是坚信 Everything is gonna be OK &amp;&amp; 唯爱与希望不可辜负。","link":"/weekly/2022/2022-09-week1.html"},{"title":"","text":"2022年9月第二周周报生活 大家中秋快乐的啦啦啦啦啦 本周状态开始在恢复了，读书之类的也继续恢复了 MD 我怎么又又又胖了？ 本周娱乐时间 德凯第9话，德凯要成！浩二叔一出情怀满分，而且这集文戏野可圈可点，保持这样的势头下去好事！ 本周的追番时间，石蒜我感觉有点小崩啊。 和女朋友开始看银魂，我觉得好看！里面很多梗要笑死我 MD 又在捡起之前看过的小说看了 闲书时间继续 养猫真麻烦，本周家里大缅因又在拉肚子。养猫多年，我已经熟练掌握了铲猫砂的时候熟练闻出家里哪只猫在闹肚子的技能（好像也没啥骄傲的呜呜呜（ 本周改善伙食，胡大的龙虾好吃，螃蟹一般。以及我爱死盒马的蒜香排骨了（ 本周好像又在见证历史了 你说我要不要买 Apple Watch Ultra？（ 本周读书时间 置身事内：中国政府与经济发展 这本书写的真的不错，对于中国很多现象有很深入的剖析。以及推荐的参考书目很不错（ 技术摆烂每一周，不过躺平的有点焦虑了，要努力了 本周还是花了一些时间在 nerdctl 上 开了新坑，参见 Issue1374， 之前 nerdctl 的一些 log driver 不支持使用 logs 命令读取，所以准备优化下 本周发布了 v0.23.0，Docker 兼容性这块得到了不少提升，这里就不艾特某位作出主要贡献的群友了（逃 nerdctl 的 log 和 DNS 这个月底之前应该会重构一波，这周思考了下方案 本周跑去给新生代的编辑器 AFFiNE 贡献了 合理利用 Docker Build Cache 进行构建加速 对于 pnpm 这类的 monorepo 方案，可能使用 Buildkit 的 Cache Mount 是个比较合适的方案，参见 PR403 对于 NX 这样的构建系统，发现一个很奇怪的现象，开了并行构建后的构建速度反而慢下来了。下周再来找找原因 本周又是和可观测性战斗的一周 前端的可观测性基建诸如 Sentry/GA/NewRelic 可能会受到用户开启 ADGuard 之类的干扰，这就导致了如果你自建了埋点系统，你自己上报的指标和第三方系统的指标存在偏差。这点需要在分析的时候注意 在线上可能会存在一些恶意的爬虫用户，在使用无头浏览器进行访问。这部份指标可能第三方的诸如 GA 这样的系统会根据特定特征进行过滤。对于自建系统，如果没有对恶意 IP 进行过滤的话，那么分析的时候也会很明显的出现数据对不上的情况出现。 本周的论文时间 ESDB: Processing Extremely Skewed Workloads in Real-time ，下周可以写个简评 重新去折腾了一下流量可观测性的东西，gRPC 的可观测性做起来真的要比 Pure HTTP 麻烦很多（蛋疼 差不多就这样吧 总结本周女王去世，也算是再一次见证历史了，世界愈发混沌，大家请务必多保重。","link":"/weekly/2022/2022-09-week2.html"},{"title":"","text":"2022年9月第三周周报嗯，向贵州高速车祸中遇难的同胞默哀 生活 本周状态还不错 减重计划进行中，不过还是要坚持坚持再坚持 不过可能因为最近作息过于不规律，我过敏性皮炎又犯了。。 本周娱乐时间 德凯第10话，梦回旧平成了卧操，人与非人种族之间的关系怎么处理。镜头处理的也很棒，演员调教的很不错，镜头的话能明显看出来是吸取了好莱坞不少巨大化 CG 的手法。我觉得是新生代里最眼前一亮的拍摄 本周石蒜十二话更新，在人耳朵两边开枪利用噪声让人失能。石蒜的细节做的真的很不错 银魂.jpg 小说《小阁老》，喜欢历史文的同学可以看看 闲书时间继续 本周的改善伙食：日式烤肉，简直是好吃好吃好吃 最近看各种新闻，也是感叹人间疾苦 一点小事，让我觉得父母也真是老了 本周的读书时间 置身事内：中国政府与经济发展 史上最伟大的交易 最近开始看一些政治经济学的书，发现还是很有意思的，能很有效的弥补我之前的知识面的缺陷 技术这周忙于工作上的事，感觉社区这块的贡献不多，下周得努力一下了 本周还是花了一些时间在 nerdctl 上 上周的新坑 Issue1374 ，我把 fluentd 的东西写完了，下周也许能搞个 PR Podman 的 DNS 解决方案 dnsname ，本质上通过一个 network 起一个 dnsmasq，来解决多容器共享 Network 中的 DNS 问题。比用 /etc/hosts 优雅一些，但是起太多 dnsmasq 也会是一个负担。有点拿不准这个 trade-off 了，下周在社区发起讨论聊一下这个话题 本周和 AFFiNE 团队的同学聊了一下开发者社区运营的问题（迷弟也见到了雪碧大佬） 开源社区核心的原则还是信息透明化，利用 RFC/Proposal 来尽可能的提早公开相关的信息，能够让更多的社区的关注者拿到足够的上下文。可能很多人对于 RFC/Proposal 有一些误解。对于不同的社区而言，有着不同的治理模式，Vote or BDFL 都是可以让人接受的。 RFC/Proposal 更大的意义在于社区成员的 comment ，而不是单纯的 Vote 对于一些大型功能的新增/重构，尽早的提出 PR，然后合理利用 Draft 或者 WIP，让大家方便 track 一个 PR，尽可能避免不声不响一个超大 PR 进来，这点 logseq 最近有个很不错的例子 PR6475 对于一个组织而言，如何避免信息黑盒是个非常值得讨论的问题 本周又是和可观测性战斗的一周 在治理 Sentry 之后，业务层面的报错源能够被用数据精确的评估，两个迭代之后，线上报错得到明显收敛。进而形成一个“报错分析-&gt;反馈业务-&gt;数据反馈更正效果”的正向循环。这也说明稳定性建设中两个基本要素的重要性 对于稳定性的评估指标一定要可量化，不可量化的指标没有任何意义 稳定性项目需要持续迭代与跟进，猪突式毕其功于一役的稳定性建设毫无意义 本周在写一个自定义指标上报的项目，如何处理业务各种变化极快的上报需求是个挺值得思考的问题 本周和 cache/DB 战斗了一些时日 tidb 5.x 的 DDL 设计也是一言难尽，具体可以参考我的推上的吐槽 本周处理了一个标准的缓存雪崩的问题。业务方存在大 key，以及所有 key 的 TTL 都是 30 的整数倍，在某一时刻集中失效后，集中写入导致 Redis 负载急剧升高，升高后的业务超时重试机制又进一步雪崩了整个系统（过于标准的八股文场景 Redis 一定要持续治理大 key 对于 TTL 一定要离散化处理 善于利用 Cloud-Vendor 的故障转移机制（self-hosted 尽可能集群化处理），先救命，再治病 总结 父母真的老了，我也得再快点而立了（","link":"/weekly/2022/2022-09-week3.html"},{"title":"","text":"2022年9月第四周简报这周算是今年过的最浪的一周了（ 生活 本周状态还不错 减重计划进行中，减了6斤了！代价就是要疯狂忌口 家里猫猫的状态也恢复了 看到有推友去急诊了，大家务必多保重身体，定期体检呀 本周的疯狂娱乐时间 德凯第十一话，笑死我嘞，好久没看到精彩的等身战了，这话文戏也不错，新时代盖亚实锤.jpg 石蒜大结局了，我感觉属于高开低走了，还是有点可惜 熬夜刷完了《摇曳露营》S1，芳文社入脑，少女们的露营太可爱了 又是熬夜刷完了《比宇宙更遥远的地方》（人称小南极/去南极），我心中的神作，少女们的热血番。人物刻画非常丰满。运镜画工细节都可圈可点。设定也很细致 银魂.jpg 有没有好看的小说推荐啊兄弟们（ 闲书时间继续 本周没有改善伙食，以及因为某只不知名要发情小猫的关系，睡眠极差，下周他将失去他的蛋蛋 本周又是天灾，今年真是多事之年，大家务必多保重啊 本周的读书时间 史上最伟大的交易 看一下经济学的书籍，的确是能让自己思维得到很大的提升 技术上周说好了要这周要多贡献一些社区的内容，但是因为某不知名公益群群友安利的番太合我胃口，所以业余时间除了写作业就去看番去了（你们这什么群啊，害人不浅啊你们） 本周开始重读 CSAPP ，写书上的作业感觉还是很有意思的。不过汇编作业写起来真的头大。Chap4 的 CPU 流水线作业估计也很炸 写汇编作业的时候，发现了 Go 好像更多去做编译速度而不是编译优化去了，导致一些 Case 下编译出来的优化不够，参见 Demo（会不会也有可能是我太菜了导致孤陋寡闻了 本周差不多把 xdp-tutorial 写完了。这个项目我建议所有对 eBPF 感兴趣的同学都可以去写一下。即便你比较熟悉 BCC 这些生态了，去写一下也不亏的。以及我在考虑参考这个写个 netfilter 的 tutorial 了 本周差不多把之前要写的一个前端上报自定义 Metric 的服务写完了。Go 在写这些涉及到数据库 DDL 变更之类的东西还是比较恶心的。 本周依旧是和可观测性战斗的一周 之前反复提到过稳定性一定要可以量化。其中有一点很重要（之前也应该提到过），即口径一定要对齐，否则毫无意义。同时量化出来的指标一定需要持续跟进矫正。比如这周有个 case，在我们一个场景里，对于疑似爬虫用户我们会做限流处理（即返回 HTTP 429），从用户视角来看，这个是个报错。但是从业务视角来看，爬虫导致的 HTTP 429 实际上并不在稳定性指标的统计口径范围内（所以最终的指标虚高）。所以需要对数据做过滤处理。 如同1中所举的 case，我们在制定指标的时候，一定要去定义起业务的具体的视角和含义。否则拍脑袋搞出来的东西毫无意义 一个稳定性指标的波动一定会在这条链路上多个服务的指标上表现出来。我们在去分析这些指标的时候，一定要去判断彼此之间是否吻合。如果不吻合，我们就需要再去深入分析看是口径定义出现了问题，还是有指标搜集出现了问题 感觉国庆可以水一篇文章出来系统性的聊聊 本周又去扫了一眼 OSDI 2022 ，发现了一些有意思的论文，感觉这两周可以读一下了(不比读 MLSys 的论文快乐多了) RESIN: A Holistic Service for Dealing with Memory Leaks in Production Cloud Infrastructure Azure 团队的文章。介绍 Azure 上针对内存泄漏的有效检测的方法 Operating System Support for Safe and Efficient Auxiliary Execution 也是国人一作的文章，设计了一套叫做 orbit 的机制，在保持可观测性与隔离性平衡的基础上，来帮助开发者比较有效的利用 auxiliary task 来完成一些运维工作，比如加固应用等。 Blockaid: Data Access Policy Enforcement for Web Applications，非常工程的一篇文章，主要是针对现在越来越复杂的数据合规场景做的，Blockaid通过在用户query和数据库访问之间添加一层能够在运行时拦截用户query并且检查访问可行性的系统，来做到与现有应用兼容的同时，能够完整的返回query结果中是否包含不合规的数据。 BlackBox: A Container Security Monitor for Protecting Containers on Untrusted Operating Systems， 也是非常工程化的文章，新的容器虚拟化的方案。好奇与 gVisor 或者 kata 的区别在哪里 差不多就这样 总结开始入冬了，大家注意保暖，注意别感冒啦！","link":"/weekly/2022/2022-09-week4.html"},{"title":"","text":"2022年10月第一周周报啊，每到假期的时候，总是最快乐的时候了 生活 本周状态还 OK，不过每周身体都会出点奇奇怪怪的问题 减重计划进行中，这周三斤左右 本周带着小白猫去绝育的路上又救助了一只新猫，我怎么就管不住我这手呢？ 本周疯狂娱乐时间 德凯第十二话，怎么才十二话就暴露身份了呢？不过文戏还是不错的 摇曳露营 S2 刷完，想考摩托车驾照去露营了 吹响，上低音号，京吹，太橘了，不过少女们的热血真的让人难以忘怀啊 银魂.jpg 间谍过家家 EP13 !，新的 OP 和 ED 好评 SSSS.电光超人古立特，六花的角色塑造真的不错 本周没有闲书（ 本周去了国家植物园！好玩！想摘里面的水果（但是忍住了 本周去吃了铁锅炖羊肉，羊汤好吃，以及吃了捞汁小海鲜！满足了！ 技术每周都在吼着要多学一些技术，但是十分钟后又去看番了，（都怪某不知名公益群群友安利的番太合我胃口，你们这什么群啊，害人不浅啊你们） 这周把 CSAPP 第三章的随堂作业差不多写了80%了，国庆再写点，国庆后应该可以开始写 Chap4 流水线的作业了。不过重学汇编对我来说弥补了之前很多想不明白的地方（比如分支预测之类的（可能流水线作业写完能有个更深的理解（ 本周接了《Dead Simple Python》的试译，国庆剩下的时间估计要在赶稿子中渡过了 仔细拜读了下 CloudFlare How we built Pingora, the proxy that connects Cloudflare to the Internet 一文，不得不说，里面不少东西的确也戳了一些痛点，感觉去 Nginx 化慢慢在成为一个潮流 Ngixn Process Worker 带来的链接无法复用的问题，当然调优不当的情况下，也会造成进程负载不均匀 扩展性的问题。 看了 AWS 团队基于 Rust 的 Quic 实现 s2n-quic。他们现在 CloudFront 产品 Quic 的支持就基于这货 这周读的 OSDI 2022 的文章 BlackBox: A Container Security Monitor for Protecting Containers on Untrusted Operating Systems，个人觉得比较野，但是的确也挺有创新性的 这篇论文主要还是论述从 Cloud Vendor 做安全容器加固的方案。所以相对而言，可以不太考虑兼容性 他们爆改了 Kernel 实现，在 ARM 的 EL2 实现了一个 Container Security Monitor，提供了一组 ABI。在内核执行，fork，exec，内存页分配，mmap，之类的操作，都会走 ABI 和 CSM 交互。CSM 会从内核中接管相关的资源并作加密处理 因为 Linux OS 在 EL1 运行，所以 EL2 的 CSM 相关的资源对操作系统不可见（同时会提供一些 buffer 来作为 IPC，其余 syscall 之类的交互使用） CSM 不会做完整的虚拟化，指令执行，内存分配释放之类的东西都还是交给 OS 来做。 相较于 gVisor 能提供更好的安全性，相较于 kata 这种 VM Container 方案，性能会更好，论文里给出的数据是不超过百15的性能损失 由于改了比较多的 Kernel，我感觉不太可能 backport 到上游 和 Intel SGX 的思路有点像，说起来 OSDI 2016 有篇 SCONE: Secure Linux Containers with Intel SGX，明天感觉可以大概读一下 工作上暂时没啥要聊的，先这样 差不多就这样 总结祝大家假期愉快啦！","link":"/weekly/2022/2022-10-week1.html"},{"title":"","text":"2022年10月第二周周报国庆的七天里你会体验到7天是如此的短，然后接着你会体会到7天是如此的长。 生活 假期摆烂，这周除了肠胃，身体没有问题了！ 减重计划继续，这周没减太多，2斤不到（国庆吃得太好了 救助的小猫接回家了，求领养！ 本周疯狂疯狂的娱乐时间 德凯总集篇，上半部分很不错，下半部分加油！ K-On 看完，笑死我了，少女们的日常 SSSS.Dynazenon 蛮不错的，和古立特梦幻联动，以及南梦芽的声优在这部里面的表现很惊艳 银魂.jpg 间谍过家家 EP14，我想养大狗狗！！！！！ 开始看前进登山和海堤日记，我对日常番真的非常喜欢！ 本周没有闲书（ 开始和妹子跳舞力全开，累死我了 本周伙食非常好（ 技术上周的我 be like 每周都在吼着要多学一些技术，但是十分钟后又去看番了 现在的我：搞什么技术，看番最重要，（都怪某不知名公益群群友安利的番太合我胃口，你们这什么群啊，害人不浅啊你们） CSAPP CHAP3 差不多搞完了，下周开始搞 Chap4 以及准备公益群的分享了，这次是业务稳定性项目落地相关的分享 开始试译《Dead Simple Python》 这周开始关注 lima，Mac 下的 Linux 虚拟机（当然也可以在 Linux 下跑），底座是裹了一层 QEMU，提了两个 Feature Request Issue1087 以及 Issue1091 和作者有一些讨论，也收获了一些东西，有兴趣的同学可以看下 对于 OSS 项目来说，保持依赖的干净还是挺重要的 保持配置的干净（ 看了下 Atlantis ，围绕 Terraform 做 GitOps 的一套方案，实现在 PR 里审批和 Apply 的收敛。不得不感叹国外的研发比国内幸福很多 Prometheus v2.39.0 发布，TSDB 那块优化了一波，我这边线上效果很明显，直接降了一半多内存 看了下 SCONE: Secure Linux Containers with Intel SGX 这篇论文，其实思路和 2022 这篇大同小异（也有可能是我倒着看论文的原因。下周准备看下 Operating System Support for Safe and Efficient Auxiliary Execution 差不多就这样 总结祝大家上班愉快（以上（","link":"/weekly/2022/2022-10-week2.html"},{"title":"","text":"2022年10月第三周周报国庆7天之后又是连续上7天班，调休调了个寂寞 生活 这周状态很差，又是噩梦+神经衰弱的一周，下周可能需要去复查了 减重也陷入了瓶颈，呜呜呜呜呜 救助的小猫状态完全恢复了，家里大缅因也完全恢复了 这周和妹子一起去了雍和宫，求了一个佛珠，相信玄学.jpg 本周的娱乐时间 德凯第14话，这什么高达剧情啊，笑死我了 银魂.jpg 间谍过家家 EP15，啊啊啊啊，我超级想养狗狗 白沙的水族馆看完了，好看！我想去冲绳玩 开始看 New Game！芳文社永远的神 本周没有闲书（有番看看什么闲书 舞力全开继续，气球人好玩！ 这周没有除去吃好吃的，呜呜呜 但是这周吃到了超级好吃的麻糬！妹子买的！妹子说我第一口吃下去的时候，笑成了个大傻子（哪有嘛 技术这周的我 be like：什么时候才能不工作啊，所有国庆前说国庆后来做的事开始攻击我了。 CSAPP 这周没啥进度，这周属实有点忙。。 《Dead Simple Python》的试译稿赶完了，交给编辑了 这周看了下 nerdctl 的一个 Issue 还挺难复现的，一个典型的信号处理的问题，参见 Issue1421，不过修起来挺好修的，留给社区的新人来作为熟悉项目的 Good First Issue 吧。 这周看了一个我觉得讲 JIT 讲的很棒的视频 Antonio Cuni - How to write a JIT compiler in 30 minutes，我觉得讲的很棒，代码仓库 jit30min ，感兴趣的同学可以看看。另外我发现，之前看 CSAPP 汇编的东西对我理解这个视频帮助不少，加深了继续看下去的信心 本周工作上遇到点内部问题需要去从 CDN 里捞各种日志，大概思考了下在接入层日志的详细度以及成本之间的 trade-off。另外搞爬虫特征真的是难搞以及我恨爬虫（爬虫食屎啦（ 基于 Atlantis 完成了内部的 Terraform 集中式的审批与执行，然后发现我们目前的 Infra 建设感觉在同行里算非常领先的了，好的 Infra 真的能极大提升研发幸福度 完全依托 AWS 我们 Infra 完全基于 Terraform 管理，审批，执行都自动化了 除了 Prometheus/Grafana 无自建 SaaS，异常搜集，拨测什么的都采购了第三方 SaaS，而且都基于 Terraform Provider 做了自动化 整体 CI 完全基于 GHA + Self-Hosted-Runner（以及部分 CD），前后端的构建发布都统一了 容器系统的 CD 都基于 ArgoCD + Kustomize，部分 EC2 的部分都完成了 Ansible 化这一套下来，用起来还是很舒服以及研发心智节约了好多 这周把家里的路由器固件升级到了自行编译的 OpenWRT 5.19，目前配置自动化备份之后，升级工作也相当轻松了 这周公益群的小伙伴分享了关于编译原理的东西，下周我也会去分享一次稳定性建设相关的内容，作为十月两场分享之一。 差不多就这样 总结无论天气还是社会大氛围寒意都越发重，大家多保重","link":"/weekly/2022/2022-10-week3.html"},{"title":"","text":"2022年10月第四周周报草，转眼十月底了，一年年的，过的真快 继续噩梦+神经衰弱的一周，日子还得过 体重终于又下去了一点点 北京的小伙伴有没有想养小猫的呀，可爱小猫求领养 有点想去打打巴柔了，下周去东边的道馆里看看 本周的娱乐时间 德凯第15话，这次的文戏很不错，我很喜欢 间谍过家家 EP16，约儿小姐做菜笑死我了（突然想起史莱姆的紫苑 刷完了 PA 社工作三部曲的白箱，好看，喵森太可爱了 开始看恋爱小行星，芳文社我爱你！ 看了下花开伊吕波，不行，真的不行，扛不住，色情作家捆绑女主那个镜头，梦回小时候 摇曳露营第三季制作决定了！ 本周看了一点闲书，还是 OK 的 本周收到了来自 Affine 的礼物，开心！ 这周又吃到了好吃的甜品，开心！ 技术这周的我 be like: 怎么又出问题了啊？怎么又出问题了啊？某群友出来挨打好不好 CSAPP 开始攻关第四章了，说实话有点看的吃劲，有没有大佬带带我啊（ 本周开始赶 PyCon China 2022 筹备了，（去年的时候我：明年在搞我是傻逼，今年我：我就是傻逼），今年暂定在12.17线上举行，欢迎大家一起来筹备 nerdctl 正式 release 1.0 啦！和去年的时候相比，整体的兼容性和稳定性都提升了不少，也有很多特色功能，欢迎大家使用。 去给 bytebase 水了点 Issue 和 PR，参见 Issue3049 和 PR3050，有一说一，2022 年了，大家的 Issue Template 该考虑 GitHub 的 Issue Form 了，体验比传统的 Issue Template 好到不知道哪里去了 这周在复习 Linux Kernel Module 的一些东西，有个东西需要走 Kernel Module 去做一些事情。发现 The Linux Kernel Module Programming Guide 这本书写得真的很好，推荐大家看看 这周给群友们搞了一下稳定性建设101的分享，分享了一些方法论。反响还行，可以开始应群友们的要求，开始准备下进阶的稳定性建设持续迭代相关的分享了 总结唉，很多事一下不知道怎么说，只能说天气转冷，记得添衣。祝我们都有光明的未来","link":"/weekly/2022/2022-10-week4.html"},{"title":"","text":"2022年10月第五周转眼间十月底了，一年快要结束了，有时看下窗外的落叶，总会有一种莫名的萧瑟感。古人云伤春悲秋，诚不欺我 本周神经衰弱终于好了一些！但是冬季嘛，肠胃又日常不对了。以及这周还是会因为自己太菜而焦虑 体重继续下降了一些 本周继续吃到了好吃的麻糬！ 本周份的伙食改善是左庭右院。他们家的牛肉饼真的顶不住 本周和妹子一起去朝阳公园走了走，看到了好可爱的小鸭子！（门前大桥下游过一群鸭（ 小区里的大秋田还认识我！叫新一，我一叫他他就过来蹭我了。（还喂了他吃小零食（想养狗 本周有了人生第一台台式机，高配，一步到位上了 i9-13900K+4090+128G RAM，圆梦童年了 本周的娱乐时间 德凯第16话，这次的文戏依旧保持很不错的水平，但是想到过几话，久弥那货又要出来了，瞬间不爽了 间谍过家家 EP17，阿尼亚好可爱啊啊啊啊啊啊啊啊 重看了下 New Game，八神光总会让我串戏到隔壁数码宝贝 看了下少女歌剧，有点摸不着头脑 准备挑战下终将成为你，以及樱花任务 这周开始打 COD19， 联机模式好玩。而且优化不错，特效全开 4K，4090 可以60度，178帧 CSGO 枪法还是不行 异常生物见闻录新动画的 PV 放出来了，真的，我心态崩了，球球了，别毁原著行不行。 这周受到其余朋友的启发，在 cal.com 开了个页面，可以让网友预约我周六晚上半个小时时间一起视频聊天。这周和两位陌生网友聊了两个小时，感觉收获不小。还是要多和人交流呀 技术这周的我 be like：怎么驱动还是不行，我要疯了 CSAPP 继续第四章，说实话看得有点吃力（有没有大佬带带我啊（ PyCon China 2022 的筹办已经正式启动了。欢迎大家报名志愿者 这周又回到了 nerdctl 的怀抱 终于把 PR1470 提了。这样让第三方的一些 Logger 也能在本地有日志方便查看 认领了 Issue1425，这个 Issue 不难，主要是苦力活 本质上还是因为 nerdctl 选择了 stateless 的实现，和 Docker 不太一样 要做兼容的话，就只能通过本地文件系统进行兼容 要保证文件不泄漏就是各种 corner case 了 使用 bytebase 的时候，顺手修了一个小 bug ，PR3176 这周给新机器装系统装了个半死，最后因为驱动问题，还是只能投到 Ubuntu 22.10 上了 这周在看一点 LDAP 的东西，bytebase 上也许有点我们自己的 LDAP 需求可以自己顺手写了，参见 Issue22 Dead Simple Python: Idiomatic Python for the Impatient Programmer 这本书的试译终于过了，这应该是我第三次试译了。年轻人第一本翻译的书，争取有机会翻译动物书 GitHub 上有个 命令行的艺术 的 repo，建议大家仔细研读，并全文背诵（逃 总结最近看了不少疫情的新闻，富士康，西宁，只能说，世道不易，大家切行且珍惜。愿我们都有光明的未来","link":"/weekly/2022/2022-10-week5.html"},{"title":"","text":"2022年11月第一周周报本来要周天交周报的，不过身体不舒服，早睡了，于是就拖延到今天了 本周感觉一般般，不好也不坏 体重没下降，不开心 这周改善伙食去吃了一家东北铁板烧，不过没有酸菜有点遗憾 我的主机成为家里的猫窝了（ 本周的娱乐时间 德凯17话，文戏水平保持的让我惊讶，有点就平成那味道，不过不要被久弥那傻x给毁了就行 间谍过家家，EP18话，一如既往的搞笑。成为霸权番是有道理的 New Game 细节真的不错，居然用的 Visual Studio+C#+Unity，很写实了 摇曳露营剧场版，啊啊啊啊啊啊，抚子吃东西我能看十年！ COD19 打全面战争模式，好玩，好玩 本周闲书时间继续 本周和一位大四的小同学一起聊了聊，然后还是很好玩的 技术这周的我 be like: 巨硬大法还是有点香的蛤？ 这周做了个死，导致 NAS 上的 Emby 数据丢了一波 做故障演练的时候，没等完全断完电就拔了盘，导致一个盘坏道 重建 RAID1 后，触发了群晖 BTRFS 的老问题，Metadata 丢失，导致文件系统损坏 重建文件系统后，所有软件都得重新安装，然后数据恢复，然后重新配置 厂商说的可靠性能信，母猪都能上树，所以还是要自己多备份，异地多活多中心（不是 这周又回到了 nerdctl 的怀抱 PR1470 说实话我现在有点纠结了，感觉很难做很好的抽象，头疼。 Issue1425 自己测试了下，走文件配置还是能比较好的 work 的 这周去给 gcgg 的 envd 项目提 Issue 了，参见的 Issue1160。整体来说就是让 envd 的 include 远程导入代码的功能支持多文件后端，比如 S3，IPFS，git+http，git+ssh 之类的。这里面核心的问题就在于怎么样去设计一个通用的文件抽象层，然后能把鉴权，存储，更新这些操作屏蔽掉。可能得看看 xuanwogg 的 opendal 的实现思路了。 这周因为是在不想用 Ubuntu（Ubuntu 发生了一个内部错误.jpg），所以体验了下 WSL2 目前开发体验还是很不错的，配合 VSCode，PyCharm 之类的，写代码之类的还是比较开心的 WSL2 的兼容性还是做的很不错了，测试了下 eBPF 和 BCC 的一些东西都能正常 work 文件系统还是一如既往的拉跨 默认的 tar 模式的导入导出如果比较大的话还是有不少问题的，具体可以参见 wsl import does not complete WSL2 的内存泄露还是没修，开 Docker + 日常开发，内存管理器上显示占用能去到 20G+ 翻译的书进展还不错，预计下周第二章能翻译完 这周开了一篇新论文 It’s Time to Replace TCP in the Datacenter。这篇文章中作者 N 评TCP，怒斥 TCP 一些 naive 的设计。还是比较好玩的，下周写个简评 差不多就这样把 总结看小说时，看到一段话“回应期待，且不要辜负善意，这是世间最幸运的事情”，深以为然 一起加油，晚安~","link":"/weekly/2022/2022-11-week1.html"},{"title":"","text":"2022年11月第二周周报这周又拖了一天周报，下次不能继续了（肯定要拖的 本周早睡一周，感觉元气满满 肠胃又在闹肚子。。 本周的娱乐时间 德凯没有更新不开心 间谍过家家 EP 19 ，说实话有点过于无厘头了 登山少女继续，不愧是和摇曳并列的神 COD19 最近在练连狙刷墙，EBR-14 手感其实挺不错的 这周 Links 又出新的视频了，啊啊啊，我想去欧洲玩！ 本周和妹子的纪念日！一起美滋滋的吃了西塔老太婆的烤肉，以及 saka 钦点蛋糕供应商廿一客！好吃！ 家里的缅因又有点小毛病，头疼 给老妈买的两个首饰到了，被她傲娇的说浪费钱（嘿嘿嘿 技术这周的我 be like：我的内存条怎么上不了 5600 Mhz 的频率啊 本周在折腾台式机 我的台式机用了 DDR5 5600 32G * 4，不过默频一直只能4800Mhz 尝试了反复调整电压，从 1.25 到 1.35 都试过，就是上不了 5600Mhz 后面默频 4800Mhz 跑 memtest86 test7 反复 failure 最后经过多方查证，应该是 D5 四通道兼容性的锅，估计要四通道超频的话，只能去 D4 了 本周把 NAS 的硬盘插满了，现在整体的配置就 16T * 2 组 RAID 1，存放重要资料，16T * 6 RAID0 放娱乐（，然后硬盘统一的都是 EXOS 企业盘系列，这下可以不折腾了，再折腾我是狗 看了下 40Gbps + RDMA 的资料，为明年家里走全光 40Gbps 改造打基础（汪汪汪 本周再基于 PEP517 的实现 PEP517 写一个静态版本分析的工具，主要场景是在推动业务升级框架的时候发现潜在的版本冲突。举个例子，我们现在 Celery 是 4.x，依赖 Kombu 4.x，如果要升级 Celery 5.x 需要升级 Kombu 5.x，但是 Kombu 5.x 锁了 redis client version &gt; 3，而我们另外一个库锁了 redis client version &lt; 3。这种版本冲突就需要提前的进行分析，不能无脑升级 这周在做 Redis 从主从模式迁移到集群模式的操作，感觉可以作为一个经典的面试题（SRE/研发通用），这不比八股文舒服多了，简单聊下我的思路 核心还是在于说避免关键实例迁移导致的缓存雪崩，所以缓存内容需要迁移，这里有两种方案 离线脚本导入 优点，实现简单，业务改动小 缺点： 离线扫 Key 可能会影响原本实例的读写 业务代码还是会有改动，在离线脚本跑完到业务上线这段 time gap 里，缓存淘汰数量不可预测 业务双写 优点： 双写可以比较好的控制缓存的淘汰数量 对原本库的负担小 缺点： 业务代码改动大 会略微增大一些请求延时 如果采用方案2进行迁移，那么怎么样确定切换时机 经验法，在 time = 10 * min(1h, maxTTL) 后进行迁移，这个时间可以根据实际情况调整，确保缓存都被 warm up 了 在2的基础上增加双读，先读新库，fallback 老库，输出缓存命中率 metric，当新老 metric 趋于一致时，可以切换 需要注意 Key 冲突的问题 这周给公益群的小伙伴做了一个简单的分享，简单聊了聊家用网络规划和存储方案设计，科普向的一些东西，反馈还不错 这周 Kubernetes 的 PR107531 几个 PR 终于合并了，Liggitt 好人！ 书的进展还不错，Chap 2 翻译完了，差不多一周多一章吧，后面可能还有提速的空间 看完 It’s Time to Replace TCP in the Datacenter 这篇论文了，好玩。作者上来一个 “Everything about TCP is wrong” 以及 “TCP is beyond repair” 笑死我了，但是仔细想想，一些场景下又很难不确实。作者主要还是讨论在大型 datacenter 的场景下，高速设备里(10Gbps or more)，tcp 的一些局限性 面向流式的设计，导致在 datacenter 的场景下，大量的 rpc 小包会吞吐很差，应用需要频繁 check boundary， 容易造成 hot spot 面向链接的设计，让负载均衡变得很难。要做链接共享有额外的 overhead。 拥塞控制是由发送方来做的，这导致在网络环境变化的时候，发送方很难及时的感知到变化（比如交换机队列阻塞了，此时发送方并不能第一时间进行调整） 不支持乱序的特性导致 Load Balancing 很难做，很容易 Hot Spot，比如你在做负载均衡的时候，你必须保证同一个五元组的 target 是一致的，不管它负载如何（实际上也是流式设计的副作用）（Google 那篇 Maglev 的论文讨论了具体的实现方案） 差不多就这样吧，这段时间摆烂有点严重，要好好思考下明年个人的 OKR 了 总结最近疫情太反复不定了，大家一定要多保重，有机会一起约饭呀！ 晚安~","link":"/weekly/2022/2022-11-week2.html"},{"title":"","text":"2022年11月第三周周报这种周报好像可以按时交了 本周继续早睡，感觉恢复的还不错 药物又开始有器官的反应了，蛋疼 本周的娱乐时间： 德凯第18话，久弥这不是会正常的写故事嘛，这集文戏可圈可点的嘛，之前干啥去了啊？以及这集打戏真的牛逼 本周间谍过家家暂时没看（ 本周主要时间都去看食梦者了，以梦想为食，好浪漫，以及三季的 OP 都好听！ COD19 的战区模式开了，emmmmmm，为什么我总是落地成盒啊，呜呜呜呜呜呜，以及 M4 终于刷出了金皮（ 下周准备看下樱花庄的女孩（ 重新看了下海猿的电影（啊，好看（ 本周继续了闲书时间 大缅因恢复啦！ 看到了资助的孩子的来信，开心 这周又和一位要毕业的小同学聊了一个多小时，以及这周我第二次被鸽了（呜呜呜 技术这周的我 be like: 网友：saka 你这个什么时候交啊我：我在写啦！ 本周继续折腾台式机 消费级主板的双通道四插槽的设计属实不靠谱，这周内存频繁不稳定翻车 综合考虑，我还是拔掉了 1/3 槽位的内存条，然后 2/4 跑在5600上 这周在网友的指点下，用 nvidia-open-dkms 成功打上了驱动，目前的发行版选择是 Garuda ，基于 Arch 的衍生版（非常有特色），内核就 Linux-Zen ，整体功耗会比 Windows 上高30-50w左右，但是使用下来还不错 在网友的催促下，开始投精力在 nerdctl 上 交了一直说的日志输出重构的 PR，参见 PR1519 ，这个合并之后，后续日志部分的的功能就可以和 stdout/stderr 完全解耦了，这样添加新的功能就非常方便了 修了一个 Bug，参见 PR1526, 这个实际上和 containerd 那边有不少关联，玄学问题。然后修的时候行为对齐了一下 Docker Issue1329 整合 AWS 的 SOCI Snapshotter 这个 feature request 被 AWS 老哥催了，这周也开始进入冲刺状态了，预计下周交 PR Issue1425 最终还是确定走本地文件的形式进行处理，反正 nerdctl 用文件记录中间状态用的挺多了，多这一个不多 本周公益群的分享又是 HomeLab 相关的，@STRRL 分享了 HomeLab 内网穿透方案，非常好玩啊哈哈哈。这样的分享多来点 本周看了下 cacule-cpu-scheduler Linux 上参考 FreeBSD ULE 的一个实现，现在是社区在 fork 维护。感觉效果一般，实际上调度性能没想象的优秀 这两周一直在查/帮人查一些奇怪的问题，再次感受到如果能合理的拿到 Linux Sys Call 的上下问该多好。比如一个经典的问题是，一个进程经常被 UID0 给 Kill 了，但是不知道是哪个进程干的好事。这种时候，如果能拿到更多的上下文会更有利于排查。（虽然找一些特殊的 Hook 点也能搞一下吧，但是麻烦） 书的进展不错，预计下周第三章翻译完 又给 Oreilly 充值了 $299，一定要看回本！ 这周公益群的群友推荐了几篇论文，大概瞅了一眼，发现有两篇比较戳我 xp，下周有时间可以写个短评： Naiad: A Timely Dataflow System 斯坦福的文章 SEDA: An Architecture for Well-Conditioned, Scalable Internet Services 差不多就这样，这周思考了下明年个人的 OKR，有点眉目，等整理完后在博客上公示一下吧 总结我一直没搞懂一个问题，三年来，这疫情怎么越防越严重了呢？我好想出去玩啊啊啊啊","link":"/weekly/2022/2022-11-week3.html"},{"title":"","text":"2022年11月第四周周报这周的周报又可以按时交了 生活 本周的早睡时间保持的不错，不过导致我看番时间少了很多 药物反应好了不少 本周的娱乐时间 德凯第19话，坂本接久弥出院了？我为什么会做这样的梦啊（不是。不过说回来，这话有个我很不爽的点就是对于战队的刻画太过于玩笑了。一个维护世界和平战队居然在队员失踪后没有响应我觉得属实离谱。目前来看奥系列两个刻画最好的战队一个是XIG 一个是 TCP，其余都有点玩笑（当然旧平成和昭和还是比新生代好了不知道多少个假面骑士（ 间谍过家家第21话，我觉得这几话质量都不太行 柯南剧场版万圣节的新娘，简而言之是长征五号柯南限定皮肤，不过比绀青还是好了太多 樱花庄的宠物女孩，可爱（ COD19 的战区模式我还是落地成盒 刷完了食梦者，哇，真的好看看看看看看看！ 本周的闲书时间继续 继续捡起《上帝的骰子》看 这周是继续在家封控的一周，得自己做很多吃的。我突然有了一个梦想，就是有钱有闲之后去开一家以不亏本就行为目标的平民餐馆 家里的小猫咪们这周偷吃火腿的偷吃火腿，拆家的拆家。收拾现场时流得泪，是当年决定养猫时脑子进的水 这周和一位群友一起聊了聊，感慨不同的人真的有不一样的人生，这样听别人讲自己的人生的经历真的很棒 技术这周的我 be like：怎么一周突然就过去了 我开始使用vim啦！经过周末两天的学习，我现在已经能正常使用了（本周报就是 vim 写的（某回忆出来挨打 我放弃了 Lunavim 的配置，因为使用还是比较复杂了，我基于 cosynvim 的模板，封装了一套自己的配置 我在配置里加了一些自己需要的功能，比如 LSP 自动安装管理，比如 GitHub Copilot 支持 LSP 现在的使用体验真的不错 基于 LUA 写配置写插件真的很爽，看起来我之前入坑 vim 的原因就是 vimscript 过程式的声明真的很符合直觉 这周群里在讨论 envd 的时候，聊到了 Docker 的几个缺陷 Dockerfile 的可复用性太差了 很多 Buildkit 支持的很有用的特性支持都很奇葩 Dockerfile 很多时候的还是面向状态的描述而不是过程的描述，进而导致其可复用性太差 这周还是花了一些时间在 nerdctl 上 Issue1425 写了一个正式的 Proposal，参见 Issue1560 Issue1329 的进度比我想的慢一些 这周和 @yuchanns 聊了下为什么 nerdctl 现在都还不支持 network connect 的原因，本质上还是因为 nerdctl 在设计之初的定位在一个弱状态的 CLI，没有 daemon，很难去维护一些重状态的操作。network connect 是个很典型的例子，将 network 和一个已经存在的 container 打通。这个过程一旦状态维护不对，那么 iptables 之类的资源泄漏的副作用会非常大 这周查了业务那边比较蛋疼的一个线上事故，对于慢查询这种东西，日志的存在过于重要 这周群里大概定了一下明年大家集体的 OKR O1: 公益 通过公益刷题/分享之类的活动，凑集超3000元捐款 O2: 开源项目 新增一位开源项目的 maintainer 全年各开源项目代码 PR 数超过 15 这周读了 SEDA: An Architecture for Well-Conditioned, Scalable Internet Services 这篇论文，有点年头了，UCB 在 SOSP 2021 上的论文（里面测试环境都还是 2G RAM（XD，不过里面的思想在后面很多的项目上都能见到影子，可能会在群论文分享会上聊下这篇文章，这里先简单记一下 这篇文章的核心还是在说处理高负载流量下的一些手段 基础的 per-request, per-thread 模形，在这篇论文的写作的时间下，Apache，IIS 之类的都是这个模型。缺陷就很明显了，请求多的时候，线程太多，导致系统的负载太高，调度 overhead 太大（经典面试八股文)，作者有个很经典的描述 “the design of these systems is still based on multiprogramming, as the focus continues to be on safe and efficient resource virtualization, rather than on graceful management and high concurrency.” 线程池模型，作者认为这个模型的缺陷在于，线程池的大小是固定的，如果线程池的大小不够，那么就会导致请求的排队，这个排队的过程会导致请求的延迟变大，如果线程池的大小太大，那么就会导致系统的负载太高，调度overhead 太大。另外一个是，需要根据不同的负载进行线程池的划分，有个很好的例子，比如有两类请求，一类是读文件再返回的重请求，一类是直接从缓存里返回的轻请求，即便在整体系统负载偏低的情况下，如果突然来了几个重请求，把所有线程池的线程都吃了。这个时候后续的轻请求就只能继续排队了，即便只需要一个线程处理就行了。换句话说线程池的公平性调度是个问题。 事件驱动模型，所有 blocking I/O 都被转化成 event，然后投递到各个子模块进行执行。每个子模块都有自己的 FSM，request context 就会被 FSM 管理起来（有没有很熟悉，实际上就是 Nginx 的经典做法）。不过缺点也很明显 request context 的维护实际上比较麻烦 一处 blocking 可能会导致全局 blocking 每增加一处子模块可能就需要修改 event dispatcher 的逻辑（ 然后提出了核心的 SEDA 模型，将一个理想系统的处理划分为若干个 stage，stage 之间通过 event queue/subroutine call 进行通信。每个 stage 的 controller 面对 resource 建模，支持批处理，这样来尽可能的扩大系统的吞吐能力。不过缺陷也比较明显 stage 的合理划分。论文本身建立在业务都是可以抽象为一个理想的 pipeline 的基础上，理想很丰满，现实很骨感。实际上这个就有点变成领域建模的意思了 批处理还是需要对任务载荷进行分类，而且受限于这篇论文所处的时代，可能没考虑批处理的原子性的问题 作者表示能正确衡量系统 bottleneck 的测试集很重要。瞎 benchmark 等于没 benchmark，点名批评现在各路 DB 动不动就 benchmark（逃 这周我被提名为 nerdctl 的 committer 了，拥有对仓库更大的权限了（意味着更大的责任了。新进群的 @yuchanns 也被提名为 reviwer 了，开心（ 写了一篇水文聊聊我的开源经历，参见 我所热爱的开源社区 。 差不多就这样 总结 愿中国青年都摆脱冷气，只是向上走。 不要听自暴自弃者的话。 能做事的做事，能发声的发声。 有一份光，发一份热。 就令萤火一般，也可以再黑暗里发一点光。 诸君保重","link":"/weekly/2022/2022-11-week4.html"},{"title":"","text":"2022年12月第1周周报本周又摆烂了，周报周一交 生活 本周又没早睡早起了，不过也没咋看新的番了 药物反应基本没有了（ 本周的娱乐时间 德凯第20话，这话我觉得有点新生代准神话的意思。圆谷传统文艺回，梦回昭和和旧平成 间谍过家家第22话，最近间谍过家家的整活差评！ COD 19 这周手感真的不行（ 周五通宵肝完了《编舟记》，啊，这种平润的激情，太和我胃口了 N 刷《摇曳露营》 本周继续的闲书时间 本周突然真的被封控了一回，断药一晚上，第二天又突然解封了奇怪的体验 啊，好想出去吃好吃的啊（啥时恢复堂食 本周的闲书 唐史并不如烟，行文风格有点类似《明朝那点事》 看着文字上的风云激荡，觉得莫名的安定。突然想去西安罗汉洞村观音禅寺，看看那颗李世民亲手种下的银杏。如果这颗树有灵，他会怎么样去讲述自己所见证的一千四百余年的岁月呢？ 技术这周的我 be like: 怎么就转眼到了一年的尾声呢 这周又花了不少时间在 nerdctl 上 给 nerdctl network 相关的 CRUD 加上了强制校验，参见 PR1569。这样一来整体 CRUD 的逻辑就自冾了。 重构日志输出的 PR1519 终于合并了，和较真老哥就应该用 Channel 还是 io.Reader battle 了三天，最后合并了。挺有意思的，较真是好事。 这周花了不少时间在 SOCI-snapshotter 的整合上，最后产出了一个 PR1601，简单聊聊这个工作 SOCI-snapshotter 是 AWS 开源的一个 containerd 的 snapshotter，本质上是实现了 FAST 2016 的论文 Slacker: Fast Distribution with Lazy Docker Containers，算是 lazy-loading（其实也不是完全的 lazy loading），改天可以开个坑聊下现在 nerdctl 支持的几种 lazy-loading 的 snapshotter 最开始花了不少时间来接入 soci，让 nerdctl 支持 nerdctl soci 系列命令 但是经过更进一步思考后发现，其实这样搞有点太重了，snapshotter only 的 command 不应该包含在 cotainerd 管理的 scope 范围内。所以这个 PR 最后又变成了一个文档 PR SOCI 的设计和其余的 lazy-loadding 的 snapshotter 不太一样，它本身的 design scope 内并不包含一个 OCI image 的概念，它又一个特殊的 index 来维护镜像与 snapshot 的映射。这也是为什么 SOCI 需要额外的一个 soci index 的命令来建立 index 的原因。这样一来其余 snapshotter 支持的 converter.ConvertFunc 接口并不会在 SOCI 的设计目标中。在这个 PR 上和 AWS 的老哥讨论了下，可能会还是要把 index 相关的逻辑放到 nerdctl 里面，这样一来 nerdctl 就可以支持 SOCI 的 index 了，达成逻辑的统一。 和社区的小伙伴电话聊了料 PR1184，挺好玩的，这样不断迭代也是开原社区的魅力之一了 本周的主要的工作都是由 vim 完成的，这里聊一下我遇到的几个问题 对于 workspace 的支持比较差，VSCode 的 workspace 是我很喜欢的功能，能够跳出文件目录的层次来组织我的日常工作（否则我可能就要开 N 个 tmux 的窗口了 重构相关的功能相比 Jetbrains 系的 IDE 还是逊色了很多 建议使用 i3 之类的桌面，不然你会觉得双手在鼠标和键盘间切换还是比较割裂的 这周之前提到的 Redis 迁移的工作终于进入尾声了，不过发现 Redis-Exporter 一个有点蛋疼的问题，就是它的 Key 的统计不是全局 count 而是按照不同的实例聚合的，有点挠头 定了下自己明年的一些 OKR，当然也只是暂时的 O1：代码能力 学习前端，成为一个前端开源项目的 maintainer 继续保持 Leetcode 每日一题，确保百分之70以上的 daily 随机 medium 及以上题目能不看题解写出来 O2: 读书 重读 TCP 卷一 读完 CSAPP，并保证每章作业完成率不低于百分之八十 剩下的书待定 O3：分享 参考 xdp-tutorial 写一个 netfilter-tutorial 全年群内分享5次以上 六篇有效博客 这周翻译进度稍微有点慢，预计下周能够完成 Chap4 和 Chap5 前半部分 本周论文时间，ATC 20 的文章 DADI: Block-Level Image Service for Agile and Elastic Application Deployment 这周看了下 pdm 和其余一些工具关于 git 依赖的实现，对挖的一个 envd 的 Issue1160 有了些想法，Proposal 开写！ 差不多就这样把。 总结疫情现在看起来是进入了开放阶段，大家多保重，保护好自己（有空一起约饭呀（","link":"/weekly/2022/2022-12-week1.html"},{"title":"","text":"2022年12月第二周周报本周没摆烂（ 生活 本周体重没增长了，算是好消息 坏消息是女朋友阳了，我估计也快了（XD（动态清零原来指阴性清零 本周更坏的是这周连续一周噩梦了，玩我呢这是？ 本周的娱乐时间 德凯21话，圆谷这不是会整活嘛！本周戴拿回归，好评！ 孤独摇滚！啊，笑死我了，芳文社我爱你啊（但是你要是不出要摇曳露营 S3 我就会很狂躁 本周 COD 新地图，然后打战区的时候被挂搞的心态崩了（导致在群里爆论（然后自罚捐款+两题 hard 了（更讨厌挂了 开始看 Do it yourself, 好玩 N 刷摇曳露营剧场版 下周三刷放学海堤日记 本周继续的闲书时间 北京放开啦！这周终于可以改善伙食了，重庆老火锅我爱你！ 本周和妹子出去玩了！开心，温室农场内搞烧烤，放空自己，超快乐的！ 本周的闲书： 唐史并不如烟 学习这周的我 be like：怎么一年过去了，我怎么还这么菜（ 这周还是花了不少时间在 nerdctl 心心念的 Issue1387 终于开始写了，尝试引入 dnsname 这个 CNI Plugin 来解决 file dns 的局限性。不过目前还是有点阻碍，估计得下周才能交 PR 了 搞了个超大的 PR ，参见 PR1639， 规模在+5,072 −4,205（Suda 你别来砍我啊（逃，主要是将 cmd 下面的东西整理了一遍，为后续重构做准备，也正好聊聊这个 PR 首先直接提超大的 PR 是不被鼓励的，这点需要告诉大家。如果你计划对某个项目进行大规模的重构，那么请在正式提交 PR 之前，和 maintainer 进行沟通，同时在 PR 中写清楚具体的重构思路（我现在这个 PR 还是 draft，我在正式提交的时候，会完整说明这个 PR），这算是基础的社区礼仪 nerdctl 之前的 faltten 的 cmd 结构，会让整体的维护和可读性都会变得比较差，这也是这个 PR 将其按照 subcommand 的方式进行重构的原因。这个 PR 会让后续的重构变得更加容易，同时也会让整体的代码更加清晰 在拆的过程中，nerdctl 之前交叉引用的情况暴露出来了，借此机会将其梳理了一下 下一步会重构 command flag 的部分（MD 又是体力活，至少会有以下两个好处 解决 command flag 满天飞带来的可维护性差的问题 能更好的利用静态语言在 Code Intelligence 上的优势（毕竟你满天飞的字符串 flag，鬼知道在哪定义的） 这周看了一个很有趣的项目，runwasi，用 containerd 来管理 WASM workload，很好，很有新意，不过目前还需要自己接入 WASN runtime，同时如同我之前在周报里说的一样，视 runtime 不同，需要自己做好实现 WASI host function 的准备 本周发现 go-redis 在 cluster 下如果要正确的获取到 cluster 里所有 Key 的数量，需要手动 ForEachMaster 来遍历 cluster 获取，这不河里，下周去看下代码和历史设计 proposal 本周在折腾 overlaybd 的东西，看得心态有点崩，注释太少了，细节和论文上也有不少对不上的 学习 Linux 的 TCMU，发现挺好玩的，virtual block device 好像还能做很多有意思的事情 本周公益群第一次线上面基，不过聊着聊着，大家就去讨论 Google Meet 为啥这么流畅了（这职业病没救了） 差不多就这样吧 总结北京阳性增加的速度有点哈人，大家都多多保重啊（","link":"/weekly/2022/2022-12-week2.html"},{"title":"","text":"2022年12月第3周周报本周史无前例的大摆烂 生活 阳了 娱乐时间 睡觉 Do It Your Self 发朋友圈告诉大家我阳了 新键盘到了，TTC 金粉好用 技术这周的我 be like：好难受啊啊啊啊 PR1639 这周被拒了，其实是个挺典型的反面教材的 巨型 PR 一定要拆分，不然 review 会很难受 无关变更降到最小 要考虑意外情况（比如这个 PR 写的时候我没考虑我也会阳了，导致这个 PR 没法被及时处理，block 其余 PR 的合入了（ 总结大家真的多保重，，","link":"/weekly/2022/2022-12-week3.html"},{"title":"","text":"2022年12月第四周周报Be postive, be patient 生活 瘦了！好耶，算是阳了后的唯一的好处了 本周身体算是开始恢复了，不过后遗症还是比较明显的 明显的体力下降 呼吸还是会有些急促 整体肠胃反应还是比较大（疯狂腹泻 本周圣诞+冬至，出去改善了伙食，伊豆的冬阴功锅底还不错！ 本周的娱乐时间 德凯23话，好看，不过这什么男酮版大古和丽娜啊？（我现在看不到后面，你变吧.mp4（戴拿：到底谁才是25周年纪念啊？那谁不是去年才致敬了吗？不过正儿八经说，德凯目前的伏笔收束我还是很满意的 孤独摇滚！啊啊啊啊好可爱，芳文社你是我爸！ Do it yourself 看完了。然后我想买钻子了 这周 CSGO 技术大进步！ N 刷摇曳露营，明年的目标就是去露营！ 本周继续的闲书时间 本周买了小米13，目前用着还挺满意的，算是年度成功购物 屏幕 size 不错，在我有 iPhone 12 Pro Max 的情况下，是个比较好的补充 非曲面屏！这点很重要！ 续航很不错 本周的闲书 唐史并不如烟 学习这周体力还没恢复，所以算是摆烂的心安理得了 这周还是花了点时间在 nerdctl 上 上周 PR1639 关闭后，这周反思了下这个 PR 里犯的一些问题 巨型 PR 一定要拆分，不然 review 会很难受 无关变更降到最小 要考虑意外情况（比如这个 PR 写的时候我没考虑我也会阳了，导致这个 PR 没法被及时处理，block 其余 PR 的合入了（ 说白了还是失去了对于社区的敬畏 虽然之前的 PR 有点糟，但是重构 CMD 这块的处理还是势在必行的。这周正式提出了 proposal Issue1680 整体的思路是先将 flag 处理于注册解耦，确保在逻辑入口处不会依赖 cobra 将辅助方法抽出来成单独的 package 在前面两个的基础上，就能比较干净的将 cmd 按照 subcommand 的方式拆分 不过具体的一些拆分进度社区也还在讨论中，有兴趣的同学可以关注下，预计元旦前正式放任务出来 这周在折腾 WASM Runtime，自己基于 wasmtime 简单糊了点 host function 以及改了下字节码处理的一些东西。说不定下周有时间的话可以去和 runwasi 打通一下。WASM 还是比较好玩的 这周在折腾 SLI/SLO 的一些东西，PromQL 的标准化的能力真的不行，以及 Prometheus 还是要善用 Recording Rule 这周开始重学 Kernel 那块的知识了（算是提前启动了自己 2023 的 OKR） Linux Kernel Development 3rd Edition 这本书讲的不错，虽然版本有点老，但是主线很清晰 Bootlin 家的 Bootlin-Elixir 阅读代码的体验是真的不错，除了 index 非常非常慢（我用了一周多才跑完 Linux 的 Index），不过目前体验下来相当值得 善于利用 eBPF 这样的工具来在内核打点调试，能更好的加深你对代码的理解 没时间打周赛，就开始刷之前已经举办过的周赛的题了，我还是太菜了 差不多就这样吧 总结现在感觉没有阳的都成稀有动物了，大家多保重，多关注血氧，我们春天见！","link":"/weekly/2022/2022-12-week4.html"},{"title":"","text":"2022年12月第五周周报这是2022年最后一周了，突然有点舍不得 生活 没有瘦，状态还是一般 新冠的后遗症还在持续， 体力还是有点问题 时不时的咳嗽 本周在2022最后一天和妹子去了环球影城，啊哈哈哈，算是这几年过得最快乐的跨年了 环球影城感觉主要还是套壳的欢乐谷这样的传统游乐场，感觉互动性不算太强 侏罗纪，哈利波特是最值得玩的，功夫熊猫说实话很让人失望 哈利波特区的夜景很好看 本周的娱乐时间 德凯没有更新，不爽 孤独摇滚完结了，心里空落落的 重新开始 N 刷放学后海堤日记，这部番感觉是画鱼比画人精细，但是真的很好看 CSGO 这周是我沉迷的游戏，我自己的定位和移动能力得到了挺大的提升。说起来，单论射击游戏的纯粹性，我觉得 CSGO 是天花板了 本周继续闲书时间了 本周改善了好多伙食，包括不仅限于老火锅，环球影城的黑暗料理（不是） 本周的闲书 唐史并不如烟 感觉这个系列太大了，争取Q1读完吧（ 技术这周还算有些产出了 这周花了不少时间在 nerdctl 上 Proposal Issue1680 正式开始了，核心就一点，把 cmd 这摊子屎山给扬了 确定了整体的重构后的目录结构和对齐了具体的重构目标：把 flag process 彻底和 cmd logic 分离 创建任务的时候全部转成 issue 了，搞了一次手动 spam ddos 有社区的同学开始推进 PR 了，比如 PR1773，但是对于多人协作下的重构工程而言，由 Proposal 的提出者或者其余相对 Senior 的同学提出一个示例 PR 是比较好的体验 所以我有了 PR1774，用 Apparmor 这个相对功能齐全的子命令作为示例，给社区同学进行参考，又引申出其余的问题 在一个重构 PR 进行的时候，中间可能会涉及到许多通用的 helper function 需要重新抽出来。这个时候意味着在实际上的重构进程开始之前需要由 Proposal 的提出者或者其余相对 Senior 的同学做好一些前置工作。不然很难达成 One PR for One Thing 的目标。 即便是已经在 Proposal 里达成的一些细节的共识，也许要在 PR 里针对性的进行细化和不断迭代。比如对于目录结构，在 PR 里最初的共识和有了一个参考 PR 后大家觉得更合理的结构还是有些差异的 然后我开始补了一些前置性的工作，参考 PR1779 以及 PR1780。对于 PR1780 实际上也有一些很有价值的讨论，我大概说一下，大家有兴趣可以去看原始讨论 因为 Go 的大道至简的特性+nerdctl 原本的一些不合理的设计，1780 在现阶段没法引入一个很好的抽象，导致引入了不少 duplicate code，进而导致整个 PR 的规模到了 +833 −150（当然比之前的 +5,072 −4,205 要好很多了（逃。有 maintainer 提出了引入一个 warpper function 来减少 duplicate code。不过我和另外一个 maintainer 的看法是对于 PR 来说可能规模大不意味着做的事多（同理规模小不意味做的事少），在现阶段，保持 PR 里 step 尽可能的少会有助于进一步的维护。而在引入全局通用的抽象之前，保持一些 duplicate code 是可以接受的。实际上这个两方都有道理，具体的 trade-off 看团队的风格达成共识即可 本周继续折腾 SLI/SLO，对于业务来说，找到合理的 SLI 以及约定合理的 SLO 的阈值非常的重要 本周重新看了下 Firecracker 那块的一些东西。我发现现在用 Rust 基于 QEMU 去裁减一些东西后做一个轻量化的 VM Runtime 感觉是挺流行的一个事啊。不过我自己对于这种方案在通用没有特定硬件支持的情况的效能持有怀疑态度 本周继续看 Linux Kernel Development 3rd Edition，开始搞进程调度那块的东西。不过说起来社区各种调度的 patch 有点多。Zen 之类的，感觉有余力可以去扒一下，不过有没有内核大佬带我给上游提点 patch 啊 本周体力恢复了，翻译书搞完了 Chap5 前半部分，元旦后要疯狂补进度了 准备开始自己的年终总结了 总结2022 对于我来说勉强算没有虚度，希望23年一切会更好吧，祝大家新年快乐！","link":"/weekly/2022/2022-12-week5.html"},{"title":"","text":"2022 周报 2022年2月第一周 2022年2月第二周 2022年2月第三周 2022年2月第四周 2022年3月第一周 2022年3月第二周 2022年3月第三周 2022年3月第四周 2022年4月第一周 2022年4月第二周 2022年4月第三周 2022年4月第四周 2022年5月第一周 2022年5月第二周 2022年5月第三周 2022年5月第四周 2022年5月第五周 2022年6月第一周 2022年6月第二周 2022年6月第三周 2022年6月第四周 2022年7月第一周 2022年7月第二周 2022年7月第三周 2022年7月第四周 2022年7月第五周 2022年8月第一周 2022年8月第二周 2022年8月第三周 2022年8月第四周 2022年9月第一周 2022年9月第二周 2022年9月第三周 2022年9月第四周 2022年10月第一周 2022年10月第二周 2022年10月第三周 2022年10月第四周 2022年10月第五周 2022年11月第一周 2022年11月第二周 2022年11月第三周 2022年11月第四周 2022年12月第一周 2022年12月第二周 2022年12月第三周 2022年12月第四周 2022年12月第五周","link":"/weekly/2022/index.html"},{"title":"","text":"2023 年第一周周报转眼来到了2023年了，新年快乐！（不是 生活 新冠对于我的副作用就是抗抑郁的药物的副作用比之前大了很多，一周吃完药吐了好几次 没有瘦，但是新年药开始运动了！ 本周带家里新入职的小猫咪去绝育了（XDDD 本周一起去做了宫灯，放在家里超好看的！ 吃了超级不好吃的串串！把你招牌上的成都味给我去了。不知道的还以为成都的人都在吃猪食 和群友每天组队背单词半小时，感觉单词还是要继续背的 本周的娱乐时间 COD19 好像被我遗忘了 CSGO 大进步！预瞄，买了一把新枪 M4-彼岸花，Manjusaka 的 Manjusaka 之枪 妹子被我安利成功放学后海堤日记Hhhhhh，开心! 继续闲书时间 写了自己的年终总结 本周的闲书时间 唐史并不如烟 读史总会让人舒服 技术这周的我 be like：重构什么啊，重构时留的泪，是之前脑子进的水 这周大部分的业余时间都放在 nerdctl 上了，预计这样的节奏还会持续两个月 一周肝了/合并进去了 8个 PR，心里憔悴，，我当时开坑脑子进水了。。参见 脑子进水记录 比起预期来，重构的过程还是有点波折。不过很多事情感觉也没法提前预估 业务剩下的时间去搞了搞 Envoy 的 WASM，Envoy WASM 支持走 Host Function 进行 gRPC Call，这样扩展性的确会更好，但是走 host function 进行调用的话，性能会有一定的损耗，所以这个方案还是需要慎重考虑的 帮群友业余时间排查了一个比较冷门的 ext4 的问题。参见 排查一个特殊的 No space left on device 在准备答应群友很久的关于物联网的分享，但是没考虑好什么时候讲（ 本周继续看 Linux Kernel Development 3rd Edition，继续搞进程调度那块的东西。 翻译的书稿进度还行，尽快得赶上来 差不多这样 总结祝大家 2023 一切顺利啦！","link":"/weekly/2023/2023-week1.html"},{"title":"","text":"2023 年第10周周报本次周报终于用我的 PC 写出了（台式机：说好的爱呢） 生活 本周身体状态还行，除了一如既往的没事闹肚子以外 本周养狗记录 吃屎一次，定点大小便基本学会了 分清左右手了 边牧简直心机，想要吃好吃的时候，会自己主动去正确的地方尿尿，然后兴冲冲的跑过来找你要吃的 M2 本周记录 目前发现 M2 的键盘是真的不行，高强度用到现在，发现已经打油了（难道是我手的问题？ ARM 的生态还是和 X86 有差异。Intel 你能不能加把劲（ 本周的娱乐时间 打了几把 CSGO，手感奇差无比。说起来，起源2快出了？ 把知否看完了 开始看我们大家的河合庄，笑死（ 小说看了下诡秘之主，没看下去（ 命运石之门（感觉不是还合我胃口（ 本周出去吃了寿喜烧！好吃！ 继续背单词 还在继续瘦（ 技术这周的杂事真的多，感觉自己每周都在变菜，我会不会以后没人要啊呜呜呜 runc 和 nerdctl 这周都没花太多时间去顾及。不行，这是个问题得改 这周背 IaC 各种折腾： Terraform 这种 IaC 其 module 和云厂商交互控制台的逻辑存在一些 GAP。有些时候这点在不注意的时候就会翻车 IaC 其实是对状态的一个描述。如果遇上了云服务商自动给你升级小版本这样的骚操作，那么不注意的话可能会导致你的 Infra 有重建的可能 TF 自己的 DSL 在对于一些动态性描述的场景会有比较大的问题。我在考虑有部份组件考虑迁移到 Pulumi 去管理。但是状态同步又是一个问题。 这周在思考一些 IaC 未来可能是什么样的，记录下碎碎念吧 其实 IaC 现在在面临高速迭代的业务的时候，Infra 和 Domain Bussiness 分离其实会导致研发速度还是被人所制约。 今年开始，一个新生的概念提出来了，Infrastructure from Code，业务代码生成基础设施。这一点可以看两篇文章 Infrastructure from Code: the New Wave of Cloud Infrastructure Management 和 Framework-defined infrastructure IfC 这一新生事物我自己也还在观望，不过目前能看出来几点的限制 极度吃内部的代码基建。如果框架，业务逻辑描述不收敛，那么 IfC 无从谈起 边界糢糊后带来的稳定性问题 写了篇水文，聊聊自己眼中的 IaC，参见 简单聊聊 IaC：Infrastructure as Code 继续读 Unikernels: Beyond Containers to the Next Generation of Cloud，写的真的挺好玩的 读论文 SOCK: Rapid Task Provisioning with Serverless-Optimized Containers 新开了坑，作为 《Python编程 : 从入门到实践》第三版中译本的审阅。 整理 MVP 续期资料 准备26号北京的 Python meetup 的演讲 差不多就这样 总结总结一下不知道些什么好，就这么总结了（你打我啊","link":"/weekly/2023/2023-week10.html"},{"title":"","text":"2023 年第11周周报我又当鸽王了（ 生活 本周身体状态还行，开始吃补品了，效果还行 本周养狗记录 本周小狗顺利定点大小便！ 小狗学东西好快啊。这周你问他想出来玩吗，想出来给右手，不想出来给左手，他能分清！ 小狗去牧羊啦！ 小狗被精致翻新了，虽然第二天牧羊就 GG 了（ 这周和妹子和小狗一起去露营办公了，下下周还想去！ 本周的娱乐时间 CSGO 继续枪法摆烂（ 在看纪录片冰冻星球2 看了来自回忆同学推荐的番，《我们大家的河合庄》，非常惊喜 套了一层工业化的壳，但是制作很用心 内涵很不错，吐槽很到位，而且令人惊喜的是思辩的深度和角度不错 虽然有不少成人段子，但是控制的恰到好处，而且很贴合剧情，非常见监督和脚本的功力 目前在我的心里地位应该是仅次于昭和奥和平成奥以及摇曳露营两部曲 命运石之门还是没太看下去 本周出去吃了日式烧肉！好吃 继续背单词 还在瘦啦！ 妹子给我买了赛文55周年限定版的金古桥！开心！ 技术又是杂事颇多的一周，不过也还是要继续努力呀（不能继续摆烂变菜了 把 MVP 续期资料填完了（XD，累死我了 这周帮网友查了一个很教科书的半连接全链接队列满了导致 SYN DROP 的问题，还是比较好玩的 这周去系统的看了下 eBPF CO-RE，这里推荐两篇文章 一篇是 eBPF 怪兽 Brendan Gregg 的 BPF binaries: BTF, CO-RE, and the future of BPF perf tools 第二篇是 Facebook 的 BPF Portability and CO-RE 这周开始折腾 DNS 系统的一些东西，估计后面不短的时间内都要和 DNS 系统打交道了 系统看了下 DNSSEC 及关联的几个 RFC DNS Security Extensions (DNSSEC)RFC 9364 去了解了下 EDNS 的一些东西，参见 Extension Mechanisms for DNS (EDNS0) 看了下 Client Subnet in DNS Queries 这样的有趣的一些应用 重新捡起 COREDNS 和 ADGuardDNS 看一下他们的实现，目前 Homelab 的 DNS 用的我有点头大，所以可能想自己写一个自己的 DNS for homelab 把 Unikernels: Beyond Containers to the Next Generation of Cloud 读完了 《Python编程 : 从入门到实践》第三版中译本的审阅结束 继续翻译 Chap7 公益群有小伙伴被 promote 成 Apache Committer 了，开心 差不多就这样 总结春天来了，多出去走走呀","link":"/weekly/2023/2023-week11.html"},{"title":"","text":"2023 年第12周周报本周是麻上加麻的一周 以及沉痛悼念杰克奥特曼人间体乡秀树扮演者团时朗老先生因肺癌去世，享年74岁。夕阳下的战士与奥特之星永远活在我们心中 生活 本周心态崩了几次，原因见开头，麻了，哭了几次了 本周养狗记录 小狗这星期又学会好多东西，开心 这星期小狗和小区里的其余小狗狗一起玩的很开心（当然我和其余主人也有话题能聊（有不少互联网社畜 这周爆冲的机会少了不少，好事 导致我新的 16 寸的 Mac 屏幕坏了，身价+799 本周去租了下的房子，下周搬家 130+ 的房子，生活质量极大提升 一梯一户，人车分流，小狗估计能更开心的玩了 本周的娱乐时间 CSGO 状态终于恢复一点了，没那么摆烂了 纪录片冰冻星球2 重刷了黑影坠落，每次看到 1st SFOD-D（美国陆军第一特种部队 Delta 分队，俗称的三角洲）的 Master Sergeant Gary Gordon 和 Sergeant First Class Randy Shughart 面对摩加迪沙数百部族武装人员，在不确定援军何时能够到来的情况下，仍自愿到在二号坠机地点守卫机组人员，一直坚持到全部子弹打光最后英勇牺牲的刻画时，总是很难过。以及这部片子每次看都能发现新的希捷 重刷白沙的水族馆 重刷我们大家的河合庄 冰果还是没能看下去 本周出去吃了好吃的糟粕醋火锅！ 继续背单词 体重均衡 技术都说了这周是麻上加麻了 这周完成了在北京的一个分享，聊了聊 IaC 这周处理了内部的安全隐患，再次强调下关于帐号的几个原则 所有 API Key 最小权限颗粒度 代码里不放 API Key，收到 CICD 注入环境变量 开发过程需要测试的每个人都独立的 API Key 员工离职后回收个人所有 API Key 应用服务请不要固定 AK/SK，请走角色扮演/ServiceAccount Binding 等形式获取云资源的访问。参见 服务账户的 IAM 角色 这周 Sentry 又炸了，我目前看起来觉得可能在这种上报服务前自己过一层限流之类的策略，对于整个系统的稳定性还是有好处的 这周 OpenAI 出来的一个事故报告很有趣 March 20 ChatGPT outage: Here’s what happened Python 的 asyncio 还是不太成熟，redis-py 这样已经很成熟的库了还有这种 race condition，也是很让人头疼的 具体修复可以参见 AsyncIO Race Condition Fix 这周整理了下关于 nerdctl 在四月份想做的一些事 之前的重构工程差不多完成了，四月份可以彻底收掉 我准备开始 DNS 这一块的重构工作，会写一个简单的 CNI 插件 将 nerdctl 接入到 OpenAI，这样对于容器内的一些日志可以快速辅助定位问题 Chap7 差不多赶完了 这周继续开始捡起 Linux Kernel Development 3rd Edition 来看 差不多就这样 总结团时朗先生千古。 最后复习一下奥特五大誓言吧 不要饿着肚子去上学 好天气需要晒被子 过马路时注意交通 不依赖别人的力量 不要赤脚走在地上","link":"/weekly/2023/2023-week12.html"},{"title":"","text":"2023 年第13周周报本周还是上条当麻的一周（ 生活 本周身体状态又在波动，头疼，是不是该一点之前睡觉啊（ 本周的养狗记录 小狗这周的学习以巩固为主 响片课程开始后，小狗和人的互动性也提升了很多 感觉边牧还挺喜欢学习的 搬新家啦 130+的大空间，狗和猫都玩的更开心啦 把家里的网络重新规划了一下，整体质量提升很多 有了一个小机柜来处理 NAS 这些设备 下周准备开关之类的弱电重新改造一下 本周的娱乐时间 重新打了一下使命召唤，感觉新赛季完全变了一个游戏 CSGO 挂逼真多 重刷白箱 小说没啥好看的，难过 准备下周开始看看星际牛仔（ 本周因为搬家累个半死，所以没出去吃好吃的 搬家时发现之前买的各类钻子真好用，准备一步到位上个博世的无刷电钻了（就是有点肉疼，2k+) 继续背单词 体重在下降 技术 这周又在做内部的安全控制，大概记录下常用的一些管控点 CI/CD 的收口需要注意，确保机器上的敏感信息降到最小（实际上理论上来说，机器上不存放敏感信息是完全能 OK 的，不过考虑到 MacOS 这样奇葩的构建环境，还是考虑将敏感信息控制到最小吧） 员工个人帐号权限的绑定与权限最小化 测试帐号与生产环境的隔离 数据查询平台等需要有足够的 audit 手段 这周在用业余时间给妹子改进之前的小组机器人 在测试接入 OpenAI，不过感觉对于灵活场景，fine-tuning 出来的效果一般（不过也可能是我数据集太小了） 而且在对于模型微调的时候，好像还不支持给予 negative 的数据源进行反向训练（不然可以形成个完整的 auto-pipeline） 这周在 homelab 的优化上花了不少时间 家里本来准备上 UBNT 交换机+ AP 方案，不过经过判断发现效果可能不佳，所以暂时放起掉 目前家里选择 XDR10280 * 1 + XDR 6080 * 2 走有线 Mesh 的方式，目前看起来即便在工作间，不开门的情况下，也能完成千兆的速率 下一步考虑走光线暗线，完成我 40Gbps 内网的梦想，不过具体的实施方案还没搞好 本周开始把精力放回了 nerdctl 上 这周帮人排查了一个 iptables 的问题，参见 Can’t access the exposed port from LAN in bridge mode.。比较经典的问题。突然想起之前 calico-felix 一个我不太理解的设计“每次启动时会扫 iptables，把自己的 iptables 放在最前面”。之前不理解，现在感叹，工程老炮的吃屎经验果然不是白吃的 社区居然有同学找到了去年4月因为 CRIU 不太成熟导致搁置的 checkpoint 的支持，参见 nerdctl checkpoint command support。我看了下，之前在 5.1x 上因为 overlayfs 导致的 CRIU crash 在内核里已经秀了，参见 re-apply missing overlayfs SAUCE patch。而且 K8S 的 Checkpoint 也已经支持了，nerdctl 也最好跟进了。暂时作为个 P1 的事情，争取下下周之前搞完吧 本周 Chap7 翻译完工，开始 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结这周躺在新家的沙发上，小狗躺在脚边，妹子在旁边练琴。我自工作以来好像没法想到比这个更好的生活。但是说实话我自己有一种不真切感与不安全感，生怕那一天因为我自己太菜了，或者其余原因，这种如梦似幻的生活便离我远去（也因为这个从噩梦中惊醒）。 这周其实还是没从团时郎先生的离世（没想到坂本龙一也离世了）中缓过劲来。曾经想过如果如果做了好事，能有机会站在历代奥特曼人间体面前告诉他们，从小看他们影片长大的孩子，没辜负他们的教诲，没辜负奥特精神（我也不知道我算不算没辜负，先姑且算个合格的吧）。目前看来会有遗憾了（虽然可能也不会有这样的机会吧） 差不多这样吧，愿奥特之星在每个人心目中闪耀","link":"/weekly/2023/2023-week13.html"},{"title":"","text":"2023 年第14周周报最近越来越忙了（ 生活 本周身体和状态继续一般，精力比较差（ 本周的养狗记录 小狗继续被猫猫揍了（果然狗和猫聊不到一块去（ 小狗肠胃炎，上吐下泻，又是去医院奔波（ 小狗35斤啦！正在奔着40斤去 小狗本周被夸好看*6 新家改造计划： 思考了下，感觉全屋光在现阶段意义不大，所以暂时搁置 下一步是将家里的开关全部更换一遍 小米的洗衣机用起来很不错，烘干是刚需（ 本周的娱乐时间 使命召唤打不动了，挂哥和 CSGO 一样多 本周难得打了下 CSGO，又是挂哥，麻了 本周看完美星球2，好看 本周新番，跃动青春，感觉还不错 小说重新复习了下斗破苍穹，土豆的爽点把控能力真的牛逼 本周没有出去吃好吃的（呜呜呜 本周给妹子上了小米 13 Pro 顶配，双mi fan家庭继续（ 继续背单词 体重没太大变化（ 技术要多努力一些了，，不然要继续变垃圾了（ 本周在 OpenDAL 上花了一些时间 做了以下 SMB 支持的调研，Issue1877，放弃支持了。一个是需要依赖 libsma，GPL 和 Apache License 出现了冲突。要想实现完整的 SMB Protocol 还是有点蛋疼的。 接了 Dropbox 支持的活，Issue1098 ，虽然有官方的 SDK，但是因为其是全 block 的设计，所以需要自己实现文件相关的 API 兼容 Tokio 的 async 支持 给 OpenDAL Python binding 做了 benchmark，参见 PR1882，OpenDAL 走 PyO3 做的 async 支持在性能上比 Gevent Patch 要好很多。不得不说 Rust 还是猛啊 麻了，我的 Azure OpenAI 还没申请下来 本周开了一个 openai-translator 的坑，支持全文搜索，参见 Issue565，原本以为在 SQLite 里做全文索引会比较麻烦，但是发现官方其实提供了比较好的支持，参见 SQLite FTS5 Extension。目前微信的消息记录的全文索引也是基于这个做的，看起来性能能做的很好 本周发现了一个很好玩的 Rime 配置，把我自己的配置替换了，参见 rime-ice 本周贡献了自己真正意义上一个前端项目的 PR，React 的 Lib SWR，参见 PR2550 本周阅读了 juicefs 这篇 浅析三款大规模分布式文件系统架构设计 这篇文章，挺有收获的，建议大家都可以去看看。顺便了解到 Facebook 在 Fast’21 发布的一个工作，参见 Facebook’s Tectonic Filesystem: Efficiency from Exascale 下周可以锐评一下 发现 Python 一个 web UI 库 niceui，我觉得挺有意思的，将 Python 处理为 Vue 的 component 23333 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 gcgg 团队招人，详情参见 TensorChord 2023 实习招聘（暑期与长期） 总结有很多话想说，但是却不知道怎么说。始终还是处在对于现有生活的不真切感，以及觉得自己太过于垃圾的焦虑中苟活。不过 Everything is gonna be OK. 愿我们记住共同走过的岁月，记住爱，记住时光","link":"/weekly/2023/2023-week14.html"},{"title":"","text":"2023 年第15周周报这周状态终于恢复了点了（ 生活 身体状态一般，但是 saka の 根性复苏了（ 本周养狗记录 小狗越来越天使啦 本周小狗出门被跨*5 本周小狗超级招猫嫌（没有边界感的小狗） 下周准备继续带小狗出去露营啦 本周娱乐时间 CSGO 终于没遇到挂哥了 本周开始看贞观攻略的小说（历史文还是和我胃口的（ 跃动青春，好看！感谢回忆同学和铁妹联合强力推荐！ 本周继续看完美星球2 新奥特曼 4K BlueRay 的资源出了！重看了一遍，奥特曼你真的就这么喜欢人类吗？ 本周出去吃了海底捞和糟粕醋火锅 本周买了新的工具 新上了博世的 18v50 的钻，比我之前的 Go2 握持和发力上感觉更舒服 买了新的电焊机（ 继续背单词 体重在慢慢下降 技术本周正儿八经技术没咋搞，乱七八糟的技术搞了一堆 本周家庭大改造之 saka 的技术之旅 家里的插座全部换了一遍。（祖传的插座技能 家里部分房间重新网络布线，请跟我一起唱橙白橙 绿白蓝 蓝白绿 棕白棕 本周因为小狗拆了狗笼，尝试点了电焊技能.jpg 我是不是可以考虑35岁之后去搞装修（绝不坑人（我们利润率不超过 10%.jpg 本周的傻逼瞬间，和布线有关 插了线后发现不亮，因为昨天把插座插松了，以为是插座的问题，换了插座才发现是交换机插座线 静态 IP 分配的时候，ip route 没看到 default 规则，然后才想起忘了设置 Gateway了 本周工作上最头疼的几个事 依赖公网 P2P Peering 的一些场景会链接非常不稳定，头疼，会导致一些场景的启动和同步速度非常慢。下周可以研究下能不能占座 Electron 的崩溃搜集，整体上方案还是依赖 Sentry 了。不过有个细节需要注意下，对于 Native 的 Runtime 层， crash report 需要依赖 debug symbol，虽然 Electron 官方提供了各平台符号表下载，但是还是推荐走 Sentry 自己 repo 同步符号表 Windows 怎么样让进程崩溃，在做一些测试。我自己测试了 Windbg 改内存不太 work，思来想去还是可能打个有问题的包，手动点击触发吧。怀恋 Posix（SIGSEGV 解决战斗） 因为一直在搞崩溃搜集相关的东西，所以重新看了下 ELF 相关的东西 本周我的 Azure OpenAI 终于拿到权限了，正在重新 fine tuning 自动回复的语料（ 说起来，这周又被 Terraform 坑了一次，你们一定要锁版本！即便是 AWS 这样使用面很广的 module 也会出现升级后直接废字段而不 deprecated warning 的！麻了，听我说谢谢你！ 这两周有一部分的工作在做存算分离相关的东西。实际上在一些场景下存算分离所带来的成本的优势在 cloud vendor 下并不明显（主要是 EBS/NVME 之类的太贵啦！AWS IO Express2 是 EBS GP3 16000 价格的10倍，麻了）。而且会引入多层网络 I/O 会导致系统复杂度提升。对于可观测性也有了比较高的要求、 这周看了=一篇很不错的文章，实用 Web API 规范 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结菜鸡也好，垃圾也罢，生活嘛，总得缓慢向前","link":"/weekly/2023/2023-week15.html"},{"title":"","text":"2023 年第16周周报继续生活（ 生活 本周身体状态终于好了一些了 本周养狗记录 小狗本周平均每天被人夸两次 在搬了新家后，小狗也变得愈发天使了 不过这周估计又是吃猫粮翻车了，去医院打针（肠胃炎属于自己作死2333 本周娱乐时间 完美星球刷完了，好看 复习排球少年，教练我也想打篮球 贞观攻略看完了（ 试图玩新游戏但是失败 本周家里新增大件电器：Sony X91K 75寸。HDR 效果就是好顶！ 本周都没怎么出去玩，也没吃好吃的，麻了（ 继续背单词 体重平衡（ 技术本周工作强度有点大，所以感觉业余时间少了不少 本周花了不少时间在 AI 新事物的折腾上 利用手上的数据，尝试在 Azure OpenAI 上基于 curie fine tune 模型，发现几个问题 Azure OpenAI 现在的训练速度非常慢。我万把条数据集，训练了大半天 curie 这些模型，实际上在最终的生成质量上还是有点蛋疼的 这周深度用了下 ChatGPT 在一些对话生成上，情绪和整体风格的倾向说实话还很 old style？也许是我 prompt 不对？ 对于日常工作的确有效率上的提升 说起来不知道开源的模型 fine tune 的效果咋样 这周发现 Azure 稳定性做的很有意思的一点，引起了一些思考 在我 fine tune 模型的时候，我的操作触发了 Azure 后端的 internal error，进而返回500 在我准备提工单的时候，Azure 的 Health Checking 提醒我，我的账户在 OpenAI 服务下遇到问题。如果是因为这个问题想要去提工单的话可以稍晚等服务恢复。 实际上这一点是很难做到的。这似乎意味着微软做到了 per account 级别的 traffic health monitoring。那么这意味着需要从入口开始对流量进行标记，染色。不过这一样一套链路其实我觉得效果也是立竿见影的？ 本周工作的碎碎念 千万不要信 AWS 说的什么 Redis 无损升级的鬼话。实际上升级带来的 AOF 丢失，DNS 迁移都一样存在。而且缓存层的迁移需要格外注意缓存击穿的问题。直接 DB 炸了（ 吃屎 K8S 的一周，说实话 K$S 面对有有状态拓扑的场景真的是一言难尽。即便能抽出 Operator 也是吃屎一样的存在 本周 Sentry 又 quota limit 了，还是老老实实做隔离限流吧。 继续看 Facebook’s Tectonic Filesystem: Efficiency from Exascale 这篇论文。感觉可以用 PDFChat 之类的 AI 工具辅助一下？ 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结很久没有静下心来写代码看书了，这样不好，需要赶紧调整了","link":"/weekly/2023/2023-week16.html"},{"title":"","text":"2023 年第17周周报本周稍微开心一点，有不少可以折腾的了 生活 本周的肠胃一如既往的波动，感觉是目前吃的来士普，德巴金，劳拉西泮的组合对我肠胃负担比较重了。感觉可以去复查一下了？ 本周的养狗记录 本周小狗的皮毛进一步油光水滑，被很多人夸好看 小狗的情绪进一步稳定 本周小狗有了不少新玩具Hhhhhh 本周好友赠送的 12 代 i7 NUC 到货，美滋滋 自己新买了两个零刻的机器，一个12代的 i5， 一个 AMD 的 6800HX ，我叛变了 Intel .jpg 本周的娱乐活动 CSGO 继续，我发现一个规律，我白天打的时候胜率高，凌晨打的时候胜率贼低 COD19 第三赛季，emmmmm，感觉完全玩不来了 玩了下无人升空，还不错感觉 本周小说重新开始看黎明之剑，琥珀太可爱了 跃动青春，好看（ 本周出去吃了老火锅和伊豆火锅，不过饭量真的是越来越小了，吃不动了（ 准备换个人体工学椅子了（ 继续背单词 体重继续下降 看到了资助学生的近况，非常开心 技术 本周玩 Homelab 完了个痛！ 首先零刻的两台机器我买的都是准系统版本，自己给买了 64GRAM + 1T SSD 然后最开始本来想直接 Ubuntu 三台机器，但是想了想，还是 PVE 都一把梭 AMD 6800HX 在安装 PVE 的时候，会有一些问题，需要自己手动处理下 xorg 的配置文件。 PVE 官方居然提供了 6.2 的内核，这个就很赞了。6.2 对于大小核调度上的优化还是很不错的 不过 PVE 目前的易用性还是很不足的，比如要退出集群，或者其余一些常用操作，都不太 work 这周自己重新编译了 OpenWRT ，由于上游还不支持 6.2 的内核，所以只能做 6.1 的支持。目前用上了感觉还不错。另一个花絮就是这次固件是在自己的 NUC 上编译的。比默认的 GHA Runner 节约了4倍的时间，开心。 本周在 Ubuntu 23.04 用自己的脚本初始化环境的时候，发现了 Python 3.11 在行为上的游戏欸改变。参见 PEP 668 – Marking Python base environments as “externally managed”，简而言之在 PEP 668 中引入了一个叫作 externally managed 的概念。可以理解为如果发行版激活了这一特性，除非你使用发行版自带的包管理安装对应的包，你用 pip install 包到保护路径将会报错。这个对于目前的一些上游项目的行为影响还是有的。已经去提出相对应的 issue 本周开始重新捡起 runc 之前的 Burst 的 PR 开始写了，搞低版本的 CI 真头疼 本周里工作比较好玩的东西 K8S 跨版本升级目前来说还是比较麻烦的，不过这类底座升级都一样（ AWS CSI 的天坑，参见 Issue1569。简而言之是在一些情况下，如果 Node Group 里的 Node 出现异常，Pod 被驱逐后，有可能 binding 的 volume 无法自动释放，需要人手工 force deattach。麻了，这属实是天坑 本周 Sentry Proxy 正式上线了，不过发现不同 SDK 的行为有一些不一致，需要手动去在 Proxy 层做兼容。这就是逆向的蛋疼么 本周开始准备做 systemd CPU Burst 的支持，不过说实话能不能顺利做完我心里没底 开始写 OpenDAL 的 Dropbox 的支持，Rust 写起来还真是不太熟练.jpg 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 跑去看了眼 langchain 的东西，感觉可以把我 Logseq 上的一些笔记什么导入进去进行搜索来着（ 总结生活不易，一起加油（","link":"/weekly/2023/2023-week17.html"},{"title":"","text":"2023 年第18周周报本周还是玩的比较开心的 生活 本周肠胃还是很脆弱，预计下周去复查一下 本周的养狗记录 狗粮不能喂多了！否则会拉稀！ 小狗带去狗狗营地，跑的贼欢 皮毛越来越好看了 本周开始和妹子互相给对方准备一些小礼物，然后封在小纸条里，这样在对方难过的时候就可以抽点小惊喜！ 本周的椅子到货了！果然一分钱一分货 本周的娱乐时间 黎明之剑，N 刷 CSGO继续，最近预瞄的能力有所进步 无人升空好玩 复习强风吹拂，教练我也想跑步.jpg 跃动青春，好看 五一人太多了，没出去吃好吃的 体脂27，啊啊啊啊啊，我不活了 继续背单词 给被捐助的学生额外补了一些钱，希望她高考顺利 女朋友送我了奥特曼！ 技术 本周 Homelab 的折腾时间 这周 PVE 偶发性的会出现某个 Node 的 known_hosts ，导致集群模式下跨节点的服务登录有点问题 开始用 Prometheus 做各个 VM 的监控，什么叫 Exporter 暴露癖啊 基于 CloudInit 做了一批基础镜像，这样不需要手动安装点点点点了，我之前为什么这么傻逼呢？ 这周 runc CPU Burst 的 PR PR3749 终于有了比较大的进展 首先完成了高版本内核的测试的通过，（Ubuntu 2004 和 2204 行为并不一致），得很 Dirty 的处理以下 Centos 7/8 的测试终于过了，蛋疼，也是得额外做一个 skip 处理，麻了 这周把 nerdctl 的之前提的 checkpoint，Issue956 写完了，不过没提 PR，几个原因： 现在 containerd 的 checkpoint 支持太过于原始了 Docker，Podman 实际上是自己完整写了一套逻辑。我也在考虑有没有必要这么做 systemd 之前提的对 CPU Burst 的支持，参见 Issue26658，被扔给自己做了。这周开始看代码做一些前期的调研工作，但是 systemd 的代码看起来真恶心（ OpenDAL 的 Dropbox 支持开始进入冲刺阶段！ 本周工作上有趣的事情 AWS 的 NVME 机型是真不行，又不支持快照，又难用 发现需要对于 CDN 和 WAF 做不少的优化，应该是可以提升不少吞吐以及 anti bot 的效果会更好 docker 的新 network 的网段默认从 172 开始，很容易和云厂商的 VPC 段冲突，拉黑拉黑 本周比较好玩的一个讨论：Connection reset while connecting https://storage.googleapis.com with rustls 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结放假真好，可惜要恰饭就要上班（呜呜呜","link":"/weekly/2023/2023-week18.html"},{"title":"","text":"2023 年第19周周报本周状态开始恢复了一些，以及我29岁啦！ 生活 这周去医院复查了 双相，混合发作，不过整体在好转 来士普的停了，换成喹硫平，劳拉西泮加量，德巴金不变 加了碳酸锂 肝功有点问题，但是不大 新药组合第一周有点得劲 本周的养狗记录 养狗真不后悔 小狗和我们熟悉度也增加了不少，果然还是需要互相磨合 本周突然水逆了起来，我一晚上经历了 电脑坏了 耳机断了 捡屎的时候袋子破了弄手上了。麻了 本周29岁啦！和妹子一起出去吃好吃的啦！ 本周开始戒碳水，多喝纯净水，刷脂 我坚持了运动！你们快夸我！快快快！ 本周娱乐时间 黎明之剑 看完了（ 河合庄 N 刷，互相救赎的感觉太棒了 CSGO 这周手感还行 Netflix Ultraman S3 出来了，悬疑营造的不错 继续背单词 看《通信简史》 技术本周状态开始恢复了一些欸！、 妈耶！！我 runc 的的 PR 终于被 LGTM ！参见 PR3749 这个 PR 是接盘 Intel 老哥的 PR ，为 RUNC 做 CPU Burst 的支持 CPU burst 主要是可以通过调度，让 Burstable 的应用更少的被 Throttle 这个 PR 呢，我接盘的时候，代码量不算大，但是主要吧就是跨内核版本（4.x,5.x,6.x)，以及跨 Distribution （Ubuntu，Centos）的测试难写，不过好歹是 LGTM 了一个了 这周去给 OpenDAL 写 Dropbox 的支持了，参见 PR2264 Rust 真的太难了呜呜呜，感谢 @Xuanwo GG 和铁妹的耐心教导 Dropbox 的不少 API 真的很奇葩，比如把参数放在了 Header 里，我？？？ 对于 OpenAPI，我还是喜欢 Rest 风格的，RPC 风格的太蛋疼了 感谢 gcgg 的新文章 你真的需要一个（专门的）向量数据库么，去大概了解了下VectorDB 的一些东西 gcgg 的文章里有两篇 Reference 讲 Embedding 的，写的也很棒，参见 What is an embedding, anyways? 和 Enhancing ChatGPT With Infinite External Memory Using Vector Database and ChatGPT Retrieval Plugin 这周加入了 Logseq 团队的一个项目，准备给 Logseq 做 Local Langchain 的支持 这周在折腾一些的 DNS 上的东西： ADGuard 团队自己实现了一个 DNSProxy，500来行，大部分场景够用，但是我觉得还是可以在优化下设计 dns 这个库的老哥是真的猛啊，实现了一堆 DNS 的 RFC 支持 我发现了 Switch 的模拟器，Ryujinx，这群老哥真的过于猛了 这周有个很好玩的事情，事情是这样，我新买了一个 2T SSD，然后准备把 Home 迁移过去，在迁移之前 df -h 看 Home 大概是 192G，mount SSD 到一个目录，拷贝完 Home 后发现是330G。最开始没想明白为啥。后面才想起，我 Home 原本挂载的时候开了 zstd 压缩，但是新盘挂载的时候没开。所以解决方法很简单，执行 sudo btrfs fi defragment -r -v -czstd /home 压缩一下就行了（逃 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结转眼间29岁啦。奔三前最后一年啦。想说很多，但是又不知道说些什么。感谢所有陪我一路走过来的人，父母，女友，密友，公益群天天给我推番但是听我一天6罐零度后又急得不行的憨憨的群友们。感谢你们所有人对于我的包容，支持。或者说，感谢你们对于我的溺爱 对于新的一年，好像也没什么太大的愿望吧，控制病情，努力工作，多健身，多和群友一起看番。再多做一些事，能帮更多的人 優しさを失わないでくれ。弱い者を労り互いに助け合い。 どこの国の人とも友達になろうとする気持ちを失わないでくれ。 たとえその気持ちが何百回裏切られようとも。 それが私の最後の願いだ。 热忱之心不可泯灭。要体恤、帮助弱者。与任何国家的人都能成为朋友，别失去这份热心，纵使它已被背叛了千百回。 这就是我最后的愿望。 回应期待，且不要辜负善意，这是时间最幸运的事情。希望我能做到","link":"/weekly/2023/2023-week19.html"},{"title":"","text":"2023 年第二周周报2023 年第二周，出去玩了个爽 生活 妹子回家了，呜呜呜呜呜 这周去住了一次五星酒店，看北京的夜景！以及在清晨的薄雾中吃早餐，感觉非常爽 这周在次去了国家博物馆，无论去多少次，看到很多文物都会不由自主的想要跪下与落泪 和妹子一起去了颐和园，滑病车真的太健身了属于是（以及说来惭愧，来北京6年了，这是我第一次去颐和园（ 本周的娱乐时间 CSGO 的预瞄技术有提升，买了把新的 AK 饰品，可燃冰（改名叫夏虫不可语冰） 德凯25话更新了，我觉得文戏处理的还不错，反派洗白的套路和我之前预计的差不多 本周重新刷完了希灵帝国，大眼珠子的想象力真的不错 妹子沉迷放学后海堤日记了（逃 吃了宜宾招待所，牛肉火锅等许多好吃的。说起来宜宾招待所的豌豆尖酥肉汤，鱼香肉丝都很不错 本周继续的闲书时间（ 当然本周继续和群友组队背单词了 本周的闲书时间 唐史并不如烟 技术这周都去放阳了，学习效率低了很多了 这周的大部分业余时间还是放在了 nerdctl 上了 我自己贡献了6+个 PR，然后 Review 其余同学的 PR，麻了 到目前发现，社区新同学的参与热情都挺高的。截至目前给社区吸引了4位新的贡献者，而且贡献的质量都很不错。这再次验证了我之前的一个观点，Good First Issue 的质量能有效的决定社区同学的参与热情。没有好的 Good First Issue 和新人引导的社区会迟早慢慢走向死水（爆论 这周另外一部分大头的业余时间去搞了搞 PyO3 的东西，之前说要给 xuanwo 的 OpenDAL 贡献东西。和 xuanwo 讨论了下，我想试着封装一套 Python 的 SDK 出来，这样 Python 用户也能享受一套语义使用多种存储后端的便利性 这周在做一些安全相关的治理，碎碎念一下 所有个人的 AK/SK 的权限 scope 一定要收口，缩小爆炸半径 S3 之类的文件上传的操作尽可能收敛到 CI/CD 上，避免个人手动操作 Release 出去的二进制文件一定要给出对应的 checksum 数据，避免供应链上的替换 对于 CDN 之类的二进制分发的需求，最好在用户点击时生成临时的带签名的 URL，避免中间人攻击 整理了下自己之前搞的一些 eBPF 的脚本，感觉可以做个自己的工具箱了 本周继续看 Linux Kernel Development 3rd Edition，之前关于 CFS 的一些疑问算是得到了解答 又重新捡起 CSAPP 第四章看了（我好笨啊 Chap 5 终于翻译完了（板载 这两周没搞啥技术的东西，感觉新年就这样还是不行，摆烂也得下半年再开始摆对吧 总结新的一年，希望能去更多的地方玩！以上！","link":"/weekly/2023/2023-week2.html"},{"title":"","text":"2023 年第20周周报什么鬼，这都20周了？ 生活 本周精神状态继续给大家汇报下吧 新药开始适应了，虽然副作用真的很大 不过睡眠还是一般， 情绪稳定性换药后好像的确是在好转 本周养狗记录 小狗目前的喜怒哀乐表现的非常明显，喜欢吃苹果，讨厌回笼子里睡觉 和家里猫咪的关系还是一般（ 本周没有出去吃好吃的呜呜呜 我用一周多把体脂下降了百分之三！快夸我！ 喝水计划继续（ 本周娱乐时间： 没啥书看，开始看足球小说了 跃动青春继续（ 电锯人看完了不好看， CSGO 手感如果用数字量化那就是正弦函数了（ 继续背单词 看《通信简史》 妹子给我买了大水枪！ 技术本周好像状态继续网上走 聊聊工作上的好玩的东西： 这周基于 Cloudfront + Lambda 去做了一个根据 UserAgent 自动分流的东西。差不多就是满足特定 UserAgent 的话转发到一个 ALB 下，否则回源到 S3。Lambda 这一套体验还是不错。不过在 CDN 的场景下，一个是调试异常困难。一个是文档很少，不少 CF 里专属的字段映射在 Lambda 的 event 会有很大的变化 这周继续给 OpenDAL 卖命.jpg，参见 PR2264 Dropox 的 RPC API 也真是食屎。。把控制参数放 Header 里。。我宁愿他做成两段式请求。。 我以为我会写 Rust 的时候，Rust 总会告诉你你是个傻逼 感谢 Xuanwo 哥哥的指导，解答了我一个疑问，参见 gist。小声自恋下，我这算不算提问模板（ 本周是在 Homelab 上持续吃屎的一周 首先重做了我内部的 OS 模板，这样新增的镜像自动带 Node Exporter 和 Docker 搞了套 PVE 的 Discovery，差不多思路是轮训 PVE 的 API，将节点信息写在 JSON 里，这样就可以利用 Prometheus 的 file_sd 去发现了 最后 AMD 是傻逼！为什么傻逼呢？听我说啊 首先，AMD Zen2/Zen3/Zen4 CPU 的温度传感器在 Linux 下实际上没有太 work，你执行 lm-sensors 的命令的时候，只能获取到磁盘之类的外设的温度 在 Linux 中，管理各个外设温度这些传感器的模块叫 hwmon（Hardware Monitor 的缩写），参见 The Linux Hardware Monitoring kernel API。lm-sensors 之类的工具实际上就是走 hwmon 获取的 在 hwmon 中，不同的硬件有不同的实现，其中 AMD 的 Temperature Driver 是叫作 K10temp，参见 Kernel driver k10temp 截止到 Linux 6.2（上游内核或者我自己编译的内核都不行），像我的 6900HX 在 Linux 下都没法独到具体的核心温度 我以为是 6.2 的 k10temp 还没有实现对比较新 CPU 的兼容，于是我将 Kernel 主线里的 k10temp 做了一些修改并移植到了低版本内核上，通过打 module 的形式替换。结果还是 GG，不管怎么 patch 都不行 当我放弃 k10temp ，将目光投向了另外一个 zen temperature 的支持 zenpower 后，我自己修改安装后还是不 work。。 我放弃在我的 Linux 机器上看 AMD CPU 的温度了，AMD 就是傻逼！ 这周我将我台式机的 Linux 系统替换成我自己精简优化过的内核了 base on xanmod 的性能补丁 把 AMD 相关的优化全部禁用了 Target 设置为 13th Intel 开了 O3 移除了除我需要的外的所有驱动 开了 btrfs 的新特性和调试特性 Nginx 支持 QUIC 了，去看了一眼实现。 这周我在准备下周图灵出版社的一个直播，顺便去看了下 Python 3.11 的很重要的一个功能的实现：PEP 659 – Specializing Adaptive Interpreter。指令特化其实是 JIT 的基础，我觉得 659 完全落地后，对于官方的 JIT 应该是个很大的利好 这周我的 OpenAI 有了 GPT4-32K 模型的权限了，试了下感觉还挺好用的 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结整个就业市场越来越难了，祝大家都一切安好吧","link":"/weekly/2023/2023-week20.html"},{"title":"","text":"2023 第21周周报又是一周过去了，这一周一周的过的真快 生活 继续汇报一下精神状态 新药适应还行，副作用还有，但是好了很多 睡眠好了一些 双相本周发作了一次，很烦 本周养狗记录 本周小狗学会叫爸爸妈妈啦（用按钮） 本周小狗又开始了拆东西 本周还是没吃东西（ 本周体脂下降到了24%，以下 本周喝水计划继续 本周娱乐时间 翔阳.奥特曼 N 刷，今年十二月要出和音驹比赛的 S5 了，开心 跃动青春，继续好看 本周小说慌，哭死了 CSGO 开始打 Bot 练习，现在12bot 一面墙，100Bot 击杀最快要1分18s，还是太菜了 重新打了一局使命召唤，现在越来越难完了。想去打一下 Squard 了23333 继续背单词 看《通信简史》 技术 本周把自己的工作间拾掇了拾掇，买了新的爱格升支架，屏幕布局转上下双屏了 本周给我的 Keychron 的键盘换了一套轴体，改成凯华的蓝莓冰淇淋 Pro 了，比默认的佳达隆的 G 红 Pro 要好用的多 本周的工作上比较好玩的一些事 这周踩了 Fluentbit 的一个坑。我们 Fluentbit 目前是 sidecar 模型直接走 ES 写入。但是 Fluentbit 默认的写入行为不保证 Retry 的幂等性（或者说 OpenSearch？），在遇到一些不规范日志往 ES/OpenSearch 写返回非 200 后，Fluentbit 会进行重试，这个时候可能会存在日志重复的情况。这个时候建议把 Generate_ID 配置打开，官方地说明为“When enabled, generate _id for outgoing records. This prevents duplicate records when retrying.” 本周我们遇到了一个 AWS 上很奇怪的情况。就是我们 CDN 在刷新缓存 CreateInvalidaion 的时候，时不时的会提示我们被 throttle 了。我们最开始很一头雾水。因为理论上我们没有很高频的离线任务的场景。后续我们才发现是我们 S3 Trigger 的一个 Lambda 导致的。最后处理一下代码就好了。这实际上凸显了一个问题，我们对于 Lambda 上的一些调用实际上掌控力度很差，实际上我们可能需要考虑将 AWS 的一些 API 做一下透明代理，增强链路上的可观测性 本周的 Homelab 折腾 AMD 在我各种移植驱动后，还是没法正确的获取到温度，我放弃了 我的一个 VM （跑一些高内存占用构建）在内存利用很高的情况下，KVM 会出现 page panic，很奇怪，下周找时间复现一下查一下 这周优化了一下我的 OpenWRT 固件构建的逻辑，应该能压缩不少网络传输的 overhead 这周发现 httpx 有一个很有趣的 API 设计问题，发 discussion 讨论了一下，虽然讨论的结果不慎理想，参见 Keep the API style consensus for request model in httpx 这周开始搞 Prometheus 的一些东西 发起了对 PVE service discovery 支持的 proposal，参见 Support service discovery for the PVE platform 这周在写相关代码的时候，发现 PVE 的 API 设计真的很恶心，像 Go 这种没有官方 SDK 使用的真的写起来蛋疼（ 本周去图灵出版社做了一次分享，卖了一波书，PPT 在这里 Python 的进化之路，整体介绍了一下这几年 Python 的一些演进，当然我自己也借此机会去重新看了下 CPython 的源码 这周基于 stow 把我的 dotfiles 统一管理了，参考 【译】使用 GNU stow 管理你的点文件，之前怎么管理我的 dotfile ，或者说 MPV 的播放配置（里面有各种 Lua 脚本，sharder，和特殊配置）一直让我挺头疼的，我现在的 dotfile 仓库 参见 感谢 @xuanwo 的推荐，1Password 在管理终端 credentials 很好用，参见 2023-21: 我的 1Password 密钥管理实践 本周在和群友聊天的过程中，我在写的 Homelab DNS Server 的平替 Dashboard 有了解决方案，参见 rsuite-admin-template 我自己开始维护了一个 aur 上我自己的 xanmod 内核的构建脚本，这样用起来会更舒服一些 继续翻译 Chap8 继续 Linux Kernel Development 3rd Edition 学习 总结感觉整体状态在好转，不过依旧还是个废掉的菜鸡，下周得继续加油了（","link":"/weekly/2023/2023-week21.html"},{"title":"","text":"2023 年第三周周报2023 年第三周，春节回家了 生活 三年没回家了，春节回成都啦（XD 家里的菜还是好吃的一笔的，外面吃的川菜都什么猪食 啊啊啊啊啊，我要疯了，我回成都后吃的回锅肉居然都是青椒回锅肉，疯球了，这什么预制菜害人啊 过年终于可以放烟花了（XD 本周的娱乐时间 CSGO 本周手感奇差（XD 德凯大结局了，咋说呢，有点高开低走的味道，还是有点失望。反派洗白的套路过于传统，最终话反派压迫力是够了，但是有点虎头蛇尾。不过比起扳机超人的 TV 还是好了不少。不过思麦鲁超人也变成可靠的先辈了啊 小说最近又不知道看什么了，迷茫 我第 N 刷摇曳露营了！ 本周的闲书时间献给了摇曳露营漫画（XD 背单词当然继续了（XD 技术虽然这周春节放羊了好几天，不过学习进度还行 本周的大部分时间还是放在了 nerdctl 上了 重构进度终于超过百分之五十了，不容易 这一周还是肝了6个 PR，大年三十继续肝 PR，麻了 其实再重构过程里，有不少 Issue 和 PR 的讨论可以去看看，比如 Issue1889 关于函数参数设计的一些讨论。多人协作的重构就是这样，很多东西没法预先考虑清楚。具体的语义上的设计需要在 PR 中反复打磨 这周继续在看 OpenDAL binding 的东西，碎碎念一下 Rust 能提供 C 兼容 API，简直利好。不管 Python 生态和 Node.js 生态都能无缝接入 CGO 去死 Rust 的 async/await 还没想好怎么接入到 Python 的 async 生态来，先做 sync 吧（虽然 PyO3 支持 tokio 做 async 后端转成 sync，不过还是不清真了 春节剩下几天可以冲个 MVP 看看 继续看 Linux Kernel Development 3rd Edition 这本书写的是真的不错 翻译开始 Chap6 了（233333333 看了个很有趣的项目，sqlcommenter，通过给执行的 SQL 注入 comment 的形式，可以支持传入 TraceId 这些信息，进而更进一步增强 SQL 的可观测性。很有趣。 eBPF 的可观测性新项目 netdata 有点意思 差不多这样 总结新年祝大家新年快乐！","link":"/weekly/2023/2023-week3.html"},{"title":"","text":"2023 年第四周周报2023 年第四周，开始回北京继续相守一个人的时光了 生活 在家呆的实在是不太适应，所以抓紧时间回北京了 不过回北京后还是怀恋成都的川菜 妹子回来啦，带来了山东的好吃的！ 本周的娱乐时间 CSGO 本周开始打竞技模式了，发现强度要比休闲模式要低很多是什么鬼。以及为什么在我连胜几局后总会告诉我要休息20个小时左右啊啊啊 本周妹子正式入坑摇曳露营了（芳文社永远的神 这周看完了群友推荐的《排球小子》，问：大古和翔阳之间的区别在于什么？答：大古需要神光棒才能变身成奥特曼，而翔阳不需要 小说找不到看的，又把之前的大医凌然拿出来看了（XDD 本周的闲书时间继续摇曳露营漫画.jpg 背单词继续 技术这周还是在搞一些技术的东西，不过为了不显得太摆烂，还是做了一些技术和学习上的事 本周的大部分时间还是放在 nerdctl 上了 这周整体的重构进度超过百分之80了，有不少群友的贡献，太猛了 这周摆烂了，只写了两个 PR （理直气壮 这周和 Suda 商量了一下，我来作为 Release Manager 负责 v1.2.0 的发版。正好规范一下 Release 的流程。提了一个 Proposal，参见 [Proposal] Release v1.2.0 作为 Release Manager 一个比较重要的职责是在版本发布后进行回归测试，确保整体 Release 正常进行。这周其实 nerdctl 就发现了两个隐藏的由新功能引入的副作用，参见 [Regression in v0.23.0] Compose does not print detailed error 和 [Regression in main] nerdctl logs exits silently for nerdctl run w/o -d。其实进一步我在思考，这种其实简单 e2e 没法测试出来的问题，怎么样进行自动化的测试比较合理 这周有个比较有趣的讨论，参见 [Refactor] simplify the stdout rewrite logic in image.Save。如果在重构过程中发现之前的一些不合理但是 work 的代码应该怎么办 本周继续在看 OpenDAL binding 的东西。说好的 MVP 鸽了 本周继续搞 sqlcommenter 的东西，踩了一些坑，简单碎碎念一下 sqlcommenter 支持了 Django/SQLAlchemy 这样 Python 主流的 ORM 框架。不过对于 peewee 这类的框架还没有做好支持。实际上我在做 peewee 的支持的时候发现，peewee 并没有设计 SQLAlchemy 这样比较完善的事件回调机制。如果你想在外部 Hook 掉 SQL 的执行过程，注入一些东西的话，只能按需去 override 具体的 SQLDriver 的 execute_sql 方法。参见 [Feature Request] Event hooks for peewee。其实我觉得这样的设计其实并不 make sense。不过这个时候也看出来 SQLAlchemy 的一些设计优势了。 将 TraceId 之类的信息注入到 SQL Comment 里实际上是个比较取巧的做法。在使用云上的一些数据库的时候（比如 AWS Aurora/Aliyun PolarDB），其性能分析器能否将完整的带 Comment 的 SQL 暴露出来还待测试 翻译继续 Chap6. 水了一篇文章 从一个重构项目中能学到什么东西 差不多这样 总结明天又要上班了（死了算了","link":"/weekly/2023/2023-week4.html"},{"title":"","text":"2023 年第五周周报2023 年第五周，生活多了点惊喜 生活 这周又开始做一些奇奇怪怪的梦了。比如印象很深的一个是我回高中母校表示想要复读。曾经的班主任扔给我三道微积分说做出来就能写。于是我梦醒之后马上下单普林斯顿微积分（XD 这周家里入职一只狗狗！ 名叫林克，是一只边牧，五个月大 在一个靠谱犬舍买的，冠军血统，所以骨量贼大（比小区里一只四岁边牧的爪子还粗一圈 贼聪明，到家第一天就已经学会看眼色了 每天遛狗被迫规范作息（ 本周的娱乐活动 CSGO 周一打完定级赛了，白银2，枪法还有待加强，不过这周后面都没时间打游戏了 排球少年 S4 看完了。教练我要打排球！（不是 和妹子继续看摇曳露营，N 刷之后发现作者除了喜欢战队系列以外，居然还喜欢我英这种民工漫（猛男的归宿都是少女心.jpg 有没有推荐的小说啊（ 本周吃徽菜，吃臭鳜鱼，吃的很开心（ 背单词继续（XD 开始每周写点日报总结一天规划明天，感觉还行（ 技术这周继续在搞一些技术上的东西（ 本周一部分时间在 nerdctl 上 v1.2.0 正式发布了，这个版本的变动真的贼大 这个星期的主要的工作在 review PR 上了，下周开始会开始再写一些 PR，把重构工作做个收尾，这样我就可以腾出时间去做其余的项目了。 本周继续搞 sqlcommenter 的东西 测试了一下云上的一些数据库，对于 Comment 中的一些信息也还是能透出的，这点不错 把 peewee 的支持做完了，不过没打算开源出来 redash 之类的工具也利用了同样的思路来做, SQL Track 突然有个问题，对于 Redis 合并中，需要检查 Key 的冲突。我在考虑，有没有可能搞个静态分析工具来自动化 这周又搞在搞一个 eBPF 的场景 场景很简单，attach 到内核里，对于给特定四元组和特定特征的 TCP 报文进行替换。 首先来说，这个需求毫无疑问优先考虑 TC 了，XDP 的能力还是不在包处理这块 难点在于这样几个 对于匹配语法的设计，因为 eBPF 的 Stack 和循环能力都有限制，写起来会很麻烦。而且也不支持正则之类的通用的东西 sk_buff 非线性区 payload 的处理 一段特征报文可能分散在多个 sk_buff 中，怎么样合理的处理也是比较脑壳疼的 看了下在内核里通过 Kernel Module 给 eBPF 提供带正则支持的 helper function 的实现，参见 linux-regex-module。比较 trick，但是有一些参考价值 继续翻译 Chap6 继续看 Linux Kernel Development 3rd Edition 差不多就这样 总结很多人时常沉迷于35岁失业焦虑不可自拔。我觉得这样除了让自己更为痛苦以外毫无作用。所以做好当前的事情，但行好事，莫问前程。至于未来？随缘吧","link":"/weekly/2023/2023-week5.html"},{"title":"","text":"2023 年第六周周报2023的第六周，时间过的真的好快。。。 生活 在有了小狗后，生活作息规律了许多 养小狗真的是很废心力的一件事 边牧是真的聪明，握手，坐下，趴下啥的一个小时不到就能学会。不过他配合不配合你全看他心情 在猫面前真的是卑微的不行 非常擅长看眼色，对我和对妹子的态度完全不一样 捡屎是真臭啊 本周的娱乐活动 终于有点时间打 CSGO 了，不过这周枪马的不行 和妹子刷完了摇曳露营 重新开始看咒术回战（ 求小说推荐啊！ 本周和群友约饭，以及盒马的杨汁甘露非常好喝 背单词继续 写日志的时候发现 lgoseq 的 template 真的好用 技术本周还是在折腾一些之前搞过的东西 这周没咋投精力在 nerdctl 上，不过社区的小伙伴是真的猛，重构进程基本可以告一断落了 这周工作上很大一部分精力还是在落地 sqlcommenter 上 生产环境上终于完整的上了 traceId 注入的功能，顺便额外注入了每个服务独立的账户，最后验证下来效果不错 本体 sqlcommenter 的全局变量的传递方式还是有所局限，可能更合理的是利用 contextvar 这周又有点新的场景需要迭代，对于 Celery 这种异步任务的场景，怎么样合理的将 trace 信息在各个任务间传递下去。现在 Celery 在元数据这块暴露的信息太少了 上周末搞的 TC eBPF 的东西，这周进展不大 特征匹配来说，循环的使用不可缺少，但是在 eBPF 这种要过内核验证器，尽可能保证内核安全的场景下，用 loop 简直非常痛苦。虽然在高版本内核有一些支持改进，但是如果你要考虑向下兼容，就很麻烦 Linux 5.3 之后支持了 bounded loop，参见 bpf: introduce bounded loops 5.17 之后有了一个新的 helper function，参见 bpf: Add bpf_loop helper，虽然很难用就是了 如果要考虑去做更好的比如正则替换之类的功能支持，虽然在上周介绍了 linux-regex-module，但是这种方式是通过 patch 内核的方式进行的，和之前介绍的 《XRP: In-Kernel Storage Functions with eBPF》 这篇工作中作者的思路一样。。这样通用性比 Linux Kernel Module 又低了一个 level。不过 eBPF 这种以稳定性为第一优先级的东西，要做扩展的易用性必然会在其余方面取舍。。 重读 ATC19 的论文 @Extension Framework for File Systems in User space eBPF 的一个项目 ebpf-verifier 很有意思，将验证器从内核中独立出来，可以让开发者自己去验证不同版本的兼容性。 看了下 Envoy WASM 的根基 proxy-wasm-cpp-host，里面 API 设计蛮有趣的 继续 Chap6 的翻译 继续看 Linux Kernel Development 3rd Edition 公益群的小伙伴来了一次赴日工作居住指南的分享，好耶！ 差不多就这样 总结我下周绝对不拖延了！","link":"/weekly/2023/2023-week6.html"},{"title":"","text":"2023 年第七周周报2023的第七周，春天来了 生活 作息规律，但是肠胃毛病又犯了（XDDD 养狗痛苦并快乐着 狗改不了吃屎，古人诚不欺我 座和握手学得非常扎实 坏习惯还得慢慢的改 不过狗狗真的喜欢被抱抱（我也喜欢被女朋友抱.jpg 本周的娱乐时间 和妹子开始刷小南极 有狗之后游戏时间少的不行 重看异常生物见闻录 咒术回战刷完了（我想编程热血青年（ 和妹子去看了舞剧五星出东方 优缺点都比较明显 编舞和舞美非常不错，有不少桥段比较创新（我个人觉得比舞剧李白要好很多） 剧情有点啦跨，配乐一般 立意有点无厘头 背单词继续 迷上了盒马的糖葫芦 我没有瘦，呜呜呜呜呜呜 技术这两周都没搞新东西了，有点不行，后面要努力了 这周花了点时间在 nerdctl 上 帮群友排查了 CI/CD 失败的问题，最后发现 GHA 不支持 IPV6，具体可以参考我的 Twitter 推文 花了点时间在 envd 上，给他们提了一个需求，结果当天下午就完成了，顺手 review 了一下 PR，参见 PR1486 讨论 API 设计果然是很好玩的一件事 这周继续完善 MetaData in SQL Comment 的事情上 辅助定位了一个 AWS Aurora 的奇葩的全文索引导致实例挂掉的问题 注入更多的信息来帮助定位生产问题。之前的一个疑问就是某个 SQL 从哪个 Pod 来的现在终于可以知道了（逃 发现了一个很有趣的抓包项目 tc-dump，从 Ingress 和 Egress 抓包，不必注册 socket 高到哪里去了（eBPF save my ass（ Chap6 的翻译终于告一段落 深感于自己网络基础的薄弱，重开了自顶向下方法这本书的坑（要学的好多啊，学不动了怎么办 继续 Linux Kernel Development 3rd Edition 差不多就这样 总结下周绝对不拖延了！（真的！","link":"/weekly/2023/2023-week7.html"},{"title":"","text":"2023 年第8周周报本次周报由 M2 Max MBP 赞助写出 生活 本周甲流发作，难受了好几天，以及建议大家家里都备着一些奥司他韦 本周小狗的犯罪记录 自行学会开锁，偷跑出笼子，拆家撕书 吃了我的 Airpods Pro 一只 拆了我台式机网卡 由于小狗拆了台式机网卡+马上急用电脑，去买了一台 M2 Max + 32G 的丐版 MBP，体验如下： 续航和发热真的比我之前的 i9 16寸好到不知道哪里去了 发烧的时候适合用来物理降温（逃 有一些生态还是有点不兼容（或者说用起来不利索），毕竟异构了 本周的娱乐时间 和妹子一起看知否 没玩游戏，教练我想打 CSGO（不过看了些 CSGO 相关的纪录片 看完了异常生物见闻录 重看了排球少年，我发现翔阳对二传的吸引力简直无敌。应该没有二传能拒绝翔阳的诱惑.jpg 本周只能继续在家吃烤肉了 背单词继续 还是没有瘦（去死啦我自己（ 技术今年不知道咋回事，在开年就在技术上有点瓶颈，感觉学不进去东西。算是废了 这周花了点时间在 nerdctl 上 调研了一圈，发现 Azure Pipeline 可能是最适合作为 IPV6 载体的平台，提了一个 Proposal PR2031 看起来社区在重构 run 命令上遇到点问题，得一起讨论下下一步的重构计划了 这周又去给 envd 蹭 PR 了 我发现我经历的很多项目都不知道用 GitHub Form 来做 Issue Template，这是何苦呢？参见 PR1494 在内部推广 envd 的时候，发现了 flag 设计上的一点小问题，顺手修了一下，参见 PR1495 这周开始接手 runc 里 CPU Burst 的 PR PR3205 后续的开发工作，参见 PR3749。 这个 PR 其实 Hung 了很久了，如果能在容器环境内大范围的普及，对于 API Server 这种 CPU 占用率比较离散但是又有不少突发请求的场景，Burst 在实测同样资源下能给 PT99 等指标带来 30% 以上的提升（阿里的数据） 这个 PR 最大头的工作还是在于后续的测试的跟进，毕竟依赖于特定的内核 发现一个很有趣的新项目，libxev，zig 写的一个 event loop，支持 io_uring 这样的新东西，感觉可以继续观望下 这周把 NewRelic 上的拨测迁移到了 AWS 的 CloudWatch 上，我发现 CloudWatch 的拨测还是比 NewRelic 的高到不知道哪里去了。当然 Custom Script 的 API 极为难用 开始了 chap7 的翻译 继续 Linux Kernel Development 3rd Edition 差不多就这样了 总结怎么样，你看这周没拖延了吧","link":"/weekly/2023/2023-week8.html"},{"title":"","text":"2023 年第9周周报本次周报躺在床上写出 生活 本周甲流好的差不多，但是又犯诺如，疯狂上吐下泻 本周养狗记录 小狗吃屎一次 小狗因为我妹子回来太兴奋了，把自己摔骨裂了 M2 本周记录 32G 用起来因为是统一内存的锅，所以有条件建议还是 64G M2 Max 的风扇策略很保守，要在100度上下一段时间才会开始到1500左右的转速，所以不会有很大的声音 XDR 的显示效果的确不错 本周的娱乐时间 重看明末边军一小兵 打了一会儿 CSGO，发现枪法随缘了，白银4了 开始看纪录片冰冻星球2，BBC 爸爸我爱你 排球少年三刷完毕（我好想磕 CP 啊（ 本周出去吃了还不错的云南菜，玫瑰米酿很好喝！ 继续背单词 瘦了点（finally 技术还是不能太躺平了，不然就会越来越菜了。菜到最后就没人要了呜呜呜 这周花了不少时间在 runc 里 CPU Burst 的 PR PR3749 上 写着测试发现，systemd 居然还不支持 cgroup 的 CPU burst 特性，赶紧去提了个 Issue。参见 Issue26658。不过我看了下源码，感觉可能实现难度还好。下周有机会可以区尝试实现下 写 e2e 测试真的是要了我亲命了（ 这周沉迷于 Affine，这周跑去给他们提了7+个 Issue，参见 Issue。希望雪碧不要打死我 这周一直在折腾 MacOS 的 CI/CD 机器。我只想化身为蒙古上单库克版（ 看了下 BaseCamp 新出的一套工具 mrsk 的代码 某种意义上算是 docker-compose 的一个自定义版 默认会集成 traefik 作为南北向的流量入口 某种意义上的确是能减轻入门的门槛。但是在 Cloud 时代（无论是云上还是云下），很多时候容器化或者其余方案的二进制编排与分发更多的时候还是吃诸如构建规范化这些软性的治理的东西 天下苦 K8$ 久已。但是我觉得目前有些又有点为反而反的意思了 拜读 gcgg 早年的文章 Unikernel: 从不入门到入门。写的非常棒 吃了 gcgg 安利，去开始读 Unikernels: Beyond Containers to the Next Generation of Cloud 一书 本周群友推荐了好几篇论文 Orleans: Distributed Virtual Actors for Programmability and Scalability SOCK: Rapid Task Provisioning with Serverless-Optimized Containers 继续 Chap7 的翻译 继续 Linux Kernel Development 3rd Edition 差不多就这样 总结最近状态不太对，专注度和效率都降低了不少，需要尽快调整过来","link":"/weekly/2023/2023-week9.html"},{"title":"","text":"2023 周报 2023年第一周 2023年第二周 2023年第三周 2023年第四周 2023年第五周 2023年第六周 2023年第七周 2023年第八周 2023年第九周 2023年第十周 2023年第十一周 2023年第十二周 2023年第十三周 2023年第十四周 2023年第十五周 2023年第十六周 2023年第十七周 2023年第十八周 2023年第十九周 2023年第二十周 2023年第二十一周","link":"/weekly/2023/index.html"}]}