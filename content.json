{"pages":[{"title":"","text":"","link":"/404.html"},{"title":"About","text":"关于我自己一个喜欢编程的香港记者，热爱 Python ,讨厌 Java ,重度拖延症晚期，想学很多东西，但是总觉得智商不够。渴望被这个世界温柔的拥抱着，也学会了去温柔的拥抱这个世界。 之前在某外卖厂，师从松鼠奥利奥中微服务中间件相关的开发 然后 transfer 到某云，从事公有云的中间件开发，欢迎各位大佬提工单 现在在太极图形，欢迎有兴趣的同学一起来做一些有意思的事情 和几个朋友搞了一个播客，『捕蛇者说』 Jincher 家的大狗","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"","text":"Manjusaka 个人简历联系方式 Email：me@manjusaka.me 个人信息 本科/西华大学/信息工程（2012 ~ 2016） 工作年限：4 年 技术博客：https://manjusaka.itscoder.com 期望职位：中间件开发 期望城市：北京 About Me 自驱力较强，能主动去学习新技术并且推动落地实现，并回馈社区。 PyCon China 2018/2019/2020 组织者，讲师。微软 Python 方向 MVP。 多年 Python 经验，具备良好的编程规范 。熟练掌握 Python 相关的技术开发工作。了解 Go/Java，阅读过部分 Go/CPython 代码。 熟练使用 Flask/Gunicorn/Gevent/Django/Celery 等 Python Web 开发过程中所使用的工具与技术栈，并阅读过 Flask/Greenlet 等项目源码 了解 Kubernetes 及其周边生态。能利用 Kubernetes 进行 PaaS 平台开发。阅读过部分 Kubernetes 源码 了解 TCP/HTTP1.x ，并阅读过开源社区部分实现 了解 Linux Kernel，并阅读过部分实现。同时对 Linux 中的一些新技术如 eBPF 有所涉猎并有落地经验 能力较为全面，能同时承担技术社区 PR/布道/运营等工作 工作经历2019.11-至今 阿里云（内部调转）工作内容： 对开源网关项目（Zuul，Spring Cloud Gateway，Kong CE）进行云上托管 对开源监控项目（Prometheus）进行云上托管 对开源项目进行功能增强（网关配置热更新，无损升级，协议转换等） 作为 SRE 一号位，负责产品稳定性建设 工作成果： 在任职期间，通过制定线上操作 SOP 来避免人为故障。建立完整监控体系，保证故障及时感知。同时进行周期性故障演练。任职期间所负责产品无故障，无投诉，单月线上服务冒烟数收敛百分之50。 结合阿里云已有公有云产品，基于开源产品实现功能增强（如 Pod 无损升级等） 基于 eBPF 初步实现完整的协议栈级别的链路监控 所负责的产品，完成从0到1的商业化。 参与产品运营，包含客户支持，技术推广等。数次直播 PV 为 BU 内 Top5 2019.02-2019.11 饿了么负责饿了么内部服务治理中心 Huskar 相关开发工作 工作内容： 基于 Zookeeper 构建饿了么内部服务治理中心 维护 Python Zookeeper SDK 并回馈上游 构建周边稳定性服务，维护 Huskar 稳定性 完成上云迁移 工作成果： 在向集团上云迁移过程中，无事故发生 基于 Go 完成保护网关，在特定网络波动导致大规模重连（单机房规模不低于 100k）时，保证后端服务正常运行 2017.02-2018.11 北京闪银科技有限公司负责风控系统基础设施开发及构建。SRE 一号位 工作内容： 负责通用数据请求网关的搭建 负责基础开发框架的封装 构建全链路追踪系统 风控系统稳定性建设 工作成果： 在接手通用数据请求网关的建设后，通过推动升级 runtime ，fix 开源组件等手段，将线上服务异常率从日千分之五降至日十五万分之一，请求时间 PT99 提升50%，内存占用下降30% 通过封装基础脚手架，建立完整 CI/CD 流程，将由人为/低级错误导致的线上故障数从月均一到两次收敛至零 基于 Zipkin 构建 Tracing 系统，封装基础框架，实现调用链路的全覆盖。支持研发人员排查某次异常调用所有周边依赖","link":"/resume/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"外国语学院-情况说明","text":"情况说明2018年4月23日上午，有微信公众号发布我院岳昕同学的《公开信》。学院第一时间向有关老师和同学了解情况，现作说明如下： 2018年4月22日下午和晚上，学院辅导员出于对学生的关心，通过多种方式、多次联系岳昕同学，均未能联系上，这种情况下，辅导员感到担忧。到23:30左右，该同学还未回到寝室。出于对同学安全的关心，辅导员与同学的母亲联系，询问岳昕同学是否回家等情况。该同学的母亲表示，孩子并未回家；之后，家长拨打电话未接，发微信未回，因此感到着急，随后赶到宿舍。此时，该同学已回到宿舍，母亲与其进行沟通。因担心影响其他同学休息，母亲决定和同学一起回家。 2018年4月23日上午，学院关注到网络上的相关信息后，与家长进一步沟通了情况，表达了关心。学院的老师与家长一样，都真心爱护学生、关心学生，既关心学生的学习，也关心学生的健康、安全与成长，老师和家长是善意的，态度也是一致的。 据了解，该同学已提交了毕业论文的部分初稿，指导老师也非常关心，给予了积极评价和悉心指导，并希望注意写作进度。 学院是学生成长成才的家园，老师们尽一切努力关心爱护学生，而这份关爱也意味着责任所在，学院、老师与家长都同样担负着教育的责任。与此同时，我们始终尊重每一位同学的基本权利、努力保障每一位同学的合法权益。 我们感谢师生、校友对学院工作的关心。 北京大学外国语学院2018年4月23日","link":"/posts/2018/04/24/A-Fact-Sheet-By-PKU/"},{"title":"申请公开信息事件经过（量产基地）","text":"转自公众号：量产基地文章已被4044月22日，我采访了岳昕和其他一同申请信息公开的同学，希望了解关于信息公开结果的更多情况。就在采访结束后的凌晨一点，岳昕的母亲与辅导员来到她的宿舍，将她叫醒，要求她删除手机电脑中所有与信息公开事件相关的资料，并要求她书面保证不再介入此事。随后，她被母亲带回家中，无法返校。 12个小时后，岳昕通过公众号“深约一丈”发表了面对北京大学师生及北京大学外国语学院的公开信，表示自己因学校的介入“恐惧而震怒”，并希望学院作出公开说明。 “我不能打一百分” 4月9日上午10点整，十名同学（实际到场八名）向北京大学提交关于1998年7月前后讨论沈阳‘师德’问题的系列会议记录的信息公开申请。4月20日，提交申请的同学们，及其余十五名通过邮件申请信息公开的同学均收到了来自北京大学信息公开办公室的答复。 岳昕是申请信息公开的十名同学之一。4月20日中午11时30分，她和其他同学一样，被邀请前往自己所在院系行政楼的会议室内。学工老师向她出示了校方的答复函，并问：“这个回复可以打一百分吗？” 岳昕回答：“我肯定不能打一百分。” 老师说：“如果我们学校的同学都不能打一百分，那校外的人会打多少分呢？是不是会打负分呢？” 岳昕说，她和同学们收到的答复，其实并无太多有效信息——答复函中表示，学校已在2018年4月8日向社会主动公开了《关于给予沈阳行政警告处分的决定》和中文系《关于给予沈阳警告处分的决定》，并给出查询网址链接。但对于信息申请中提及的“党委相关会议记录”“西城区公安局对此事的调查结果通报”“中文系相关会议记录”及“沈阳在大会上公开检讨的内容”，均表示现有档案并无相关信息。答复函承认当时学校和院系管理工作并不规范健全，并以“不断加强师德建设，认真落实立德树人的根本任务”作为结尾。 答复函中援引了《高等学校信息公开办法》第十八条（三）：“不属于本校职责范围的或该信息不存在的，应当告知申请人”。 岳昕想拿走答复函的纸质文件，被老师拒绝了：“给你有百害而无一利，搞不好你以后不能顺利毕业。”岳昕说，由于她参与过大量媒体工作的缘故，校方或许担心她将文件交给媒体。她从这样的拒绝中读出不信任，并再次和学工老师协商。院系学工老师在斟酌后，最终将纸质文件交给她。 在得到回复的25人中，只有岳昕和另一位同学保留了纸质文件。张震林是同样曾申请信息公开的同学之一，当他申请保留纸质文件时，院系学工老师以“规定”必须将纸质文件留在院系里为由，拒绝了他，并表示学生如有需要，可以随时来院系查看。张震林无奈同意。事后他翻阅文件，发现并无具体规定禁止。 在收取回复函期间，同学们被禁止录音。岳昕表示学工老师曾试图拿起她的手机检查，她于是将手机往自己的方向“拨了一下”。但出于种种顾虑，部分同学依然保留了当时的录音。同学们的顾虑并非全无缘由——对张震林来说，保留录音曾成为他保护自己的方式之一。“什么是明确的表态呢？” 在4月9日张震林申请信息公开的当天，院系辅导员曾给他打电话，表示学校学工部曾找自己了解学生情况，被她“挡回去了”。辅导员随后联系了张震林的同学，次日下午，同学转告张震林，称提请信息公开一事背后或有境外势力支持，让他谨言慎行。张震林“感到害怕”，于是主动联系辅导员，4月11上午9点，张震林和辅导员见面谈话。看见有两个手机倒扣在桌面上，他怀疑辅导员在现场秘密录音。因此，他也录了音。 据张震林回忆，辅导员先和他聊了聊学业及职业规划，并问他是否认识提请信息公开的其他同学。这些问题以“你显然和xxx交情不浅”的方式提出。张震林认为“这些都是诱导性提问”，他承认自己由于同处一个专业，和一位同学关系不错，但是“她一直认为我们是一个组织的”，对于这个问题，他表示否认。 辅导员向张震林提出“三条指控”：前去提交信息公开申请表的同学统一着装且佩戴口罩，极可能是有组织有预谋的行动，而张震林是他们的组织者之一；这一行动受到了境外组织的资金支持，张震林作为组织者对此知情；张震林在当天联系了境外媒体。据岳昕回忆，当时前去的同学并非统一着装，“我穿了一件米色风衣”；有部分同学因害怕被媒体拍到的缘故佩戴口罩，但并非人人如此；提交信息公开申请表的当天早上有十余家媒体在校门口以“拍摄花鸟”为由试图进校，其中确有外媒。但是张震林表示对这件事毫不知情。而是后来辅导员告诉他的。 张震林自认并不活跃，只是一同去提交信息公开申请。在被辅导员质问时，甚至觉得不可思议，“笑得很开心”——他随后否认以上指控。约谈现场有另一名他不认识的同学在场，“明显偏向辅导员一方”。这令他觉得很奇怪。 辅导员并不相信张震林，最后表示“我护你到什么程度取决于你说了多少”。事后，她再次联系张震林的同学，希望同学劝说张震林坦白实情。张震林认为她并不信任自己，因为他“说话吞吐且前后不一”“当提到有外媒时表现得很镇定，像是早已经知道了一样”。他还得知，辅导员对他的同学说，会向学校汇报关于他的约谈材料，如果他的同学“表现得不错”，材料将不会涉及被辅导员找来劝说自己的那位同学。 张震林托同学转告辅导员自己已坦白一切，但辅导员又找到另一位同学劝说张震林，再次询问他和境外组织的关系。张震林为此十分恐惧，4月12日，他向辅导员发微信表示自己的确和境外组织无任何瓜葛。辅导员以几段语音回复他：“心情差可以找我聊天”“反性侵可以从别的渠道”“给我一个明确的表态”。 “什么是明确的表态呢？”张震林问。 辅导员拒绝在微信上回答，要求面谈。 4月13日早上，辅导员给他的父亲打去电话，告知相关情况并希望他来学校。当晚八点左右，父亲到达张震林宿舍楼下，但张震林已因害怕辅导员来宿舍找自己的缘故，选择去北京八中附近的一家宾馆住宿。父亲和辅导员及张震林所在院系的党委副书记老师见面。次日，张震林见到父亲，父亲并不相信辅导员的几条指控，但依然为他担忧。“他有想调和的意思。”张震林认为，父亲是被“吓住了”。 张震林从辅导员处要到老师的电话，通话中，他表示自己有当天约谈的录音，希望学院能为此公开道歉并消除不良影响。一番协商后，老师当面向他道歉，并向他的父亲与同学澄清此事。次日，张震林和熟识的同学聊起此事，向公众号“深约一丈”讲述了这件事。4月14日，公众号“深约一丈”发表文章《沈阳事件近期情况汇总》，文章中提及这次约谈，随后以“违反网络信息安全法”为由被删除，公众号被禁言七天。 岳昕表示，通过家长渠道去做学生工作是“非常常见的威胁手法”。4月20日她收取学校答复时，也被学工老师提醒：学工部门有权不经过学生直接联系家长。 “我们对此表示遗憾” 岳昕对自己的经历感到“非常憋屈”。提请信息公开后，她同样多次收到了来自院系辅导员的约谈要求。4月10日，学工老师不断给她打电话，她因忙于毕业论文的缘故未能接听，回复短信表示已经收到消息。 4月11日晚上10点左右，岳昕在寝室换上睡衣准备开始写论文，学工老师出现在寝室门口，将她带去约谈。 此次约谈后不久，她得知张震林同被约谈的消息。正值规划毕业去向之际，母亲打来电话，她害怕母亲知情，又害怕母亲的顾虑会让她放弃自己喜爱的工作，心理压力大到一度不敢同母亲通话。她开始同朋友商量“最坏的可能性与对策”——4月15日下午，她正在思考此事时，社会学系的一位老师发来邀请，希望她能列席参与次日的“反性骚扰暂行规定学生意见征集会”，晚上10点，她收到了具体的时间地点信息。 据后来学工老师与她的约谈内容，学工老师称，在相关老师向她发出邀请时，曾有其他老师提出质疑。学工老师认为这是锻炼和成长机会，于是同意岳昕参加。张震林从岳昕处得知意见征集会的消息，向老师申请参加，得到了主办方的同意。 4月16日，意见征集会举行，该会议并未公开，而是邀请了学生常代表和部分学生。公众号“北门静悄悄”为此发文询问为何不公开征集意见。会上讨论了性骚扰的概念界定、反性骚扰专门委员会的代表产生、性骚扰的投诉与受理、调解机制、师生恋问题、资源对接、委员会监督、保护、保密及信息公开等问题。岳昕及张震林作为列席成员，提出许多问题。 主持人不希望同学在会后带走材料或将材料传到网上，但并未规定是否可以将会议内容录音或者在网上转述。4月17日晚上，张震林在未名BBS“三角地”板块和自己的公众号“境外事例”上发表了会议内容记录，并在未名BBS“校长信箱”板块提出自己的8条建议，校方回复说：“同学，你好！学校专家组会对你的意见进行研究， 感谢你的建议。” 北京大学学生会常代会回复了BBS的会议内容记录帖子：“有网友未经会议主办方确认，自行在互联网上发布本次会议‘记录整理’，我们对此表示遗憾。” 据北京大学新闻中心报道，2018年4月17日下午，学校召开第935次校长办公会议，专题研究反性骚扰暂行规定。 4月19日，老师再次约谈张震林，询问BBS的帖子是否由他发布。张震林认为他“管不着”，但又不想撒谎，于是拒绝回答。老师询问次日上午，他是否有时间来院系一趟——时间定在11时30分。次日11时25分，张震林来到院系办公楼门口，发现老师已久候多时。 他随后收到了11天前，关于信息公开申请的回复函。院长、党委书记和副书记及其他学工老师均在场。张震林认为，整个学院“精锐部队”全部出动的阵仗，可能会让很多同学觉得“压力非常大”。 老师在他吃午餐时，私下提醒他“这事到此为止，不要公开，也不要发BBS说自己收到回复的事。” 岳昕则表示，在得到回复的同时，老师们也略带威胁地同她聊天，甚至屡屡谈及“顺利毕业”的话题。 回顾从提请信息公开申请至收到回复函的过程，岳昕首先进行了反思：她对自己的表现还是不太满意，在她认为学工老师谈话有明显不合理、体现权力不对等的地方，她表现得还不够强硬。同时，她也认为自己没能发动更多同学：只有十名同学当场提交信息公开申请表，十五名同学通过邮箱提交申请表，这个数量并不算多；她也未能和更多同学交流自己的想法。但她始终认为，做事应当有始有终，才能让敢于发声的同学不致失望，也不会让其他同学们失去对自己的信任。毕业在即，她认为这才是一个爱北大的同学该做的事——“而不是为了120周年校庆歌舞升平”。 目前，同学们可以通过校长信箱或BBS其他版面向学校提出自己的意见，也可参与学生会主办的“我的校园我做主”座谈会参与校园事务，当然也可以申请信息公开。但岳昕认为这还远远不够——同学发在BBS上的意见，往往不能从根本上解决问题，“可能只是扣掉后勤工友的工资”。她曾参与过第三次“我的校园我做主”座谈会，那次座谈会针对保卫部和共享单车的管理问题，但她参与之后，却认为同学们在座谈会上很难做好充足的准备，发声较为分散，容易被校方目为幼稚，“依然是信息不对等的结果”。 由于她是4月9日上午第三名提交信息公开申请表的同学，她拿到的回复函抬头为“北大信息公开[2018]3号”。这意味着在本次申请信息公开之前，尚无其他同学就其他可能关心的事件向学校申请信息公开。 4月19日下午16时44分，她再次被学工老师约谈。最后，她们在未名湖边聊了四个多小时。老师不断劝说她考虑家人的感受，“不必走到申请信息公开的最后一步”“反正最后也会告诉你”。岳昕暂时没有回复她。 4月20日5时33分，岳昕向老师发信息，表示仍希望走完信息公开的整个流程。 7时09分，她收到短信回复：“你真的想清楚了吗？还是再想想吧。” 不是尾声的尾声 4月23日下午12时57分，公众号“深约一丈”刊载岳昕的公开信；13时18分，公众号显示“此内容因违规无法查看”。","link":"/posts/2018/04/24/A-Fact-Sheet/"},{"title":"岳昕：致北大师生与北大外国语学院的一封公开信","text":"岳昕：致北大师生与北大外国语学院的一封公开信北京大学的老师和同学：你们好！我是2014级外国语学院的岳昕，是4月9日早上向北京大学递交《信息公开申请表》的八位到场同学之一。我拖着极疲惫的身躯写下这段文字，说明近来发生在我身上的一些事情。 一4月9日之后，我不断被学院学工老师、领导约谈，并两次持续到凌晨一点甚至两点。在谈话中，学工老师多次提到“能否顺利毕业”、“做这个你母亲和姥姥怎么看”、“学工老师有权不经过你直接联系你的家长”。而我近期正在准备毕业论文，频繁的打扰和后续的心理压力严重影响了我的论文写作。 二4月20日中午，我收到了校方的回复。外国语学院党委书记、学工老师、班主任在场，党委书记向我宣读了学校对于本次信息公开申请的答复： 讨论沈阳师德的会议级别不够记录 公安局调查结果不在学校的管理范围里 沈阳公开检讨的内容因中文系工作失误也没有找到 这样的回复结果令我失望。但毕业论文提交即将截止，我只能先将心思放在论文写作上。 三4月22日晚上十一点左右，辅导员突然给我打来电话，但因为时间已晚，我并没有接到。凌晨一点，辅导员和母亲突然来到我的宿舍，强行将我叫醒，要求我删除手机、电脑中所有与信息公开事件相关的资料，并于天亮后到学工老师处作出书面保证不再介入此事。有同楼层的同学可以作证。随后，我被家长带回家中，目前无法返校。我和母亲都彻夜未眠。学校在联系母亲时歪曲事实，导致母亲受到过度惊吓、情绪崩溃。因为学校强行无理的介入，我和母亲关系几乎破裂。学院目前的行动已突破底线，我感到恐惧而震怒。申请信息公开何罪之有？我没有做错任何事，也不会后悔曾经提交《信息公开申请表》，行使我作为北大学生的光荣权利。二十年孺慕情深，我爱我的母亲。面对她的嚎啕痛哭、自扇耳光、下跪请求、以自杀相胁，我的内心在滴血。在她的哀求下我只能暂时回到家中，但原则面前退无可退，妥协不能解决任何问题，我别无他法，只有写下这篇声明，陈述原委。情绪激动，请大家原谅我的语无伦次。 四在此，我正式向北京大学外国语学院提出以下诉求： 北京大学外国语学院应公开书面说明越过我向家长施压、凌晨到宿舍强行约谈我、要求我删除申请信息公开一事的相关资料所依据的规章制度，对此过程中违法违规操作予以明确，并采取措施避免此类事件再次发生。 北京大学外国语学院应立即停止一切对我家人的施压行为，向我已经遭受惊吓的母亲正式道歉并澄清事实，帮助修复因此事导致的家庭紧张关系。 北京大学外国语学院必须公开书面保证此事不会对本人毕业一事产生影响，并不会再就此事继续干扰我的论文写作进程。 北京大学外国语学院负责消除此事对本人学业、未来就业和家人的其他一切不良影响。 北京大学外国语学院应明确就以上诉求进行公开书面回复，给关注此事的大家一个交代。 我将保留通过法律手段进一步追究相关个人和单位责任的一切权利，包括但不限于向北京大学和上级主管部门举报外国语学院严重违反校纪的行为。 北京大学外国语学院14级本科生岳昕2018年4月23日","link":"/posts/2018/04/24/A-public-letter-to-PKU/"},{"title":"Swift 3 中的函数参数命名规范指北","text":"原文地址：Function Naming In Swift 3 原文作者：Pablo Villar 译文出自：掘金翻译计划 译者：Zheaoli 校对者：Kulbear, Tuccuay 昨天，我开始将这个 Jayme 迁移到 Swift 3。这是我第一次将一个项目从 Swift 2.2 迁移至 Swift 3。说实话这个过程十分的繁琐，由于 Swift 3 在老版本基础上发生了很多比较大的改变，我不得不承认眼前这样一个事实，除了花费较多的时间以外，没有其余的捷径可走。不过这样的经历也带来一点好处：我对 Swift 3 的理解变得更为深入，对我来讲，这可能是最好的消息了。😃 在迁移代码的过程中，我需要做出很多的选择。更为蛋疼的是，整个迁移过程并不是修改代码那么简单，你还需要用耐心去一点点适应 Swift 3 中带来的新变化。某种意义上来讲，修改代码只是整个迁移过程的开始而已。 如果你已经决定将你的代码迁移到 Swift 3 ，我建议你去看看这篇文章来作为你万里长征的第一步。 如果一切顺利的话，在不久以后，我将回去写一篇博客来记录下整个迁移过程中的点点滴滴，包括我所作出的决定等等。但是眼前，我将会把注意力集中在一个非常非常重要的问题上：怎样正确的编写函数签名. 开篇首先，让我们来看看在 Swift 3 与 Swift 2 相比函数命名方式的差异吧。 在 Swift 2 中，函数中的第一个参数的标签在调用时可以省略，这是为了遵循这样一个 good ol’ Objective-C conventions 标准。比如我们可以这样写代码： 1234// Swift 2func handleError(error: NSError) { }let error = NSError()handleError(error) // Looks like Objective-C 在 Swift 3 中调用函数时，其实也是有办法省略第一个参数的标签的，但默认情况下不是这样： 12345// Swift 3func handleError(error: NSError) { }let error = NSError()handleError(error) // Does not compile!// ⛔ Missing argument label 'error:' in call 当遇到这样的情况时，我们第一反应可能是下面这样的： 123456// Swift 3func handleError(error: NSError) { }let error = NSError()handleError(error: error) // Had to write 'error' three times in a row!// My eyes already hurt 🙈 当然如果这样做，你肯定会很快意识到你的代码将将会变得有多坑爹。 如同前面所说的一样，在 Swift 3 中，我们是可以在调用函数时，将第一个参数的标签省略的，但是记住，你要去明确的告诉编译器这一点： 12345// Swift 3func handleError(_ error: NSError) { }// 🖐 Notice the underscore!let error = NSError()handleError(error) // Same as in Swift 2 你可能在使用 Xcode 自带的迁移工具进行迁移时遇到这样的情况。 注意，在函数签名中的下划线的意思是：告诉编译器，我们在调用函数时第一个参数不需要外带标签。这样，我们可以按照 Swift 2 中的方式去调用函数。 此外，你需要意识到，Swift 3 之所以修改了函数编写方式，是为了保证其一致性与可读性：我们不在需要对不同的参数区别对待。我想这可能是你遇到的第一个问题。 好了，现在代码可以编译运行了，但是你必须知道，你需要反复的去阅读 Swift 3 API design guidelines 一文。 ☝️ 一点微小的人生经验：你需要随时去诵读 Swift 3 API design guidelines 一文，这会为你解锁 Swift 开发的新体位。 第二步，精简你的代码 让我们再来看看之前的代码: 为了精简我们的代码，你可以将你的代码进行修剪一番，比如去除函数名里的类型信息等。 12345// Swift 3func handle(_ error: NSError) { /* ... */ }let error = NSError()handle(error) // Type name has been pruned// from function name, since it was redundant 如果你想让你的代码变得更短，更精悍，更明了的话，我给你们讲，作为一个钦定的开发者，一定要去反复诵读这篇 Swift 3 API design guidelines 文章到可以默写为止。 要注意让函数的调用过程是清晰、明确的，我们根据以下两点来确定函数的的命名和参数： 我们知道函数的返回类型 我们知道参数所对应的类型（比如在上面这个例子中，我们毫无疑问的知道其参数所属的类型是 NSError）。 更多的一些问题现在请睁大眼睛看清楚我们下面所讨论的东西。 ⚠️ 上面我们所讲的东西并没有包括所有可能出现的情况，换句话说，你可能遇到这样一种特殊情况，即，一个参数的类型没有办法直观的体现其作用。 让我们考虑下面这样一种情况： 123// Swift 2func requestForPath(path: String) -&gt; URLRequest { }let request = requestForPath(&quot;local:80/users&quot;) 如果你想将代码迁移到 Swift 3 ，那么根据已有的知识，你可能会这么做： 123// Swift 3func request(_ path: String) -&gt; URLRequest { }let request = request(&quot;local:80/users&quot;) 讲真，这段代码看起来可读性很差，让我们稍微修改下： 123// Swift 3func request(for path: String) -&gt; URLRequest { }let request = request(for: &quot;local:80/users&quot;) OK，现在看起来舒服多了，但是并没有解决我上面提到的问题。 在我们调用这个函数的时候，我们怎样很直观的知道我们需要给这个参数传递一个 Web Url 呢？你所能提前知道的是你需要传递一个 String 类型的变量进去，但是你并不清楚你需要传递一个 Web Url 进去。 同理，我们在一个大型项目中，我们需要很清楚的明白每个参数的作用所在，但是很明显，目前我们还没有解决这个大问题，比如: 你怎么知道一个 String 类型的变量代表着 Web Url。 你怎么知道一个 Int 类型的变量代表着 Http 状态码。[String: String] 你怎么知道一个 [String: String] 类型的变量代表着 Http Header。 等等…。 ⚠️ 综上，我给你们一点微小的人生经验吧: 谨慎精简你的代码 ✄ 回到代码上，我们可以给参数添加上相对应的标签来解决这个问题，好了看看下面这个代码： 12func request(forPath path: String) -&gt; URLRequest { }let request = request(forPath: &quot;local:80/users&quot;) 好了，现在代码看起来是不是更清楚，可读性更强了呢？ 🎉 恭喜~ 讲真，看到这里其实你可以关闭浏览器了，但是事实上，下面才是最精华的部分。 好了，让我们来看看关于函数参命名的用词问题： 12func request(forPath path: String) -&gt; URLRequest { }// The word 'path' appears twice 这段代码看起来不错，但是如果你想让其变得更好，那么请看接下来的部分。 你所不知道的小技巧这个小技巧很简单：在上下文中反映参数的类型及作用，这样你就可以无脑的精简你的代码了。 呐，我们来看看下面这段代码。 1234typealias Path = String // To the rescue!func request(for path: Path) -&gt; URLRequest { }let request = request(for: &quot;local:80/users&quot;) 在这个例子中，参数的类型和参数的作用表达达成了一个完美的统一，因为你在上下文中为 String 赋予了一个别名叫做 Path。 现在，你的函数看起来还是依旧的精简，可读性较高，但是却不重复。 以此类推，你可以使用同样的方式来书写一些优美的代码，比如： 1234typealias Path = Stringtypealias StatusCode = Inttypealias HTTPHeader = [String: String]// etc... 如你所见，你可以尽情的写精简而优美的代码了。 不过，请记住，凡事走向极端便变了味了：这个小技巧会为你的代码添加额外的负担，特别是你们代码存在多重嵌套的情况下。因此请记住，如果你无脑的使用这样的小技巧的话，那么你可能会付出一些惨痛的代价。 结论很多时候，你在使用 Swift 3 时，命名函数的时候你会遇到很多困难。 积累一些代码片段可能会帮助你很多： 123456789101112131415func remove(at position: Index) -&gt; Element { }employees.remove(at: x)func remove(_ member: Element) -&gt; Element? { }allViews.remove(cancelButton)func url(forPath path: String) -&gt; URL { }let url = url(forPath: &quot;local:80/users&quot;)typealias Path = String // Alternativefunc url(for path: Path) -&gt; URL { }let url = url(for: &quot;local:80/users&quot;)func entity(from dictionary: [String: Any]) -&gt; Entity { /* ... */ }let entity = entity(from: [&quot;id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;John&quot;])","link":"/posts/2016/10/09/Function-Naming-In-Swift-3/"},{"title":"听说你会 Python ？","text":"前言最近觉得 Python 太“简单了”，于是在师父川爷面前放肆了一把：“我觉得 Python 是世界上最简单的语言！”。于是川爷嘴角闪过了一丝轻蔑的微笑（内心 OS：Naive！，作为一个 Python 开发者，我必须要给你一点人生经验，不然你不知道天高地厚！）于是川爷给我了一份满分 100 分的题，然后这篇文章就是记录下做这套题所踩过的坑。 1.列表生成器描述下面的代码会报错，为什么？ 1234567class A(object): x = 1 gen = (x for _ in xrange(10)) # gen=(x for _ in range(10))if __name__ == &quot;__main__&quot;: print(list(A.gen)) 答案这个问题是变量作用域问题，在 gen=(x for _ in xrange(10)) 中 gen 是一个 generator ,在 generator 中变量有自己的一套作用域，与其余作用域空间相互隔离。因此，将会出现这样的 NameError: name 'x' is not defined 的问题，那么解决方案是什么呢？答案是：用 lambda 。 12345678class A(object): x = 1 gen = (lambda x: (x for _ in xrange(10)))(x) # gen=(x for _ in range(10))if __name__ == &quot;__main__&quot;: print(list(A.gen)) 或者这样 1234567class A(object): x = 1 gen = (A.x for _ in xrange(10)) # gen=(x for _ in range(10))if __name__ == &quot;__main__&quot;: print(list(A.gen)) 补充感谢评论区几位提出的意见，这里我给一份官方文档的说明吧：The scope of names defined in a class block is limited to the class block; it does not extend to the code blocks of methods – this includes comprehensions and generator expressions since they are implemented using a function scope. This means that the following will fail: 123class A: a = 42 b = list(a + i for i in range(10)) 参考链接 Python2 Execution-Model:Naming-and-Binding ， Python3 Execution-Model:Resolution-of-Names。据说这是 PEP 227 中新增的提案，我回去会进一步详细考证。再次拜谢评论区 @没头脑很着急 @涂伟忠 @Cholerae 三位的勘误指正。 2.装饰器描述我想写一个类装饰器用来度量函数/方法运行时间 1234567891011import timeclass Timeit(object): def __init__(self, func): self._wrapped = func def __call__(self, *args, **kws): start_time = time.time() result = self._wrapped(*args, **kws) print(&quot;elapsed time is %s &quot; % (time.time() - start_time)) return result 这个装饰器能够运行在普通函数上： 12345678@Timeitdef func(): time.sleep(1) return &quot;invoking function func&quot;if __name__ == '__main__': func() # output: elapsed time is 1.00044410133 但是运行在方法上会报错，为什么？ 1234567891011class A(object): @Timeit def func(self): time.sleep(1) return 'invoking method func'if __name__ == '__main__': a = A() a.func() # Boom! 如果我坚持使用类装饰器，应该如何修改？ 答案使用类装饰器后，在调用 func 函数的过程中其对应的 instance 并不会传递给 __call__ 方法，造成其 mehtod unbound ,那么解决方法是什么呢？描述符赛高 123456789class Timeit(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print('invoking Timer') def __get__(self, instance, owner): return lambda *args, **kwargs: self.func(instance, *args, **kwargs) 3.Python 调用机制描述我们知道 __call__ 方法可以用来重载圆括号调用，好的，以为问题就这么简单？Naive！ 12345678class A(object): def __call__(self): print(&quot;invoking __call__ from A!&quot;)if __name__ == &quot;__main__&quot;: a = A() a() # output: invoking __call__ from A 现在我们可以看到 a() 似乎等价于 a.__call__() ,看起来很 Easy 对吧，好的，我现在想作死，又写出了如下的代码， 1234567a.__call__ = lambda: &quot;invoking __call__ from lambda&quot;a.__call__()# output:invoking __call__ from lambdaa()# output:invoking __call__ from A! 请大佬们解释下，为什么 a() 没有调用出 a.__call__() (此题由 USTC 王子博前辈提出) 答案原因在于，在 Python 中，新式类（ new class )的内建特殊方法，和实例的属性字典是相互隔离的，具体可以看看 Python 官方文档对于这一情况的说明 For new-style classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object’s type, not in the object’s instance dictionary. That behaviour is the reason why the following code raises an exception (unlike the equivalent example with old-style classes): 同时官方也给出了一个例子： 123456789101112class C(object): passc = C()c.__len__ = lambda: 5len(c)# Traceback (most recent call last):# File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;# TypeError: object of type 'C' has no len() 回到我们的例子上来，当我们在执行 a.__call__=lambda:&quot;invoking __call__ from lambda&quot; 时，的确在我们在 a.__dict__ 中新增加了一个 key 为 __call__ 的 item，但是当我们执行 a() 时，因为涉及特殊方法的调用，因此我们的调用过程不会从 a.__dict__ 中寻找属性，而是从 tyee(a).__dict__ 中寻找属性。因此，就会出现如上所述的情况。 4.描述符描述我想写一个 Exam 类，其属性 math 为 [0,100] 的整数，若赋值时不在此范围内则抛出异常，我决定用描述符来实现这个需求。 123456789101112131415161718192021222324252627282930class Grade(object): def __init__(self): self._score = 0 def __get__(self, instance, owner): return self._score def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: self._score = value else: raise ValueError('grade must be between 0 and 100')class Exam(object): math = Grade() def __init__(self, math): self.math = mathif __name__ == '__main__': niche = Exam(math=90) print(niche.math) # output : 90 snake = Exam(math=75) print(snake.math) # output : 75 snake.math = 120 # output: ValueError:grade must be between 0 and 100! 看起来一切正常。不过这里面有个巨大的问题，尝试说明是什么问题为了解决这个问题，我改写了 Grade 描述符如下： 12345678910111213class Grad(object): def __init__(self): self._grade_pool = {} def __get__(self, instance, owner): return self._grade_pool.get(instance, None) def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: _grade_pool = self.__dict__.setdefault('_grade_pool', {}) _grade_pool[instance] = value else: raise ValueError(&quot;fuck&quot;) 不过这样会导致更大的问题，请问该怎么解决这个问题？ 答案1.第一个问题的其实很简单，如果你再运行一次 print(niche.math) 你就会发现，输出值是 75 ，那么这是为什么呢？这就要先从 Python 的调用机制说起了。我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。好的，现在回到我们的问题，我们发现，在我们的类 Exam 中，其 self.math 的调用过程是，首先在实例化后的实例的 __dict__ 中进行查找，没有找到，接着往上一级，在我们的类 Exam 中进行查找，好的找到了，返回。那么这意味着，我们对于 self.math 的所有操作都是对于类变量 math 的操作。因此造成变量污染的问题。那么该则怎么解决呢？很多同志可能会说，恩，在 __set__ 函数中将值设置到具体的实例字典不就行了。那么这样可不可以呢？答案是，很明显不得行啊，至于为什么，就涉及到我们 Python 描述符的机制了，描述符指的是实现了描述符协议的特殊的类，三个描述符协议指的是 __get__ , ‘set‘ , __delete__ 以及 Python 3.6 中新增的 __set_name__ 方法，其中实现了 __get__ 以及 __set__ / __delete__ / __set_name__ 的是 Data descriptors ，而只实现了 __get__ 的是 Non-Data descriptor 。那么有什么区别呢，前面说了， 我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。 但是，这里没有考虑描述符的因素进去，如果将描述符因素考虑进去，那么正确的表述应该是我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。其中如果在类实例字典中的该属性是一个 Data descriptors ，那么无论实例字典中存在该属性与否，无条件走描述符协议进行调用，在类实例字典中的该属性是一个 Non-Data descriptors ，那么优先调用实例字典中的属性值而不触发描述符协议，如果实例字典中不存在该属性值，那么触发 Non-Data descriptor 的描述符协议。回到之前的问题，我们即使在 __set__ 将具体的属性写入实例字典中，但是由于类字典中存在着 Data descriptors ，因此，我们在调用 math 属性时，依旧会触发描述符协议。 2.经过改良的做法，利用 dict 的 key 唯一性，将具体的值与实例进行绑定，但是同时带来了内存泄露的问题。那么为什么会造成内存泄露呢，首先复习下我们的 dict 的特性，dict 最重要的一个特性，就是凡可 hash 的对象皆可为 key ，dict 通过利用的 hash 值的唯一性（严格意义上来讲并不是唯一，而是其 hash 值碰撞几率极小，近似认定其唯一）来保证 key 的不重复性，同时（敲黑板，重点来了），dict 中的 key 引用是强引用类型，会造成对应对象的引用计数的增加，可能造成对象无法被 gc ，从而产生内存泄露。那么这里该怎么解决呢？两种方法第一种： 1234567891011121314class Grad(object): def __init__(self): import weakref self._grade_pool = weakref.WeakKeyDictionary() def __get__(self, instance, owner): return self._grade_pool.get(instance, None) def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: _grade_pool = self.__dict__.setdefault('_grade_pool', {}) _grade_pool[instance] = value else: raise ValueError(&quot;fuck&quot;) weakref 库中的 WeakKeyDictionary 所产生的字典的 key 对于对象的引用是弱引用类型，其不会造成内存引用计数的增加，因此不会造成内存泄露。同理，如果我们为了避免 value 对于对象的强引用，我们可以使用 WeakValueDictionary 。第二种：在 Python 3.6 中，实现的 PEP 487 提案，为描述符新增加了一个协议，我们可以用其来绑定对应的对象： 123456789101112class Grad(object): def __get__(self, instance, owner): return instance.__dict__[self.key] def __set__(self, instance, value): if 0 &lt;= value &lt;= 100: instance.__dict__[self.key] = value else: raise ValueError(&quot;fuck&quot;) def __set_name__(self, owner, name): self.key = name 这道题涉及的东西比较多，这里给出一点参考链接，invoking-descriptors , Descriptor HowTo Guide , PEP 487 , [what`s new in Python 3.6](https://docs.python.org/3.6/whatsnew/3.6.html#pep-487-descriptor-protocol-enhancements) 。 5.Python 继承机制描述试求出以下代码的输出结果。 12345678910111213141516171819202122232425262728293031class Init(object): def __init__(self, value): self.val = valueclass Add2(Init): def __init__(self, val): super(Add2, self).__init__(val) self.val += 2class Mul5(Init): def __init__(self, val): super(Mul5, self).__init__(val) self.val *= 5class Pro(Mul5, Add2): passclass Incr(Pro): csup = super(Pro) def __init__(self, val): self.csup.__init__(val) self.val += 1p = Incr(5)print(p.val) 答案输出是 36 ，具体可以参考 New-style Classes , multiple-inheritance 6. Python 特殊方法描述我写了一个通过重载 new 方法来实现单例模式的类。 1234567891011121314class Singleton(object): _instance = None def __new__(cls, *args, **kwargs): if cls._instance: return cls._instance cls._isntance = cv = object.__new__(cls, *args, **kwargs) return cvsin1 = Singleton()sin2 = Singleton()print(sin1 is sin2)# output: True 现在我有一堆类要实现为单例模式，所以我打算照葫芦画瓢写一个元类，这样可以让代码复用： 12345678910111213141516171819class SingleMeta(type): def __init__(cls, name, bases, dict): cls._instance = None __new__o = cls.__new__ def __new__(cls, *args, **kwargs): if cls._instance: return cls._instance cls._instance = cv = __new__o(cls, *args, **kwargs) return cv cls.__new__ = __new__class A(object): __metaclass__ = SingleMetaa1 = A() # what`s the fuck 哎呀，好气啊，为啥这会报错啊，我明明之前用这种方法给 __getattribute__ 打补丁的，下面这段代码能够捕获一切属性调用并打印参数 123456789101112131415161718192021class TraceAttribute(type): def __init__(cls, name, bases, dict): __getattribute__o = cls.__getattribute__ def __getattribute__(self, *args, **kwargs): print('__getattribute__:', args, kwargs) return __getattribute__o(self, *args, **kwargs) cls.__getattribute__ = __getattribute__class A(object): # Python 3 是 class A(object,metaclass=TraceAttribute): __metaclass__ = TraceAttribute a = 1 b = 2a = A()a.a# output: __getattribute__:('a',){}a.b 试解释为什么给 getattribute 打补丁成功，而 new 打补丁失败。如果我坚持使用元类给 new 打补丁来实现单例模式，应该怎么修改？ 答案其实这是最气人的一点，类里的 __new__ 是一个 staticmethod 因此替换的时候必须以 staticmethod 进行替换。答案如下： 1234567891011121314151617181920class SingleMeta(type): def __init__(cls, name, bases, dict): cls._instance = None __new__o = cls.__new__ @staticmethod def __new__(cls, *args, **kwargs): if cls._instance: return cls._instance cls._instance = cv = __new__o(cls, *args, **kwargs) return cv cls.__new__ = __new__class A(object): __metaclass__ = SingleMetaprint(A() is A()) # output: True 结语感谢师父大人的一套题让我开启新世界的大门，恩，博客上没法艾特，只能传递心意了。说实话 Python 的动态特性可以让其用众多 black magic 去实现一些很舒服的功能，当然这也对我们对语言特性及坑的掌握也变得更严格了，愿各位 Pythoner 没事阅读官方文档，早日达到装逼如风，常伴吾身的境界。","link":"/posts/2016/11/18/Someone-tell-me-that-you-think-Python-is-simple/"},{"title":"Python 描述符入门指北","text":"很久都没写 Flask 代码相关了，想想也真是惭愧，然并卵，这次还是不写 Flask 相关，不服你来打我啊（就这么贱，有本事咬我啊这次我来写一下 Python 一个很重要的东西，即 Descriptor （描述符） 初识描述符老规矩，Talk is cheap,Show me the code. 我们先来看看一段代码 1234567891011121314151617181920212223class Person(object): &quot;&quot;&quot;&quot;&quot;&quot; #---------------------------------------------------------------------- def __init__(self, first_name, last_name): &quot;&quot;&quot;Constructor&quot;&quot;&quot; self.first_name = first_name self.last_name = last_name #---------------------------------------------------------------------- @property def full_name(self): &quot;&quot;&quot; Return the full name &quot;&quot;&quot; return &quot;%s %s&quot; % (self.first_name, self.last_name)if __name__==&quot;__main__&quot;: person = Person(&quot;Mike&quot;, &quot;Driscoll&quot;) print(person.full_name) # 'Mike Driscoll' print(person.first_name) # 'Mike' 这段代大家肯定很熟悉，恩，property 嘛，谁不知道呢，但是 property 的实现机制大家清楚么？什么不清楚？那还学个毛的 Python 啊。。。开个玩笑，我们看下面一段代码 1234567891011121314151617181920212223242526272829303132333435class Property(object): &quot;Emulate PyProperty_Type() in Objects/descrobject.c&quot; def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(&quot;unreadable attribute&quot;) return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError(&quot;can't set attribute&quot;) self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(&quot;can't delete attribute&quot;) self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__) 看起来是不是很复杂，没事，我们来一步步的看。不过这里我们首先给出一个结论：Descriptors 是一种特殊 的对象，这种对象实现了 __get__ ，__set__ ，__delete__ 这三个特殊方法。 详解描述符说说 Property在上文，我们给出了 Propery 实现代码，现在让我们来详细说说这个 1234567891011121314151617181920212223class Person(object): &quot;&quot;&quot;&quot;&quot;&quot; #---------------------------------------------------------------------- def __init__(self, first_name, last_name): &quot;&quot;&quot;Constructor&quot;&quot;&quot; self.first_name = first_name self.last_name = last_name #---------------------------------------------------------------------- @Property def full_name(self): &quot;&quot;&quot; Return the full name &quot;&quot;&quot; return &quot;%s %s&quot; % (self.first_name, self.last_name)if __name__==&quot;__main__&quot;: person = Person(&quot;Mike&quot;, &quot;Driscoll&quot;) print(person.full_name) # 'Mike Driscoll' print(person.first_name) # 'Mike' 首先，如果你对装饰器不了解的话，你可能要去看看这篇文章，简而言之，在我们正式运行代码之前，我们的解释器就会对我们的代码进行一次扫描，对涉及装饰器的部分进行替换。类装饰器同理。在上文中，这段代码 123456@Propertydef full_name(self): &quot;&quot;&quot; Return the full name &quot;&quot;&quot; return &quot;%s %s&quot; % (self.first_name, self.last_name) 会触发这样一个过程，即 full_name=Property(full_name) 。然后在我们后面所实例化对象之后我们调用 person.full_name 这样一个过程其实等价于 person.full_name.__get__(person) 然后进而触发__get__() 方法里所写的 return self.fget(obj) 即原本上我们所编写的 def full_name 内的执行代码。 这个时候，同志们可以去思考下 getter() ,setter() ,以及 deleter() 的具体运行机制了=。=如果还是有问题，欢迎在评论里进行讨论。 关于描述符还记得之前我们所提到的一个定义么：Descriptors 是一种特殊的对象，这种对象实现了 __get__ ，__set__ ，__delete__ 这三个特殊方法。然后在 Python 官方文档的说明中，为了体现描述符的重要性，有这样一段话：“They are the mechanism behind properties, methods, static methods, class methods, and super(). They are used throughout Python itself to implement the new style classes introduced in version 2.2. ” 简而言之就是 先有描述符后有天，秒天秒地秒空气。恩，在新式类中，属性，方法调用，静态方法，类方法等都是基于描述符的特定使用。 OK，你可能想问，为什么描述符是这么重要呢？别急，我们接着看 使用描述符首先请看下一段代码 123456class A(object): #注：在 Python 3.x 版本中，对于 new class 的使用不需要显式的指定从 object 类进行继承，如果在 Python 2.X（x&gt;2)的版本中则需要 def a(self): passif __name__==&quot;__main__&quot;: a=A() a.a() 大家都注意到了我们存在着这样一个语句 a.a() ，好的，现在请大家思考下，我们在调用这个方法的时候发生了什么？OK？想出来了么？没有？好的我们继续首先我们调用一个属性的时候，不管是成员还是方法，我们都会触发这样一个方法用于调用属性 __getattribute__() ,在我们的 __getattribute__() 方法中，如果我们尝试调用的属性实现了我们的描述符协议，那么会产生这样一个调用过程 type(a).__dict__['a'].__get__(b,type(b))。好的这里我们又要给出一个结论了：“在这样一个调用过程中，有这样一个优先级顺序，如果我们所尝试调用属性是一个 data descriptors ，那么不管这个属性是否存在我们的实例的 __dict__ 字典中，优先调用我们描述符里的 __get__ 方法，如果我们所尝试调用属性是一个 non data descriptors，那么我们优先调用我们实例里的 __dict__ 里的存在的属性，如果不存在，则依照相应原则往上查找我们类，父类中的 __dict__ 中所包含的属性，一旦属性存在，则调用 __get__ 方法，如果不存在则调用 __getattr__() 方法”。理解起来有点抽象？没事，我们马上会讲，不过在这里，我们先要解释下 data descriptors 与 non data descriptors，再来看一个例子。什么是 data descriptors 与 non data descriptors 呢？其实很简单，在描述符中同时实现了 __get__ 与 __set__ 协议的描述符是 data descriptors ，如果只实现了 __get__ 协议的则是 non data descriptors 。好了我们现在来看个例子： 123456789101112131415161718192021222324252627import mathclass lazyproperty: def __init__(self, func): self.func = func def __get__(self, instance, owner): if instance is None: return self else: value = self.func(instance) setattr(instance, self.func.__name__, value) return valueclass Circle: def __init__(self, radius): self.radius = radius pass @lazyproperty def area(self): print(&quot;Com&quot;) return math.pi * self.radius * 2 def test(self): passif __name__=='__main__': c=Circle(4) print(c.area) 好的，让我们仔细来看看这段代码，首先类描述符 @lazyproperty 的替换过程，前面已经说了，我们不在重复。接着，在我们第一次调用 c.area 的时候，我们首先查询实例 c 的 __dict__ 中是否存在着 area 描述符，然后发现在 c 中既不存在描述符，也不存在这样一个属性，接着我们向上查询 Circle 中的 __dict__ ，然后查找到名为 area 的属性，同时这是一个 non data descriptors ，由于我们的实例字典内并不存在 area 属性，那么我们便调用类字典中的 area 的 __get__ 方法，并在 __get__ 方法中通过调用 setattr 方法为实例字典注册属性 area 。紧接着，我们在后续调用 c.area 的时候，我们能在实例字典中找到 area 属性的存在，且类字典中的 area 是一个 non data descriptors，于是我们不会触发代码里所实现的 __get__ 方法，而是直接从实例的字典中直接获取属性值。 描述符的使用描述符的使用面很广，不过其主要的目的在于让我们的调用过程变得可控。因此我们在一些需要对我们调用过程实行精细控制的时候，使用描述符，比如我们之前提到的这个例子 12345678910111213141516171819202122232425262728293031323334class lazyproperty: def __init__(self, func): self.func = func def __get__(self, instance, owner): if instance is None: return self else: value = self.func(instance) setattr(instance, self.func.__name__, value) return value def __set__(self, instance, value=0): passimport mathclass Circle: def __init__(self, radius): self.radius = radius pass @lazyproperty def area(self, value=0): print(&quot;Com&quot;) if value == 0 and self.radius == 0: raise TypeError(&quot;Something went wring&quot;) return math.pi * value * 2 if value != 0 else math.pi * self.radius * 2 def test(self): pass 利用描述符的特性实现懒加载，再比如，我们可以控制属性赋值的值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Property(object): &quot;Emulate PyProperty_Type() in Objects/descrobject.c&quot; def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(&quot;unreadable attribute&quot;) return self.fget(obj) def __set__(self, obj, value=None): if value is None: raise TypeError(&quot;You can`t to set value as None&quot;) if self.fset is None: raise AttributeError(&quot;can't set attribute&quot;) self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(&quot;can't delete attribute&quot;) self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__)class test(): def __init__(self, value): self.value = value @Property def Value(self): return self.value @Value.setter def test(self, x): self.value = x 如上面的例子所描述的一样，我们可以判断所传入的值是否有效等等。 总结Python 中的描述符可以说是新式类调用链中的根基，所有的方法，成员，变量调用时都将会有描述符的介入。同时我们可以利用描述符的特性来将我们的调用过程变得更为可控。这一点，我们可以在很多著名框架中找到这样的例子。 参考1.《Python Cookbook》 8.10 章 P2712.《Descriptor HowTo Guid》3.《Python 黑魔法》","link":"/posts/2016/10/12/Something-about-Descriptor/"},{"title":"Swift 3.0 新增安全特性的一点普及","text":"原文链接 : WWDC 2016: Increased Safety in Swift 3.0 原文作者 : Matt Mathias 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: llp0574, thanksdanny 在 Swift 发布之后，Swift 的开发者一直在强调，安全性与可选择类型是 Swift 最为重要的特性之一。他们提供了一种nil的表示机制，并要求有一个明确的语法在可能为nil的实例上使用。 可选择类型主要以下两种: Optional ImplicitlyUnwrappedOptional 第一种做法是一种安全的做法：它要求我们去拆解可选类型变量是为了访问基础值。第二种做法是一种不安全的做法：我们可在不拆解可选择类型变量的情况下直接访问其底层值。比如，如果在变量值为 nil 的时候，使用 ImplicitlyUnwrappedOptional 可能会导致一些异常。 下面将展示一个关于这个问题的例子： 1234let x: Int! = nilprint(x) // Crash! `x` is nil! 在 Swift 3.0 中，苹果改进了 ImplicitlyUnwrappedOptional 的实现，使其相对于以前变得更为安全。这里我们不禁想问，苹果到底在 Swift 3.0 对 ImplicitlyUnwrappedOptional 做了哪些改进，从而使 Swift 变得更为安全了呢。答案在于，苹果在编译器对于 ImplicitlyUnwrappedOptional 进行类型推导的过程中进行了优化。 在 Swift 2.x 中的使用方式让我们来通过一个例子来理解这里面的变化。 123456789101112struct Person { let firstName: String let lastName: String init!(firstName: String, lastName: String) { guard !firstName.isEmpty &amp;&amp; !lastName.isEmpty else { return nil } self.firstName = firstName self.lastName = lastName }} 这里我们创建了一个初始化方法有缺陷的结构体 Person 。如果我们在初始化中不给实例提供 first name 和 last name 的值的话，那么初始化将会失败。 在这里 init!(firstName: String, lastName: String) ，我们通过使用 ! 而不是 ? 来进行初始化的。不同于 Swift 3.0，在 Swift 2.x 中，我们用过利用 init! 来使用 ImplicitlyUnwrappedOptional 。不管我们所使用的 Swift 版本如何，我们应该谨慎的使用 init!。一般而言，如果你能允许在引用生成的为nil的实例时所产生的异常，那么你可以使用 init! 。因为如果对应的实例为 nil 的时候，你使用 init! 会导致程序的崩溃。 在 .* 中，这个初始化方法将会生成一个 ImplicitlyUnwrappedOptional&lt;Person&gt; 。如果初始化失败，所有基于 Person 的实例将会产生异常。 比如，在 Swift 2.x 里，下面这段代码在运行时将崩溃。 12345// Swift 2.xlet nilPerson = Person(firstName: &quot;&quot;, lastName: &quot;Mathias&quot;)nilPerson.firstName // Crash! 请注意，由于在初始化器中存在着隐式解包，因此我们没有必要使用类型绑定（译者注1： optional binding ）或者是自判断链接（译者注2： optional chaining ）来保证 nilPerson 能被正常的使用。 在 Swift 3.0 里的新姿势在 Swift 3.0 中事情发生了一点微小的变化。在 init! 中的 ! 表示初始化可能会失败，如果成功进行了初始化，那么生成的实例将被强制隐式拆包。不同于 Swift 2.x ，init! 所生成的实例是 optional 而不是 ImplicitlyUnwrappedOptional 。这意味着你需要针对不同的基础值对实例进行类型绑定或者是自判断链接处理。 123456// Swift 3.0let nilPerson = Person(firstName: &quot;&quot;, lastName: &quot;Mathias&quot;)nilPerson?.firstName 在上面这个示例中，nilPerson 是一个 Optional&lt;Person&gt; 类型的实例。这意味着如果你想正常的访问里面的值，你需要对 nilPerson 进行拆包处理。这种情况下，手动拆包是个非常好的选择。 安全的类型声明这种变化可能会令人疑惑。为什么使用的 init! 的初始化会会生成 Optional 类型的实例？不是说在 init! 中的 ! 表示生成 ImplicitlyUnwrappedOptional 么？ 答案是安全性与声明之间的依赖关系。在上面这段代码里（ let nilPerson = Person(firstName: &quot;&quot;, lastName: &quot;Mathias&quot;) ）将依靠编译器对 nilPerson 的类型进行推断。 在 Swift 2.x 中，编译器将会把 nilPerson 作为 ImplicitlyUnwrappedOptional&lt;Person&gt; 进行处理。讲道理，我们已经习惯了这种编译方式，而且它在某种程度上也是有道理的。总之一句话，在 Swift 2.x 中，想要使用 ImplicitlyUnwrappedOptional 的话，就需要利用 init! 对实例进行初始化。 然而，某种程度上来讲，上面这种做法是很不安全的。说实话，我们从没有任何钦定 nilPerson 应该是 ImplicitlyUnwrappedOptional 实例的意思，因为如果将来编译器推导出一些不安全的类型信息导致程序运行出了偏差，等于，你们也有责任吧。 Swift 3.0 解决这类安全问题的方式是在我们不是明确的声明一个 ImplicitlyUnwrappedOptional 时，会将 ImplicitlyUnwrappedOptional 作为 optional 进行处理。 限制 ImplicitlyUnwrappedOptional 的实例传递这种做法很巧妙的一点在于限制了隐式解包的 optional 实例的传递。参考下我们前面关于 Person 的代码，同时思考下我们之前在 Swift 2.x 里的一些做法： 1234567// Swift 2.xlet matt = Person(firstName: &quot;Matt&quot;, lastName: &quot;Mathias&quot;)matt.firstName // `matt` is `ImplicitlyUnwrappedOptional&lt;person&gt;`; we can access `firstName` directly&lt;/person&gt;let anotherMatt = matt // `anotherMatt` is also `ImplicitlyUnwrappedOptional&lt;person&gt;`&lt;/person&gt; anotherMatt 是和 matt 一样类型的实例。你可能已经预料到这种并不是很理想的情况。在代码里，ImplicitlyUnwrappedOptional 的实例已经进行了传递。对于所产生的新的不安全的代码，我们务必要多加小心。 比如，在上面的代码中，我们如果进行了一些异步操作，情况会怎么样呢？ 1234567// Swift 2.xlet matt = Person(firstName: &quot;Matt&quot;, lastName: &quot;Mathias&quot;)matt.firstName // `matt` is `ImplicitlyUnwrappedOptional&lt;person&gt;`, and so we can access `firstName` directly&lt;/person&gt;... // Stuff happens; time passes; code executes; `matt` is set to nillet anotherMatt = matt // `anotherMatt` has the same type: `ImplicitlyUnwrappedOptional&lt;person&gt;`&lt;/person&gt; 在上面这个例子中，anotherMatt 是一个值为 nil 的实例，这意味着任何直接访问他基础值的操作，都会导致崩溃。这种类型的访问确切来说是 ImplicitlyUnwrappedOptional 所推荐的方式。那么我们如果把anotherMatt 换成 Optional&lt;Person&gt; ，情况会不会好一些呢？ 让我们在 Swift 3.0 中试试同样的代码会怎样。 1234567// Swift 3.0let matt = Person(firstName: &quot;Matt&quot;, lastName: &quot;Mathias&quot;)matt?.firstName // `matt` is `Optional&lt;person&gt;`&lt;/person&gt;let anotherMatt = matt // `anotherMatt` is also `Optional&lt;person&gt;`&lt;/person&gt; 如果我们没有显示声明我们生成的是 ImplicitlyUnwrappedOptional 类型的实例，那么编译器会默认使用更为安全的 Optional。 类型推断应该是安全的在这个变化中，最大的好处在于编译器的类型推断不会使我们代码的安全性降低。如果在必要的情况下，我们选择的一些不太安全的方式，我们必须进行显示的声明。这样编译器不会再进行自动的判断。 在某些时候，如果我们的确需要使用 ImplicitlyUnwrappedOptional 类型的实例，我们仅仅需要进行显示声明。 1234// Swift 3.0let runningWithScissors: Person! = Person(firstName: &quot;Edward&quot;, lastName: &quot;&quot;) // Must explicitly declare Person!let safeAgain = runningWithScissors // What`s the type here? runningWithScissors 是一个值为 nil 的实例，因为我们在初始化的时候，我们给 lastName 了一个空字符串。 请注意，我们所声明的 runningWithScissors 实例是一个 ImplicitlyUnwrappedOptional&lt;Person&gt; 的实例。在 Swift 3.0 中，Swift 允许我们同时使用 Optional 和 ImplicitlyUnwrappedOptional 。不过我们必须进行显示声明，从而告诉编译器我们所使用的是 ImplicitlyUnwrappedOptional 。 不过幸运的是，编译器不再自动将 safeAgain 作为一个 ImplicitlyUnwrappedOptionalThankfully 实例进行处理。相对应的是，编译器将会把 safeAgain 变量作为 Optional 实例进行处理。这个过程中，Swift 3.0 对不安全的实例的传播进行了有效的限制。 一些想说的话ImplicitlyUnwrappedOptional 的改变可能是处于这样一种原因：我们通常在 macOS 或者 iOS 上操作利用 Objective-C 所编写的API，在这些API中，某些情况下，它们的返回值可能是为 nil，对于 Swift 来讲，这种情况是不安全的。 因此，Swift 正在避免这样的不安全的情况发生。非常感谢 Swift 开发者对于 ImplicitlyUnwrappedOptional 所进行的改进。我们现在可以非常方便的去编写健壮的代码。也许在未来某一天，ImplicitlyUnwrappedOptional 可能会彻底的从我们视野里消失。= 写在最后的话如果你想知道更多关于这方面的知识，你可以从这里this proposal获取一些有用的信息。你可以从 issue 里获得这个提案的作者的一些想法，同时通过具体的变化来了解更多的细节。同时那里也有相关社区讨论的链接。","link":"/posts/2016/07/23/Swift-3-0-%E6%96%B0%E5%A2%9E%E5%AE%89%E5%85%A8%E7%89%B9%E6%80%A7%E7%9A%84%E4%B8%80%E7%82%B9%E6%99%AE%E5%8F%8A/"},{"title":"聊聊 sk_buff 中一个冷门字段: nohdr","text":"今天遇到一个很有意思的问题，“nohdr 字段到底有什么用”，在这里写个水文简单记录一下 正文前情提要首先来说，不管介绍再冷门的字段，既然涉及到 SKBUFF ，那么就得先来对 sk_buff 做个简单的介绍 简而言之，sk_buff 是 Linux 网络子系统的核心数据结构，从链路层到我们最终对数据包的操作，背后都离不开 sk_buff sk_buff 要完全讲解基本就相当于把 Linux 网络系统完全讲解了，所以讲完是不可能讲完的，这辈子都不可能的！ 简单聊几个关键，可能会帮助大家理解我们本文提到的冷门字段 nohdr 的关键字段吧 首先来讲，最重要的三个字段：data ，mac 和 nh ，分别代表着当前 sk_buff 的数据区的起始地址，L2 header 的起始地址，L3 Header 的起始地址。用一个图方便大家理解 看了图的同学可能会有点明白了，实际上在内核里，也是一层一层的通过指针偏移，不断的添加新的 header 来处理网络请求。和我们直觉相符。可能有同学会问，我既然知道 L3 Header 的起始地址，IP 之类的 L3 协议的 header 长度是固定的。我是不是可以算出 L4 的偏移，然后手动处理。 Bingo，内核里有 tcphdr 的数据结构（对应 IP 是 iphdr ），你根据偏移，手动 cast 就可以手动处理。不过详细做法以后再聊 接着两个比较重要的字段，是 len 和 data_len ，这两个字段都是标识数据长度，但是简要来说，len 代表着当前 sk_buff 所有数据的长度（即包含当前协议的 header 和 payload），data_len 代表当前有效数据长度（即当前协议 payload 长度） OK，前情提要到此结束 关于 nohdr花开两朵，各表一支。聊了 sk_buff 一些预备知识，我们来聊一下 nohdr 这个字段。说实话这个字段真的很冷门 首先官方对此有对应描述 The ‘nohdr’ field is used in the support of TCP Segmentation Offload (‘TSO’ for short). Most devices supporting this feature need to make some minor modifications to the TCP and IP headers of an outgoing packet to get it in the right form for the hardware to process. We do not want these modifications to be seen by packet sniffers and the like. So we use this ‘nohdr’ field and a special bit in the data area reference count to keep track of whether the device needs to replace the data area before making the packet header modifications. 嗯，这段属实有点拗口。首先 TSO 大家肯定有所所了解。利用网卡来对大数据包进行分段（具体 Linux 下 GSO/TSO 的实现可以改天鸽一篇文章来聊），那么在这种情况下，网卡可能会需要对 header 部分进行一点小的修改来完成分片的操作。 但是有些时候，我们对于 L4 这一层的包，我并不需要关心其被修改的 Header ，只需要关心其 payload，那么怎么搞。这个时候就是 nohdr 发挥作用了。 在这里， nohdr 生效还需要配合另外一个字段，dataref 。 dataref 是一个计数字段，其具体的含义是指当前 data 字段所指向的数据区，被多少个 sk_buff 所引用。在这里有两种情况 在 nohdr 为 0 的情况下，dataref 值为数据区的引用计数 在 nohdr 为 1 的情况下，高16位，是数据区中 payload 数据区的引用计数，低16位是数据区的引用计数 对此官方有这样的描述 12345678/* We divide dataref into two halves. The higher 16 bits hold references * to the payload part of skb-&gt;data. The lower 16 bits hold references to * the entire skb-&gt;data. It is up to the users of the skb to agree on * where the payload starts.* * All users must obey the rule that the skb-&gt;data reference count must be * greater than or equal to the payload reference count.* * Holding a reference to the payload part means that the user does not * care about modifications to the header part of skb-&gt;data.*/ #define SKB_DATAREF_SHIFT 16 #define SKB_DATAREF_MASK ((1 &lt;&lt; SKB_DATAREF_SHIFT) - 1) 实际上这里也不太难理解为什么这么设计。首先来说，我们在内核里去获取数据包的时候，有些时候不需要去关心具体的 header，只需要关心具体的 payload。 而我们对于 payload 的引用计数，也需要单独的处理来保证其正确性。这样确保我们的数据还没处理完的时候。数据片不会被内核提前释放。当然这里需要大家在处理这块的时候需要保证数据区的引用计数要大于 payload 的引用计数（感觉这里像约定大于配置的做法？（当然这里不遵守约定的后果就是你内核 dump 了2333 在最后，我们的内核也通过 dataref 来在合适的时机释放数据区的内存空间，释放条件是满足以下其一即可 !skb-&gt;cloned: skb 没有 被 clone !atomic_sub_return(skb-&gt;nohdr ? (1 &lt;&lt; SKB_DATAREF_SHIFT) + 1 : 1, &amp;skb_shinfo(skb)-&gt;dataref) 即在 nohdr 为 1 的时候通过 dataref-(1 &lt;&lt; SKB_DATAREF_SHIFT) + 1) 判断是否需要释放数据区。而 nohdr 为 0 的时候通过 dataref-1 来决定是否需要释放数据区 总结水文差不多就这样。。nohdr 真的是个很冷门的字段。嗯，因为这篇水文的一些 reference 是在地铁上查的。。我就懒得列在文章里了。。差不多这样。。写题去了。。","link":"/posts/2021/11/22/a-litte-introduction-about-nohdr-filed-in-skbuff/"},{"title":"日常辣鸡水文:一个关于 Sanic 的小问题的思考","text":"日常辣鸡水文:一个关于 Sanic 的小问题的思考睡不着，作为一个 API 复制粘贴工程师来日常辣鸡水文一篇 正文最近迁移组内代码到 Sanic ，遇到一个很有意思的情况 首先标准的套路应该是这样的 1234567891011121314151617181920212223242526272829303132333435363738394041from sanic import Sanic,reponseapp=Sanic(__name__)def return_value(controller_fun): &quot;&quot;&quot; 返回参数的装饰器 :param controller_fun: 控制层函数 :return: &quot;&quot;&quot; async def __decorator(*args, **kwargs): ret_value = { &quot;version&quot;: server_current_config.version, &quot;success&quot;: 0, &quot;message&quot;: u&quot;fail query&quot; } ret_data, code = await controller_fun(*args, **kwargs) if is_blank(ret_data): ret_value[&quot;data&quot;] = {} else: ret_value[&quot;success&quot;] = 1 ret_value[&quot;message&quot;] = u&quot;succ query&quot; ret_value[&quot;data&quot;] = ret_data ret_value[&quot;update_time&quot;] = convert_time_to_time_str(get_now()) print(ret_value) return response.json(body=ret_value, status=code) return __decoratorasync def test1(): return {&quot;a&quot;:1&quot;}@return_valueasync def test2(): return await test1(),200@app.route(&quot;/wtf&quot;)async def test3(): return await test2() 中规中举，没什么太大问题 不过如果上面的代码变成下面这样 12345678910111213from sanic import Sanic,reponseapp=Sanic(__name__)async def test1(): return {&quot;a&quot;:1&quot;}@return_valueasync def test2(): return await test1()@app.route(&quot;/wtf&quot;)def test3(): return test2() 一般会以为这样会产生报错的，因为没有 await test2() ，直接 return test2() 的话，返回的是一个 Coroutine 的对象，这样应该是会抛错的，但是实际上是正常运行的，最开始很迷，不过后面看了下 Sanic 中关于 handle_request 的部分，有点意思 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172async def handle_request(self, request, write_callback, stream_callback): &quot;&quot;&quot;Take a request from the HTTP Server and return a response object to be sent back The HTTP Server only expects a response object, so exception handling must be done here :param request: HTTP Request object :param write_callback: Synchronous response function to be called with the response as the only argument :param stream_callback: Coroutine that handles streaming a StreamingHTTPResponse if produced by the handler. :return: Nothing &quot;&quot;&quot; try: # -------------------------------------------- # # Request Middleware # -------------------------------------------- # request.app = self response = await self._run_request_middleware(request) # No middleware results if not response: # -------------------------------------------- # # Execute Handler # -------------------------------------------- # # Fetch handler from router handler, args, kwargs, uri = self.router.get(request) request.uri_template = uri if handler is None: raise ServerError( (&quot;'None' was returned while requesting a &quot; &quot;handler from the router&quot;)) # Run response handler response = handler(request, *args, **kwargs) if isawaitable(response): response = await response except Exception as e: # -------------------------------------------- # # Response Generation Failed # -------------------------------------------- # try: response = self.error_handler.response(request, e) if isawaitable(response): response = await response except Exception as e: if self.debug: response = HTTPResponse( &quot;Error while handling error: {}\\nStack: {}&quot;.format( e, format_exc())) else: response = HTTPResponse( &quot;An error occurred while handling an error&quot;) finally: # -------------------------------------------- # # Response Middleware # -------------------------------------------- # try: response = await self._run_response_middleware(request, response) except: log.exception( 'Exception occured in one of response middleware handlers' ) # pass the response to the correct callback if isinstance(response, StreamingHTTPResponse): await stream_callback(response) else: write_callback(response) 核心代码是这样一段 1234567891011handler, args, kwargs, uri = self.router.get(request)request.uri_template = uriif handler is None: raise ServerError( (&quot;'None' was returned while requesting a &quot; &quot;handler from the router&quot;))# Run response handlerresponse = handler(request, *args, **kwargs)if isawaitable(response): response = await response 大概就是，首先按照 route-&gt;add_route 的顺序注册对应的处理函数和 URL 到一个映射里，然后当请求发过来时，取出对应的 handler ，然后进一步处理 最开始正常的中规中矩的做法里 123@app.route(&quot;/wtf&quot;)async def test3(): return await test2() 注册的 handler 是 test3 这个函数，然后执行 response = handler(request, *args, **kwargs) ，初始化了一个 Coroutine 对象，紧接着这个对象是 awaitable 的，于是进入后面的 response = await response 流程。 好了，来看看非主流的做法 123@app.route(&quot;/wtf&quot;)def test3(): return test2() 老规矩，先注册，然后取出 test3 这个函数作为 handler ，然后执行，因为是普通函数，于是 response 的值便是 test3 中初始化的那个 Coroutine 对象，然后同样是 awaitable 的，进入后面的 response = await response 流程。 两种方式殊途同归，这也解释了为什么第二中不清真的方式也能得到正确的结果 思考Sanic 这样的处理方式，相当于增强了整个框架的容错性。也可能让用户写出向之前那样不清真的代码。不过我也没法说这个是好是坏，各有看法吧。不过有一点是肯定的，在 debug 模式下，如果用户利用 app.route 添加了一个非 async 的函数，是有必要抛出一个 warning 的，不过，Sanic 还有，PR 已经提出，就不知道合不合了。。。 好了，就先这样吧。。明天还得搬砖，溜了，溜了。。","link":"/posts/2018/02/23/a-little-idea-about-sanic/"},{"title":"关于 Node.js 中 execSync 的一点问题","text":"很久没写水文了，昨天帮人查了一个 Node.js 中 execSync 这个函数特殊行为的问题，很有趣，所以大概记录下来水一篇文章 背景首先老哥给了一张截图 首先基本问题可以抽象为在 Node.js 中利用 execSync 这个函数执行 ps -Af | grep -q -E -c &quot;\\\\-\\\\-user-data-dir=\\\\.+App&quot; 这样一条命令的时候，Node.js 时不时会报错。具体堆栈大概为 12345678910Uncaught Error: Command failed: ps -Af | grep -q -E -c &quot;\\-\\-user-data-dir=\\.+App&quot; at checkExecSyncError (child_process.js:616:11) at Object.execSync (child_process.js:652:15) { status: 1, signal: null, output: [ null, &lt;Buffer &gt;, &lt;Buffer &gt; ], pid: 89073, stdout: &lt;Buffer &gt;, stderr: &lt;Buffer &gt;} 但是同样的命令在终端上并不会有类似的现象。所以这个问题有点困扰人 分析首先先看一下 Node.js 文档中对 execSync 的描述 The child_process.execSync() method is generally identical to child_process.exec() with the exception that the method will not return until the child process has fully closed. When a timeout has been encountered and killSignal is sent, the method won’t return until the process has completely exited. If the child process intercepts and handles the SIGTERM signal and doesn’t exit, the parent process will wait until the child process has exited.If the process times out or has a non-zero exit code, this method will throw. The Error object will contain the entire result from child_process.spawnSync().Never pass unsanitized user input to this function. Any input containing shell metacharacters may be used to trigger arbitrary command execution. 大意就是，这个函数通过子进程来执行一个命令，在命令执行超时之前会一直等待。OK 没有问题。那接下来，我们先来看一下上面提到的报错堆栈以及 execSync 的实现代码 12345678910111213141516171819202122232425262728293031323334function execSync(command, options) { const opts = normalizeExecArgs(command, options, null); const inheritStderr = !opts.options.stdio; const ret = spawnSync(opts.file, opts.options); if (inheritStderr &amp;&amp; ret.stderr) process.stderr.write(ret.stderr); const err = checkExecSyncError(ret, opts.args, command); if (err) throw err; return ret.stdout;}function checkExecSyncError(ret, args, cmd) { let err; if (ret.error) { err = ret.error; } else if (ret.status !== 0) { let msg = 'Command failed: '; msg += cmd || ArrayPrototypeJoin(args, ' '); if (ret.stderr &amp;&amp; ret.stderr.length &gt; 0) msg += `\\n${ret.stderr.toString()}`; // eslint-disable-next-line no-restricted-syntax err = new Error(msg); } if (err) { ObjectAssign(err, ret); } return err;} 我们能看到，这里 execSync 在执行完命令执行代码后，会进入 checkExecSyncError 来检查子进程的 Exit Status Code 是否为0，不为0则认为命令执行出错，然后抛出异常。 看起来没有问题，那么也就是我们执行命令的时候出错了？那我们验证下吧 对于这种涉及 Linux 下 Syscall 问题排查的工具（这个问题在 Mac 等环境下也存在，不过我为了方便排查，跑去 Linux 上复现了），除了 strace 好像也暂时找不到更成熟方便的工具了（虽然基于 eBPF 也能做，但是说实话自己现撸绝对没 strace 的效果好。 那么上命令 1sudo strace -t -f -p $PID -o error_trace.txt tips: 在使用 strace 的时候可以利用 -f 参数，可以 trace 被 trace 进程创建的子进程 好了执行命令，成功拿到整个 syscall 的调用链路，OK 开始分析 首先我们将目光很快定位到了最关键的部分（因为整个文件太长，有将近 4K 行，我就直接挑重点部分分析了） 12345678910...894259 13:21:23 clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f12d9465a50) = 896940...896940 13:21:23 execve(&quot;/bin/sh&quot;, [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;ps -Af | grep -E -c \\&quot;\\\\-\\\\-user-da&quot;...], 0x4aae230 /* 40 vars */ &lt;unfinished ...&gt;...896940 13:21:24 &lt;... wait4 resumed&gt;[{WIFEXITED(s) &amp;&amp; WEXITSTATUS(s) == 1}], 0, NULL) = 896942896940 13:21:24 --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=896942, si_uid=1000, si_status=1, si_utime=0, si_stime=0} ---896940 13:21:24 rt_sigreturn({mask=[]}) = 896942896940 13:21:24 exit_group(1) = ?896940 13:21:24 +++ exited with 1 +++ 首先这里科普一下，Node.js 中没有直接使用 fork 来创建新的进程，而是使用 clone 来创建新的进程，至于两者之间的差别，要详细说的话，可以单独水一篇超长文了（我先立个 flag）这里先用官方的说法大概描述下 These system calls create a new (“child”) process, in a manner similar to fork(2).By contrast with fork(2), these system calls provide more precise control over what pieces of execution context are shared between the calling process and the child process. For example, using these system calls, the caller can control whether or not the two processes share the virtual address space, the table of file descriptors, and the table of signal handlers. These system calls also allow the new child process to be placed in separate namespaces(7). 用简短的概括性的话来描述就是,clone 提供了 fork 近似的语义,不过通过 clone ,开发者能更细粒度的控制进程/线程创建过程中的细节 OK, 这里我们看到 894259 这个主进程通过 clone 创建了 896940 这个进程。在执行过程中, 896940 这个进程利用 execve 这个 syscall 通过 sh (这是 execSync 的默认行为)我们的命令 ps -Af | grep -q -E -c &quot;\\\\-\\\\-user-data-dir=\\\\.+App&quot;。 OK，我们也看到了，896940 在退出的时候，的确是以 1 的 exit code 退出的，和我们之前的分析一致。那么换句话说，在我们执行命令的时候，有 error 的出现。那么这里的 error 出现在哪呢？ 我们分析一下命令，如果熟悉常见 shell 的同学可能发现了，我们的命令中实际上使用了管道操作符 | ，不精确的来说，当这个操作符出现的时候，前后两个命令将分别在两个进程执行，然后通过 pipe 进行 IPC。那么换句话说，我们可以很快定位这两个进程，直接快速搜了一下文本 12345678910...896941 13:21:23 execve(&quot;/bin/ps&quot;, [&quot;ps&quot;, &quot;-Af&quot;], 0x564c16f6ec38 /* 40 vars */) = 0...896942 13:21:23 execve(&quot;/bin/grep&quot;, [&quot;grep&quot;, &quot;-E&quot;, &quot;-c&quot;, &quot;\\\\-\\\\-user-data-dir=\\\\.*&quot;], 0x564c16f6ecb0 /* 40 vars */ &lt;unfinished ...&gt;...896941 13:21:24 &lt;... exit_group resumed&gt;) = ?896941 13:21:24 +++ exited with 0 +++...896942 13:21:24 exit_group(1) = ?896942 13:21:24 +++ exited with 1 +++ OK，我们发现 896942 即执行 grep 的进程直接以 exit code 1 退出了。那么这是为什么呢？？看了下 grep 的官方文档，，卧操，差点吐血 Normally, the exit status is 0 if selected lines are found and 1 otherwise. But the exit status is 2 if an error occurred, unless the -q or –quiet or –silent option is used and a selected line is found. Note, however, that POSIX only mandates, for programs such as grep, cmp, and diff, that the exit status in case of error be greater than 1; it is therefore advisable, for the sake of portability, to use logic that tests for this general condition instead of strict equality with 2. 如果 grep 没有匹配到数据，那么会以 1 作为 exit code 退出进程。。如果匹配到了，则0退出。。但是，但是，卧操，卧操。。按照标准语义，exit code 1 的含义难道不是 Operation not permitted 吗？？完全不按基本法出牌！ 总结实际上通篇看了下来，我们可以总结出两个原因 Node.js 在对 POSIX 相关 API 进行抽象封装的时候，直接按照了标准语义，给用户兜底了。虽然从理论上讲这应该是个应用自决的行为 grep 没有按照基本法办事 说实话我也不知道怎么去评价这两方面谁更坑一点。按照前面所说么处理子进程的 exit code 从理论上讲这应该是个应用自决的行为，但是 Node.js 自己做了一层封装，在节省用户心智的同时，遇到一些非标场景，也会有不小的隐患了。。 只能说不断根据不同的场景做 trade-off 吧 好了，这篇文章就到这里了，因为是临时起义，所以我就懒得将相关 Reference 列在文里了。差不多这样吧，水文目标达成.jpg","link":"/posts/2021/08/24/a-little-problem-about-posix-node-js-execsync/"},{"title":"为什么 Python 的 Type Hint 没有流行起来","text":"在知乎上看到一个很有意思的问题，为什么TypeScript如此流行，却少见有人写带类型标注的Python？ 虽然我没忍住在知乎上输出了答案，但是为了以防万一，我在博客上扩展，与更新一下 BTW 最近上线真的心力憔悴，写个文章放松下 开始其实这个答案很简单，历史包袱与 ROI，在了解为什么有这样的现象之前，首先我们要去了解 Type Hint 能给我们带来什么，然后我们需要去了解 Type Hint 的前世今生 在现在这个时间点（2020.03）来看，Type Hint 能给我们带来肉眼可见的收益是 通过 annotation ，配合 IDE 的支持，能让我们在代码编辑的时候的体验更好 通过 mypy/pytype 等工具的支持，我们能在 CI/CD 流程中去集成静态类型检查 通过 pydantic 以及很多新式框架的支持，我们能够减少很多重复的工作 可能大家以为从 Python 3.5 引入 PEP 484 开始，Python Type Hint 便已经成熟。但是实际上，这个时间比大家想象的短的多 好了，我们现在要去回顾一下整个 Type Hint 发展史上的关键节点 PEP 3107 Function Annotations PEP 484 Type Hints PEP 526 Syntax for Variable Annotations PEP 563 Postponed Evaluation of Annotations PEP 3107如同前面所说，大家最开始认识 Type Hint 的时间应该是14 年 9 月提出，15 年 5 月通过的 PEP 484 。但是实际上雏形早的多，PEP 484 的语法实际上来自于 06 年提出，3.0 引入的 PEP 3107 所设计的语法，参见 PEP 3107 – Function Annotations 在 PEP 3107 中，对于这个提案的目标，有这样一段描述 Because Python’s 2.x series lacks a standard way of annotating a function’s parameters and return values, a variety of tools and libraries have appeared to fill this gap. Some utilise the decorators introduced in “PEP 318”, while others parse a function’s docstring, looking for annotations there.This PEP aims to provide a single, standard way of specifying this information, reducing the confusion caused by the wide variation in mechanism and syntax that has existed until this point. 说人话就是，为了能够给函数的参数或者返回值添加额外的元信息，大家五花八门各显神通，有用 PEP 318 装饰器的，有用 docstring 来做的。社区为了缓解这个现象，决定推出新的语法糖，来让用户能够方便的为参数签名和返回值添加额外的信息 最后形成的语法如下 12def foo(a: 'x', b: 5 + 6, c: list) -&gt; max(2, 9): pass 是不是很眼熟？ 没错，3107 实际上奠定了后续 Type Hint 的基调 可标注 作为 function/method 信息的一部分，可 inspect runtime 但是新的疑惑就来了，为什么这个提案经常被人忽略？还是，我们需要放在具体的时间点来看 这个提案提出时间最早可以追溯至06年，在 PEP3000 这个可能是 Python 历史上最著名的提案（即宣告 Python 3 的诞生）中确定在 Python 3 中引入，08年正式发布 在这个时间点下，3107 面临着两个问题： 在06-08这个时间点上，社区最主要的精力都在友(ji)好(lie)的讨(si)论(bi)，我们为什么要 Python 3？以及为什么我们要迁到 Python 3 3107 实际上只是告诉大家，你可以标注，你可以方便的获取标注信息，但是怎么样去抽象一个类型的表示，如一个 int 类型的 list ，这种事，还是依靠社区自行发展，换句话说，叫做放养 问题1，无解，只能依靠时间去慢慢推动。问题2，促成了 PEP 484 的诞生 PEP 484PEP 484 这个提案大家应该都有一定程度上的了解了，在此不再描述提案的具体内容 PEP 484 最大的意义在于， 在继承了 PEP 3107 奠定的语法和基调之上，将 Python 的类型系统进行了合理的抽象，这也是重要的产物 typing，直到这时，Python 中的 type hint 才有了基本的官方规范，同时达到了基本的可用性，这个时间点是 15 年 9 月（9月13，Python 3.5.0 正式 Release） 但是实际上 PEP 484 在这个时间点也只能说基本满足使用，我来举几个被诟病的例子 首先看一段代码 12345from typing import Optionalclass Node: left: Optional[Node] right: Optional[Node] 这段代码实际上很简单对吧，一个标准的二叉树节点的描述，但是放在 PEP 484 中，这段代码暴露出两个问题 无法对变量进行标注。如同我前面所说的一样，PEP 484 本质上是 PEP 3107 的一个扩展，这个时候 hint 的范围仅限于 function/method ，而在上面的代码中，在 3.5 时期，我是无法对我的 left 和 right 的变量进行标注的，一个编程语言的基本要素之一的变量，无法被 Type Hint ，那么一定程度上我们可以说这样一个 type hint 的功能没有闭环 循环引用，字面意义，在社区/StackOverflow 上如何解决 Type Hint 中的循环引用这个问题，一度让人十分头大。社区：What the fuck? 所幸，Python 社区意识到了这个问题，推出了两个提案来解决这样的问题 PEP 526问题1 促成了 PEP 526 – Syntax for Variable Annotations 的诞生，16 年 8 月提出，16 年 9 月被接受。16 年 9 月在 BPO-27985 实现。在我印象里，这应该是 Python 社区中数的出来的争议小，接收快，实现快的 PEP 了 在 526 中，Python 正式允许大家对变量进行标注，无论是 class attribute 还是普通的 variable 12class Node: left: str 这样是可以的， 12def abc(): a:int = 1 这样也是可以的 在这个提案的基础上，Python 官方也推动了 PEP 557 – Data Classes 的落地，当然这是后话 话说回来，526 只解决了上面的问题1，没有解决问题2，这个事情，将会由 PEP 563 来解决 PEP 563为了解决循环引用的问题，Python 引入了 PEP 563 – Postponed Evaluation of Annotations，17 年 9 月社区提出，17 年 11 月被接受，18 年 1 月在 GH-4390 中实现。 在 563 之后，我们上面的代码可以这么写了 12345from typing import Optionalclass Node: left: Optional[&quot;Node&quot;] right: Optional[&quot;Node&quot;] 嗯，484 中的两个问题，终于被解决了 总结以 PEP 563 作为重要分割点，Python 最早在 18 年 1 月之后才初步具备完整的生态和生产可用性，如果考虑 release version，那么应该是 18 年 6 月，Python 3.7 正式发布之后的事了。 在 Python 3.6/7 之后，社区也才开始围绕 Type Hint 去构建一套生态体系， 比如利用 PEP 526 来高效的验证数据格式，参见 pydantic 顺带一提，这货也是目前很火的一个新型框架（也是我目前最喜欢的一个框架）FastAPI 的根基 各大公司也开始跟进，例如 Google 的 pytype ，微软推出了 pyright 来提供在 VSCode 上的支持 还有许许多多优秀的如 starlette 这样库 直到这时，Python + Type Hint 的真正的威力才开始挥发出来。这样才开始能回答大家这样一个问题：“我为什么要切换到 Type Hint”，我猜在 IDE 里写的爽肯定不是一个重要原因 要知道，我们在做技术决策时候，一定是因为这个决策能给我们带来足够的 benefit，换句话说，有足够的 ROI，而不是单纯的因为，我们喜欢它 这样看起来，到现在，满打满算一年半不超过两年的时间。对于一个用户习惯养成周期来说，这太短了。更何况还有一大堆的 Python 2 代码在那放着23333 话说回来，作为对比，TypeScript Release 时间可以上溯至 12 年 10 月，发布 0.8 版本，当时的 TS 应该是具备了相对完整地类型系统。 TS 用了 8 年，Python 可能也还有很长的路要走 当然，这个答案也只是从技术和历史的角度聊聊这个问题。至于其余的很多因素，包括社区的博奕与妥协等，暂还不在这个答案的范围内，大家有兴趣的话，可以去 python-idea，python-dev，discuss-python 这几个地方去找一找历史上关于这几个提案的讨论，非常有意思。 最后，TS 成功还有一个原因，它有个好爸爸&amp;&amp;它爸爸有钱（逃 嗯，差不多就这样吧，最近干活干的心里憔悴的我，也就只能写点垃圾水文了压压惊，平复心情了。。","link":"/posts/2020/03/20/a-simple-history-about-type-hint-in-python/"},{"title":"当我们在聊 CI&#x2F;CD 时，我们在聊什么？","text":"本文实际上是在群内第二次分享的内容。这次其实想来聊聊，关于 CI/CD 的一些破事和演进过程中我们所需要遇到的一些问题，当然本文中是一个偏新手向的文章和一点点爆论，随便看看就好。 开宗明义，定义先行在我们谈论一个事物之前，我们需要对这个事物给出一个定义，那我们先来看一下我们今天要聊的 CI 与 CD 的定义。 首先，CI 指 Continuous Integration ，在中文语境中的表述是持续集成。而 CD 在常见语境下可能是两种意思：Continuous Delivery 或 Continuous Deployment，与之对应的表述是持续交付/持续部署。这里借用一下 Brent Laster 在 What is CI/CD?1 中给出的定义 Continuous integration (CI) is the process of automatically detecting, pulling, building, and (in most cases) doing unit testing as source code is changed for a product. CI is the activity that starts the pipeline (although certain pre-validations—often called “pre-flight checks”—are sometimes incorporated ahead of CI).The goal of CI is to quickly make sure a new change from a developer is “good” and suitable for further use in the code base.Continuous deployment (CD) refers to the idea of being able to automatically take a release of code that has come out of the CD pipeline and make it available for end users. Depending on the way the code is “installed” by users, that may mean automatically deploying something in a cloud, making an update available (such as for an app on a phone), updating a website, or simply updating the list of available releases. 光看定义，可能大家还是会很懵逼，那么下面我们用一些实际的例子来给大家从头捋一遍 CI/CD 那些事 Re：从0开始构建流程这个标题好像起的有点草，不过不管了。首先我们假定这样一个最简单的需求 我们基于 Hexo 构建了一个个人的博客系统。其中包含我们所需要发布的文章，我们配置的主题。我们需要将其发布到具体的 Repo 上。 好了，基于这个需求，我们来从0到0玩一圈吧（笑（ 构建原生之初可能这里有很多人会问，为啥会选择 Hexo 来作为我们的切入点。原因很简单啊！因为它够简单啊！ 言归正传，首先 Hexo 有两个命令 hexo g &amp;&amp; hexo d ，分别是根据当前目录下的 Markdown 文件来生成静态的网页。然后将生成的产物根据配置推送到对应的 repo 上 OK，那么我们在最原始的阶段一个构建的流程就是这样 用一个编辑器，开开心心的写文章 然后在本地终端执行 hexo g &amp;&amp; hexo d 问题来了，现在有些时候提交了博客，但是忘了执行生成命令怎么办？或者是我每次都需要敲重复的命令很麻烦怎么办？那就让我们把整个过程自动化一下吧。Let’s rock! 更进一步的构建OK，我们先来假设一下，我们如果完成了自动化，我们现在发布一个博客的工作流应该编程什么样的 我们编写一个 Markdown 文件，推送到 GitHub 仓库里的 Master 分支上 我们的自动任务开始构建我们博客，生成一系列静态文件和样式 将我们的静态文件和样式推送到我们的站点 Repo/CDN 等目标位置 好了，那么这里有两个核心的问题 在我们推送代码的时候，自动开始构建 在构建完成后，推送产物 那我们现在基于 GitHub Action 来配置一套我们的自动化构建任务 1234567891011121314151617181920212223name: Build And Publish Blogon: push: branches: [ master ]jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v1 with: node-version: '12.x' - name: Install Package run: npm install -g hexo-cli &amp;&amp; npm install - name: Generate Html File run: hexo g - name: Deploy 🚀 uses: JamesIves/github-pages-deploy-action@3.7.1 with: GITHUB_TOKEN: ${{ secrets.PUBLSH_TOKEN }} BRANCH: gh-pages # The branch the action should deploy to. FOLDER: public # The folder the action should deploy. 我们能看到这段配置实际上完成了这样一些事情 在我们往 master 分支提交代码的时候触发构建 拉取代码 安装构建所需依赖 构建生成静态文件 推送静态文件 如果上面任何一个步骤失败了，都将取消后面步骤的执行。实际上这样一个简单的任务已经包含了一个 CI &amp;&amp; CD 所包含的基础要素（这里 CD 我并未严格区分 Continuous Delivery/Continuous Deployment) 与已有的代码持续的构建与集成 集成中区分多个 phase。每个 phase 将依赖上个 phase 结果。 将构建产物交付/部署出去。交付/部署的成功依赖于集成的成功 那么在这里，我们将博客系统换成一个我们工程中的例子。将 Hexo 换成我们的 Python 服务。将新增博文换成我们的新增的代码。将构建命令换成 mypy/pylint 等检查工具。你看，CI/CD 实际上和你想象的复杂的系统，是不是有很大差别？ 这里可能有很多人会提出这样一个问题，如果说这里我们将这些命令，不用线上的形式触发。而在本地用 Git Hook 等形式进行实现。那么这算不算一种 CI 与 CD 呢？我觉得毫无疑问算的，从我的视角来看，CI/CD 核心的要素在于通过可以重复，自动化的任务，来尽早暴露缺陷，减轻人为因素所带来的不必要的事故发生。 这个开发过份傻逼却不谨慎首先抛出一个最基础的爆论，然后我们接着往下谈 所有人都有傻逼的时候，而且这个傻逼的时候可能还会很多。 在这样一个爆论的情况下，我们来回顾一下上面举基于 Hexo 去构建一个个人博客系统的例子中，如果我们不选择通过一种收敛的，自动化的系统去解决我们的构建，发布需求。那么我们哪些环节会出现风险 最基础的，写完博客，忘了构建，忘了发布 比如我们升级一下依赖中的 Hexo 版本或者主题版本，我们没有测试，导致构建出来的样式失效 我们的 Markdown 有问题，导致构建失败 比如多个人维护一个博客的情况下，我们每个人都需要保存目标仓库/CDN的密钥等信息。导致信息泄漏等 将基于 Hexo 去构建一个个人博客系统的例子切换成我们日常开发的场景，那么我们可能遇到的问题会更多。简单举几个 没法很快速的回滚 没法溯源具体的构建/发布记录 没有自动化的任务，研发懒得跑测试或者 lint 导致代码腐化 高峰期上线导致事故 嗯，这些问题大家是不是都很熟悉？大概就是，我起了，构建了，出事故了，有啥好说的23333 讲到这里的大家实际上有没有发现一个问题？我在这篇文章中，没有对 CI 与 CD 进行区分？从我的视角来看，CI/CD 本质上是践行的同一个事。即 对于研发流程与交付流程的收敛 从我的视角来看，去构建一个 CI/CD 系统核心的目标在于 通过收敛入口以及自动化的任务触发，尽可能减轻人为因素所带来的系统不稳定性 通过快速，多次，可重复，无感知的任务，尽可能的在较早阶段暴露系统中的问题 在这样两个大目标的前提下，我们便会根据不同的业务场景，采用不同的手段与形式丰富我们 CI/CD 中的内容，包括不仅限于 在 CI 阶段自动化的单元测试，E2E 测试等 在 CI 阶段周期性的 Nighty Build 等 在 CD 阶段进行发布管控等 不过无论我们怎么样去构建一个 CI/CD 系统，或者选择什么样的粒度去进行 CI/CD。我觉得一个合格的 CI/CD 系统与机制 都需要遵照这样几个原则（个人向总结） 入口的收敛，SOP 的建立。如果不达成这点共识，研发能够通过技术手段绕过 CI/CD 系统那么便又回到的了我们本章的标题（这个研发过份傻逼却不谨慎） 对于业务代码无侵入 集成任务/发布任务一定要是自动化，可重复的 可回溯的历史记录与结果 可回溯的构建集成产物 从上到下的支持 那么遵照我总结的这样几个原则，我们来迭代一下我们之前的博客的发布过程 1234567891011121314151617181920212223242526272829303132333435name: Build And Publish Blogon: push: branches: [ master ] pull_request: branches: [ master ]jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v1 with: node-version: '12.x' - name: Install Package run: npm install -g hexo-cli &amp;&amp; npm install - name: Generate Html File run: hexo g - name: Deploy To Repo🚀 if: ${{ github.ref == 'refs/heads/master'}} uses: JamesIves/github-pages-deploy-action@3.7.1 with: GITHUB_TOKEN: ${{ secrets.PUBLSH_TOKEN }} BRANCH: gh-pages # The branch the action should deploy to. FOLDER: public # The folder the action should deploy. - name: Upload to Collect Repo uses: JamesIves/github-pages-deploy-action@3.7.1 with: GITHUB_TOKEN: ${{ secrets.PUBLSH_TOKEN }} BRANCH: build-${{ github.run_id }} # The branch the action should deploy to. FOLDER: public # The folder the action should deploy. 在这段更改后的构建流程中，我选择以 PR 为粒度去触发 CI 流程，并将历史产物进行存储，而在合并主分支后，新增发布流程。在这样一来，我在博客构建发布的时候，便能够通过回溯的历史产物来验证我框架升级，新增博文等操作的正确性。同时依托 GitHub Action，我能很好的完成历史构建的回溯 嗯，这样便可以尽可能避免我傻逼的操作所带来的各种副作用（逃 进击的构建：终章好了，啥都没有，傻眼了吧。。。。 只是开个玩笑。实际上本文到这差不多就可以告一段落了。实际上大家通过这篇文章可以发现一个问题。就是实际上构建一个 CI/CD 系统可能并不会涉及很多，很高深的技术问题(极少数的场景除外）无论是传统的 Jenkins，还是新生的 GitHub Action，GitLab-CI，亦或者是云厂商提供的服务都能很好的帮助我们去构建一套贴合业务的 CI/CD 系统。但我之前在推特上发表了的一个爆论“CI/CD 的建立往往不是一个技术问题，而是一个制度问题，更可以称为是一个想法问题”。 所以，我希望我们每个人都能认识到我们都会犯错这样一个事实。然后尽可能的将自己所负责的系统的开发流程与交付流程尽可能的收敛与自动化。让一个 CI/CD 真正称为我们日常工作中的一部分。 差不多这样，溜了，溜了。","link":"/posts/2021/04/11/a-simple-introduce-about-ci-cd/"},{"title":"简单聊聊进程中的信号处理 V2","text":"上次写了一个水文简单聊聊进程中的信号处理 ，师父看了后把我怒斥了一顿，表示上篇水文中的例子太 old style, too simple ,too naive。如果未来出了偏差，我也要负泽任的。吓得我连和妹子周年庆的文章都没写，先赶紧来重新水一篇文章，聊聊更优秀，更方便的信号处理方式 前情提要首先来看看，之前那篇文章中的例子 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); deletejob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;}int main(int argc, char **argv) { int pid; sigset_t mask_all, prev_all; sigfillset(&amp;mask_all); signal(SIGCHLD, handler); while (1) { if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); addjob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); }} 再来复习下几个关键的 syscall signal1: 信号处理函数，使用者可以通过这个函数为当前进程指定具体信号的 Handler。当信号触发时，系统会调用具体的 Handler 进行对应的逻辑处理。 sigfillset2: 用于操作 signal sets（信号集）的函数之一，这里的含义是将系统所有支持的信号量添加进一个信号集中 fork3: 大家比较熟悉的一个 API 了，创建一个新的进程，并返回 pid 。如果是在父进程中，返回的 pid 是对应子进程的 pid。如果子进程中，pid 为0 execve4: 执行一个特定的可执行文件 sigprocmask5：设置进程的信号屏蔽集。当传入第一个参数为 SIG_BLOCK 时，函数会将当前进程的信号屏蔽集保存在第三个参数传入的信号集变量中，并将当前进程的信号屏蔽集设置为第二个参数传入的信号屏蔽集。当第一个参数为 SIG_SETMASK 时，函数会将当前进程的信号屏蔽集设置为第二个参数设置的值。 wait_pid6: 做一个不精确的概括，回收并释放已终止的子进程的资源。 好了，复习完关键点之后，开始进入本文的关键部分。 更优雅的信号处理手段更优雅的 handler首先再来看看上面信号处理部分的代码 123456789101112131415void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); deletejob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;} 这里我们为了保证 handler 不被其余的信号打断，所以我们在处理的时候使用 sigprocmask + SIG_BLOCK 来做信号屏蔽。这样看起来逻辑上没啥问题，但是有个问题。当我们有其余很多不同 handler 的时候，我们势必会生成很多重复冗余的代码。那么我们有没有更优雅的方法来保证我们的 handler 的安全呢？ 有（超大声（好，很有精神！（逃。隆重介绍一个新的 syscall -&gt; sigaction7 废话不多说，先上代码 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { deletejob(pid); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;}int main(int argc, char **argv) { int pid; sigset_t mask_all, prev_all; sigfillset(&amp;mask_all); struct sigaction new_action; new_action.sa_handler=handler; new_action.sa_mask=mask_all; signal(SIGCHLD, handler); while (1) { if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); addjob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); }} 好！很有精神！大家可能发现了，我们这段代码相较于之前的代码增加了关于 sigaction 相关的设置。难道？ yep，在 sigaction 中，我们可以通过设置 sa_mask 来设置当信号处理函数执行期间，进程将阻塞哪些信号。 你看，这样我们的代码是不是相较于之前更为优雅了。当然，sigaction 还有很多其余很有用的设置项，大家可以下来了解一下。 更快速的信号处理方式在我们上面的例子中，我们已经解决了优雅的设置信号处理函数这样的问题，那么我们现在又面临了一个全新的问题。 如上面所说，我们信号处理函数在执行时，我们选择阻塞了其余的信号。那么这里存在一个问题，当我们在信号处理函数中的逻辑耗时较长，且不需要原子性（即需要和信号处理函数保持同步），而且系统中的信号发生频率较高。那么我们这样的做法将会导致进程的信号队列不断增加，进而导致不可预料的后果。 那么我们这里有什么更好的方法来处理这件事呢？ 假设，我们打开一个文件，在信号处理函数中只完成一件事，就是往这个文件中写一个特定的值。然后我们轮询这个文件，如果一旦发生变化，那么我们读取文件中的值，判断具体的信号，做具体的信号处理，这样是不是既保证了信号的妥投，又保证我们信号处理逻辑将阻塞信号的代价降至最低了？ 当然，当然，社区知道大家嫌写代码难，所以专门给大家提供了一个船新的 syscall -&gt; signalfd8 老规矩，先来看看例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/signalfd.h&gt;#include &lt;sys/wait.h&gt;#define MAXEVENTS 64void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }int main(int argc, char **argv) { int pid; struct epoll_event event; struct epoll_event *events; sigset_t mask; sigemptyset(&amp;mask); sigaddset(&amp;mask, SIGCHLD); if (sigprocmask(SIG_SETMASK, &amp;mask, NULL) &lt; 0) { perror(&quot;sigprocmask&quot;); return 1; } int sfd = signalfd(-1, &amp;mask, 0); int epoll_fd = epoll_create(MAXEVENTS); event.events = EPOLLIN | EPOLLEXCLUSIVE | EPOLLET; event.data.fd = sfd; int s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); while (1) { int n = epoll_wait(epoll_fd, events, MAXEVENTS, 1); if (n == -1) { if (errno == EINTR) { fprintf(stderr, &quot;epoll EINTR error\\n&quot;); } else if (errno == EINVAL) { fprintf(stderr, &quot;epoll EINVAL error\\n&quot;); } else if (errno == EFAULT) { fprintf(stderr, &quot;epoll EFAULT error\\n&quot;); exit(-1); } else if (errno == EBADF) { fprintf(stderr, &quot;epoll EBADF error\\n&quot;); exit(-1); } } printf(&quot;%d\\n&quot;, n); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (sfd == events[i].data.fd) { struct signalfd_siginfo si; ssize_t res = read(sfd, &amp;si, sizeof(si)); if (res &lt; 0) { fprintf(stderr, &quot;read error\\n&quot;); continue; } if (res != sizeof(si)) { fprintf(stderr, &quot;Something wrong\\n&quot;); continue; } if (si.ssi_signo == SIGCHLD) { printf(&quot;Got SIGCHLD\\n&quot;); int child_pid = waitpid(-1, NULL, 0); deletejob(child_pid); } } } if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } addjob(pid); }} 好了，我们来介绍下这段代码中的一些关键点 signalfd 是一类特殊的文件描述符，这个文件可读，可 select 。当我们指定的信号发生时，我们可以从返回的 fd 中读取到具体的信号值。 signalfd 优先级比信号处理函数低。换句话说，假设我们为信号 SIGCHLD 注册了信号处理函数，同时也为其注册了 signalfd 那么当信号发生时，将优先调用信号处理函数。所以我们在使用 signalfd 时，需要利用 sigprocmask 设置进程的信号屏蔽集。 如前面所说，该文件描述符可 select ，换句话说，我们可以利用 select9, poll10, epoll1112 等函数来对 fd 进行监听。在上面的的代码中，我们就利用 epoll 对 signalfd 进行监听 当然，这里额外要注意的一点是，很多语言不一定提供了官方的 signalfd 的 API（如 Python），但是也有可能提供了等价的替代品，典型的例子就是 Python 中的 signal.set_wakeup_fd13 在这里也给大家留一个思考题：除了利用 signalfd ，还有什么方法可以实现高效，安全的信号处理？ 总结私以为信号处理是作为一个研发的基本功，我们需要安全，可靠的处理在程序环境中遇到的各种信号。而系统也提供了很多设计很优秀的 API 来减轻研发的负担。但是我们要知道，信号本质上是通讯手段的一种。而其天生的弊端便是携带的信息较少。很多时候，当我们有很多高频的信息传递需要去做的时候，这个时候可能利用信号并不是一个很好的选择。当然这个并没有定论。只能 case by case 的去做 trade-off。 差不多就这样吧，本周第二篇水文混完（逃 Reference [1]. Linux man page: signal [2]. Linux man page: sigfillset [3]. Linux man page: fork [4]. Linux man page: execve [5]. Linux man page: sigprocmask [6]. Linux man page: waitpid [7]. Linux man page: sigaction [8]. Linux man page: signalfd [9]. Linux man page: select [10]. Linux man page: poll [11]. Linux man page: epoll_ctl [12]. Linux man page: epoll_wait [13]. Python Documentation: signal.set_wakeup_fd","link":"/posts/2020/11/07/a-simple-introduction-about-signal-process-in-linux-v2/"},{"title":"简单聊聊进程中的信号处理","text":"最近在某个技术群里帮人分析了 Linux 编程下信号处理的一段代码。我自己觉得这段代码是挺不错的一个例子，所以写个简单的水文，用这段代码聊聊 Linux 中的信号处理 正文我们首先来看一看这一段代码 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }void handler(int sig) { int olderrno = errno; sigset_t mask_all, prev_all; pid_t pid; sigfillset(&amp;mask_all); while ((pid = waitpid(-1, NULL, 0)) &gt; 0) { sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); deletejob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); } if (errno != ECHILD) { printf(&quot;waitpid error&quot;); } errno = olderrno;}int main(int argc, char **argv) { int pid; sigset_t mask_all, prev_all; sigfillset(&amp;mask_all); signal(SIGCHLD, handler); while (1) { if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all); addjob(pid); sigprocmask(SIG_SETMASK, &amp;prev_all, NULL); }} 实际上这段代码是比较典型的信号处理的代码，为了引出后续的内容，我们先来复习一下，这段代码中几个关键的 syscall signal1: 信号处理函数，使用者可以通过这个函数为当前进程指定具体信号的 Handler。当信号触发时，系统会调用具体的 Handler 进行对应的逻辑处理。 sigfillset2: 用于操作 signal sets（信号集）的函数之一，这里的含义是将系统所有支持的信号量添加进一个信号集中 fork3: 大家比较熟悉的一个 API 了，创建一个新的进程，并返回 pid 。如果是在父进程中，返回的 pid 是对应子进程的 pid。如果子进程中，pid 为0 execve4: 执行一个特定的可执行文件 sigprocmask5：设置进程的信号屏蔽集。当传入第一个参数为 SIG_BLOCK 时，函数会将当前进程的信号屏蔽集保存在第三个参数传入的信号集变量中，并将当前进程的信号屏蔽集设置为第二个参数传入的信号屏蔽集。当第一个参数为 SIG_SETMASK 时，函数会将当前进程的信号屏蔽集设置为第二个参数设置的值。 wait_pid6: 做一个不精确的概括，回收并释放已终止的子进程的资源。 OK 了解完这样一些关键的 syscall 后，这段代码那么基本上不难理解了。但是要吃透这段代码，我们还需要去复习一下一些 Linux 或者说 POSIX 中的机制： 由 fork 创建出来的子进程，会继承父进程中的很多东西。就本文中聊的信号一部分来说，子进程会继承父进程的信号屏蔽集和信号处理函数的相关设置 execve 执行后，会重设当前进程的程序段与堆栈。所以在上面的代码中我们执行 /bin/date 后，子进程会被重设。信号处理函数等设置也会被重设 每个进程都有信号屏蔽集，在信号屏蔽集中的信号被触发时，会进入一个队列，暂时不会触发进程的信号处理，此时信号处于 pending 状态。在取消对应信号的屏蔽与阻塞后，再次触发进程的信号处理机制。如果进程显式声明忽略信号，那么不会触发信号的处理。（Tips：关于信号队列这一点，这是一个 POSIX 1. 的约定。在 POSIX 中将这种机制称为可靠信号，当阻塞期间，有多个信号发生时，会进入一个可靠队列确保信号能被妥投。 Linux 支持可靠信号，其余 Unix/类 Unix 不一定支持） 子进程退出后，会给所属的父进程传递一个 SIGCHLD1 信号，父进程在接受到这种信号后，需要调用 wait_pid6 函数对子进程进行处理。否则未被回收的子进程，会成为一个僵尸进程，也就是通常说的 Z 进程 OK，到现在，大家在掌握这些东西后，对于上面的代码应该能完整明白了。不过可能大家还有一个疑惑，为什么在这段代码中需要调用 sigprocmask5 设置进程的信号屏蔽集来阻塞信号呢？这涉及到另一个问题。 如前面所说，信号在触发时，进程会”跳转“对应的信号处理函数进行处理。但是信号处理函数处理完后的行为会怎么样呢？依照 Linux 中的设计，可能会出现两种情况 对于可重入函数而言，信号处理函数返回后会继续处理 对于不可重入函数而言，会返回 EINTR1 OK 大家这里应该对我们为什么会在这里使用 sigprocmask5 有具体的了解了，实际上是为了保证我们的一些函数能够正常的执行完，不会被信号处理所打断。当然这里也有其余的问题，如果信号触发特别密集的情况下，这里的处理会带来额外的 cost。所以还是需要根据不同的场景做 trade-off 了。 好了。差不多就这样吧，福报久了真没力气写文章，💊。下一篇文章应该就是我最近做内核协议栈监控的一些吃屎记录了（flag++（逃。 Reference [1]. Linux man page: signal [2]. Linux man page: sigfillset [3]. Linux man page: fork [4]. Linux man page: execve [5]. Linux man page: sigprocmask [6]. Linux man page: waitpid","link":"/posts/2020/10/24/a-simple-introduction-about-signal-process-in-linux/"},{"title":"2018，我，2019，未来","text":"2018，我，2019，未来本来以为年前没有机会发这篇年终总结以及新年展望。不过目前去向已经确定，所以准备还是写一篇文章纪念下2018这一年，赶在农历戊戌年的末尾发一篇文章出来吧。 2018，我2018 的这一年算是很特殊的一年，我的本命年，24岁，第二次有意识的状态下过农历狗年。但是可能因为我一整年都没穿红内裤吧。所以整个人生算是起起伏伏，跌跌撞撞。 原本以为自己可能有很多话想说，但是真正要写的时候却发现不知道从何说起。所以干脆随意记点流水账吧 工作一整年的工作算是有条不紊的进行，去年，阿不，前年一整年都在做基础组件和快照这类基础服务的开发。今年因为搭档出差和转组的关系接触了下业务。虽然期间发生了不少的意外，但是也算是有一个不错的结果。也认识了不少的朋友。 但是最大的收获还是在于对自己的认知发生了改变。曾经以为自己能妥善的处理业务与基础服务的不同场景。过早的把自己定位在一个万金油的角色上，但是就目前看来，当时的认知是错误的。我自己对于业务可能兴趣并不是太大，可能需要去专注的做基础服务的开发学习，才是比较好的出路。 而这样的认知的改变也直接造成了我后续的计划的改变和未来走向的变动 在工作之余，我选择建立了一些私人项目来进行练手，在这样一个从0到1的过程中，进一步确认了我的想法。 不过今年学到最深的一句话还是“做一件事，做一件有结果的事” 不过今年的认知的改变和身体原因直接造成了我选择裸辞在家休息，也直接让我存款降为0，这也是年初没有想到的。 感情11月，在光棍节之前脱单，算是在知乎上认识的一位很优秀的女孩子，山东人，中国音乐学院的钢琴生。 她是一个很优秀的女孩子，至少比我优秀的多。至少我现在都还在思考一个问题：“当时她怎么眼瞎看上我的” 最开始我有点担心，某种意义上讲，我们算是两个不同世界的人，这样凑在一起是不是不太好，会不会缺少话题。但是目前看起来，我们彼此都在适应对方，我会告诉她互联网圈的趣闻，她告诉我艺术圈的趣闻，一起看话剧/音乐剧，一起吐槽分析一起做菜，一起买糖葫芦，目前看来这样的日子很舒服，很幸福。 对了，可能有朋友在担心，她会不会需要我照顾人之类的，实话实说，她是一个很独立的女孩子，也很会照顾我。在离职后两个月里，我几次生病（有几次还比较惨）都是她在照顾我，所以请大家放心啦 至于照片，她说你敢放就打死你，出于男女朋友间和谐生活的角度考虑，我就不单独放啦！ 技术2018年初给自己立了几个目标 《Unix 网络编程卷一》刷一遍 配合 15-213 刷《深入理解计算机系统》 CPython 源码……也争取吃一下吧…… 目前来看，完成度都还不错 UNP 刷了 CSAPP 没有刷，但是刷了 AUP（Unix 高级编程》 CPython 源码刷了一部分，并且作为作业向 CPython 提交两个代码补丁，一个文档补丁 另外还有些奇奇怪怪的收获 作为 PyCon China 2018 的主要参与者，筹办了北京场，成都场 作为讲师参加成都场的活动，做了一次关于黑 asyncio 的演讲 另外再去阅读了一些奇奇怪怪的源码，比如 Redis/Nginx 之类的。也写了点 Golang/Java 服务。 所以整体来看，预期达到并有所超出，但是缺陷也很明显，局限于某一部分分支，而缺乏方法论，系统设计方面的学习，可能这也是一位 HR 对我说我的格局可能还是相对较小的原因所在。 2019，未来在18年11月因为个人规划选择离开上家公司后，经过两个月休息，目前去向已经定了。去饿了么基础框架部 Huskar 组师从张江阁（松鼠奥利奥/tongseek）老师做服务注册治理平台方向的开发。整体薪资和定级出于隐私原因，我就不在这里透露了。只能说比我预期的还要好一些。所以在昨晚得到消息的时候很明确的告诉 HR 选择接受 Offer。 所以未来几年的发展方向应该会是在中间件和分布式服务这一块深入耕耘，同时也会在业余时间折腾不同的东西。 关于2019，首先争取多回家几次，陪陪父母。然后和女朋友好好的走下去，多去玩不同的地方，吃不同的好吃的，看不同音乐剧。 关于职业和技术方面。在我看来这次 Offer 算是一个破例 Offer 了（因为小跳大，而且只有两年经验非科班，不过因为三轮面试官对我的评价都蛮高，所以最后结果让我有一些惊喜）。所以先在饿了么立足，拿一个不错的绩效这会是一个首要的目标，为了我自己，也为了三位面试官不会后悔他们给出的评价。 社区方面，我会持续为 CPython 提交 Patch，争取有天能晋升成为核心开发者。同时我也会全程参与 PyCon China 2019 的筹办，恩，继续去学习一些技术之外的事 其余的话，就是继续补补计算机基础，补一些这次面试暴露出来的薄弱点，然后去按部就班学习新的东西。 结束前天饿了么三面结束后，我给师父说谢谢你带我不断地学习。师父说谢你自己啊白痴。其实年前一个月的面试不管结果如何，我最开心可能还是我终于有资格能认为自己是个合格的开发者了吧。算不上优秀，但是能算及格。这也是市场对我的肯定吧 我问师父，你徒弟没给你丢脸吧？师父说丢什么？你很厉害的。恩所以下一步的目标大概就是朝着有资格说自己是个不错的开发者努力了吧。 这一年，跌跌撞撞的走过来，哭过很多次，也难受过很多次，但是最后算是有个 happy ending，也算是一种幸运了。期间承蒙许多人的照顾，父母，女朋友，师父，leader，同事，我的密友们。至少在我想哭的时候，会有人陪着我。也在不断地支持我去看看更大的世界。 不管2019年这一年，会怎么样，会不会如同18年一样的不顺，我还是得努力让自己的每一天都变得精彩一点，有意义的一点。为我自己，也为所有照顾帮助过我的人。我不想让我和他们失望。 有道是，诗酒趁年华","link":"/posts/2019/01/01/about-me-2019/"},{"title":"但行好事，莫问前程","text":"这篇文章，实际上本应该在 2020 年结束时写完的。不过我是一个拖延症患者，而且写这篇文章时会想起一些已经离去的人，所以一直不愿动笔，拖到了现在。不过在农历新年的末尾，还是得写出来，给自己去过一年一个总结吧。 开篇怎么说呢，2020其实是很操蛋的一年，我无数次的在内心问候”2020，我日你仙人“（山本大佐表示很赞） 但是2020呢，其实又是挺好的一年，让我估计后面会无数次的从2020这一年里学到的东西中受益。 所以吧，我把这暂且称为薛定谔2020？（算了，这 TM 就是 Manjusaka 的2020 工作大年三十了，先说点难过的吧，熟悉我的朋友都知道，我其实2020的职业生涯从职级的角度上来说可以用职场败犬来形容 Yep，没错，我被降级了，而且晋升还他娘挂了。你说我气不气 你要说我心态没有炸实际上是不可能的，在19年调转时刚得知降级结果那段时间心态简直爆炸。当时我老板说了一句印象非常深刻的话”你怎么焦虑成这样了，我记得你当时刚进来时不是这样的啊？“ 在心里接受降级后，开始继续努力搬砖争取晋升回来后，然后，晋升又挂了。我\b？？？？？？ 不过说回来，其实这段经历，可能在刚刚发生那一会儿乃至后续的一段时间，让我一度焦虑狂躁。但是从我现在的视角去看，其实也能反思很多东西。 实际上经历过这一段很特殊的经历后，我自己的心态已经算是相对平和，某种意义上比刚去阿里云时来得更为沉得住气。如同前段时间我给老板说过的一样，现在的我更希望去做一些自己认可的事，而不是纯粹的为了半年后的晋升去做什么事。往往机关算尽，到头却功亏一篑。不值当 嘛，当时在调转之初，我老板送我的一句话，我觉得非常受用： 你现在所经历的事情在当下你看来可能无法接受，但是如果将时间放大到整个一生这几十年的尺度来看，其实可能也不算什么大事了。 \b抛开职级不谈，说实话在阿里云一年多时间里，虽然也经历了客服支持，连续加班等非常惨痛的经历。不过就我而言，可以说还是蛮享受以及这段时间的工作。 在云这边我主要的工作是和同事一起从0开始做了一款公有云上的网关产品。在去年组织结构变动后，又跑来做监控服务了。 在这么样的一个过程中，我的角色从在饿了么时期的一个纯粹的一个对内的 infra RD 的角色切换成了一个云产品的研发，我觉得这样一个角色的转变其实是能让我从一个不同的视角去审视 infra 这件事。如果说在对内时期，你做的东西可以通过一些非技术上的手段去强行 push 落地的话。那么在做云产品的时候，如果你产品无法提供足够 OK 的成本，特性，是不会有客户给你产品买单的（当然，你爸要是客户老板当我没说）。 这样一个观点的转变，让我更能从客户的视角去系统的思考我们做的事的意义。而通过客户支持的经历让我能贴近客户真实的使用场景与业务。我觉得这样一段经历对于我来说是非常重要的。 另外一个方面是我在云这段时间，有足够的场景和驱动力，去对内核，eBPF ，SystemTap 等一些相对冷门和深入的技术进行探究。我觉得也是非常不错的。 不过提到工作我就没法逃避的一个事实（也是我迟迟不想写这篇文章的原因）就是，2020，我失去的我最早期刚进入职场时的领路人，也是我最好的朋友。如果说，我师父在我职业早期送我最重要的礼物是说”学习任何一个东西需要系统化“，那么他送给我的礼物就是正确的职业态度，正确的提问方式等很多很细碎，但是足够让我受益终生的东西。So，RIP &amp;&amp; 2020 我日你仙人*2。 感情和荆澈同学的感情步入了第三年，依旧非常甜蜜。荆澈同学一如既往的照顾我，比如我身上现在的行头都是荆澈同学给我买的，而此文由荆澈同学给我买的 HHKB Hybrid 写出！ 而且我们第一次一起去拍了美美的情侣照！有一说一，情侣照成片拿到手后，我最喜欢干的一件事就是换微信头像Hhhh（逃 不过说实话，很多时候觉得我自己还不算是一个合格的男朋友，很多时候小事和小细节非常不注意（= =我也很绝望）会无意间让她非常不开心，用她吐槽我的话来说，就是”败兴大王“，而且之前自己作死导致19年身体一直时好时坏，也让荆澈同学操了不少心。 所以，有些时候我也在想，荆澈同学没把我扔出去也是神奇Hhhh。不过我自己也定了很多目标，会一点点的改掉自己很多不好的习惯！不过这里突然想引用一下每年给荆澈同学一封信中我经常说的一句话 亲爱的，我感激并享受着你的爱 啊，对了，在年初的时候，和荆澈同学一起，列了不少今年要一起做的事，希望能一起好好的走下去。（再次表白荆澈同学！） 技术嘛，实际上去年年初的时候，因为各种原因没有给自己立下什么 flag，所以大概聊聊自己过去一年多做的一些事吧。 读书方面的话，我自己比较印象深刻的有这样几本 Design Data Intensive Application Kubernetes in Action BPF Performance Tools 然后因为各种原因，复习/新读了几篇论文，比如印象比较深刻的 Cloud Programming Simplified A Berkeley View on Serverless Computing, Maglev A Fast and Reliable Software Network Load Balancer。 社区方面的话，今年一如既往的参与了 PyCon China 2021 的筹办，参与了两次 Meetup 分享。收获了微软的 MVP ，算是完成了一个软粉的夙愿。 整体来看吧，可能输出没有之前多，而且也有点偷懒了，具体原因后面会说。，不过可能还算是一个合格的答卷。希望21年能够继续在技术上勇猛精进吧 生活嘛，实际上生活上这一年多的变化还是蛮大的，首先正儿八经的讲，我没有穿迷彩了！（实际上是荆澈同学看不下去了包办了我全身的行头！） 在年初的时候，家里加入了一个新的成员，一只“憨厚“”老实“”稳重“的红虎斑缅因猫年年，这样一来，我们家就有了四只猫！年年说实话给我们的生活带来了很多的乐趣（除了踩在我肚子上的时候） 而过去一年，在生活上比较大的一个变化是，我第一次意识到了，除了技术和睡觉，生活也很重要。开始时不时的和亲爱的荆澈同学一起打打游戏（一起动森，一起分手厨房），一起看看剧，逛逛淘宝啥的。说实话，非常幸福，这也是我之前从未有过的体验。Hhhhhh 另外一方面，2020年，我做了一件对我来说可能是比较重要的一个事，在17年确诊抑郁后，当时分析成因可能是因为来源于自己小时候被性侵后的 PTSD。所以我选择将我自己的经历不断以文字的形式公开出来。而去年，我选择参与进华师的一个研究，我面向研究者，完整的，深入的直面了当时我被性侵的细节，心里状况，反思及建议 这件事实际上，对于我来说意义也非常重要。我希望能通过自己和其余人的努力，能让国内被性侵儿童心里干涉相关的研究能够进一步发展。这里非常非常谢谢亲爱的荆澈同学对我的支持，几次因为小时候的事极度丧的时候，都是荆澈同学一直在给我拥抱并安慰。而且荆澈同学也非常支持并鼓励我讲自己的经历分享出来。同时我们决定，将自己参加研究所得到的相关收入捐赠给公益基金会。 但行好事，莫问前程嗯，差不多流水帐就这样一些，说实话，2020年，对于我来说其实是很特殊的一年，有过哭泣，有过坚强，想过放弃，但是又继续走了下去 这一年，或者说过去四年，能跌跌撞撞的走到现在，有很多的人想感谢，亲爱的荆澈同学，师父，几位密友，捕蛇者说的搭档，历任的 Leader，推上认识的朋友。感谢你们一路相伴，带我看了更大的世界。然后，其实我想第一次在这里谢谢自己，谢谢自己在无数次想过一了百了之后，还是坚持到了今天。 前两天，我看着我招行账单，上面显示说，我自己的收入比19年高了不少，当时心里百味杂陈，给亲爱的说“我们日子会越过越好的” 嗯，Everything is gonna be OK 如果说四年前，在17年的总结中，我送给自己的话是 “诗酒趁年华”。那么在经历过挫折，反思，成长，可能不算丰富，但是对我来说却意义重大的四年后。我选择送给自己的话，也是这篇记录的标题 但行好事，莫问前程","link":"/posts/2021/02/11/do-anything-you-want/"},{"title":"写在一周年","text":"说实话，突然体会到了老祖宗说的『光阴似箭，日月如梭』的感觉了，感觉告白还在昨天，但是转眼就一年了。所以来写篇文章纪念这短短却有很丰富的一年吧 起初说实话，我现在都没搞清楚是她撩的我，还是我撩的她，估计暂时也不会搞清楚了？ 在去年的11月7号凌晨，当时聊着双十一，然后她突然说，“要是双十一能脱单就好了”，然后我直接没过大脑的来了一句 ”我保佑你啊“，然后你能以肉眼可见的速度感觉到屏幕对面的人不开心了 然后我就纳闷了，这女生咋这么难以琢磨呢？咋这就生气了呢？我说错啥了我？等等，她不会是喜欢我吧？我这么一个沙雕程序员咋会被她看上呢？喵？然后在我反复逼问下她承认了，然后就顺其自然的在一起了 后续她吐槽我： 你咋能拷问一个女孩子喜不喜欢你呢？ 当时你说”我保佑你啊“，我第一反应，这男的白撩了 相处她是一个很棒的钢琴专业的学生，非常棒，而且堪称学霸（偶然间知道她当年高考的时候即便缺席了半年文化课，高考成绩也超一本线50多分，当时就给跪了），而我是一个屌丝学渣程序员，按道理说我们两个人是毫无交集的，所以可能很多人想问，你们两是怎么相处的？ 嘛，首先，我日常被嫌弃这是必然的，然后她有时会傻傻的，然后我也会吐槽她 讲两个故事 某天我给她讲，亲爱的你知道么？我们金牛座超屌的，她惊了：哟，你还信星座的啊？然后我兴致勃勃的给她说，你知道么，人类历史上最早有记录的超新星爆发之一就在金牛座（SN 1054，公元1054年），她无语了十分钟 某天晚上，我哄她去睡觉，我说，亲爱的，快睡了，现在快十二点了，她当时直接来了句”现在日本才11点“，我当时愣了十分钟（喵？？ 好了，其实开个玩笑啦。虽然看起来是两个世界的人，但是我们彼此都还是有不少共同点，同时我们日常也会刻意的去靠近对方的习惯。比如我们会一起去看音乐剧（音乐剧看多了的结果就是钱包抗不住），然后一起吐槽比如法扎那场演员不咋样，比如堂吉诃德的演员有啥瑕疵，当然我也会日常给她普及某些电子产品有多好用，然后给她安利哪些软件比较好用（当然她日常吐槽我：你这个之前就给我说过啦白痴！）（然后我成功把她带入了 RSS 订阅的坑） 当然，要问她为什么能看上我，唯一的理由：她眼瞎（逃 日常其实作为普通人的我们，日常其实也还是如同大部分普通人一样，一部分是温暖与惊喜，一部分是嫌弃与包容吧？ 我自己来说，作为一个程序员，特别是在这两年经历了很多重大的改变，也经历不少的关键时候，同时因为我自己职业的关系，虽不至于996，但是每天清早出门，晚上回家，一天下来精疲力竭也是常态，而且说实话我自己的身体一直欠佳，去年到今年，大大小小的毛病也一堆，在这样一个时候，有一个人陪在你的身边，我觉得这对于我来说意义重大 我之前给她说过，她给我带来最直接改变就是让我对于这个城市有了归属感，每天早晨出门的时候有一个抱抱，晚上回家的时候也有一个抱抱，这样一个颇具有仪式感的行为让我对北京这个城市有了难得归属感。让我曾经在不同的城市之间飘荡的数字游民式的想法彻底消失。 而且她也是一个擅长个给我小惊喜和有着生活情趣的人，无论是在入职之前给我准备的一束花，还是有些时候在家里养的小花，满天星，茉莉，让你觉得家里有了生机，而有时你觉得很你难过的时候，她也会duang的一下跳出来，带着你最爱吃得东西来犒劳你，让你继续安心前行 当然一段感情中，除了温暖与惊喜，就是不断的嫌弃与包容吧 比如我因为工作繁忙+自己实在是懒，日常会把家里搞的很乱，然后她一遍嘟囔着我要和你分手，一遍收拾家里。然后之前她调雪球酒，然后我贪杯多喝了几杯在床上装死狗的时候，她一边嫌弃的骂我是傻狗，一遍隔一会儿来确认下我是否有事 嘛，所以日子，等等，我突然忘了说了，其实我们两人的日常还有一个很重要的组成部分！那就是虐狗！ 啊，每当我在技术群里有意/无意的虐狗后，她总是很担(xin)心(xi)的问，你这样会不会被打啊！我说会，然后她说噢，那我就放心了 233333！ 宠物嘛，宠物是我们日常中很重要的一部分，所以我就开个单节来聊聊 首先来介绍下家里四只猫 小舅子大雄，现居于山东，喜欢唱、抓、rap、捞鱼，目前在背《沁园春.雪》 懂事的大闺女，肚兜，前流浪猫，极为懂事，在她/我身体不舒服的时候，会过去看着，也导致她经常吐槽：”你还没肚兜爱我！“ 叛逆期的外孙女，秋千，颜值波动极大，有时喜欢捣乱 我们当时因为这三只猫结实，我记得我们俩最开始的话题是如何给大雄做绝育，借着给大雄做绝育的契机，我们俩加深了感情，加深了对于彼此的认识，啊，伟大的大雄同志，万岁！ 然后突然想起关于秋千还有一个故事，去年10月，当时做保洁。秋千因为害怕躲在了家里一个未知的角落，回家的我一度以为秋千丢了，哭着满小区找秋千，亲爱的一直在安慰，别哭，别哭，我来陪你找 嗯，这三只猫，某种意义上是我们俩的媒人。嗯，纪念日开罐头 最后一只小猫，对我们来讲，是第一只一起救下来的小猫，嗯在儿童节那天，亲爱的说，这是我收到的最好的儿童节礼物 曾经的点点 现在的点点 睡死了的点点 嘛，这四只猫，分别出现在不同的时候，对于我们有着不一样的意义，当然家里养猫的日常是，当猫偷吃/打碎化妆品后，亲爱的总会对着我抱怨：”看你养的好猫！你赔我”，欲哭无泪.jpg 结束在过去这一年里，很幸运的是虽然大大小小的争端不断，没有吵架。一起养猫，一起看书，一起做陶艺，然后一起走到纪念日。在这一年里面，我们也一起立下了很多的 Flag 和对未来的希望，比如一起去加拿大读书，一起工作赚钱买 Dream House，一起去日本京都 Gap Year，等等。 虽然这句话说过很多次，但是我是还想再说一次：“亲爱的，我感激并享受着你的爱” 爱你亲爱的，一周年快乐！ 啊对了，很多人都会问，你文中的她是谁呀，再次介绍下，她叫荆澈（也是我的花名），山东人，这里两位荆澈一起，在这里向大家问好啦！","link":"/posts/2019/11/04/for-one-year-anniversary/"},{"title":"听说我有女朋友","text":"听说我有女朋友我很喜欢写听说系列文章，从入行开始写的第一篇《听说你会 Python》。所以半年了，我干脆也来写一篇《听说你有女朋友》。为啥要今天写？因为明天她送的新键盘要到了，正好老键盘退役，写点啥纪念下 怎么认识的？很多人都会问我这样的问题：”你一个技术男咋和学音乐的认识了？“，我一般这么回答：”知乎认识的“（请知乎给我广告费，明天之前打到我支付宝上 好了，正经说，我们的确算是在知乎上认识的（广告费+1）。当时我们在一个知乎群里，然后后面加上了好友。 很多时候，我这个人有个毛病，加了女孩子之后不怎么聊的。不过幸好，我们最开始有个共同的爱好，养猫。她家的是一只橘猫。我家是母女两。某种意义上讲，这三只猫是我们俩的媒人（回去加罐头） 正式开始聊起来，应该是开学的时候，她当时睡眠不太好。某天晚上，睡前戏称保佑她肯定睡得好之后。第二天她说昨晚居然还真睡得不错（直男如我我现在都不知道我的功力是不是有那么强。问她她也不说，摔） 借着这个契机，就开始聊了起来，包括不限于互相点外卖犒劳对方上课/上班的无奈。出来约了两次火锅（直男撩妹法） 11月初，当时做完18年的 PyCon，她说要送我一份礼物。找个周一兴高采烈的约了井格，拿到了第一份礼物，一条爱马仕的领带和领带夹。花纹是我最喜欢的樱花。然而我现在都还没一个合适的机会用上这条领带。所以请各位大佬行行好。如果有什么可以穿正装的场合。包括不仅限于伴郎，主持，放贷，打架，搞传销等，请务必通知我一声。 说道怎么正式在一起的。说起来也足够的戏剧性和充满了直男的无奈（她现在都还会嫌弃我） 11月6号晚上，当时突然聊到双十一的话题。她说希望这个月底能找到男朋友。我当时没有过大脑，直接说，我奶你没问题！瞬间这话一出口，能明显她感觉不开心了。我当时就在纳闷，不会吧，她不会是喜欢我吧？恩？不可能吧？于是我就正儿八经的向她反复询问这个问题。然后她承认：“没错，我是挺喜欢你的”（后来被她吐槽你咋能反复问女孩子喜不喜欢你呢！）（直男如我）。于是就这么在一起了 看完这一套下来，大家是不是觉得，我这样的人怎么都能找到女朋友呢？如果非要问我为什么能找到女朋友的话，我给大家分享三个原则： 她足够瞎 她足够瞎 她足够瞎 噢，对了，忘了向大家介绍她了，真名就不告诉大家了。她艺名叫荆澈（这也是我的花名），山东人，有很多身份啦，比如李者璈同志的女朋友，大三的钢琴生等等。总之携她向大家问好，请多指教啦！ 恋爱日常恋爱什么感觉呢？荆澈同学（非我）有一段比较好的总结：“和你谈恋爱，感觉就像养了一只大狗，这狗还贼会撒娇” 给大家分享一下我们的恋爱日常 第一次去电影院，当时在无名之辈和无敌破坏王之间纠结。她说去电影院看动画片么？于是我记着这句话了。后续，某次又要去看电影的时候，在海王和蜘蛛侠平行宇宙之间纠结。当时贼想看海王，于是我说蜘蛛侠是动画片！她说好吧，看海王。后续她吐槽，哼，我早就知道你想看海王了，当我不知道是吧。 当时她之前在参加校园卡设计大赛获奖后，某天晚上突然给我说，我奖金，一半给我买了个神仙水，一半给你买了一套海盗船键鼠（然而因为学校的坑爹，我明天才能拿到）好不好？我当时：哇！开心！等等你为啥要鼓励我玩游戏？ 某天去看莎翁的第十二夜，我最开始不习惯，然后差点睡着，荆澈同学一直在旁边捏我。 还是去看莎翁的第十二夜，当时突然想起我们买了一月份的《我，堂吉诃德》的话剧票。然后突然我问她。我们是不是要去看堂诃吉德？她一脸嫌弃，你这个文盲，是堂吉诃德！嫌弃！当时我一脸委屈的马上去京东下单了一套《堂吉诃德》（现在也没看）。后续她感叹，你这个人咋这么容易委屈呢？ 11月裸辞后，一直在家休息，在拿了饿了么的 offer 消息后，给她说，她表示：早知道了。我说，窝草？她说，哼我可是梦见自己中考分数的女人！怎么样！棒不棒。我：棒棒棒，半仙，，半仙 两个人都喜欢吃糖葫芦，草莓的。有次一起去看新房子（租的），然后将草莓糖葫芦放在新屋子里忘了拿！ 某天晚上，你要是不给我吃糖葫芦我就通宵！她：哦，熬得住你就熬吧（这是亲女朋友么 入职之前，荆澈同学问要不要送一束花给我，我说要啊！然后她一脸无辜的问，要是你因为虐狗被开了怎么办！我一脸正气：不允许员工虐狗的公司不是好公司。第二周周一，刚到办公室，收到一束花(事实证明，这束花来的真及时) 荆澈同学回调酒，某天晚上她调了一杯雪球，我给你讲，贼好喝，当时贪嘴，喝的多了点，然后晚上就在床上装死狗。荆澈同学一脸无奈的说：以后你不准喝酒！一滴酒也不能喝！ 2月份，北京下雪，当时我没忍住，晚上出去吃饭的时候去雪地里滚了一圈。然后把手机搞丢了。第二天从派出所捡回手机。荆澈同学一脸无奈：你以后还滚不滚了？ 3月6号，下班回家。回家一看，哇一个大大的蛋糕，然后荆澈同学一脸开心的蹦出来！今天120天啦！一起庆祝一下！ 在荆澈同学回学校之前，每天下班回家都会有个抱抱（出门也有的（回学校就没了（哭 恋爱总结其实很多人都会好奇，你们完全是两个世界的人，怎么会在一起呢？说实话，起初我也担心这个问题。但是其实发现。自己很多的顾虑也是多余。恋爱是一个相互磨合与了解的过程。我们会一起去看剧，一起吐槽，给对方讲我们各自领域的故事，会分享各自的心情与家庭。当然我们也会日常打闹，互黑。 某种意义上来讲，恋爱是一个相互认可，相互成就的过程。有些时候，我笑称她眼瞎才遇上我。她会很认真的给我说，我才不瞎呢。所以很庆幸我自己能遇到这样一个可爱，知性，懂事的女孩子。我也会和她一起好好的经营这一段感情。 最后的最后，喂了这么多狗粮。还是要收尾一下的。祝每一个看这篇文章的人，如果你已经有自己的爱人，那么祝福你和你的爱人一切平安喜乐。如果你还没有遇见那个人，那么请不要着急，请相信，在不远的一天，你终将找到你所爱之人。 （另外，这篇文章的封面，就是我亲爱的荆澈同学啦！（","link":"/posts/2019/01/01/i-have-girlfriend/"},{"title":"写在黎明之前","text":"年少之成绩，或有时而可商，年少之作为，或有时而可讨，为此独立之精神，与自由之思想，纵历百十年整，亦与沱江水长流，共三光而永光 其实突然发现，没写过点什么特殊的文字，来纪念自己这一年。 今天，2月10日，距离离职完毕还有五天，距离离开成都还有11天，距离入职，还有17天。坐在自己屋子里，看着窗外的云和雨雾，慢慢的，用一点东西，来纪念自己过去的一年。 云起 Hey man, think about this world and go fuck it 16年，一开年，便遭受了一个算是影响未来的挫折吧。现在想想，从1年6个月的感情里脱身出来并不是一件很容易的事情。更何况，遇到太多的事情让我更为烦心。 其实在大学尾声的时候，来回顾自己这四年，可能也更多的算是一个 loser 吧，竞赛失败。专业课挂科四分之一，延期毕业。不过心高气傲的我，还是不喜欢自己被同为一群 loser 的人所鄙视。这种感觉不是太好。 在师父的教育下，慢慢的入了 Python 的坑，准备在毕业前找到一个能混口饭吃的工作。所谓初生牛犊不怕虎，在什么都还懵懂的时候，第一场面试的结果就给我了当头一棒。不过到现在，依然很感谢当时面试官给我的灌输的一些东西。可能这样的一些东西，让我明白了真正的程序猿所应该拥有的态度与责任吧。 所幸，第二份工作，平安的通过并入职。恩，这算是一个，还算不错的开端吧。 暗云 当激情与新鲜感退却之后，生活能剩下的只能是坚守。 在入职以后，可能离开了前面一段时间的新鲜期，便跟之前一样，进入了朝 9 晚 2 的生活。可能在自己以为自己会这样过下去的时候。认识了小天，猴子，和一群 Android 老司机（你说我一个写 Python 的，怎么就开始天天和 Android 的人开始搞基了呢？），然后去了掘金，开始以一个四级没过的人的身份，去翻译一点感兴趣的技术文章。然后和一群小伙伴们借着大佬后宫团的名义聚集在了一起。 如果要描述这段时光的话，应该就像现在窗外厚厚的云一样。表面四平八和，实则惊心动魄。看不到阳光的方向，却也只能随着大潮不断的前行。不知道自己的未来是什么样的，心有不甘，却也只能一步步的坚持走下去。不过，始终告诉自己，走下去。无数次的哭过，但也未曾放弃过。 其实很想感谢身边的人，公司的爽姐，涛哥，论道的朋友们，童童后宫团 12 位小伙伴们（童童，艳辉姐，羊哥，鳗鱼，大叔，盖伦哥，波波，田田，老叶，老柯，五月天，雪梨姐），在云层中看不到阳光与未来之时，这些人，用他们的宠爱和优秀，让我至少不会在随波逐流中迷失了自己的方向。 对了，如我所料，学位证延期了。 然后某人的死讯，真的是一条最好的消息。 疑云 为何而生，为何而战，为何而前行。 和优秀的人相处久了，你总是不由自主的想去变得和他们一样的优秀。长久的遗传下来的自卑心却告诉自己，这些东西都离我太过遥远。心中的不安，不甘，自卑，狂妄交织在一起，产生出了一种难以明说的滋味。 其实这样一段时间，体验到了一个词的含义“悲喜参半”，悲的话，其实主要算是可惜吧，那段时间多愁善感，用朋友的话来说，你真成了一贱人的（不过我啥时候不是？）。喜得话应该算是再这样一种不断的挣扎于交集中，真正确定了自己想做什么，该做什么。 对了，最后成功的拿到了学位证。 云开 在黑暗中飞翔的鸟儿，终将会得到主救赎，自由的沐浴在阳光下 拿到学位证之后，想出去看看更大的世界的心变得越发的狂野。写简历，投简历，找内推。一场场面试下来，可以说是体力与精神的双重考验。 不过在面试中，最大的收获，算是对自己的肯定吧。面试的内容和结果都告诉自己，自己貌似，也成为了，一个还算不错的新人？恩，长久被自卑覆盖的心，在这一段时间里体会了阳光的温暖。可能以后，不会在自卑中沉沦了呢？ 恩，最后，很庆幸能遇到一些愿意给我机会的面试官，这样一种特殊的恩赐，算是，命运送给我，最好的新年礼物吧。 最后还有十天就离开成都了，还有很多想说，还有很多不知道怎么说。对未来依旧一无所知，但是会很少再去迷茫。 休对故人思故国，且将新火试新茶。诗酒趁年华。恩，诗酒趁年华！","link":"/posts/2017/02/10/it-s-my-way/"},{"title":"Leetcode BiWeekly Contest 19 题解","text":"例行 Leetcode 周赛，这周双周赛，两场赛打下来，有点酸爽，先写个 BiWeekly 19 Contest 的题解吧 1342. Number of Steps to Reduce a Number to Zer题面： Given a non-negative integer num, return the number of steps to reduce it to zero. If the current number is even, you have to divide it by 2, otherwise, you have to subtract 1 from it. 示例： 123456789Input: num = 14Output: 6Explanation: Step 1) 14 is even; divide by 2 and obtain 7. Step 2) 7 is odd; subtract 1 and obtain 6. Step 3) 6 is even; divide by 2 and obtain 3. Step 4) 3 is odd; subtract 1 and obtain 2. Step 5) 2 is even; divide by 2 and obtain 1. Step 6) 1 is odd; subtract 1 and obtain 0. 这个题题面很简单，一个非负整数，偶数除2，奇数减1，求需要多少步到0 暴力写 12345678910111213class Solution: def numberOfSteps(self, num: int) -&gt; int: count = 0 if num == 0: return count result = num while result &gt; 1: if result % 2 == 0: result = int(result / 2) else: result -= 1 count += 1 return count + 1 Number of Sub-arrays of Size K and Average Greater than or Equal to Threshold题面： Given an array of integers arr and two integers k and threshold.Return the number of sub-arrays of size k and average greater than or equal to threshold. 示例： 123Input: arr = [2,2,2,2,5,5,5,8], k = 3, threshold = 4Output: 3Explanation: Sub-arrays [2,5,5],[5,5,5] and [5,5,8] have averages 4, 5 and 6 respectively. All other sub-arrays of size 3 have averages less than 4 (the threshold) 给定一个数组和一个长度 k，和一个阈值 threshold ，求这个数组中的所有长度为 K 且平均数大于等于阈值的子数组的个数。这个题，暴力写很简单，一个简单的数组的拆分，sum(arr[i:i+k])/k &gt;= threshold 即可，但是这里有个问题，如果实时求和，那么时间复杂度为 O(M*K) M 为数组的长度，这个时候暴力会 T 因此需要做个小技巧的优化。可以考虑这样这样一个做法，假设当前 i 及其后 k 个数的和为 sum[i]，那么有这样一个公式，sum[i]=sum[i-1]-arr[i]+arr[i+k-1]，这样每次计算和都是 O(1) 的复杂度，那么整体就是一个 O(N) 的做法 好了，暴力开写 12345678910111213141516171819202122from typing import Listclass Solution: def numOfSubarrays(self, arr: List[int], k: int, threshold: int) -&gt; int: if not arr: return 0 length = len(arr) sum_threshold = [0.0] * length count = 0 last_index = length for i in range(length - k, -1, -1): if i == length - k: total_sum = sum(arr[i:i + k]) else: total_sum = sum_threshold[i + 1] - arr[last_index] + arr[i] sum_threshold[i] = total_sum if total_sum / k &gt;= threshold: count += 1 last_index -= 1 return count 1344. Angle Between Hands of a Clock题面： Given two numbers, hour and minutes. Return the smaller angle (in sexagesimal units) formed between the hour and the minute hand. 示例： 12Input: hour = 12, minutes = 30Output: 165 求某个时刻，时针与分针的夹角，，，啊，，我的上帝呀，一度梦回小升初。。。一个数学题，首先科普如下知识 普通钟表相当于圆，其时针或分针走一圈均相当于走过360°角； 钟表上的每一个大格（时针的一小时或分针的5分钟）对应的角度是：360°/12=30°； 时针每走过1分钟对应的角度应为：360°/(12*60)=0.5°； 分针每走过1分钟对应的角度应为：360°/60=6°。 好了，那么就暴力做吧 12345class Solution: def angleClock(self, hour: int, minutes: int) -&gt; float: hour %= 12 result = abs((minutes * 6) - (hour * 30 + minutes * 0.5)) return result if result &lt; 360 else 360 - result 1345. Jump Game IV题面： Given an array of integers arr, you are initially positioned at the first index of the array.In one step you can jump from index i to index: i + 1 where: i + 1 &lt; arr.length. i - 1 where: i - 1 &gt;= 0. j where: arr[i] == arr[j] and i != j.Return the minimum number of steps to reach the last index of the array.Notice that you can not jump outside of the array at any time. 示例： 123Input: arr = [100,-23,-23,404,100,23,23,23,3,404]Output: 3Explanation: You need three jumps from index 0 --&gt; 4 --&gt; 3 --&gt; 9. Note that index 9 is the last index of the array. 还是跳格子，给定一个数组，里面会有一些具体的值，现在从 index = 0 的地方起跳，跳跃规则如下： 在 i+1 或 i-1 都在数组的范围内 如果存在 index=j 且 arr[i]==arr[j] 且 i!=j 的时候，可以直接从 i 跳到 j 求从 index=0 跳到 index=arr.length-1 最小的次数 这题我还是没 A，后面琢磨了下，一个搜索的题目 构建一个字典，值为key，index 为 value（相同的值之间可以直接跳） 利用一个 set 来保存跳过的点 从 index = 0 开始进行 BFS ，求每个点在一步之内可以跳到哪个点，然后不断的 BFS 直到到达终点 更新被访问过的点 emmmm，好吧 BFS ，开写吧 123456789101112131415161718192021222324252627282930313233from typing import List, Setimport collectionsclass Solution: def minJumps(self, arr: List[int]) -&gt; int: length = len(arr) if not arr or length == 1: return 0 value_index = collections.defaultdict(list) for index, value in enumerate(arr): value_index[value].append(index) visited: Set[int] = set() traversal_queue = collections.deque([(0, 0)]) result = 0 while traversal_queue: next_step_queue = collections.deque() for _ in range(len(traversal_queue)): cur_index, cur_step = traversal_queue.popleft() cur_value = arr[cur_index] visited.add(cur_index) for next_index in [cur_index + 1, cur_index - 1] + value_index[ cur_value ]: if (length &gt; next_index &gt;= 0) and (next_index not in visited): if next_index == length - 2: return cur_step + 2 if next_index == length - 1: return cur_step + 1 next_step_queue.append((next_index, cur_step + 1)) del value_index[cur_value] traversal_queue = next_step_queue return result 总结这周的题，还是不难，但是需要小心，比如第二题我太大意直接暴力吃了一发T，然后第三题没仔细读题（求最小的度数）吃了一发 WA，不过和第二天周赛比起来，真的是幸福，175 第三题的题面直接让心态崩了，，明天写题解。 好了，滚去睡觉","link":"/posts/2020/02/11/leetcode-biweekly-contest-19/"},{"title":"Leetcode Weekly Contest 174 题解","text":"最近因为生病好久没刷题，今早开始打了一场 Leetcode 的周赛，来写个题解，今早状态还行，，BTW 以后每周都会打周赛，争取写题解 Leetcode 1341. The K Weakest Rows in a Matrix描述： Given a m * n matrix mat of ones (representing soldiers) and zeros (representing civilians), return the indexes of the k weakest rows in the matrix ordered from the weakest to the strongest.A row i is weaker than row j, if the number of soldiers in row i is less than the number of soldiers in row j, or they have the same number of soldiers but i is less than j. Soldiers are always stand in the frontier of a row, that is, always ones may appear first and then zeros. Example 1: 12345678910111213141516Input: mat = [[1,1,0,0,0], [1,1,1,1,0], [1,0,0,0,0], [1,1,0,0,0], [1,1,1,1,1]], k = 3Output: [2,0,3]Explanation: The number of soldiers for each row is: row 0 -&gt; 2 row 1 -&gt; 4 row 2 -&gt; 1 row 3 -&gt; 2 row 4 -&gt; 5 Rows ordered from the weakest to the strongest are [2,0,3,1,4] 题面很简单，其实这道题就是二进制的处理，Python 里面就暴力出奇迹了 123456789101112from typing import Listclass Solution: def kWeakestRows(self, mat: List[List[int]], k: int) -&gt; List[int]: if not mat: return [] number = [] for i in range(len(mat)): number.append((int(&quot;&quot;.join([str(x) for x in mat[i]]), 2), i)) number.sort() return [x for _, x in number[0:k]] 1342. Reduce Array Size to The Half描述： Given an array arr. You can choose a set of integers and remove all the occurrences of these integers in the array.Return the minimum size of the set so that at least half of the integers of the array are removed. 12345Input: arr = [3,3,3,3,5,5,5,2,2,7]Output: 2Explanation: Choosing {3,7} will make the new array [5,5,5,2,2] which has size 5 (i.e equal to half of the size of the old array).Possible sets of size 2 are {3,5},{3,2},{5,2}.Choosing set {2,7} is not possible as it will make the new array [3,3,3,3,5,5,5] which has size greater than half of the size of the old array. 这个题题面也很简单，给定一个数组，选择一组数字移除，被移除后的数组数量小于等于之前的一半，求最少选择多少数字能达到要求 哈希表，O(N) 的做法 12345678910111213141516171819from typing import Listclass Solution: def minSetSize(self, arr: List[int]) -&gt; int: if not arr: return 0 counter = {} for i in arr: counter[i] = counter.setdefault(i, 0) + 1 counter = {k: v for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)} total_count = 0 result_count = 0 for i, count in counter.items(): total_count += count result_count += 1 if total_count &gt;= len(arr) / 2: break return result_count 1343. Maximum Product of Splitted Binary Tree描述： Given a binary tree root. Split the binary tree into two subtrees by removing 1 edge such that the product of the sums of the subtrees are maximized.Since the answer may be too large, return it modulo 10^9 + 7. Example 1: 123Input: root = [1,2,3,4,5,6]Output: 110Explanation: Remove the red edge and get 2 binary trees with sum 11 and 10. Their product is 110 (11*10) 这个题的题面也很简单，给定一个带值的二叉树，移除某个二叉树的边，使之分割成为两个新的二叉树，求两个二叉树和的乘积最大 最开始很多人会被这道题唬到，但是实际上这道题就是一个二叉树的遍历，无论前中后序遍历，先遍历一次二叉树，求出二叉树节点值的总和，以及每个节点的左子树的和 left_sum 以及右子树的总和 right_sum 然后再次遍历，result=max((total_sum-left_sum)*left_sum),(total_sum-right_sum)*right_sum),result) 暴力求解即可 123456789101112131415161718192021222324252627282930313233class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def maxProduct(self, root: TreeNode) -&gt; int: total_sum = self.sum_node(root) result = 0 stack = [] node = root while node or stack: while node: stack.append(node) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) result = max(result, ((total_sum - node.right_sum) * node.right_sum)) node = node.left node = stack.pop() node = node.right if node: result = max(result, ((total_sum - node.right_sum) * node.right_sum)) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) return result % (10 ** 9 + 7) def sum_node(self, root: TreeNode) -&gt; int: if not root: return 0 left_sum = self.sum_node(root.left) right_sum = self.sum_node(root.right) root.left_sum = left_sum root.right_sum = right_sum return left_sum + right_sum + root.val BTW 这段代码的 type hint 使用其实有点问题，我后面比赛完了改了一版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869from typing import Optional, Tuple, Listclass TreeNode: val: int left: Optional[&quot;TreeNode&quot;] right: Optional[&quot;TreeNode&quot;] def __init__(self, x): self.val = x self.left = None self.right = Noneclass TreeNodeWithSum: val: int left: Optional[&quot;TreeNodeWithSum&quot;] right: Optional[&quot;TreeNodeWithSum&quot;] left_sum: int right_sum: int def __init__( self, x: int, left: Optional[&quot;TreeNodeWithSum&quot;], right: Optional[&quot;TreeNodeWithSum&quot;], left_sum: int, right_sum: int, ): self.val = x self.left = left self.right = right self.left_sum = left_sum self.right_sum = right_sumclass Solution: def maxProduct(self, root: TreeNode) -&gt; int: total_sum,new_root = self.sum_node(root) result = 0 stack:List[TreeNodeWithSum] = [] node = new_root while node or stack: while node: stack.append(node) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) result = max(result, ((total_sum - node.right_sum) * node.right_sum)) node = node.left node = stack.pop() node = node.right if node: result = max(result, ((total_sum - node.right_sum) * node.right_sum)) result = max(result, ((total_sum - node.left_sum) * node.left_sum)) return result % (10 ** 9 + 7) def sum_node( self, root: Optional[TreeNode] ) -&gt; Tuple[int, Optional[TreeNodeWithSum]]: if not root: return 0, None left_sum, new_left_node = self.sum_node(root.left) right_sum, new_right_node = self.sum_node(root.right) return ( left_sum + right_sum + root.val, TreeNodeWithSum( root.val, new_left_node, new_right_node, left_sum, right_sum ), ) BTW，这道题因为数据太大，需要对 10^9+7 取模，我智障的忘了取模，WA 了两次，罚时罚哭。。。我真的太菜了。。 1344. Jump Game V描述： Given an array of integers arr and an integer d. In one step you can jump from index i to index:i + x where: i + x &lt; arr.length and 0 &lt; x &lt;= d.i - x where: i - x &gt;= 0 and 0 &lt; x &lt;= d.In addition, you can only jump from index i to index j if arr[i] &gt; arr[j] and arr[i] &gt; arr[k] for all indices k between i and j (More formally min(i, j) &lt; k &lt; max(i, j)).You can choose any index of the array and start jumping. Return the maximum number of indices you can visit.Notice that you can not jump outside of the array at any time. 12345Input: arr = [6,4,14,6,8,13,9,7,10,6,12], d = 2Output: 4Explanation: You can start at index 10. You can jump 10 --&gt; 8 --&gt; 6 --&gt; 7 as shown.Note that if you start at index 6 you can only jump to index 7. You cannot jump to index 5 because 13 &gt; 9. You cannot jump to index 4 because index 5 is between index 4 and 6 and 13 &gt; 9.Similarly You cannot jump from index 3 to index 2 or index 1. 这题的题面是这样，一个数组，里面有若干值，你可以从任意一个位置开始跳跃，一次只能跳一个，跳的时候需要满足规则，假定你从数组 i 位置起跳，每次可跳的范围是 x，那么你需要满足 i+x &lt; arr.length 和 0&lt;x&lt;=d i-x &gt;=0 和 0&lt;x&lt;=d 同时假设你从 i 跳往 j，那么你需要保证 arr[i]&gt;arr[j] 且 i 到 j 中的每个元素都满足 arr[j]&lt;x&lt;arr[i]，求最多能跳多少个元素 最开始觉得这题是一个双头 DP 的题，嫌写起来恶心就懒得写，，但是后面比赛完了发现其实这个题其实单 DP 就能解决的，因为我们只能从高往低跳，于是我们可以先将元素排序后依次遍历，可以得出公式为 dp[i]=max(dp[i]+dp[j]+1) 其中 j 是从 i 起可以到达的索引值，DP 部分的复杂度为 O(DN) 但是因为需要提前排序，因此整体的时间复杂度为 O(logN+DN) 123456789101112131415from typing import Listclass Solution: def maxJumps(self, arr: List[int], d: int) -&gt; int: length = len(arr) dp = [1] * length for a, i in sorted([a, i] for i, a in enumerate(arr)): for di in [-1, 1]: for j in range(i + di, i + d * di + di, di): if not (0 &lt;= j &lt; length and arr[j] &lt; arr[i]): break dp[i] = max(dp[i], dp[j] + 1) return max(dp) 总结很久没刷题了，手还是有点生，在前面几个签到题上花了时间，，而且犯了低级错误，，所以以后一定要坚持刷题了。。BTW 这次的周赛题感觉都很简单，感觉像是被泄题后找的 Backup，好了就先这样吧，我继续卧床养病了。。","link":"/posts/2020/02/02/leetcode-weekly-contest-174/"},{"title":"Leetcode Weekly Contest 176 题解","text":"emmmm，我的拖延症没救了，顺便加上这周沉迷 Kotlin ，这篇本应该周一就写完的题解拖到现在，= =然而这周双周赛，，我又得写两篇题解了。。。 1351. Count Negative Numbers in a Sorted Matrix题面： Given a m * n matrix grid which is sorted in non-increasing order both row-wise and column-wise.Return the number of negative numbers in grid. 示例： 123Input: grid = [[4,3,2,-1],[3,2,1,-1],[1,1,-1,-2],[-1,-1,-2,-3]]Output: 8Explanation: There are 8 negatives number in the matrix. 题面很简单，给定一个矩阵，矩阵横/纵向都是递减的，求这个矩阵中负数的个数，这个题，因为横/纵向的数据规模都是小于100的，那就没啥说的了，，直接暴力，横向遍历，然后遇到负数就停止遍历 123456789101112131415from typing import Listclass Solution: def countNegatives(self, grid: List[List[int]]) -&gt; int: if not grid: return 0 n_length = len(grid[0]) result = 0 for item in grid: for i in range(n_length): if item[i] &lt; 0: result += n_length - i break return result 1352. Product of the Last K Numbers题面: 12345678910Implement the class ProductOfNumbers that supports two methods:1. add(int num)Adds the number num to the back of the current list of numbers.2. getProduct(int k)Returns the product of the last k numbers in the current list.You can assume that always the current list has at least k numbers.At any time, the product of any contiguous sequence of numbers will fit into a single 32-bit integer without overflowing. 示例 12345678910111213141516171819Input[&quot;ProductOfNumbers&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;getProduct&quot;,&quot;getProduct&quot;,&quot;getProduct&quot;,&quot;add&quot;,&quot;getProduct&quot;][[],[3],[0],[2],[5],[4],[2],[3],[4],[8],[2]]Output[null,null,null,null,null,null,20,40,0,null,32]ExplanationProductOfNumbers productOfNumbers = new ProductOfNumbers();productOfNumbers.add(3); // [3]productOfNumbers.add(0); // [3,0]productOfNumbers.add(2); // [3,0,2]productOfNumbers.add(5); // [3,0,2,5]productOfNumbers.add(4); // [3,0,2,5,4]productOfNumbers.getProduct(2); // return 20. The product of the last 2 numbers is 5 * 4 = 20productOfNumbers.getProduct(3); // return 40. The product of the last 3 numbers is 2 * 5 * 4 = 40productOfNumbers.getProduct(4); // return 0. The product of the last 4 numbers is 0 * 2 * 5 * 4 = 0productOfNumbers.add(8); // [3,0,2,5,4,8]productOfNumbers.getProduct(2); // return 32. The product of the last 2 numbers is 4 * 8 = 32 题面很简单，设计一个数据结构，提供一个 add 方法，让用户能够往里面添加速度，提供一个 getProduct 方法，让用户能求倒数K个数的乘积，这题没啥好说的，直接暴力写，中间加个 hashmap 作为缓存 1234567891011121314151617181920212223242526272829303132333435363738394041from typing import List, Dictimport bisectfrom operator import mulfrom functools import reduceclass ProductOfNumbers: _value: List[int] _cache_result: Dict[int, int] _cache_index: List[int] def __init__(self): self._value = [] self._cache_result = {} self._cache_index = [] def add(self, num: int) -&gt; None: self._value.append(num) self._cache_index.clear() self._cache_result.clear() def getProduct(self, k: int) -&gt; int: if k in self._cache_result: return self._cache_result[k] cache_index = bisect.bisect(self._cache_index, k) - 1 if cache_index &gt;= 0: last_k = self._cache_index[cache_index] result = self._cache_result[last_k] for i in range(1, cache_index + 1): temp_last_k = last_k + i if temp_last_k &gt;= len(self._value): break result *= self._value[-last_k] else: temp_value = ( self._value[-1 : -k - 1 : -1] if k &lt;= len(self._value) else self._value ) result = reduce(mul, temp_value, 1) bisect.bisect_left(self._cache_index, k) self._cache_result[k] = result return result 1353. Maximum Number of Events That Can Be Attended题面： Given an array of events where events[i] = [startDayi, endDayi]. Every event i starts at startDayi and ends at endDayi.You can attend an event i at any day d where startTimei &lt;= d &lt;= endTimei. Notice that you can only attend one event at any time d.Return the maximum number of events you can attend. 示例 1234567Input: events = [[1,2],[2,3],[3,4]]Output: 3Explanation: You can attend all the three events.One way to attend them all is as shown.Attend the first event on day 1.Attend the second event on day 2.Attend the third event on day 3. 给定一个数组，数组中每个元素 x 代表一个活动，x[0], x[i] 代表该活动的起始与结束时间，一个用户一天只能参加一个活动，求用户最多能参加多少个活动。经典的一个贪心题目，首先对活动列表以结束时间进行排序，然后依次遍历每个时间，确认具体哪一天可以参加，整体时间复杂度为 O(max(nlogn,n*m)) 12345678910111213141516171819202122from typing import List, Dictclass Solution: def maxEvents(self, events: List[List[int]]) -&gt; int: if not events: return 0 events_size = len(events) if events_size == 1: return 1 events = sorted(events) _day_map: Dict[str, bool] = {} _event_map: Dict[int, bool] = {} count = 0 for i in range(events_size): for j in range(events[i][0], events[i][1]+1): temp = &quot;{}&quot;.format(j) if temp not in _day_map and i not in _event_map: count += 1 _day_map[temp] = True _event_map[i] = True return count 1354. Construct Target Array With Multiple Sums题面 Given an array of integers target. From a starting array, A consisting of all 1’s, you may perform the following procedure : let x be the sum of all elements currently in your array. choose index i, such that 0 &lt;= i &lt; target.size and set the value of A at index i to x. You may repeat this procedure as many times as needed.Return True if it is possible to construct the target array from A otherwise return False. 示例： 1234567Input: target = [9,3,5]Output: trueExplanation: Start with [1, 1, 1] [1, 1, 1], sum = 3 choose index 1[1, 3, 1], sum = 5 choose index 2[1, 3, 5], sum = 9 choose index 0[9, 3, 5] Done 这题算是一个数学题吧，我们首先知道数组中所有的元素的和一定大于数组中每个元素（这不是废话），然后我们假定有这样一个数组 [1,1,9,17,63] ，我们可以往回迭代上一个数组结构是 [1,1,9.17,33] ，然后我们还可以向前迭代一次 [1,1,9,17,5] 然后当前的数字已经不再是数组中最大的数字，于是我们开始寻找下一个数组中最大的数字进行迭代 这里我们也可以发现，数组中最大数字的最原始版本的值是当前数字对其余数字的和的模，于是我们就这样一直迭代就 OK 了 好了，上代码 123456789101112131415161718192021import heapqfrom typing import Listclass Solution: def isPossible(self, target: List[int]) -&gt; bool: if not target: return False total = sum(target) target = sorted([-x for x in target]) heapq.heapify(target) while True: a = -heapq.heappop(target) total -= a if a == 1 or total == 1: return True if a &lt; total or total == 0 or a % total == 0: return False a %= total total += a heapq.heappush(target, -a) 总结这次的题还是周赛的常规水平，然而我刷题实在是太少了QAQ","link":"/posts/2020/02/23/leetcode-weekly-contest-176/"},{"title":"日常辣鸡水文:关于 logging 的进程安全问题","text":"日常辣鸡水文:关于 logging 的进程安全问题团队聚餐喝了点酒，作为一个垃圾文档工程师来写一篇日常水文 正文现在团队的日志搜集方式从原本的 TCP 直传 logstash 的方式改进为写入一个单文件后，改用 FileBeat 来作为日志搜集的前端。但是这样时常带来一个问题，即日志丢失 嗯，我们线上服务是 Gunicorn 启用多个 Worker 来处理的。这就有个问题了，我们都知道，logging 模块是 Thread Safe 的，在标准的 Log Handler 内部加了一系列锁来确保线程安全，但是 logging 直写文件是不是进程安全的呢？ 分析我们写文件的方式是用的是 logging 模块中自带的 FileHandler ，首先看看它源码吧 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class FileHandler(StreamHandler): &quot;&quot;&quot; A handler class which writes formatted logging records to disk files. &quot;&quot;&quot; def __init__(self, filename, mode='a', encoding=None, delay=False): &quot;&quot;&quot; Open the specified file and use it as the stream for logging. &quot;&quot;&quot; # Issue #27493: add support for Path objects to be passed in filename = os.fspath(filename) #keep the absolute path, otherwise derived classes which use this #may come a cropper when the current directory changes self.baseFilename = os.path.abspath(filename) self.mode = mode self.encoding = encoding self.delay = delay if delay: #We don't open the stream, but we still need to call the #Handler constructor to set level, formatter, lock etc. Handler.__init__(self) self.stream = None else: StreamHandler.__init__(self, self._open()) def close(self): &quot;&quot;&quot; Closes the stream. &quot;&quot;&quot; self.acquire() try: try: if self.stream: try: self.flush() finally: stream = self.stream self.stream = None if hasattr(stream, &quot;close&quot;): stream.close() finally: # Issue #19523: call unconditionally to # prevent a handler leak when delay is set StreamHandler.close(self) finally: self.release() def _open(self): &quot;&quot;&quot; Open the current base file with the (original) mode and encoding. Return the resulting stream. &quot;&quot;&quot; return open(self.baseFilename, self.mode, encoding=self.encoding) def emit(self, record): &quot;&quot;&quot; Emit a record. If the stream was not opened because 'delay' was specified in the constructor, open it before calling the superclass's emit. &quot;&quot;&quot; if self.stream is None: self.stream = self._open() StreamHandler.emit(self, record) def __repr__(self): level = getLevelName(self.level) return '&lt;%s %s (%s)&gt;' % (self.__class__.__name__, self.baseFilename, level) 嗯，其中关注的点是 _open 方法，以及 emit 方法，首先科普一个背景知识，在我们用 logging 输出日志的时候，logging 模块会调用对应 Handler 中的 handle 方法，在 handle 方法中，会调用 emit 方法输出最后的日志。于是我们如果使用 FileHandler 的话，那么就是先触发 handle 方法的调用，然后触发 emit 方法，在调用 _open 方法获取一个 file point 后，调用父类（更准确的描述书 MRO 上一级）StreamHandler 中 emit 方法。 来看看 StreamHandler 中的 emit 方法吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class StreamHandler(Handler): &quot;&quot;&quot; A handler class which writes logging records, appropriately formatted, to a stream. Note that this class does not close the stream, as sys.stdout or sys.stderr may be used. &quot;&quot;&quot; terminator = '\\n' def __init__(self, stream=None): &quot;&quot;&quot; Initialize the handler. If stream is not specified, sys.stderr is used. &quot;&quot;&quot; Handler.__init__(self) if stream is None: stream = sys.stderr self.stream = stream def flush(self): &quot;&quot;&quot; Flushes the stream. &quot;&quot;&quot; self.acquire() try: if self.stream and hasattr(self.stream, &quot;flush&quot;): self.stream.flush() finally: self.release() def emit(self, record): &quot;&quot;&quot; Emit a record. If a formatter is specified, it is used to format the record. The record is then written to the stream with a trailing newline. If exception information is present, it is formatted using traceback.print_exception and appended to the stream. If the stream has an 'encoding' attribute, it is used to determine how to do the output to the stream. &quot;&quot;&quot; try: msg = self.format(record) stream = self.stream stream.write(msg) stream.write(self.terminator) self.flush() except Exception: self.handleError(record) def __repr__(self): level = getLevelName(self.level) name = getattr(self.stream, 'name', '') if name: name += ' ' return '&lt;%s %s(%s)&gt;' % (self.__class__.__name__, name, level) 嗯很简单，就是调用我们之前获取的 file point 往文件中写入数据 问题就在这里，在 FileHandler 中，_open 函数中调用 open 函数时，所选择的 mode 是 'a' ，也就是通常而言的 O_APPEND 模式。我们知道，通常而言 O_APPEND 可以视作进程安全的，因为 O_APPEND 能够保证内容不被别的 O_APPEND 写操作所覆盖。但是这里为什么会出现日志丢失的情况呢？ 原因是在 POSIX 中存在着一种特殊设计，在 《POSIX Programmers Guide》 一书中对此描述如下： A write of fewer than PIPE_BUF bytes is atomic; the data will not be interleaved with data from other processes writing to the same pipe. A write of more than PIPE_BUF may have data interleaved in arbitrary ways. 这段话翻译大概就是，在 POSIX 中存在着一个变量叫做 PIPE_BUF ，这个变量大小为 512 ，在进行写入操作时，如果大小小于 PIPE_BUF 值的写操作，是具有原子性的，即不可被打断，因此不会和其余进程写入的值产生混乱，而如果写入的内容大于 PIPE_BUF ，则操作系统不能保证这一点。 在 Linux 操作系统中，这个值发生了一点变化 POSIX.1 says that write(2)s of less than PIPE_BUF bytes must be atomic: the output data is written to the pipe as a contiguous sequence. Writes of more than PIPE_BUF bytes may be nonatomic: the kernel may interleave the data with data written by other processes. POSIX.1 requires PIPE_BUF to be at least 512 bytes. (On Linux, PIPE_BUF is 4096 bytes.) 即大于 4K 的写入操作都不能保证其原子性，可能会发生数据紊乱。 而发生数据紊乱后，其日志格式不固定，最终造成解析端没法解析，从而最终日志丢失。 这里我们复现一下，首先测试代码 最后这种操作之前从未想过，今天算是打开了新的大门，最后感谢 @依云 前辈的指点= =如果没有前辈的提醒，完全想不到即便是 O_APPEND 模式下，数据也不能保证安全。 Reference文中参考了两处参考资料，链接如下 1.OReilly POSIX Programmers Guide 2.Linux Man: PIPE","link":"/posts/2018/02/23/logging-process-safe/"},{"title":"菜鸟阅读 Flask 源码系列（1）：Flask的router初探","text":"文章来源：itsCoder 的 WeeklyBolg 项目 itsCoder主页：http://itscoder.com/ 作者：写代码的香港记者 审阅者：Brucezz 前言没有一个完整的开源项目的的阅读经验的程序猿是一个不合格的程序猿，虽然曾经阅读过部分诸如 Redis 等项目的源码，但是还没有过一个完整的开源项目的阅读经验，因此在经过某个前辈的不断安利后，我决定用 Flask 来作为阅读开源源码计划的开始。而这一个系列的文章，将作为我自己的阅读笔记，来巩固自己曾经所没有重视的 Python 的很多细节。 关于 Flask关于 Flask 的背景知识，就不需要太多的描述了，网上已经有很多的资料了。在使用 Flask 的时候，我们经常用如下的方式来设置我们的自定义的路由： 12345678910111213141516171819202122232425262728293031323334353637##Flask官方Example中flaskr项目部分代码app = Flask(__name__)@app.route('/')def show_entries(): db = get_db() cur = db.execute('select title, text from entries order by id desc') entries = cur.fetchall() return render_template('show_entries.html', entries=entries)@app.route('/add', methods=['POST'])def add_entry(): if not session.get('logged_in'): abort(401) db = get_db() db.execute('insert into entries (title, text) values (?, ?)', [request.form['title'], request.form['text']]) db.commit() flash('New entry was successfully posted') return redirect(url_for('show_entries'))@app.route('/login', methods=['GET', 'POST'])def login(): error = None if request.method == 'POST': if request.form['username'] != app.config['USERNAME']: error = 'Invalid username' elif request.form['password'] != app.config['PASSWORD']: error = 'Invalid password' else: session['logged_in'] = True flash('You were logged in') return redirect(url_for('show_entries')) return render_template('login.html', error=error) 那么问题来了，上面的例子中，我们知道 app.route('xxxx',methods=['xxx']) 将会设置我们对应的方法与对应 url 的关联，那么这样一种做法是怎样生效的呢？ Flask 源码阅读让我们看看最开始的 router 是什么样子的首先让我们从 Flask 这里获取 flask 源码，然后我们将版本号切换至最初的 0.1 版（git tag为8605cc310d260c3b08160881b09da26c2cc95f8d） 小tips：阅读开源项目时，如果当前版本太过于复杂，可以切换至项目最初发布时的版本，然后根据每次项目版本发布的 Release Note 来进行跟进。 在 flask.py 文件里，我们能看到如下的的结构 讲真这个时候我们就可以看到 Flask 里的 route 的核心代码了 1234567def route(self, rule, **options): def decorator(f): self.add_url_rule(rule, f.__name__, **options) self.view_functions[f.__name__] = f return f return decorator 我们能很清楚的看到之前代码中的 app.route('/') 本质上是调用了一个装饰器来对我们对应的方法进行请求与方法之间进行关联。在 route 方法被触发后，进一步来调用 add_url_rule 来注册我们所设定的url。 1234def add_url_rule(self, rule, endpoint, **options): options['endpoint'] = endpoint options.setdefault('methods', ('GET',)) self.url_map.add(Rule(rule, **options)) 在最初版本的 Flask 中， Router 的实现就这么简单暴力 两个关于装饰器的知识点第一个：很多人肯定想问，在之前的代码里，我们没有调用相关的方法，例如 index() ，那么装饰器为什么会被触发呢？ 答：首先，请大声告诉我，装饰器的作用是什么？很明显嘛，在不修改原有代码的基础上，对函数进行一次封装，然后实现为原有方法增加一些功能的特殊实现。是不是感觉很抽象？来我们看个例子 12345678910111213def testDe1(func): def de(a, b, c): func(a, b, c) print('1') print('2') return de@testDe1def test2(a, b, c): print(a+b+c)if __name__ == '__main__': test2(1,2,3) 来，告诉我，这段代码的输出应该是什么？答案是 2,6,1，看到这里，你是不是感觉似乎明白了些什么？是的没错，上面的例子其实等价于 12345678910111213def testDe1(func): def de(a, b, c): func(a, b, c) print('1') print('2') return dedef test2(a, b, c): print(a+b+c)if __name__ == '__main__': testDe1(test2)(1,2,3) 那么我们换个例子 123456789101112def testDe1(func): def de(a, b, c): func(a, b, c) print('1') print('2') return de@testDe1def test2(a, b, c): print(a+b+c)if __name__ == '__main__': pass 这段代码的输出会是什么？是的没错，这段代码的输出是 2 。看到这里你是不是感觉更明白些什么？恩，在 Python 中，使用函数装饰器的时候，等于先行调用了装饰函数一次，具体来讲在使用装饰器后，装饰器会用装饰后的函数来进行一个替换，即在什么也不做的情况下，会产生这样一个调用 test2=testDe1(test2)，接着如果在 __main__ 中添加一段代码 test2(1,2,3),是不是就等价于 testDe1(test2)(1,2,3) 。看到这里是不是彻底明白了？恩，来，我们再来复习下前面的例子 1234567@app.route('/')def show_entries(): db = get_db() cur = db.execute('select title, text from entries order by id desc') entries = cur.fetchall() return render_template('show_entries.html', entries=entries) 上面这段代码里发生了什么？是不是有一个调用为 show_entries=app.route('/')(show_entries)？看到这里是不是很清楚了呢？ 第二个，在第一个小 tip 的基础之上，我们来讲一个关于装饰器传参的问题可能很多人不清楚装饰器传参的使用情景，首先如前面所说装饰器的最根本的作用在于 在不修改原有代码的基础上，对函数进行一次封装，然后实现为原有方法增加一些功能的特殊实现 现在假设我们需要对函数的运行时间进行输出，这个时候我们该怎么办 123456789101112def testTime(func): def dec(*args,**kwargs): flag=time.time() func(*args,**kwargs) print(time.time()-flag) return decdef func(): passif __name__=='__main__': func() 如前所述，前面这段代码等价于 test(func)(),那么这个时候我们想给我们时间输出以一定的单位进行格式化怎办，修改上面装饰器代码如下 12345678910111213def testtime(time=None): def dec1(func): def dec2(*args,**kwargs): flag=time.time() func(*args,**kwagrs) flag2=time.time() if time is not None: print((flag2-flag)/time) else: print(flag2-flag) return dec2 return dec1 写到这里，大家是不是明白了带参数的装饰器的使用情景呢？ 后记Flask 的路由系统相对简单，其本质是利用带参数的装饰器来进行相应的路由记录，同时利用装饰器的包装特性，将我们的对应的处理函数进行包装，同时加入路由表中，一旦触发我们所注册的路由，便可调用我们所对应的处理函数。","link":"/posts/2016/08/09/reading-the-fucking-flask-source-code-Part1/"},{"title":"简单安利 Rime 输入法","text":"唉，最近因为气胸大过年的住院，春节颓废了好久，今天开始回北京，干脆来安利一个输入法– Rime 碎碎念如同大多数人一样，我之前也是使用搜狗输入法作为自己的主力输入法，但是搜狗输入法的一些缺陷让我放弃了使用搜狗输入法 作为传统艺能，搜狗输入法隐私保护成迷，在 MacOS 上某几个版本的搜狗在寻求获取我的通讯录和日历读取权限 作为传统艺能，搜狗输入法的广告推送实在是一言难尽，特别是在 Windows 上，已经禁了一些组件，但是还是防不胜防 因为和港澳台和国外社区朋友的交流需要，我需要输入法能够比较好的支持繁体，而搜狗输入法的繁体支持也是一言难尽 搜狗输入法的定制能力也着实不满足我的需求。。 因此我在18年开始在寻求一种开源，可控，可定制，对简/繁输入都比较友好的输入法。经过寻找之后，Rime 输入法进入了我的视线，经过一年多的使用，我觉得这个真的是一款非常棒的输入法 Rime 是什么？Rime （又名 中州韻）是一款开源的跨平台的输入法引擎，完全开源，完全可定制，你甚至可以基于 Rime 的源码，来封装一套自己的输入法引擎。同时因为 Rime 极其高的定制性，你可以基于 Rime 制作自己的输入法。 Rime 的优势主要在于通过配置文件的方式，对扩展提供了极好的支持，而且繁体支持非常棒 举个例子 在这里，「才」「纔」不一樣。还有很多的例子，大家可以自行体验。 但是 Rime 成也极高的定制性，败也极高的定制性，对于使用者而言，纯 YAML 配置文件的定制方式，准入门槛太高 让你的 Rime 更好用首先上一下我的 Rime 配置的效果 好了，我们开始来聊聊怎么安装配置 Rime Rime 基础安装没啥好说的，从官网 下载对应平台的安装包安装即可，在 MacOS 下，Rime 的配置在 ~/Library/Rime 下，大家可以用 VSCode 之类的文本编辑器打开对应的目录，进行编辑 官方并不建议直接修改原始的配置文件，因为输入法更新时会重新覆盖默认配置，可能导致某些自定义配置丢失；推荐作法是创建一系列的 patch 配置，通过类似打补丁替换这种方式来实现无感的增加自定义配置； Rime 配色Rime 的配色管理文件是 squirrel.custom.yaml，我自己使用了网友贡献的即刻黄配色 想要切换皮肤配色只需要修改 style/color_scheme 为相应的皮肤配色名称既可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051patch: app_options: &quot;com.runningwithcrayons.Alfred-3&quot;: ascii_mode: true com.google.android.studio: ascii_mode: true com.jetbrains.intellij: ascii_mode: true show_notifications_when: appropriate # 状态通知，适当(appropriate)，开（always）关（never） style: color_scheme: jike preset_color_schemes: apathy: name: &quot;冷漠 / Apathy&quot; author: &quot;LIANG Hai &quot; horizontal: true # 水平排列 inline_preedit: true #单行显示，false双行显示 candidate_format: &quot;%c\\u2005%@\\u2005&quot; # 编号 %c 和候选词 %@ 前后的空间 corner_radius: 5 #候选条圆角 border_height: 0 border_width: 0 back_color: 0xFFFFFF #候选条背景色 font_face: &quot;PingFangSC-Regular,HanaMinB&quot; #候选词字体 font_point: 16 #候选字词大小 text_color: 0x424242 #高亮选中词颜色 label_font_face: &quot;STHeitiSC-Light&quot; #候选词编号字体 label_font_point: 12 #候选编号大小 hilited_candidate_text_color: 0xEE6E00 #候选文字颜色 hilited_candidate_back_color: 0xFFF0E4 #候选文字背景色 comment_text_color: 0x999999 #拼音等提示文字颜色 jike: name: 即刻黄 author: Ryekee back_color: 0x11E4FF corner_radius: 5 #候选条圆角 border_height: 0 border_width: 0 candidate_format: &quot;%c\\u2005%@\\u2005&quot; candidate_text_color: 0x362915 comment_text_color: 0x000000 font_face: &quot;PingFangSC-Regular,HanaMinB&quot; font_point: 16 #候选字词大小 hilited_candidate_back_color: 0xF4B95F hilited_candidate_text_color: 0xFFFFFF horizontal: true inline_preedit: true label_font_face: &quot;STHeitiSC-Light&quot; label_font_point: 12 text_color: 0xFFFFFF Rime 快捷键字符在 Rime 中，可以设置一些快捷键帮助输入一些特殊字符和表情。默认自带了很多， 比如输入 /bg 会给出八卦图案的列表 比如输入 /xl 会给出希腊字符的列表 更多的快捷输入可以参看 symbols.yaml 下的列表，其中一些比较好玩的给大家看看 12345678910111213141516#月份、日期、曜日等 '/yf': [ ㋀, ㋁, ㋂, ㋃, ㋄, ㋅, ㋆, ㋇, ㋈, ㋉, ㋊, ㋋ ] '/rq': [ ㏠, ㏡, ㏢, ㏣, ㏤, ㏥, ㏦, ㏧, ㏨, ㏩, ㏪, ㏫, ㏬, ㏭, ㏮, ㏯, ㏰, ㏱, ㏲, ㏳, ㏴, ㏵, ㏶, ㏷, ㏸, ㏹, ㏺, ㏻, ㏼, ㏽, ㏾ ] '/yr': [ 月, 火, 水, 木, 金, 土, 日, ㊊, ㊋, ㊌, ㊍, ㊎, ㊏, ㊐, ㊗, ㊡, ㈪, ㈫, ㈬, ㈭, ㈮, ㈯, ㈰, ㈷, ㉁, ㉀ ]#時間 '/sj': [ ㍘, ㍙, ㍚, ㍛, ㍜, ㍝, ㍞, ㍟, ㍠, ㍡, ㍢, ㍣, ㍤, ㍥, ㍦, ㍧, ㍨, ㍩, ㍪, ㍫, ㍬, ㍭, ㍮, ㍯, ㍰ ]#天干、地支、干支 '/tg': [ 甲, 乙, 丙, 丁, 戊, 己, 庚, 辛, 壬, 癸 ] '/dz': [ 子, 丑, 寅, 卯, 辰, 巳, 午, 未, 申, 酉, 戌, 亥 ] '/gz': [ 甲子, 乙丑, 丙寅, 丁卯, 戊辰, 己巳, 庚午, 辛未, 壬申, 癸酉, 甲戌, 乙亥, 丙子, 丁丑, 戊寅, 己卯, 庚辰, 辛巳, 壬午, 癸未, 甲申, 乙酉, 丙戌, 丁亥, 戊子, 己丑, 庚寅, 辛卯, 壬辰, 癸巳, 甲午, 乙未, 丙申, 丁酉, 戊戌, 己亥, 庚子, 辛丑, 壬寅, 癸卯, 甲辰, 乙巳, 丙午, 丁未, 戊申, 己酉, 庚戌, 辛亥, 壬子, 癸丑, 甲寅, 乙卯, 丙辰, 丁巳, 戊午, 己未, 庚申, 辛酉, 壬戌, 癸亥 ]#節氣 '/jq': [ 立春, 雨水, 驚蟄, 春分, 清明, 穀雨, 立夏, 小滿, 芒種, 夏至, 小暑, 大暑, 立秋, 處暑, 白露, 秋分, 寒露, 霜降, 立冬, 小雪, 大雪, 冬至, 小寒, 大寒 ]#單位 '/dw': [ Å, ℃, ％, ‰, ‱, °, ℉, ㏃, ㏆, ㎈, ㏄, ㏅, ㎝, ㎠, ㎤, ㏈, ㎗, ㎙, ㎓, ㎬, ㏉, ㏊, ㏋, ㎐, ㏌, ㎄, ㎅, ㎉, ㎏, ㎑, ㏍, ㎘, ㎞, ㏎, ㎢, ㎦, ㎪, ㏏, ㎸, ㎾, ㏀, ㏐, ㏓, ㎧, ㎨, ㎡, ㎥, ㎃, ㏔, ㎆, ㎎, ㎒, ㏕, ㎖, ㎜, ㎟, ㎣, ㏖, ㎫, ㎳, ㎷, ㎹, ㎽, ㎿, ㏁, ㎁, ㎋, ㎚, ㎱, ㎵, ㎻, ㏘, ㎩, ㎀, ㎊, ㏗, ㏙, ㏚, ㎰, ㎴, ㎺, ㎭, ㎮, ㎯, ㏛, ㏜, ㎔, ㏝, ㎂, ㎌, ㎍, ㎕, ㎛, ㎲, ㎶, ㎼ ]#貨幣 '/hb': [ ￥, ¥, ¤, ￠, ＄, $, ￡, £, ৳, ฿, ₠, ₡, ₢, ₣, ₤, ₥, ₦, ₧, ₩, ₪, ₫, €, ₭, ₮, ₯, ₰, ₱, ₲, ₳, ₴, ₵, ₶, ₷, ₸, ₹, ₺, ₨, ﷼ ] 而我参考漠然的配置，在 luna_pinyin_simp.custom.yaml 中添加了一些配置 12345678910111213punctuator: import_preset: symbols symbols: &quot;/fs&quot;: [½,‰,¼,⅓,⅔,¾,⅒] &quot;/dq&quot;: [🌍,🌎,🌏,🌐,🌑,🌒,🌓,🌔,🌕,🌖,🌗,🌘,🌙,🌚,🌛,🌜,🌝,🌞,⭐,🌟,🌠,⛅,⚡,❄,🔥,💧,🌊] &quot;/jt&quot;: [⬆,↗,➡,↘,⬇,↙,⬅,↖,↕,↔,↩,↪,⤴,⤵,🔃,🔄,🔙,🔚,🔛,🔜,🔝] &quot;/sg&quot;: [🍇,🍈,🍉,🍊,🍋,🍌,🍍,🍎,🍏,🍐,🍑,🍒,🍓,🍅,🍆,🌽,🍄,🌰,🍞,🍖,🍗,🍔,🍟,🍕,🍳,🍲,🍱,🍘,🍙,🍚,🍛,🍜,🍝,🍠,🍢,🍣,🍤,🍥,🍡,🍦,🍧,🍨,🍩,🍪,🎂,🍰,🍫,🍬,🍭,🍮,🍯,🍼,🍵,🍶,🍷,🍸,🍹,🍺,🍻,🍴] &quot;/dw&quot;: [🙈,🙉,🙊,🐵,🐒,🐶,🐕,🐩,🐺,🐱,😺,😸,😹,😻,😼,😽,🙀,😿,😾,🐈,🐯,🐅,🐆,🐴,🐎,🐮,🐂,🐃,🐄,🐷,🐖,🐗,🐽,🐏,🐑,🐐,🐪,🐫,🐘,🐭,🐁,🐀,🐹,🐰,🐇,🐻,🐨,🐼,🐾,🐔,🐓,🐣,🐤,🐥,🐦,🐧,🐸,🐊,🐢,🐍,🐲,🐉,🐳,🐋,🐬,🐟,🐠,🐡,🐙,🐚,🐌,🐛,🐜,🐝,🐞,🦋] &quot;/bq&quot;: [😀,😁,😂,😃,😄,😅,😆,😉,😊,😋,😎,😍,😘,😗,😙,😚,😇,😐,😑,😶,😏,😣,😥,😮,😯,😪,😫,😴,😌,😛,😜,😝,😒,😓,😔,😕,😲,😷,😖,😞,😟,😤,😢,😭,😦,😧,😨,😬,😰,😱,😳,😵,😡,😠] &quot;/ss&quot;: [💪,👈,👉,👆,👇,✋,👌,👍,👎,✊,👊,👋,👏,👐] &quot;/dn&quot;: [⌘, ⌥, ⇧, ⌃, ⎋, ⇪, , ⌫, ⌦, ↩︎, ⏎, ↑, ↓, ←, →, ↖, ↘, ⇟, ⇞] &quot;/fh&quot;: [©,®,℗,ⓘ,℠,™,℡,␡,♂,♀,☉,☊,☋,☌,☍,☑︎,☒,☜,☝,☞,☟,✎,✄,♻,⚐,⚑,⚠] &quot;/xh&quot;: [＊,×,✱,★,☆,✩,✧,❋,❊,❉,❈,❅,✿,✲] 设置输入法大家可以在 default.custom.yaml 中设置自己喜欢的输入法，我目前使用的是明月拼音，默认切换输入法的快捷键是 Ctrl+~ 但是因为这个快捷键和 VSCode 快捷键冲突，所以我将其改为 Ctrl+Shift+F12 1234567patch: menu: page_size: 8 schema_list: - schema: luna_pinyin_simp # 朙月拼音 简化字 &quot;switcher/hotkeys&quot;: - &quot;Control+Shift+F12&quot; 调教词库这里引用漠然的讲解： Rime 默认的词库稍为有点弱，我们可以下载一些搜狗词库来进行扩展；不过搜狗词库格式默认是无法解析的，好在有人开发了工具可以方便的将搜狗细胞词库转化为 Rime 的格式(工具点击这里下载)；目前该工具只支持 Windows(也有些别人写的 py 脚本啥的，但是我没用)，所以词库转换这种操作还得需要一个 Windows 虚拟机；转换过程很简单，先从搜狗词库下载一系列的 scel 文件，然后批量选中，接着调整一下输入和输出格式点击转换，最后保存成一个 txt 文本光有这个文本还不够，我们要将它塞到词库的 yaml 配置里，所以新建一个词库配置文件 luna_pinyin.sougou.dict.yaml，然后写上头部说明(注意最后三个点后面加一个换行) 123456789101112# Rime dictionary# encoding: utf-8# 搜狗词库 目前包含如下:# IT计算机 实用IT词汇 亲戚称呼 化学品名 数字时间 数学词汇 淘宝词库 编程语言 软件专业 颜色名称 程序猿词库 开发专用词库 搜狗标准词库# 摄影专业名词 计算机专业词库 计算机词汇大全 保险词汇 最详细的全国地名大全 饮食大全 常见花卉名称 房地产词汇大全 中国传统节日大全 财经金融词汇大全---name: luna_pinyin.sougouversion: &quot;1.0&quot;sort: by_weightuse_preset_vocabulary: true... 接着只需要把生成好的词库 txt 文件内容粘贴到三个点下面既可；但是词库太多的话你会发现这个文本有好几十 M，一般编辑器打开都会卡死，解决这种情况只需要用命令行 cat 一下就行 1cat sougou.txt &gt;&gt; luna_pinyin.sougou.dict.yaml 最后修改 luna_pinyin.extended.dict.yaml 中的 import_tables 字段，加入刚刚新建的词库既可 123456789101112131415---name: luna_pinyin.extendedversion: &quot;2016.06.26&quot;sort: by_weight #字典初始排序，可選original或by_weightuse_preset_vocabulary: true#此處爲明月拼音擴充詞庫（基本）默認鏈接載入的詞庫，有朙月拼音官方詞庫、明月拼音擴充詞庫（漢語大詞典）、明月拼音擴充詞庫（詩詞）、明月拼音擴充詞庫（含西文的詞彙）。如果不需要加載某个詞庫請將其用「#」註釋掉。#雙拼不支持 luna_pinyin.cn_en 詞庫，請用戶手動禁用。import_tables: - luna_pinyin # 加入搜狗词库 - luna_pinyin.sougou - luna_pinyin.poetry - luna_pinyin.cn_en - luna_pinyin.kaomoji 在我的配置中，我加入了来自搜狗的医学，古诗词，军事等词库（逃 快捷键设置这里参考了 Rime 作者的一个 Gist 对快捷键做了一些配置 12345678ascii_composer/good_old_caps_lock: trueascii_composer/switch_key: Caps_Lock: commit_code Control_L: noop Control_R: noop # 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态 Shift_L: commit_code Shift_R: noop 总结经过这一系列折腾下来，我们 Rime 应该就能满足我们日常的使用了，文中的配置都可以直接用我放在 GitHub 上的配置实现开箱即用 RimeConfig 可能有人想问，为什么对于一个输入法都需要这么多的时间进行调教？是这样，我觉得对于一些关系我们日常使用的基础工具，花一定量的时间去寻找合适自己，并且将其按照的自己的需求进行调教，是一件非常有意义的事。在后续的工作生活学习中，这也将极大的提升我们的幸福感与效率 嗯差不多这样吧，新年第一篇文章，祝大家新年快乐！","link":"/posts/2020/01/28/simple-config-for-rime-input/"},{"title":"简单聊聊 SQL 中的 Prepared Statements","text":"好久没写文章了，新年还是得写点技术水文来保证下状态，正好最近遇到一个比较有意思的问题，就来简单聊聊一下关于 MySQL 中 Prepared Statements 吧 开始gorm 是大家在使用 Go 开发时的比较常用的 ORM 了，最近在使用 gORM 的时候遇到一个很有意思的问题。首先我大概描述一下这个问题 在使用 gORM 的 Raw 方法进行 SQL 查询时，构造了如下类似的 SQL 1select * from demo where match(name) AGAINST('+?' IN BOOLEAN MODE) 在随后传入参数的时候，返回 Error : sql: expected 0 arguments, got 1。而其余的诸如如下的查询就正常执行 1select * from demo where name = ? 最开始我以为这是 gORM 中拼接 SQL 模块的问题，但是看了下代码后发现一个很有趣的逻辑。gORM 中并没有拼接 Raw SQL 的相关逻辑，它会直接调用 Golang 中的标准库 database/sql 来进行 SQL 的处理，而 database/sql 将会直接调用对应数据库驱动的实现，我们先来看看在 databse/sql 中关于 Query 的逻辑。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func (db *DB) queryDC(ctx, txctx context.Context, dc *driverConn, releaseConn func(error), query string, args []interface{}) (*Rows, error) { queryerCtx, ok := dc.ci.(driver.QueryerContext) var queryer driver.Queryer if !ok { queryer, ok = dc.ci.(driver.Queryer) } if ok { var nvdargs []driver.NamedValue var rowsi driver.Rows var err error withLock(dc, func() { nvdargs, err = driverArgsConnLocked(dc.ci, nil, args) if err != nil { return } rowsi, err = ctxDriverQuery(ctx, queryerCtx, queryer, query, nvdargs) }) if err != driver.ErrSkip { if err != nil { releaseConn(err) return nil, err } // Note: ownership of dc passes to the *Rows, to be freed // with releaseConn. rows := &amp;Rows{ dc: dc, releaseConn: releaseConn, rowsi: rowsi, } rows.initContextClose(ctx, txctx) return rows, nil } } var si driver.Stmt var err error withLock(dc, func() { // 比较有意思的地方 si, err = ctxDriverPrepare(ctx, dc.ci, query) }) if err != nil { releaseConn(err) return nil, err } ds := &amp;driverStmt{Locker: dc, si: si} rowsi, err := rowsiFromStatement(ctx, dc.ci, ds, args...) if err != nil { ds.Close() releaseConn(err) return nil, err } // Note: ownership of ci passes to the *Rows, to be freed // with releaseConn. rows := &amp;Rows{ dc: dc, releaseConn: releaseConn, rowsi: rowsi, closeStmt: ds, } rows.initContextClose(ctx, txctx) return rows, nil}} 在 database/sql 执行 QueryDC 逻辑时，会调用 ctxDriverPrepare 方法来进行 SQL Query 的预处理，我们来看看这段逻辑 123456789101112131415func ctxDriverPrepare(ctx context.Context, ci driver.Conn, query string) (driver.Stmt, error) { if ciCtx, is := ci.(driver.ConnPrepareContext); is { return ciCtx.PrepareContext(ctx, query) } si, err := ci.Prepare(query) if err == nil { select { default: case &lt;-ctx.Done(): si.Close() return nil, ctx.Err() } } return si, err} 在其中，ctxDriverPrepare 会调用 ci.Prepare(query) 来执行对应 SQL Driver 实现的 Prepare 或者 PrepareContext 方法来对 SQL 预处理，在 go-mysql-driver 中，对应的实现是这样 12345678910111213141516171819func (mc *mysqlConn) PrepareContext(ctx context.Context, query string) (driver.Stmt, error) { if err := mc.watchCancel(ctx); err != nil { return nil, err } stmt, err := mc.Prepare(query) mc.finish() if err != nil { return nil, err } select { default: case &lt;-ctx.Done(): stmt.Close() return nil, ctx.Err() } return stmt, nil} 这一段的逻辑是 go-mysql-driver 会向 MySQL 发起 prepared statement 请求，获取到对应的 Stmt 后将其返回 在 stmt 中包含了对应的参数数量，stmt name 等信息。在这里，SQL 会将 ? 等参数占位符进行解析，并告知客户端需要传入的参数数量 问题也出在这里，我们重新看一下之前的 SQL 1select * from demo where match(name) AGAINST('+?' IN BOOLEAN MODE) 在这里，我使用了 MySQL 5.7 后支持的 Full Text Match ，在这里，我们待匹配的字符串 +? 会被 MySQL 解析成为一个待查询的字符串，而不会作为占位符进行解析，那么返回 stmt 中，需要传入的参数数量为0，而 database/sql 会在后续的逻辑中对我们传入的参数和需要传入的参数数量进行匹配，如果不一致则会抛出 Error 。 好了，问题找到了，那么 Prepared Statement 究竟是什么东西，而我们为什么又需要这个？ Prepared Statement什么是 Prepared Statement？其实大致的内容前面已经聊的比较清楚了，我们来重新复习下：Prepared Statement 是一种 MySQL（其余的诸如 PGSQL 也有类似的东西）的机制，用于预处理 SQL，将 SQL 和查询数据分离，以期保证程序的健壮性。 在 MySQL 官方的介绍中，Prepared Statement 有如下的好处 Less overhead for parsing the statement each time it is executed. Typically, database applications process large volumes of almost-identical statements, with only changes to literal or variable values in clauses such as WHERE for queries and deletes, SET for updates, and VALUES for inserts. Protection against SQL injection attacks. The parameter values can contain unescaped SQL quote and delimiter characters. 简而言之是： 提升性能，避免重复解析 SQL 带来的开销 避免 SQL 注入 MySQL 的 Prepared Statement 有两种使用方式，一种是使用二进制的 Prepared Protocol（这个不在今天的文章的范围内，改天再写篇文章来聊聊 MySQL 中的一些二进制协议） ，一种是使用 SQL 进行处理 在 Prepared Statement 中有着三种命令 PREPARE 用于创建一个 Prepared Statement EXECUTE 用于执行一个 Prepared Statement DEALLOCATE PREPARE 用于销毁一个 Prepared Statement 这里需要注意一点的是，Prepared Statement 存在 Session 限制，一般情况下一个 Prepared Statement 仅存活于它被创建的 Session 。当连接断开，者在其余情况下 Session 失效的时候，Prepared Statement 会自动被销毁。 接下来，我们来动手实验下 怎么使用 Prepared Statement首先我们先创建一个 测试表 1234567create table if not exists `user`( `id` bigint(20) not null auto_increment, `name` varchar(255) not null, primary key (`id`)) engine = InnoDB charset = 'utf8mb4'; 然后插入数据 1insert into user (`name`) values ('abc'); 好了，我们先按照传统的方式进行查询下 123select *from userwhere name = 'abc'; 好了，我们现在来使用 Prepared Statement 首先使用 Prepared 关键字创建一个 statement 123set @s = 'select * from user where name=?';PREPARE demo1 from @s; 然后使用 Execute 关键字来执行 Statement 123set @a = 'abc';EXECUTE demo1 using @a; 嗯，还是很简单的对吧 为什么要使用 Prepared Statement？其中一个很重要的理由是可以避免 SQL Injection Attack （SQL 注入）的情况出现，而问题在于，为什么 Prepared Statement 能够避免 SQL 注入？ 其实很简单，我们将 Query 和 Data 进行了分离 还是以之前的表作为例子 在没有手动处理 SQL 和 参数的情况下，我们往往使用字符串拼接，那么这样会利用 SQL 语法来构造一些非法 SQL，以 Python 为例 12b = &quot;'abc';drop table user&quot;a = f&quot;select * from user where name={b}&quot; 那么这样一段代码将会生成这样的 SQL 1select * from user where name='abc';drop table user 嗯，，，，数据库从入门到删表跑路.pdf 那么，我们来使用 Prepared Statement 来看看 123set @a = '\\'abc\\';drop table user';EXECUTE demo1 using @a; 然后我们最后执行的语句是 1select * from user where name='\\'abc\\';drop table user' 因为我们将 Query 与 Query Params 在结构上进行了区分，这个时候我们无论输入什么，都会将其作为 Query Params 的一部分进行处理，从而避免了注入的风险 Prepared Statement 的优劣好处显而易见 因为数据库会对 Prepared Statement 进行缓存，从而免去了客户端重复处理 SQL 带来的开销 避免 SQL Injection Attack 语义清楚 缺点也有不少 Prepared Statement 的二进制协议存在客户端兼容的问题，有些语言的客户端不一定会对 Prepared Statement 提供二进制的协议支持 因为存在两次与数据库的通信，在密集进行 SQL 查询的情况下，可能会出现 I/O 瓶颈 所以具体还是要根据场景来做 Trade-off 了 碎碎念飞机上写下这篇文章算是作为新年的一个新开始吧，争取多写文章，规范作息，好好照顾女朋友。对了，通过这段时间的一些折腾（比如解析 Binlog 之类的），突然发现 MySQL 是个宝库，后面会写几篇文章来聊聊踩坑 MySQL 中的 Binlog 和 Protocol 中的一些坑和好玩的地方（嗯 Flag ++，千万别催稿（逃 好了，今晚就先这样，飞机要落地了，我先关电脑了（逃","link":"/posts/2020/01/05/simple-introdution-about-sql-prepared/"},{"title":"年轻人第一台 Mac，来自一个开发者的 Macbook Pro 2019 16寸简评","text":"从工作开始，一直就想买个 Mac，但是一直没有买成，虽说有公司配发的 Mac（这也让我从 Macbook Pro 2015 13寸到 Macbook Pro 2017 15寸，到 Macbook Pro 2017 15寸，到 Macbook Pro 2018 13寸，到 Macbook Pro 2018 15寸用了个遍，23333），但是没有自己的 Mac始终是一个比较遗憾的事，所以这次新款 Mac 出来后，就瞬时公司员工优惠（官网95折）+12期免息分期入手了，现在我来从一个开发者的角度来给一个简单的评测吧 正文为什么会考虑 Mac写这个文章之前，我需要介绍下我买 Mac 的背景。目前我的主力机是来自蓝天的准系统，P775TM，配置是 i7 8700+32G+512G SSD+1070。工作系统是 Manjaro（一个 Linux 发型版）。在目前开发的时候觉得非常舒服，但是我也遇到了几个问题 太重了，有些时候临时有事需要出门带着并不方便 Linux 下日常软件的缺少还是会给开发带来一些不便 所以在11月之后，我就在慎重考虑需要换一台电脑，而我自己是偏基础设施的后端开发者，所以对 Unix/类 Unix 比较好的支持是必须的。同时因为有些时候会参与一些大型的开源项目，我也需要电脑有足够的性能来支撑多虚拟机以提供多平台的调试能力。因此当时摆在我面前的有两个选择 买一个硬件友好的机器，装黑苹果和 Linux 双系统 买一个 Macbook Pro 由于我算是有点版权洁癖，黑苹果严格意义上来讲是违规的。因此 Macbook Pro 是我最好的选择了，恰逢新款的 16 寸的 Macbook Pro 上市，我就决定入手 我最后选择的配置是 i9 2.3 Ghz + 64G RAM + 1T SSD + 5500M 4G。选择这样的配置的逻辑是这样的，因为根据目前的评测，这一款 Mac 散热表现不错。因为我日常会在本地编译调试东西，所以果断选择了 i9，1T SSD 则是默认标配。而我不是视频工作者，同时我也不会在 Mac 上玩游戏， 所以显存 4G 版显卡对我来说完全够用。唯一的纠结点在于 RAM，16G 肯定不够用，到底是 32G 还是 64G 这是个问题。考虑了下，咬咬牙上了 64G （后面也说明这是一个正确的决策） 定了之后果断分期下单，18号下单，23号拿到机器（这里要感谢女朋友大力支持，要不是她，我估计最后也舍不得买）。 好了，开始进入开箱，评测环节 开箱23号一早跑去公司拿了快递，然后回家开箱 献祭一只猫后，打开快递箱 苹果的包装一如既往的简洁 下面几张图，是真机原貌 机器到手默认是 MacOS 10.15.1 即 MacOS Catalina 。不得不说，这一代 MacOS 与 iOS 都是 Bug 奇多，堪称 BugOS = =，让人怀疑果家的项目管理是不是彻底失效了 至于机器外观，我选择的是银色版，不得不说，颜值是真高，但是也容易脏qaq，可能也有朋友关系 Macbook Pro 16 寸有多大，下面做了一个图大家可以感受一下 从上往下分别是： 女朋友的 Macbook Air 公司配发的 Macbook Pro 2018 15 寸（后面还会有他的细分） Macbook Pro 16 寸 蓝天 P775TM 从这样一个角度大家能看出来，其实新款的 Macbook Pro 16 寸比 Macbook Pro 15 寸只大出一点点，而在新款出来后，官网也下架了 Macbook Pro 15 寸，所以看起来以后 Macbook Pro 16 寸就是 15 寸的替代品 使用体验其实到手时间还相对较短，比较深的体验暂时没有2333，这一部分可能就相对较短了 屏幕是保持了苹果一如既往的水准，很舒服，不过这一代默认显示分辨率是 1792x1120， 较 15 寸的 1680x1050 更大，很多人可能会觉得字体比较小，需要额外调整（此处推荐 RDM 键盘终于舒服了，蝶式键盘对于我来说有点敲钢板的感觉（不过各有所爱啦） ESC 键的恢复对于 VIM 党来说是重大利好（逃（然而我不是（逃x2，2333333 性能测试首先，我们来看下新款 Mac 的 CPU 测试成绩 不过，说实话，我觉得利用日常的一些场景来做测试可能更有价值，所以我选取了两个项目来做编译测试，一个是 Dubbo （基于 Dubbo 2.7.4.1 进行编译, Maven 3.6.2, JDK 8),另外一个是基于 CPython Master 最新代码进行编译，下面是编译命令 Dubbo: mvn clean package -U -Dmaven.test.skip=true (两台机器都已经搞定依赖) CPython: ./configure --with-pydebug --with-openssl=/usr/local/opt/openssl &amp;&amp; make -j 两台机器分别为 最后的测试结果如下图所示（单位都为秒） 看起来性能提升还是很明显，不过因为编译涉及到频繁的小文件读写，所以导致差距没有理论上的那么大，但是随着代码规模的扩大，i9 的优势会更为明显。后续有时间我会找几个科学计算的例子来进行场景补充 说道编译，大家可能也关心新款的 Mac 的散热怎么样，我大概测试了一下， 结论为，降频无法避免，但是满负载的时候，能在81度左右的温度，将 CPU 频率稳定在 3.5 Ghz。虽然这个结果还是没有其余的高性能本那么显眼，但是对于一个轻薄高性能本来说，我觉得还算不错，毕竟凡事总得做一个 trade-off 对吧 最后，测一下硬盘速度 嗯，还是一如既往的暴力。。。 啊对了，其实有朋友可能会关心，64G 的内存是否浪费，嗯，我看了下，其实对我而言，不算浪费 这是我写这篇文章时候的内存使用率，机器上开了三个 IDEA 项目，两个 Goland 项目，一个 PyCharm 项目，浏览器窗口若干，VSCode 窗口若干，用 Docker 跑了三个 ElasticSearch 节点做 HA 测试，一个 Kibana 节点，如果后续在开几个虚拟机，可能 64G 对我来说就是非常适合的了。不过每个人的场景不一样，这里我也就不对大家的选机做一个建议了（不然选错了可能就要被打QAQ） 总结其实 Macbook Pro 是个水桶机，整体配置相对均衡，适合大多数场景（需要 Office 的除外），而且今年的官方也提供了更为灵活的配置选项让大家来进行组合。所以我建议如果对于大尺寸 Mac 有需求的朋友，其实真的可以考虑入手 千言万语汇成一句话： 最后再次感谢女朋友对我的支持！mua！","link":"/posts/2019/11/25/simple-test-about-new-macbook-pro/"},{"title":"asyncio 笔记","text":"来源 annotated-py-asyncio 阅读补充:1. 基本概念:1.1 协程: “协程 是为非抢占式多任务产生子程序的计算机程序组件，协程允许不同入口点在不同位置暂停或开始执行程序”。 从技术的角度来说，“协程就是你可以暂停执行的函数”。 如果你把它理解成“就像生成器一样”，那么你就想对了。 1.2 事件循环: 事件循环 “是一种等待程序分配事件或消息的编程架构”。 基本上来说事件循环就是，“当A发生时，执行B”。 或许最简单的例子来解释这一概念就是用每个浏览器中都存在的JavaScript事件循环。 当你点击了某个东西（“当A发生时”），这一点击动作会发送给JavaScript的事件循环，并检查是否存在注册过的 onclick 回调来处理这一点击（“执行B”）。 只要有注册过的回调函数就会伴随点击动作的细节信息被执行。 事件循环被认为是一种循环是因为它不停地收集事件并通过循环来发如何应对这些事件。 1.3 Python 的事件循环: 对 Python 来说，用来提供事件循环的 asyncio 被加入标准库中。 asyncio 重点解决网络服务中的问题，事件循环在这里将来自套接字（socket）的 I/O 已经准备好读和/或写作为“当A发生时”（通过selectors模块）。 除了 GUI 和 I/O，事件循环也经常用于在别的线程或子进程中执行代码，并将事件循环作为调节机制（例如，合作式多任务）。 如果你恰好理解 Python 的 GIL，事件循环对于需要释放 GIL 的地方很有用。 事件循环提供一种循环机制，让你可以“在A发生时，执行B”。 基本上来说事件循环就是监听当有什么发生时，同时事件循环也关心这件事并执行相应的代码。 Python 3.4 以后通过标准库 asyncio 获得了事件循环的特性。 1.4 async, await: 将 async/await 看做异步编程的 API 基本上 async 和 await 产生神奇的生成器，我们称之为协程， 同时需要一些额外的支持例如 awaitable 对象以及将普通生成器转化为协程。 所有这些加到一起来支持并发，这样才使得 Python 更好地支持异步编程。 相比类似功能的线程，这是一个更妙也更简单的方法。 在 Python 3.4 中，用于异步编程并被标记为协程的函数看起来是这样的： 12345# This also works in Python 3.5.@asyncio.coroutinedef py34_coro(): yield from stuff() Python 3.5 添加了types.coroutine 修饰器，也可以像 asyncio.coroutine 一样将生成器标记为协程。你可以用 async def 来定义一个协程函数，虽然这个函数不能包含任何形式的 yield 语句；只有 return 和 await 可以从协程中返回值。 12async def py35_coro(): await stuff() 你将发现不仅仅是 async，Python 3.5 还引入 await 表达式（只能用于async def中）。虽然await的使用和yield from很像，但await可以接受的对象却是不同的。await 当然可以接受协程，因为协程的概念是所有这一切的基础。但是当你使用 await 时，其接受的对象必须是awaitable 对象：必须是定义了await()方法且这一方法必须返回一个不是协程的迭代器。协程本身也被认为是 awaitable 对象（这也是collections.abc.Coroutine 继承 collections.abc.Awaitable的原因）。这一定义遵循 Python 将大部分语法结构在底层转化成方法调用的传统，就像 a + b 实际上是a.add(b) 或者 b.radd(a)。 为什么基于async的协程和基于生成器的协程会在对应的暂停表达式上面有所不同？主要原因是出于最优化Python性能的考虑，确保你不会将刚好有同样API的不同对象混为一谈。由于生成器默认实现协程的API，因此很有可能在你希望用协程的时候错用了一个生成器。而由于并不是所有的生成器都可以用在基于协程的控制流中，你需要避免错误地使用生成器。 用async def可以定义得到协程。定义协程的另一种方式是通过types.coroutine修饰器 – 从技术实现的角度来说就是添加了 CO_ITERABLE_COROUTINE标记 – 或者是collections.abc.Coroutine的子类。你只能通过基于生成器的定义来实现协程的暂停。 awaitable 对象要么是一个协程要么是一个定义了await()方法的对象 – 也就是collections.abc.Awaitable – 且await()必须返回一个不是协程的迭代器。 await表达式基本上与 yield from 相同但只能接受awaitable对象（普通迭代器不行）。async定义的函数要么包含return语句 – 包括所有Python函数缺省的return None – 和/或者 await表达式（yield表达式不行）。async函数的限制确保你不会将基于生成器的协程与普通的生成器混合使用，因为对这两种生成器的期望是非常不同的。 1.5 关于 python 协程 和 golang 的对比讨论: From Python to Go and Back Again PPT 关于此PPT 的观点: go 比 pypy 性能高不了多少, 但是复杂度和调试难度增加很高 结尾鼓吹 rust. 异步库参考: hyper curio 将asyncio看作是一个利用async/await API 进行异步编程的框架 David 将 async/await 看作是异步编程的API创建了 curio 项目来实现他自己的事件循环。 允许像 curio 一样的项目不仅可以在较低层面上拥有不同的操作方式 （例如 asyncio 利用 future 对象作为与事件循环交流的 API，而 curio 用的是元组） 2. 源码模块:2.1 (futures.py)[./futures.py]2.1.1 参考: Python 3.5 协程究竟是个啥 译: Python 3.5 协程究竟是个啥 译: github 译: 博客 Python 协程：从 yield/send 到 async/await future 源码剖析 concurrent.futures 源码阅读笔记（Python） concurrent.futures 是一个异步库 concurrent.futures — Asynchronous computation 2.1.2 生成器（Generator）VS 迭代器（iterator）: improve-your-python-yield-and-generators-explained 译文:提高你的Python: 解释‘yield’和‘Generators（生成器）’ 在Python之外，最简单的生成器应该是被称为协程（coroutines）的东西。 generator是用来产生一系列值的 yield则像是generator函数的返回结果 yield唯一所做的另一件事就是保存一个generator函数的状态 generator就是一个特殊类型的迭代器（iterator） 和迭代器相似，我们可以通过使用next()来从generator中获取下一个值 通过隐式地调用next()来忽略一些值 生成器与迭代器的关系 生成器(generator)是一个特殊的迭代器，它的实现更简单优雅。yield 是生成器实现 next() 方法的关键 python黑魔法—迭代器（iterator） 生成器","link":"/posts/2017/06/07/some-note-for-asyncio/"},{"title":"关于 Kubernetes 和容器化的一些随想","text":"这段时间在不少群里争论过关于 Kubernetes 和容器化的一些事，干脆总结下一些碎碎念作为一个概括吧。本文仅代表个人立场，不代表商业观点 容器化目前很主流的一个观点，是能上容器尽可能上容器，说实话这个想法实际上是有一定的合理性的，去 review 这个想法，我们需要去看一下容器这个东西，给我们带来了什么样的改变 容器首先毫无疑问，会给我们带来非常多的好处： 真正意义上让开发与生产环境保持一致是一种非常方便的事，换句话说，开发说的“这个服务在我本地没啥问题”是一句有用的话了 让部署一些服务变的更为方便，无论是分发，还是部署， 能做到一定程度上的资源隔离与分配 那么，看起来我们是不是可以无脑用容器？不，不是，我们需要再来 Review 一下，容器化后我们所要面临的一些弊端： 容器安全性问题，目前最主流的容器实现（此处点名 Docker）本质上而言还是基于 CGroups + NS 来进行资源与进程隔离。那么其安全性将会是一个非常值得考量的问题。毕竟 Docker 越权与逃逸漏洞年年有，年年新。那么这意味着我们是需要去有一个系统的机制去规范我们容器的使用，来保证相关的越权点能被把控在一个可控的范围内。而另一个方向是镜像安全问题，大家都是面向百度/CSDN/Google/Stackoverflow 编(fu)程(zhi)选手，那么势必会出现一个情况，当我们遇到一个问题，搜索一番，直接复制点 Dockerfile 下来，这个时候，将会存在很大的风险点，毕竟谁也不知道 base image 里加了啥料不是？ 容器的网络问题。当我们启动若干个镜像后，那么容器之间的网络互通怎么处理？而大家生产环境，肯定不止一个机器那么少，那么跨主机的情况下，怎么样去进行容器间的通信，同时保证网络的稳定性？ 容器的调度与运维的问题，当我一个机器高负载的时候，怎么样去将该机器上的一些容器调度到其余的机器上？而怎么样去探知一个容器是否存活？如果一个容器 crash 了，怎么样重新拉起？ 容器具体的细节问题，比如镜像怎么样构建与打包？怎么样上传？乃至说怎么样去排查一些 corner case 的问题？ 我们做一个业务决策的时候，我们肯定不会是因为某个技术够先进，够舒服，而是需要去衡量这个业务决策的 ROI，同时在利弊之间做一个 Trade-Off，用容器化这件事来说吧，我们来思考下我们可能迁移容器常见的几个误区： 我们想对利用容器做资源隔离！那么问题来了，用 systemd + cgroup 这样简便的方法做和容器之间有什么区别？是容器的成本更低？ 我们想践行 Devops 所以想上容器化！实际上 Devops 和容器化关联并不算大，它更多的是一种方法论，一个团队之间内部协作的一套方法论。不精确的来讲，是通过自动化，流程改进，SOP 引入等手段，将一套服务的分发与运维更为简便化。换句话说，在我们去践行 Devops 这一套方法论的时候，实际上不是一个技术问题，而是一个制度问题（讲个笑话，Devops 的开发不需要写脚本）。在其中，无论是我们传统的 Ansible 等运维手段，还是一些自动化测试的方法与框架，都可以成为 Devops 的一部分。那么这里还是一个问题，我们为什么要用容器？是因为传统的工具践行 Devops 的 cost 远高于用容器化的？ 从这两个例子大家能看出来，当我们去做容器化这件事的时候，一定要思考的问题是，容器化是真正解决了我们什么痛点，还是只是因为它看起来够先进，够屌，能为我简历背书？ Kubernetes前面聊到容器化的几个问题，促成了以 Kubernetes 为代表的容器编排体系的诞生。大家在想，哇，既然解决了这个问题，那么我们再来聊聊这个问题 首先我已经忽略掉自建 Kubernetes 集群的场景了，因为那不是一般人能 Hold 住的。那么我们来看一下，依托公有云使用的情况吧，以阿里云为例，点开页面，然后我们见到这样张图 好了，提问： VPC 是什么？ Kubernetes 1.16.9 和 1.14.8 有什么区别 Docker 19.03.5 和阿里云安全沙箱 1.1.0 是什么，有什么区别 专有网络是什么？ 虚拟交换机是什么？ 网络插件是什么？Flannel 和 Terway 又是什么？有什么区别？当你翻了翻文档，然后文档告诉你，Terway 是阿里云基于 Calico 魔改的 CNI 插件。那么 CNI 插件是什么？Calico 是什么？ Pod CIDR 是什么怎么设？ Service CIDR 是什么怎么设？ SNAT 是什么怎么设？ 安全组怎么配置？ Kube-Proxy 是什么？iptables 和 IPVS 有什么区别？怎么选？ 是不是和你想象的一键点点点有很大区别？你可能说，我们小公司不管这些，暴力出奇迹，一键全默认。。。。emmmm，那上什么 Kubernetes 啊。。好了，假设你上了后，来，我们继续算账 你得有个镜像仓库吧，不贵，中国区基础版780一个月 你集群内的服务需要暴露出去用吧？行叭，买个最低规格的 SLB，简约型，每个月200 好了，你每个月日志得花钱吧？假设你每个月20G日志，不多吧？行，39.1 你集群监控要不要？好，买，每天50w条日志上报吧？行，不贵，975 一个月 算一下，一个集群吧，(780+200+39.1+975)*12=23292.2 不算集群基础的 ENI，ECS 等费用，美滋滋 而且会衍生很多其余的问题，具体的话，大家可以去 Kubernetes 的 Issue 区看一下盛况 总结写这个文章，并不为吐槽或者喷人，只是想表明一个观点，借用我比较喜欢的一篇文章中台，我信了你的邪 | 深氪 中的一句话 到了去年底，阿里巴巴董事长兼CEO张勇在湖畔大学分享时也说：如果一个企业奔着中台做中台，就是死。 逍遥子是不是说过这句话待考，但我很赞同，同时我认为一个企业奔着技术先进性去搞技术，就是死 ，毕竟技术是需要为业务服务的，而技术的进步很大程度上依赖业务的沉淀与需求 好了，这应该是我写过最水的文章了，先这样吧。继续搬砖了","link":"/posts/2020/06/29/some-tips-about-kubernetes-and-container/"},{"title":"怎么样去理解 Python 中的装饰器","text":"怎么样去理解 Python 中的装饰器首先，本垃圾文档工程师又来了。开始日常的水文写作。起因是看到这个问题如何理解Python装饰器？，正好不久前给人讲过这些，本垃圾于是又开始新的一轮辣鸡文章写作行为了。 预备知识首先要理解装饰器，首先要先理解在 Python 中很重要的一个概念就是：“函数是 First Class Member” 。这句话再翻译一下，函数是一种特殊类型的变量，可以和其余变量一样，作为参数传递给函数，也可以作为返回值返回。 12345678def abc(): print(&quot;abc&quot;)def abc1(func): func()abc1(abc) 这段代码的输出就是我们在函数 abc 中输出的 abc 字符串。过程很简单，我们将函数 abc 作为一个参数传递给 abc1 ，然后，在 abc1 中调用传入的函数 再来看一段代码 1234567def abc1(): def abc(): print(&quot;abc&quot;) return abcabc1()() 这段代码输出和之前的一样，这里我们将在 abc1 内部定义的函数 abc 作为一个变量返回，然后我们在调用 abc1 获取到返回值后，继续调用返回的函数。 好了，我们再来做一个思考题，实现一个函数 add ，达到 add(m)(n) 等价于 m+n 的效果。这题如果把之前的 First-Class Member 这一概念理清楚后，我们便能很清楚的写出来了 12345def add(m): def temp(n): return m+n return tempprint(add(1)(2)) 嗯，这里输出就是 3 。 正文看了前面的预备知识后，我们便可以开始今天的主题了 先来看一个需求吧现在我们有一个函数 12345def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 现在我们要给这个函数加上一些代码，来计算这个函数的运行时间。 我们大概一想，写出了这样的代码 1234567import timedef range_loop(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_result 先且不论，这样计算时间是不是准确的，现在我们要给如下很多函数加上一个时间计算的功能 12345678910111213141516171819import timedef range_loop(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_resultdef range_loop1(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_resultdef range_loop2(a,b): time_flag=time.time() for i in range(a,b): temp_result=a+b print(time.time()-time_flag) return temp_result 我们初略一想，嗯，Ctrl+C,Ctrl+V。emmmm 好了，现在你们不觉得这段代码特别脏么？我们想让他变得干净点怎么办？ 我们想了想，按照之前说的 First-Class Member 的概念。然后写出了如下的代码 12345678910111213141516171819202122import timedef time_count(func,a,b): time_flag=time.time() temp_result=func(a,b) print(time.time()-time_flag) return temp_result def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resulttime_count(range_loop,a,b)time_count(range_loop1,a,b)time_count(range_loop2,a,b) 嗯，看起来像那么回事，好了好了，我们现在新的问题又来了，我们现在是假设，我们所有函数都只有两个参数传入，那么现在如果想支持任意参数的传入怎么办？我们眉头一皱，写下了如下的代码 123456789101112131415161718192021222324import timedef time_count(func,*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resulttime_count(range_loop,a,b)time_count(range_loop1,a,b)time_count(range_loop2,a,b) 好了，现在看起来，有点像模像样了，但是我们再想想，这段代码实际上改变了我们的函数调用方式，比如我们直接运行 range_loop(a,b) 还是没有办法获取到函数执行时间。那么现在我们如果不想改变函数的调用方式，又想获取到函数的运行时间怎么办？ 很简单嘛，替换一下不就好了 12345678910111213141516171819202122232425262728import timedef time_count(func): def wrap(*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result return wrap def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_resultdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resultrange_loop=time_count(range_loop)range_loop1=time_count(range_loop1)range_loop2=time_count(range_loop2)range_loop(1,2)range_loop1(1,2)range_loop2(1,2) emmmm，这样看起来感觉舒服多了？既没有改变原有的运行方式，也输出了函数运行时间。 但是。。。你们不觉得手动替换太恶心了么？？？喵喵喵？？？还有什么可以简化下的么？？ 好了，Python 知道我们是爱吃糖的孩子，给我们提供了一个新的语法糖，这也是今天的男一号，Decorator 装饰器 说说 Decorator我们前面已经实现了，在不改变函数特性的情况下，给原有的代码新增一点功能，但是我们也觉得这样手动的替换，太恶心了，是的 Python 官方也觉得这样很恶心，所以新的语法糖来了 我们上面的代码可以写成这样了 12345678910111213141516171819202122232425262728import timedef time_count(func): def wrap(*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result return wrap@time_count def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result@time_countdef range_loop1(a,b): for i in range(a,b): temp_result=a+b return temp_result@time_countdef range_loop2(a,b): for i in range(a,b): temp_result=a+b return temp_resultrange_loop(1,2)range_loop1(1,2)range_loop2(1,2) 哇，写到这里，你是不是恍然大悟！まさか？？？是的，其实 @ 符号其实是一个语法糖，他将我们之前的手动替换的过程交给了环境执行。好了用人话描述下，@ 的作用是将被包裹的函数作为一个变量传递给装饰函数/类，将装饰函数/类返回的值替换原本的函数。 123@decoratordef abc(): pass 如同前面所讲的一样，实际上是发生了一个特殊的替换过程 abc=decorator(abc) ，好了我们来做几个题来练习下吧？ 1234567def decorator(func): return 1@decoratordef abc(): passabc() 这段代码会发生什么？答：会抛出异常。为啥啊？答：因为装饰的时候发生了替换，abc=decorator(abc) ，替换后 abc 的值为 1 。整数默认不能作为一个函数进行调用。 12345678910111213141516171819202122232425262728def time_count(func): def wrap(*args,**kwargs): time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) return temp_result return wrapdef decorator(func): def wrap(*args,**kwargs): temp_result=func(*args,**kwargs) return temp_result return wrapdef decorator1(func): def wrap(*args,**kwargs): temp_result=func(*args,**kwargs) return temp_result return wrap@time_count@decorator@decorator1 def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 这段代码怎么替换的？答：time_count(decorator(decorator1(range_loop))) 嗯，现在是不是对装饰器什么的有了基本的了解？ 扩展一下现在，我想修改下前面写的 time_count 函数，让他支持传入一个 flag 参数，当 flag 为 True 的时候，输出函数运行时间，为 False 的时候不输出时间 \b我们一步步来，我们先假设新的函数叫做 time_count_plus 我们想实现的效果是这样的 12345@time_count_plus(flag=True)def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 嗯，我们看了下，首先我们调用了 time_count_plus(flag=True) 一次，将它返回的值作为一个装饰函数来替换 range_loop ，OK 那么我们首先 time_count_plus 要接收一个参数，返回一个函数对吧 1234def time_count_plus(flag=True): def wrap1(func): pass return wrap1 好了，现在返回了一个函数来作为装饰函数，然后我们说了 @ 其实触发了一次替换过程，好那么我们现在的替换是不是 range_loop=time_count_plus(flag=True)(range_loop) 好了，现在大家应该很清楚了，我们在 wrap1 里面是不是还应该有一个函数并返回？ 嗯，最终的代码如下 123456789101112131415161718def time_count_plus(flag=True): def wrap1(func): def wrap2(*args,**kwargs): if flag: time_flag=time.time() temp_result=func(*args,**kwargs) print(time.time()-time_flag) else: temp_result=func(*args,**kwargs) return temp_result return wrap2 return wrap1@time_count_plus(flag=True)def range_loop(a,b): for i in range(a,b): temp_result=a+b return temp_result 是不是这样就清楚多啦！ 扩展两下好了，我们现在有新的需求来了 1234567891011121314m=3n=2def add(a,b): return a+bdef sub(a,b): return a-bdef mul(a,b): return a*bdef div(a,b): return a/b 现在我们有字符串 a , a 的值可能为 +、-、*、/ 那么现在，我们想根据 a 的值来调用对应的函数怎么办？ 我们煎蛋一想，嗯，逻辑判断嘛 1234567891011121314151617181920212223m=3n=2def add(a,b): return a+bdef sub(a,b): return a-bdef mul(a,b): return a*bdef div(a,b): return a/ba=input('请输入 + - * / 中的任意一个\\n')if a=='+': print(add(m,n))elif a=='-': print(sub(m-n))elif a=='*': print(mul(m,n))elif a=='/': print(div(m,n)) 但是这段代码，if else 是不是太多了点？我们仔细一想，用一下 First-Class Member 的特性，然后配合 dict 实现操作符和函数之间的关联。 12345678910111213141516m=3n=2def add(a,b): return a+bdef sub(a,b): return a-bdef mul(a,b): return a*bdef div(a,b): return a/bfunc_dict={&quot;+&quot;:add,&quot;-&quot;:sub,&quot;*&quot;:mul,&quot;/&quot;:div}a=input('请输入 + - * / 中的任意一个\\n')func_dict[a](m,n) emmmm，看起来不错啊，但是我们注册的过程能不能再简化一点？ 嗯，这个时候装饰器语法特性就能用上了 1234567891011121314151617181920212223m=3n=2func_dict={}def register(operator): def wrap(func): func_dict[operator]=func return func return wrap@register(operator=&quot;+&quot;)def add(a,b): return a+b@register(operator=&quot;-&quot;)def sub(a,b): return a-b@register(operator=&quot;*&quot;)def mul(a,b): return a*b@register(operator=&quot;/&quot;)def div(a,b): return a/ba=input('请输入 + - * / 中的任意一个\\n')func_dict[a](m,n) 嗯，还记得我们前面说的使用 @ 语法的时候，实际上是触发了一个替换的过程么？这里就是利用这一特性，在装饰器触发的时候，注册函数映射，这样我们直接根据 ‘a’ 的值来获取函数处理数据。另外请注意一点，我们这里没有必要修改原函数，所以我们没有必要写第三层的函数。 如果有熟悉 Flask 同学就知道，在调用 route 方法注册路由的时候，也是使用了这一特性 ，可以参考另外一篇很久前写的垃圾水文 菜鸟阅读 Flask 源码系列（1）：Flask的router初探 总结其实全文下来，大家应该能知道这样一点东西。Python 中的装饰器其实是 First-Class Member 概念的更进一层应用，我们将函数传递给其余函数，包裹上新的功能后再行返回。@ 其实只是将这样一个过程进行了简化而已。在 Python 中，装饰器无处不在，很多官方库中的实现也依赖于装饰器，比如很久之前写过这样一篇垃圾水文 菜鸟阅读 Flask 源码系列（1）：Flask的router初探。 嗯，今天就先写到这里吧！","link":"/posts/2018/02/23/something-about-decorator/"},{"title":"Linux 上关于 inotify 的小笔记","text":"最近还是无心写啥文章，说好的写几篇关于 Raft 的论文也因为一些事 delay 了。但是想了想还是准备写点什么，于是写个小的水文来记录下关于今天碰到的一个 Linux 内核参数的问题， 顺便做个笔记 开始我是一个不太喜欢 Mac 的人，所以我自己在家使用的开发环境是 Manjaro（这里打个广告，非常棒的发行版，堪称开箱即用，广告五毛一条）。然后代码工具就是 Jetbrains 的全家桶和 VSCode 搭配使用。 今天打开 Goland 的时候，发现 IDE 给了这样一个 Warning ，External file changes sync may be slow: The current inotify(7) watch limit is too low. 于是大家知道，我是个看着这些 warning 有强迫症的人，于是我就去查了查 简单聊聊我们平常经常会有需求，去监控一个文件或者一个目录下的变化，比如创建文件，删除文件等。我们常规的做法可能是一个直接暴力轮询的方式来做 但是这样的性能会极差。那么我们有没有什么手段来处理一下这个事么？ 有的！ Linux 提供了对应的 API 来处理这事，这就是我们今天要聊到的 inotify 按照官方的说法，inotify 其实很简单， The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory. 大意就是说 inotify 是用来监控文件系统事件的。可以使用在单个文件或者目录上。被监听的文件目录本身的变化或者内部文件的变化都在监听范围内。 在监听了对应的文件后，inotify 将返回如下事件 IN_ACCESS 文件可读 IN_ATTRIB 元数据变化 IN_CLOSE_WRITE File opened for writing was closed IN_CLOSE_NOWRITE File not opened for writing was closed IN_CREATE 被监听的目录下有文件/目录被创建 IN_DELETE 被监听的目录下有文件/目录被删除 IN_DELETE_SELF 被监听的文件/目录被删除 IN_MODIFY 文件被修改 IN_MOVE_SELF 被监听的文件/目录被移动 IN_MOVED_FROM 有文件/目录从被监听的目录中被移出 IN_MOVED_TO 有文件/目录移动至被监听的目录中 IN_OPEN 文件被打开 总共12类事件，已经能涵盖住我们常见的需求。但是 inotify 也有其自己的弊端。 不支持递归监听。举个例子，我监听 A 目录，我可以捕获到在 A 目录下创建 B 目录这个事件。但是我们没法监听到 B 目录下事件，除非将 B 目录也添加到监听队列中 Python 可用的 inotify 很少 对于第一个缺陷。常见的解决手段，是我们自行实现递归监听。当主目录下存在创建文件/目录事件的时候，我们将对应的文件/目录也添加到监听队列中。 但是这样就带来一个新的问题。如果一个非常大的项目，我们按照这样的方式去做，那么最后对应的内存损耗是很吓人的。所以在 inotify 设计之初，就通过一些内核参数做了一些限制 我们常见的有两个 /proc/sys/fs/inotify/max_queued_events 限制事件队列长度，一旦出现事件堆积，那么新的事件将被废弃 /proc/sys/fs/inotify/max_user_watches 限制每个 User ID 能够创建的 watcher 数，以免监听过多导致内存爆炸 在默认情况下 max_user_watches 的值取决于不同的 Linux 发行版，对于大多数发行版而言，其值相对较小。也就是说一旦达到限制，那么将没法添加新的 watcher。这也是 IDE 为什么会提示External file changes sync may be slow: The current inotify(7) watch limit is too low. 的原因 可以通过修改 /etc/sysctl.conf 来修改对应的参数，最后解决这个问题 最后Linux 果然是个宝库。感觉隔三差五就会遇到自己没涉及到的东西。所以还是记录下来，当作一篇水文，顺便供自己参阅","link":"/posts/2019/07/02/something-about-file-system-watch/"},{"title":"Flask 中的 Context 初探","text":"Flask 中的 Context 初探大家新年好！鉴于今年春晚非常好看，我觉得承受不起，于是来写点辣鸡水文娱乐下大家，这也是之前立的若干 Flag 中的一个 正文做过 Flask 开发的朋友都知道 Flask 中存在着两个概念，一个叫 App Context , 一个叫 Request Context 。 这两个算是 Flask 中很独特的一种机制。 从一个 Flask App 读入配置并启动开始，就进入了 App Context，在其中我们可以访问配置文件、打开资源文件、通过路由规则反向构造 URL。当 WSGI Middleware 调用 Flask App 的时候开始，就进入了 Request Context 。我们可以获取到其中的 HTTP HEADER 等操作，同时也可以进行 SESSION 等操作。 不过作为辣鸡选手而言，经常分不清为什么会存在这两个 Context ，没事，我们慢慢来说一说。 预备知识首先要清楚一点，我们要在同一个进程中隔离不同线程的数据，那么我们会优先选择 threading.local ，来实现数据彼此隔离的需求。但是现在有个问题来了，现在我们并发模型可能并不是只有传统意义上的进程-线程模型。也有可能是 coroutine(协程) 模型。常见的就是 Greenlet/Eventlet 。在这种情况下，threading.local 就没法很好的满足我们的需求。于是 Werkzeug 实现了自己的 Local 即 werkzeug.local.Local 那么 Werkzeug 自己实现的 Local 和标准的 threading.local 相比有什么不同呢？我们记住最大的不同点在于 前者会在 Greenlet 可用的情况下优先使用 Greenlet 的 ID 而不是线程 ID 以支持 Gevent 或 Eventlet 的调度，后者只支持多线程调度； Werkzeug 另外还实现了两种数据结构，一个叫 LocalStack ，一个叫做 LocalProxy LocalStack 是基于 Local 实现的一个栈结构。栈的特性就是后入先出。当我们进入一个 Context 时，将当前的的对象推入栈中。然后我们也可以获取到栈顶元素。从而获取到当前的上下文信息。 LocalProxy 是代理模式的一种实现。在实例化的时候，传入一个 callable 的参数。然后这个参数被调用后将会返回一个 Local 对象。我们后续的所有操作，比如属性调用，数值计算等，都会转发到这个参数返回的 Local 对象上。 现在大家可能不太清楚，我们为什么要用 LocalProxy 来进行操作，我们来给大家看一个例子 1234567891011121314from werkzeug.local import LocalStacktest_stack = LocalStack()test_stack.push({'abc': '123'})test_stack.push({'abc': '1234'})def get_item(): return test_stack.pop()item = get_item()print(item['abc'])print(item['abc']) 你看我们这里的输出的的值，都是统一的 1234 ，但是我们这里想做到的是每次获取的值都是栈顶的最新的元素，\b那么我们这个时候就应该用 proxy 模式了 12345678910111213from werkzeug.local import LocalStack, LocalProxytest_stack = LocalStack()test_stack.push({'abc': '123'})test_stack.push({'abc': '1234'})def get_item(): return test_stack.pop()item = LocalProxy(get_item)print(item['abc'])print(item['abc']) 你看我们这里\b就是 Proxy 的妙用。 Context由于 Flask 基于 Werkzeug 实现，因此 App Context 以及 Request Context 是基于前文中所说的 LocalStack 实现。 从命名上，大家应该可以看出，App Context 是代表应用上下文，可能包含各种配置信息，比如日志配置，数据库配置等。而 Request Context 代表一个请求上下文，我们可以获取到当前请求中的各种信息。比如 body 携带的信息。 这两个上下文的定义是在 flask.ctx 文件中，分别是 AppContext 以及 RequestContext 。而构建上下文的操作则是将其推入在 flask.globals 文件中定义的 _app_ctx_stack 以及 _request_ctx_stack 中。前面说了 LocalStack 是“线程”（这里可能是传统意义上的线程，也有可能是 Greenlet 这种）隔离的。\b\b\b\b\b\b\b同时 Flask 每个线程只处理一个请求，因此可以做到请求隔离。 当 app = Flask(__name__) 构造出一个 Flask App 时，App Context 并不会被自动推入 Stack 中。所以此时 Local Stack 的栈顶是空的，current_app 也是 unbound 状态。 1234567891011121314from flask import Flaskfrom flask.globals import _app_ctx_stack, _request_ctx_stackapp = Flask(__name__)_app_ctx_stack.top_request_ctx_stack.top_app_ctx_stack()# &lt;LocalProxy unbound&gt;from flask import current_appcurrent_app# &lt;LocalProxy unbound&gt; 作为 web 时，当请求进来时，我们开始进行上下文的相关操作。整个流程如下： 好了现在有点问题： 为什么要区分 App Context 以及 Request Context 为什么要用栈结构来实现 Context ？ 很久之前看过的松鼠奥利奥老师的博文Flask 的 Context 机制 解答了这个问题 这两个做法给予我们 多个 Flask App 共存 和 非 Web Runtime 中灵活控制 Context 的可能性。 我们知道对一个 Flask App 调用 app.run() 之后，进程就进入阻塞模式并开始监听请求。此时是不可能再让另一个 Flask App 在主线程运行起来的。那么还有哪些场景需要多个 Flask App 共存呢？前面提到了，一个 Flask App 实例就是一个 WSGI Application，那么 WSGI Middleware 是允许使用组合模式的，比如： 123456789from werkzeug.wsgi import DispatcherMiddlewarefrom biubiu.app import create_appfrom biubiu.admin.app import create_app as create_admin_appapplication = DispatcherMiddleware(create_app(), { '/admin': create_admin_app()}) 奥利奥老师文中举了一个这样一个例子，Werkzeug 内置的 Middleware 将两个 Flask App 组合成一个一个 WSGI Application。这种情况下两个 App 都同时在运行，只是根据 URL 的不同而将请求分发到不同的 App 上处理。 但是现在很多朋友有个问题，就是为什么这里不用 Blueprint ？ Blueprint 是在同一个 App 下运行。其挂在 App Context 上的相关信息都是一致的。但是如果要隔离彼此的信息的话，那么用 App Context 进行隔离，会比我们用变量名什么的隔离更为方便 Middleware 模式是 WSGI 中允许的特性，换句话来讲，我们将 Flask 和另外一个遵循 WSGI 协议的 web Framework （比如 Django）那么也是可行的。 但是 Flask 的两种 Context 分离更大的意义是为了非 web 应用的场合。Flask 官方文档中有这样一段话 The main reason for the application’s context existence is that in the past a bunch of functionality was attached to the request context for lack of a better solution. Since one of the pillars of Flask’s design is that you can have more than one application in the same Python process. 这句话换句话说 App Context 存在的意义是针对一个进程中有多个 Flask App 场景，这样场景最常见的就是我们用 Flask 来做一些离线脚本的代码。 好了，我们来聊聊 Flask 非 Web 应用的场景 比如，我们有个插件叫 Flask-SQLAlchemy然后这里有个使用场景首先我们现在有这样一个代码 123456789101112131415from flask import Flaskfrom flask_sqlalchemy import SQLAlchemydatabase = Flask(__name__)database.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'db = SQLAlchemy(database)class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return '&lt;User %r&gt;' % self.username 这里你应该注意到最开始的几个关键点，第一个，就是 database.config ，是的没错，Flask-SQLAlchemy 就是从当前的 app 中获取到对应的 config 信息来建立数据库链接。那么传递 app 的方式有两种，第一种，就是直接如上图一样，直接 db = SQLAlchemy(database) ，这个很容易理解，第二种，如果我们不传的话，那么 Flask-SQLAlchemy 中通过 current_app 来获取当前的 app 然后获取对应的 config 建立链接。那么问题来了，为什么会存在第二种这种方法呢 给个场景吧，现在我两个数据库配置不同的 app 共用一个 Model 那么应该怎么做？其实很简单 首先写 一个 model 文件，比如就叫 data/user_model.py 吧 1234567891011from flask_sqlalchemy import SQLAlchemydb = SQLAlchemy()class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return '&lt;User %r&gt;' % self.username 好了，那么在我们的应用文件中，我们便可以这样写 1234567891011121314151617181920from data.user_model import Userdatabase = Flask(__name__)database.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'with database.app_context(): db.init_app(current_app) db.create_all() admin = User(username='admin', email='admin@example.com') db.session.add(admin) db.session.commit() print(User.query.filter_by(username=&quot;admin&quot;).first())database1 = Flask(__name__)database1.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test1.db'with database1.app_context(): db.init_app(current_app) db.create_all() admin = User(username='admin_test', email='admin@example.com') db.session.add(admin) db.session.commit() print(User.query.filter_by(username=&quot;admin&quot;).first()) 你看这样是不是就好懂了一些，通过 app context ，我们 Flask-SQLAlchemy 可以通过 current_app 来获取当前 app ，继而获取相关的 config 信息 这个例子还不够妥当，我们现在再来换一个例子 12345678910111213141516171819202122from flask import Flask, current_appimport loggingapp = Flask(&quot;app1&quot;)app2 = Flask(&quot;app2&quot;)app.config.logger = logging.getLogger(&quot;app1.logger&quot;)app2.config.logger = logging.getLogger(&quot;app2.logger&quot;)app.logger.addHandler(logging.FileHandler(&quot;app_log.txt&quot;))app2.logger.addHandler(logging.FileHandler(&quot;app2_log.txt&quot;))with app.app_context(): with app2.app_context(): try: raise ValueError(&quot;app2 error&quot;) except Exception as e: current_app.config.logger.exception(e) try: raise ValueError(&quot;app1 error&quot;) except Exception as e: current_app.config.logger.exception(e) 好了，这段代码很清晰了，含义很清晰，就是通过获取当前上下文中的 app 中的 logger 来输出日志。同时这段代码也很清晰的说明了，我们为什么要用栈这样一种数据结构来维护上下文。 首先看一下 app_context() 的源码 123456789101112131415def app_context(self): &quot;&quot;&quot;Binds the application only. For as long as the application is bound to the current context the :data:`flask.current_app` points to that application. An application context is automatically created when a request context is pushed if necessary. Example usage:: with app.app_context(): ... .. versionadded:: 0.9 &quot;&quot;&quot; return AppContext(self) 嗯，很简单，只是构建一个 AppContext 对象返回，然后我们看看相关的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class AppContext(object): &quot;&quot;&quot;The application context binds an application object implicitly to the current thread or greenlet, similar to how the :class:`RequestContext` binds request information. The application context is also implicitly created if a request context is created but the application is not on top of the individual application context. &quot;&quot;&quot; def __init__(self, app): self.app = app self.url_adapter = app.create_url_adapter(None) self.g = app.app_ctx_globals_class() # Like request context, app contexts can be pushed multiple times # but there a basic &quot;refcount&quot; is enough to track them. self._refcnt = 0 def push(self): &quot;&quot;&quot;Binds the app context to the current context.&quot;&quot;&quot; self._refcnt += 1 if hasattr(sys, 'exc_clear'): sys.exc_clear() _app_ctx_stack.push(self) appcontext_pushed.send(self.app) def pop(self, exc=_sentinel): &quot;&quot;&quot;Pops the app context.&quot;&quot;&quot; try: self._refcnt -= 1 if self._refcnt &lt;= 0: if exc is _sentinel: exc = sys.exc_info()[1] self.app.do_teardown_appcontext(exc) finally: rv = _app_ctx_stack.pop() assert rv is self, 'Popped wrong app context. (%r instead of %r)' \\ % (rv, self) appcontext_popped.send(self.app) def __enter__(self): self.push() return self def __exit__(self, exc_type, exc_value, tb): self.pop(exc_value) if BROKEN_PYPY_CTXMGR_EXIT and exc_type is not None: reraise(exc_type, exc_value, tb) emmmm，首先 push 方法就是将自己推入 _app_ctx_stack ，而 pop 方法则是将自己从栈顶推出。然后我们看到两个方法含义就很明确了，在进入上下文管理器的时候，将自己推入栈，然后退出上下文管理器的时候，将自己推出。 我们都知道栈的一个性质就是，后入先出，栈顶的永远是最新插入进去的元素。而看一下我们 current_app 的源码 123456789def _find_app(): top = _app_ctx_stack.top if top is None: raise RuntimeError(_app_ctx_err_msg) return top.app current_app = LocalProxy(_find_app) 嗯，很明了了，就是获取当前栈顶的元素，然后进行相关操作。 嗯，通过这样对于栈的不断操作，就能让 current_app 获取到元素是我们当前上下文中的 app 。 额外的讲解: gg 也是我们常用的几个全局变量之一。在最开始这个变量是挂载在 Request Context 下的。但是在 0.10 以后，g 就是挂载在 App Context 下的。可能有同学不太清楚为什么要这么做。 首先，说一下 g 用来干什么 官方在上下文这一张里有这一段说明 The application context is created and destroyed as necessary. It never moves between threads and it will not be shared between requests. As such it is the perfect place to store database connection information and other things. The internal stack object is called flask._app_ctx_stack. Extensions are free to store additional information on the topmost level, assuming they pick a sufficiently unique name and should put their information there, instead of on the flask.g object which is reserved for user code. 大意就是说，数据库配置和其余的重要配置信息，就挂载 App 对象上。但是如果是一些用户代码，比如你不想一层层函数传数据的话，然后有一些变量需要传递，那么可以挂在 g 上。 同时前面说了，Flask 并不仅仅可以当做一个 Web Framework 使用，同时也可以用于一些非 web 的场合下。在这种情况下，如果 g 是属于 Request Context 的话，那么我们要使用 g 的话，那么就需要手动构建一个请求，这无疑是不合理的。 最后大年三十写这篇文章，现在发出来，我的辣鸡也是无人可救了。Flask 的上下文机制是其最重要的特性之一。通过合理的利用上下文机制，我们可以再更多的场合下去更好的利用 flask 。嗯，本次的辣鸡文章写作活动就到此结束吧。希望大家不会扔我臭鸡蛋！然后新年快乐！","link":"/posts/2018/02/23/something-about-flask-context/"},{"title":"随便聊聊 PEP570","text":"最近沉迷与 MIT 6.824 这门分布式系统的课，无心写文章。不过看到 PEP570 被接受了，决定还是写篇水文随便聊聊 PEP 570 Python 的 argument在聊 PEP570 之前，我们先要来看看 Python 的 argument 变迁 早在 Python 1.0 或更早，Python 的 argument 系统就已经支持我们现在主要使用的两种参数形式了，一种是 positional 一种是 keyword，举几个例子 1234567891011def abc(a, b, c): passabc(1, 2, 3)abc(1, 2, c=3)abc(1, b=2, c=3)abc(*(1, 2, 3))abc(**{&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3}) 这是不是我们常见的集中使用方式？ 在发展了很长一段时间后，虽然期间有一些提案对 Python 的 argument 系统做优化和增强，但是一直都被 Reject，直到 PEP3102 的出现 3102 主要引入了一个概念叫做 Keyword-Only Arguments，给个例子 有这样一个函数定义 12def abc(a, *, b, c): pass 那么这个函数只支持这样几种方式调用 1234567def abc(a, *, b, c): passabc(**{&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3})abc(a=1, b=2, c=3)abc(1, b=2, c=3) OK，大概聊完 Argument 一个迭代的过程，我们来聊聊 570 这个提案 随便聊聊 PEP 570570 做的事情其和 3102 类似，3102 是引入语法糖，让函数支持 keyword-only 的使用方式，那么 570 就是让函数支持 positional-only 的使用方式 假定有这样一个函数定义 12def abc(a, b, /, c): pass 那么 570 使得函数只支持这样的调用方式 123456def abc(a, b, /, c): passabc(1, 2, c=3)abc(1, 2, 3) 如果不这样做会怎么样呢？我们可以来试试，目前 PEP570 有一个实现，参见 bpo-36540: PEP 570 – Implementation，我们来编译测试一下，效果如下 碎碎念很多人其实没想清楚关于 570 的存在意义，PEP 570 上也提到了很多 Motivation 。不过我自己觉得，它和 3102 一样都是在践行一个理念，则 explicit is better than implicit 换句话说，如果要尽可能的保证代码风格的一致性，我们需要一定程度上语法特性的支持。而 570 和 3102 就是解决这样的问题。 所以从我自己的角度来说，我觉得 570 是个蛮重要的提案，也是很有意义的提案（都是 PEP57x ，为啥大家待遇能差这么多呢？（笑 对了，讲个段子， Python 的 Core 之一 Serhiy Storchaka 非常喜欢这个 PEP，然后在 PEP570 的实现没合并到主分支之前，就已经先把内置的一些库给改良了一下，大家可以去围观一下 PR [WIP] Use PEP 570 syntax for positional-only parameters 好了，今天的水文就到此结束了。。我写文章真的是越来越水了。。","link":"/posts/2019/04/27/something-about-pep-570/"},{"title":"关于 pyright","text":"关于 pyrightPEP 484，出来也快四年了。正好今天看到一个新库，写个短文，安利下&amp;吐槽下。 关于 PEP 484PEP 484，14年正式提出，15年正式接纳，成为 Python 3.5 以后的标准的一部分。简而言之是通过额外的语法，来为 Python 引入静态类型检查的例子 举个简单例子 1234def return_callback(flag: bool, callback: typing.Callable[[int, int], int])-&gt; int: if not flag: return None return callback(1, 2) 我们通过这样的类型静态标注，来增加可读性以及静态检查的能力。具体内容，可以参看我去年在 BPUG 上的分享的 slide。我最近也会抽出时间，详细聊聊 Type Hint 的前世今生（flag+1） 静态检查静态检查的意义在于，能及时发现低级错误，及时检查，可以很方便的集成进 CI 或者 Git Hook 中 举个简单例子 目前而言，主流的静态检查工具有两种 Python 官方和 484 配套出的 mypy Google 出的 pytype mypy 目前的问题有： 性能较差 对于新特性接入持保守态度 所以后续 Google 选择了推出自己的静态检查方案 pytype ，其性能相对于 mypy 来讲，性能和易用性也有了比较大的提升， 而目前，“开源急先锋“微软也在今天推出了自己的静态检查工具 pyright。目前在保证了对 Type Hint 周边特性兼容的情况下，宣称性能相较于 mypy 有5倍的提升 而这对于大项目的 CI 来讲是一个极大的利好 不过目前，关于 pyright 的潜在风险点可能还有这样的问题 基于 TypeScript 开发，运行环境基于 node，这可能会带来 CI 集成的难度。 易用性和可靠性还存在不足 可能和 IDE编辑器等配套的插件不足（官方也说，目前 VSCode 的插件还在开发中） 不过，pyright 还是值得大家在私下尝鲜的。后续我也会尝试阅读下 pyright 的实现，看看微软的实现思路（flag+2) 嗯，本文就到这里，这应该是我写过最水的文章了。","link":"/posts/2019/03/24/something-about-pyright/"},{"title":"聊聊 Python 中生成器和协程那点事儿","text":"文章来源：itsCoder 的 WeeklyBolg 项目 itsCoder主页：http://itscoder.com/ 作者：Manjusaka 审阅者：allenwu,Brucezz 写在前面的话本来想这周继续写写 Flask 那点破事儿的，但是想了想决定换换口味，来聊聊很不容易理解但是很重要的 Python 中的生成器和协程。 Generators 科普我猜大家对于生成器肯定并不陌生，但是为了能让我愉快的继续装逼，我们还是用点篇幅讲一下什么是生成器吧。比如在 Python 里，我们想生成一个范围 (1,100000) 的一个 list，于是我们无脑写了如下的代码出来 12345def generateList(start,stop): tempList=[] for i in range(start,stop): tempList.append(i) return tempList 注1：这里有同学提出了为什么我们不直接返回 range(start,stop)，Nice question，这里涉及到一个基础问题，range 的机制究竟是怎样的。这就要分版本而论了，在 Python 2.x 的版本中，range(start,stop) 其实本质上是预先生成一个 list ,而 list 对象是一个 Iterator ，因此可以被 for 语句所使用。然后在 Python 2.x 中还有一个语句叫做 xrange ，其生成的是一个 Generator 对象。在 Python 3 中事情发生了一点变化，可能社区觉得 range 和 xrange 分裂太过蛋疼，于是将其合并，于是现在在 Python 3 中，取消了 xrange 的语法糖，然后 range 的机制也变成生成一个 Generator 而不是 list 但是大家考虑过一个问题么，如果我们想生成数据量非常大，预先生成数据的行为无疑是很不明智的，这样会耗费大量的内存。于是 Python 给我们提供了一种新的姿势，Generator (生成器) 12345678def generateList1(start,stop): for i in range(start,stop): yield iif __name__==&quot;__main__&quot;: c=generateList1(1,100000) for i in c: print(i) 是的，Generator 其中一个特性就是不是一次性生成数据，而是生成一个可迭代的对象，在迭代时，根据我们所写的逻辑来控制其启动时机。 Generator 深入这里可能有一个问题，大家肯定想问 Python 开发者们不可能为了这一种使用场景而去单独创建一个 Generator 机制吧，那么我们 Generator 还有其余的使用场景么。当然，请看标题，对了嘛，Generator 另一个很大作用可以说就是当做协程使用。不过在这之前，我们要去深入的了解下 Generator 才能方便我们后面的讲解。 关于 Generator 中的内建方法关于 Python 中可迭代对象的一点背景知识首先，我们来看看 Python 中的迭代过程。在 Python 中迭代有两个概念，一个是 Iterable ，另一个是 Iterator 。让我们分别来看看第N次首先，Iterable 近似的可以理解成为一个协议，判断一个 Object 是否是 Iterable 的方法就是看其实现了 iter 与否，如果实现了 iter ，那么这便可以认为是一个 Iterable 对象。空谈误国，实干兴邦，让我们直接来看一段代码理解下 12345678910111213141516171819class Counter: def __init__(self, low, high): self.current = low self.high = high def __iter__(self): return self def next(self): # Python 3: def __next__(self) if self.current &gt; self.high: raise StopIteration else: self.current += 1 return self.current - 1if __name__ == '__main__': a=Counter(3,8) for c in a: print(c) 好了，让我们来看看上面这段代码里发生了什么，首先 for 语句的引用首先去判断迭代的是 Iterable 对象还是 Iterator 对象，如果是实现了 __iter__ 方法的对象，那么就是一个 Iterable 对象，for 循环首先调用对象的 __iter__ 方法来获取一个 Iterator 对象。那么什么是 Iterator 对象呢，这里可以近似的理解为是实现了 next() 方法（注：在Python3中是 next 方法)。 OK，让我们继续回到刚刚说到的那里，在上面的代码中 for 语句首先判断是一个 Iterable 对象还是 Iterator 对象，如果是 Iterable 对象那么调用其 iter 方法来获取一个 Iterator 对象，接着 for 循环会调用 Iterator 对象中的 next() （注：Python3 里是 __next__)方法来进行迭代，直到迭代过程结束抛出 StopIteration 异常。 好了，来聊聊 Generator 吧让我们先看看前面那段代码吧： 12345678def generateList1(start,stop): for i in range(start,stop): yield iif __name__==&quot;__main__&quot;: c=generateList1(1,100000) for i in generateList1: print(i) 首先我们要确定一点的是 Generator 其实也是一个 Iterator 对象。OK 让我们来看看上面这段代码，首先 for 确定 generateList1 是一个 Iterator 对象，然后开始调用 next() 方法进行进一步迭代。OK 此时你肯定想问这里面 next() 方法是怎样让 generateList1 进一步往下迭代的呢？答案在于 Generator 的内建 send() 方法。我们还是来看一段代码。 1234567def generateList1(start,stop): for i in range(start,stop): yield iif __name__==&quot;__main__&quot;: a=generateList1(0,5) for i in range(0,5): print(a.send(None)) 这里我们应该输出什么？答案就是 0,1,2,3,4 ，结果上和我们用 for 循环进行运算的结果是不是一样。好了，我们现在可以得出一个结论就是 Generator 迭代的本质就是通过内建的 next() 或 __next__() 方法来调用内建的 send() 方法。 继续吐槽 Generator 的内建方法前面我们提到一个结论 Generator 迭代的本质就是通过内建的 next() 或 __next__() 方法来调用内建的 send() 方法。 现在我们来看个例子： 12345678910111213141516def countdown(n): print &quot;Counting down from&quot;, n while n &gt;= 0: newvalue = (yield n) # If a new value got sent in, reset n with it if newvalue is not None: n = newvalue else: n -= 1if __name__=='__main__': c = countdown(5) for x in c: print x if x == 5: c.send(3) 好了这段代码的输出应该是什么？答案是 [5，2，1，0] ，是不是很迷惑？别急，我们先来看看这段代码的运行流程 简而言之就是，当我们调用 send() 函数的时候，我们 send(x) 的值会发送给 newvalue 向下继续执行直到遇到下一次 yield 的出现，然后返回值作为一个过程的结束。然后我们的 Generator 静静的沉睡在内存中，等待下一次的 send 来唤醒它。 注2：有同志问：“这里没想明白，c.send(3) 是 相当于 yield n 返回了个 3 给 newvalue ?”，好的，nice question，其实这个问题我们看前面之前的代码运行图就知道， c.send(3) 首先，将 3 赋值给 newvalue ，然后程序运行剩下的代码，直到遇到下一个 yield 为止，那么在这里，我们运行剩下完代码，在遇到 yiled n 之前，将 n 的值已经改变为 3 ,接着，yield n 即约等于 return 3。接着 countdown 这个 Generator 将所有变量的状态冻结，然后静静的呆在内存中，等待下一次的 next 或 __next__() 方法或者是 send() 方法的唤醒。 小贴士：我们如果直接调用 send() 的话，第一次请务必 send(None) 只有这样一个 Generator 才算是真正被激活了。我们才能进行下一步操作。 说说关于协程首先关于协程的定义，我们来看一段 wiki Coroutines are computer program components that generalize subroutines for nonpreemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.According to Donald Knuth, the term coroutine was coined by Melvin Conway in 1958, after he applied it to construction of an assembly program.[1] The first published explanation of the coroutine appeared later, in 1963. 简而言之，协程是比线程更为轻量的一种模型，我们可以自行控制启动与停止的时机。在 Python 中其实没有专门针对协程的这个概念，社区一般而言直接将 Generator 作为一种特殊的协程看待，想想，我们可以用 next 或 __next__() 方法或者是 send() 方法唤醒我们的 Generator ，在运行完我们所规定的代码后， Generator 返回并将其所有状态冻结。这是不是很让我们 Excited 呢！！ 关于 Generator 的一点课后作业现在我们要后序遍历二叉树，我知道看这篇文章神犇们都能无脑写出来的，让我们看看代码先： 123456789101112131415161718class Node(object): def __init__(self, val, left, right): self.val = val self.left = left self.right = rightdef visit_post(node): if node.left: return visit_post(node.left) if node.right: return visit_post(node.right) return node.valif __name__ == '__main__': node = Node(-1, None, None) for val in range(100): node = Node(val, None, node) print(list(visit_post(node))) 但是，我们知道递归深度太深的话，我们要么爆栈要么 py 交易失败，OK ，Generator 大法好，把你码农平安保，还是直接看代码： 123456789101112131415161718192021222324252627def visit_post(node): if node.left: yield node.left if node.right: yield node.right yield node.valdef visit(node, visit_method): stack = [visit_method(node)] while stack: last = stack[-1] try: yielded = next(last) except StopIteration: stack.pop() else: if isinstance(yielded, Node): stack.append(visit_method(yielded)) elif isinstance(yielded, int): yield yieldedif __name__ == '__main__': node = Node(-1, None, None) for val in range(100): node = Node(val, None, node) visit_generator = visit(node, visit_method=visit_post) print(list(visit_generator)) 看起来很复杂是不是？没事当做课后作业，大家可以在评论里给我留言，我们一起进行一下 py 交易吧~ 参考链接1.提高你的Python: 解释‘yield’和‘Generators（生成器）’2.yield大法好3.http://my.oschina.net/1123581321/blog/1605604.python的迭代器为什么一定要实现__iter__方法(关于迭代器那离，为了便于理解，我简化了一些东西，具体可以参看这个问题的高票答案)","link":"/posts/2016/09/11/something-about-yield-in-python/"},{"title":"我与 PyCon China 这两年","text":"其实这篇文章最开始动笔是写于9月份，PyCon China 2019 上海场的工作结束后。后续因为还有北京，成都场的工作，所以拖到了现在。正好我自己的三年计划刚刚落下帷幕。下一个三年计划正在开展。我也来聊聊在这三年里面，让我花费精力最多，也是最为重要的部分组成之一吧。 PyCon China，嗯这三年的时间里，我有两年都在与这个熟悉而陌生的名字关联在了一起 我与 PyCon China 的结识说实话，之前 Laike9M 的一句话让我产生了共鸣 Kenneth Reitz 曾经说，他的一年是按 PyCon 计算的。尽管围绕他有很多争议，这句话依然让我有了奇妙的共鸣。对他来说，”PyCon”自然是指 PyCon US，而对我来说，则是 PyCon China 而我18/19年这两年的时间里，有很大一部分时间都在围绕着 PyCon China 要说最早结识 PyCon，应该能追溯到的 2016 年，当时初学 Python 的我，看到 David Beazley 在 PyCon US 2009 上分享的一个名为 A Curious Course on Coroutines and Concurrency 的分享，简直惊为天人 也让我当时立下来几个 Flag 在 PyCon China 做一次分享 组织一次 PyCon China 提一个被接受的 PEP 提案 晋升成为大陆第一位 Python Core Developer（不过在16年11月，来自华为的 angwer 晋升成为大陆第一位 Core 后，这个 Flag 就有所修改了23333) 嗯，后续我也在持续关注 PyCon China 的进展，却发现，历年 PyCon China 口碑在不断的下滑，也感觉十分的痛心 在17年，PyCon China 口碑进入谷底，当时年少轻狂的我，跑去邮件组喷了一圈后，又在考虑，我自己的能力是不是足够支撑一场大型会务筹办，可能需要先试试水。所以决定和小伙伴以『Python 北京开发者社区』的名义，组织了一场『Python 北京开发者活动第一期』。借用 Thoughtworks 的场地在北京自行发起了一场活动。从事后的反馈来看，这个活动还是相对成功的，因此我也坚定了参与进 PyCon China 的意向。BTW，这次活动也收获了很多很棒的朋友，比如18年北京场的志愿者负责人姚前，19年组织者刘玉龙等人，都结识于此。 18年3月，我正式通过邮件的形式向大妈提出参与进 PyCon China 中来，7月，和 PyCon China 幕后的负责人辛庆老师见面后，我正式参与进18年 PyCon China 的筹办中来。然后入坑之后没法逃脱，，我也全程参与了19年的筹备工作 聊聊 PyCon China 这两年说实话，我不止一次向辛老师吐槽过 我觉得只有傻逼才会来参与办这个会 辛老师说： 不用你说，我们所有人都觉得自己是傻逼 办会真的太苦了。。。。我是18年北京场负责人、成都场负责人/讲师，19年北京/上海/成都负责人，成都场讲师。说实话，这两年结束之后，我基本都有半个月缓不过劲来。嗯，就是那种基本要虚脱的感觉 很多人要问，办会真的这么惨么？ 是的，很惨。我大概说一下目前 PyCon China 的运作模式吧 PyCon China 目前由一群志愿者在承担幕后的会务工作，大概从每年3到4月开始，进入今年的 PyCon China 的筹备状态 我们的工作包括不仅限于下面这样一些部分 场地的选取 讲师的征集 今年周边的设计 主题的审核以及讲师预讲服务 赞助 国内外社区沟通 媒体稿的准备 会务现场 基本进入每年的8月开始，到10月结束，基本每个人都处于高度紧张模式，基本每天大家都需要折腾到凌晨1/2点才能完成当天的准备工作（因为大家都是志愿准备工作） 而一旦各个城市开始进入会时，各位组织者也将迎来更大的挑战，无论是体能上还是具体的事务中，举个例子，我上海场/北京场都是两天睡了4个小时不到，而成都场更惨，因为日本讲师因为台风将缺席上午的分享，我需要临时做 PPT 进行补位，因此72小时也就睡了6个小时。 所以说，做会，无论是对于体能，还是自己各方面的能力，都会提出很高的要求。 为什么？其实我也不知道为什么，可能真的是傻逼吧？ 说实话，这两年 PyCon China 在我人生最重要的职业初期的两年中扮演了非常非常重要的角色，在这里一直被大家溺爱，包容着，也教会了我许许多多。所以吧，我感恩，痛苦，然后快乐着 有朋友说：以我片面的视角来看，Manjusaka 正是让 PyCon China 涅槃重生的关键人物。 其实我非常感谢大家的认可，但是不得不说，我只是这两年中微不足道的一环，在幕后，有太多让人感动并产生对这个群体的归属感的事情发生。比如18年垫付了20多万亏损的辛庆老师，做设计做到哭，但是边哭边做还是把设计成功的交出来的错姐，在背后一直默默支持的志愿者大管家倩姨，财务大管家大猫，因为台风取消航班但是依旧选择来到中国的日本小姐姐藤井美娜等等等。正是因为这样一群人的存在，PyCon China 才能不断的进步与成长 所以吧，很荣幸能在人生最重要的几年时间里有这样的一段难以忘怀的经历。嗯，一下也不知道怎么说了 关于未来关于未来的话，职业方面的话，目前刚调动到阿里云做中间件相关的开发，应该还是会继续在技术这条路上走很长的一段时间吧。然后对于 PyCon China 2020，目前的筹备工作已经开展，不过因为我自己的身体原因，还不确定是否会参与，不过不出以外的话，虽然可能没法像18/19年这样支撑多城市（因为我们各城市的志愿者也成长起来啦！），但是大家也会见到我的身影 最后两张图镇楼 最后 PyCon China 2020 已经开始筹备工作，欢迎大家加入！","link":"/posts/2019/11/03/somethng-between-me-pycon-china/"},{"title":"聊聊网络事件中的惊群效应","text":"关于惊群问题，其实我是在去年开始去关注的。然后向 CPython 提了一个关于解决 selector 的惊群问题的补丁 BPO-35517。现在大概来聊聊关于惊群问题那点事吧 惊群问题的过去惊群问题是什么？惊群问题又名惊群效应。简单来说就是多个进程或者线程在等待同一个事件，当事件发生时，所有线程和进程都会被内核唤醒。唤醒后通常只有一个进程获得了该事件并进行处理，其他进程发现获取事件失败后又继续进入了等待状态，在一定程度上降低了系统性能。 可能很多人想问，惊群效应为什么会占用系统资源？降低系统性能？ 多进程/线程的唤醒，涉及到的一个问题是上下文切换问题。频繁的上下文切换带来的一个问题是数据将频繁的在寄存器与运行队列中流转。极端情况下，时间更多的消耗在进程/线程的调度上，而不是执行 接下来我们来聊聊我们网络编程中常见的惊群问题。 常见的惊群问题在 Linux 下，我们常见的惊群效应发生于我们使用 accept 以及我们 select 、poll 或 epoll 等系统提供的 API 来处理我们的网络链接。 accept 惊群首先我们用一个流程图来复习下我们传统的 accept 使用方式 那么在这里存在一种情况，即当一个请求到达时，所有进程/线程都开始 accept ，但是最终只有一个获取成功，我们来写段代码看看 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;arpa/inet.h&gt;#include &lt;assert.h&gt;#include &lt;errno.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#define SERVER_ADDRESS &quot;0.0.0.0&quot;#define SERVER_PORT 10086#define WORKER_COUNT 4int worker_process(int listenfd, int i) { while (1) { printf(&quot;I am work %d, my pid is %d, begin to accept connections \\n&quot;, i, getpid()); struct sockaddr_in client_info; socklen_t client_info_len = sizeof(client_info); int connection = accept(listenfd, (struct sockaddr *)&amp;client_info, &amp;client_info_len); if (connection != -1) { printf(&quot;worker %d accept success\\n&quot;, i); printf(&quot;ip :%s\\t&quot;, inet_ntoa(client_info.sin_addr)); printf(&quot;port: %d \\n&quot;, client_info.sin_port); } else { printf(&quot;worker %d accept failed&quot;, i); } close(connection); } return 0;}int main() { int i = 0; struct sockaddr_in address; bzero(&amp;address, sizeof(address)); address.sin_family = AF_INET; inet_pton(AF_INET, SERVER_ADDRESS, &amp;address.sin_addr); address.sin_port = htons(SERVER_PORT); int listenfd = socket(PF_INET, SOCK_STREAM, 0); int ret = bind(listenfd, (struct sockaddr *)&amp;address, sizeof(address)); ret = listen(listenfd, 5); for (i = 0; i &lt; WORKER_COUNT; i++) { printf(&quot;Create worker %d\\n&quot;, i + 1); pid_t pid = fork(); /*child process */ if (pid == 0) { worker_process(listenfd, i); } if (pid &lt; 0) { printf(&quot;fork error&quot;); } } /*wait child process*/ int status; wait(&amp;status); return 0;} 我们来看看运行的结果 诶？怎么回事？为什么这里没有出现我们想要的现象（一个进程 accept 成功，三个进程 accept 失败）？原因在于在 Linux 2.6 之后，Accept 的惊群问题从内核上被处理了 好，我们接着往下看 select/poll/epoll 惊群我们以 epoll 为例，我们来看看传统的工作模式 好了，我们来看段代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;netdb.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#define SERVER_ADDRESS &quot;0.0.0.0&quot;#define SERVER_PORT 10087#define WORKER_COUNT 4#define MAXEVENTS 64static int create_and_bind_socket() { int fd = socket(PF_INET, SOCK_STREAM, 0); struct sockaddr_in server_address; server_address.sin_family = AF_INET; inet_pton(AF_INET, SERVER_ADDRESS, &amp;server_address.sin_addr); server_address.sin_port = htons(SERVER_PORT); bind(fd, (struct sockaddr *)&amp;server_address, sizeof(server_address)); return fd;}static int make_non_blocking_socket(int sfd) { int flags, s; flags = fcntl(sfd, F_GETFL, 0); if (flags == -1) { perror(&quot;fcntl error&quot;); return -1; } flags |= O_NONBLOCK; s = fcntl(sfd, F_SETFL, flags); if (s == -1) { perror(&quot;fcntl set error&quot;); return -1; } return 0;}int worker_process(int listenfd, int epoll_fd, struct epoll_event *events, int k) { while (1) { int n; n = epoll_wait(epoll_fd, events, MAXEVENTS, -1); printf(&quot;Worker %d pid is %d get value from epoll_wait\\n&quot;, k, getpid()); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (listenfd == events[i].data.fd) { struct sockaddr in_addr; socklen_t in_len; int in_fd; in_len = sizeof(in_addr); in_fd = accept(listenfd, &amp;in_addr, &amp;in_len); if (in_fd == -1) { printf(&quot;worker %d accept failed\\n&quot;, k); break; } printf(&quot;worker %d accept success\\n&quot;, k); close(in_fd); } } } return 0;}int main() { int listen_fd, s; int epoll_fd; struct epoll_event event; struct epoll_event *events; listen_fd = create_and_bind_socket(); if (listen_fd == -1) { abort(); } s = make_non_blocking_socket(listen_fd); if (s == -1) { abort(); } s = listen(listen_fd, SOMAXCONN); if (s == -1) { abort(); } epoll_fd = epoll_create(MAXEVENTS); if (epoll_fd == -1) { abort(); } event.data.fd = listen_fd; event.events = EPOLLIN; s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); for (int i = 0; i &lt; WORKER_COUNT; i++) { printf(&quot;create worker %d\\n&quot;, i); int pid = fork(); if (pid == 0) { worker_process(listen_fd, epoll_fd, events, i); } } int status; wait(&amp;status); free(events); close(listen_fd); return EXIT_SUCCESS;} 然后，我们用 telnet 发送一下 TCP 请求，看看效果，，我们能得到这样的结果 恩，我们能看到当一个请求到达时，我们四个进程都被唤醒了。现在为了更直观的看到这一个过程，我们用 strace 来 profile 一下 我们还是能看到，四个进程都被唤醒，但是只有 Worker 3 成功 accept ，而其余的进程在 accept 的时候，都获取到了 EAGAIN 错误， 而 Linux 文档 对于 EAGAIN 的描述是 The socket is marked nonblocking and no connections are present to be accepted. POSIX.1-2001 and POSIX.1-2008 alloweither error to be returned for this case, and do not require these constants to have the same value, so a portableapplication should check for both possibilities. 现在我们对于 EPOLL 的惊群问题是不是有了直观的了解？那么怎么样去解决惊群问题呢？ 惊群问题的现在从内核解决惊群问题首先如前面所说，Accept 的惊群问题在 Linux Kernel 2.6 之后就被从内核的层面上解决了。但是 EPOLL 怎么办？在 2016 年一月，Linux 之父 Linus 向内核提交了一个补丁 参见 epoll: add EPOLLEXCLUSIVE flag 其中的关键代码是 1234if (epi-&gt;event.events &amp; EPOLLEXCLUSIVE) add_wait_queue_exclusive(whead, &amp;pwq-&gt;wait);else add_wait_queue(whead, &amp;pwq-&gt;wait); 简而言之，通过增加一个 EPOLLEXCLUSIVE 标志位作为辅助。如果用户开启了 EPOLLEXCLUSIVE ，那么在加入内核等待队列时，使用 add_wait_queue_exclusive 否则则使用 add_wait_queue 至于这两个函数的用法，可以参考这篇文章Handing wait queues 其中有这样一段描述 The add_wait_queue( ) function inserts a nonexclusive process in the first position of a wait queue list. The add_wait_queue_exclusive( ) function inserts an exclusive process in the last position of a wait queue list. 好了，我们现在来改一下我们的代码（内核版本要在 Linux Kernel 4.5）之后 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;netdb.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#define SERVER_ADDRESS &quot;0.0.0.0&quot;#define SERVER_PORT 10086#define WORKER_COUNT 4#define MAXEVENTS 64static int create_and_bind_socket() { int fd = socket(PF_INET, SOCK_STREAM, 0); struct sockaddr_in server_address; server_address.sin_family = AF_INET; inet_pton(AF_INET, SERVER_ADDRESS, &amp;server_address.sin_addr); server_address.sin_port = htons(SERVER_PORT); bind(fd, (struct sockaddr *)&amp;server_address, sizeof(server_address)); return fd;}static int make_non_blocking_socket(int sfd) { int flags, s; flags = fcntl(sfd, F_GETFL, 0); if (flags == -1) { perror(&quot;fcntl error&quot;); return -1; } flags |= O_NONBLOCK; s = fcntl(sfd, F_SETFL, flags); if (s == -1) { perror(&quot;fcntl set error&quot;); return -1; } return 0;}int worker_process(int listenfd, int epoll_fd, struct epoll_event *events, int k) { while (1) { int n; n = epoll_wait(epoll_fd, events, MAXEVENTS, -1); printf(&quot;Worker %d pid is %d get value from epoll_wait\\n&quot;, k, getpid()); sleep(0.2); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (listenfd == events[i].data.fd) { struct sockaddr in_addr; socklen_t in_len; int in_fd; in_len = sizeof(in_addr); in_fd = accept(listenfd, &amp;in_addr, &amp;in_len); if (in_fd == -1) { printf(&quot;worker %d accept failed\\n&quot;, k); break; } printf(&quot;worker %d accept success\\n&quot;, k); close(in_fd); } } } return 0;}int main() { int listen_fd, s; int epoll_fd; struct epoll_event event; struct epoll_event *events; listen_fd = create_and_bind_socket(); if (listen_fd == -1) { abort(); } s = make_non_blocking_socket(listen_fd); if (s == -1) { abort(); } s = listen(listen_fd, SOMAXCONN); if (s == -1) { abort(); } epoll_fd = epoll_create(MAXEVENTS); if (epoll_fd == -1) { abort(); } event.data.fd = listen_fd; // add EPOLLEXCLUSIVE support event.events = EPOLLIN | EPOLLEXCLUSIVE; s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); for (int i = 0; i &lt; WORKER_COUNT; i++) { printf(&quot;create worker %d\\n&quot;, i); int pid = fork(); if (pid == 0) { worker_process(listen_fd, epoll_fd, events, i); } } int status; wait(&amp;status); free(events); close(listen_fd); return EXIT_SUCCESS;} 然后我们来看看效果 诶？为什么还是有两个进程被唤醒了？原因在于 EPOLLEXCLUSIVE 只保证唤醒的进程数小于等于我们开启的进程数，而不是直接唤醒所有进程，也不是只保证唤醒一个进程 我们来看看官方的描述 Sets an exclusive wakeup mode for the epoll file descriptor that is being attached to the target file descriptor, fd. When a wakeup event occurs and multiple epoll file descriptors are attached to the same target file using EPOLLEXCLUSIVE, one or more of the epoll file descriptors will receive an event with epoll_wait(2). The default in this scenario (when EPOLLEXCLUSIVE is not set) is for all epoll file descriptors to receive an event. EPOLLEXCLUSIVE is thus useful for avoid‐ ing thundering herd problems in certain scenarios. 恩，换句话说，就目前而言，系统并不能严格保证惊群问题的解决。很多时候我们还是要依靠应用层自身的设计来解决 应用层解决目前而言，应用解决惊群有两种策略 这是可以接受的代价，那么我们暂时不管。这是我们大多数的时候的策略 通过加锁或其余的手段来解决这个问题，最典型的例子是 Nginx 我们来看看 Nginx 怎么解决这样的问题的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758voidngx_process_events_and_timers(ngx_cycle_t *cycle){ ngx_uint_t flags; ngx_msec_t timer, delta; if (ngx_timer_resolution) { timer = NGX_TIMER_INFINITE; flags = 0; } else { timer = ngx_event_find_timer(); flags = NGX_UPDATE_TIME; } if (ngx_use_accept_mutex) { if (ngx_accept_disabled &gt; 0) { ngx_accept_disabled--; } else { if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) { return; } if (ngx_accept_mutex_held) { flags |= NGX_POST_EVENTS; } else { if (timer == NGX_TIMER_INFINITE || timer &gt; ngx_accept_mutex_delay) { timer = ngx_accept_mutex_delay; } } } } delta = ngx_current_msec; (void) ngx_process_events(cycle, timer, flags); delta = ngx_current_msec - delta; ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;timer delta: %M&quot;, delta); ngx_event_process_posted(cycle, &amp;ngx_posted_accept_events); if (ngx_accept_mutex_held) { ngx_shmtx_unlock(&amp;ngx_accept_mutex); } if (delta) { ngx_event_expire_timers(); } ngx_event_process_posted(cycle, &amp;ngx_posted_events);} 我们这里能看到，Nginx 主体的思想是通过锁的形式来处理这样问题。我们每个进程在监听 FD 事件之前，我们先要通过 ngx_trylock_accept_mutex 去获取一个全局的锁。如果拿锁成功，那么则开始通过ngx_process_events 尝试去处理事件。如果拿锁失败，则放弃本次操作。所以从某种意义上来讲，对于某一个 FD ，Nginx 同时只有一个 Worker 来处理 FD 上的事件。从而避免惊群。 总结这篇文章从去年到现在拖了很久了，惊群问题一直是我们日常工作中遇到的问题，我自己觉得，还是有必要写篇详细的笔记，记录下去年到现在的一些学习记录。差不多就这样吧，祝各位看的好。","link":"/posts/2019/03/28/somthing-about-thundering-herd/"},{"title":"简单聊聊 MySQL 全文索引","text":"最近踩 MYSQL 中文本搜索的坑踩了挺多，来写个具体的文章总结下 MYSQL 中文本搜索的一些知识点吧 模糊搜索在我们是使用 MYSQL 的过程中，总会有一些模糊搜索的需求，比如我们现在有这样一张表 1234567891011create table if not exists `user`( `id` bigint(20) not null auto_increment, `name` varchar(255) not null, `age` int not null, `update_time` timestamp not null, `create_time` timestamp not null, index (`name`), primary key (`id`)) engine = InnoDB charset = 'utf8mb4'; 现在我们需要对于 name 做一些模糊匹配的需求，比如我们需要去匹配 name 中包含 草 字，于是大家仔细一想，OK，写出了如下的 SQL 1select * from user where name like '%草%' 好了，当你行高采烈的将这段代码上线后，你发现，线上炸了，为啥？因为 MYSQL 的坑. MYSQL 的 like 查询存在这样两个限制 只有前缀匹配 ‘草%’ 和后缀匹配 ‘%草’ 才会走索引，而任意匹配则不会 当无法走索引的时候，MYSQL 会遍历全表来查询数据 当你一个表的数据规模很大的时候，那么暴力扫表必然会带来极大的开销 但是我们实际工作中这样的任意匹配的需求肯定很多，那么我们应该怎么做？或许可以尝试下全文搜索 全文搜索简单聊聊全文搜索全文搜索大家已经不太陌生了，简而言之用一种不太精确的说法就是，用一组关键词在一堆文本数据中寻找匹配项。在目前业界比较主流的全文搜索方案有： 支持全文搜索的关系行数据库 Apache Lucene 基于 Apache Lucene 的 ElasticSearch Apache Solr 后两种是目前业界主要的方案，可能很多全文搜索的需求都会考虑用 ES 或者 Solr 实现。但是这样一种方法并不是无代价的。有这样几个比较现实的问题 ES/Solr 在数据量比较大的情况下的运维问题，怎么样保证集群的 HA 将是一个很考研团队功底的问题 怎么样将 MYSQL 或其余数据源中的数据实时/离线 ETL 至 Search Engine 中 新增的学习与 Codebase 的维护成本。 新增一个依赖之后，对于系统整体的 HA 的保证 在技术决策中，我们往往需要去衡量一个选项的 ROI 来辅助决策。如果我们面对一个比较简单的搜索场景，那么选用 ES/Solr 所带来的开销将会使其 ROI 变得相对较低。因此在一些简单的场景，我们可能会更希望利用数据库本身的能力来完成我们的需求 所幸，在 MySQL 5.5 之后，其支持了一定的全文搜索的能力 MySQL 全文搜索MYSQL 全文搜索的前提是需要在表中建立一个 Full Text Index 12alter table `user` ADD FULLTEXT INDEX name_index (`name`); 注意全文索引，仅对类型为 CHAR/VARCHAR/TEXT 的字段生效。 然后，我们插入两条数据 12345insert into `user` (name, age, createTime, updateTime)values ('Jeff.S.Wang', 18, current_timestamp, current_timestamp);insert into `user` (name, age, createTime, updateTime)values ('Jeff.Li', 18, current_timestamp, current_timestamp); 好了，我们可以来看看 MYSQL 怎么进行全文查询了 首先，按照官方的定义， 1MATCH (col1,col2,...) AGAINST (expr [search_modifier]) 而 search_modifier 是所选取的匹配模式，在MYSQL中共有四种 IN NATURAL LANGUAGE MODE 自然语言模式 IN NATURAL LANGUAGE MODE WITH QUERY EXPANSION 自然语言带扩展模式 IN BOOLEAN MODE 逻辑模式 WITH QUERY EXPANSION 扩展模式 我们常用的是 自然语言模式 和 逻辑模式。 首先来聊聊 自然语言模式，很简单，顾名思义，MYSQL 会直接计算待匹配关键字，然后返回对应的值，这里引用一段官网的解释： By default or with the IN NATURAL LANGUAGE MODE modifier, the MATCH() function performs a natural language search for a string against a text collection. A collection is a set of one or more columns included in a FULLTEXT index. The search string is given as the argument to AGAINST(). For each row in the table, MATCH() returns a relevance value; that is, a similarity measure between the search string and the text in that row in the columns named in the MATCH() list. 我们来写一段 SQL 123select * from `user` where MATCH(name) AGAINST('Jeff' IN NATURAL LANGUAGE MODE) 然后我们发现能得到如下的结果 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 然后，我们来尝试匹配下用户的 LastName，比如我们想找一位姓 Wang 的用户 然后我们写出了如下的 SQL 123select * from `user` where MATCH(name) AGAINST('Jeff' IN NATURAL LANGUAGE MODE) 得到如下结果 id name age updateTime createTime 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 然后我们开始尝试，去搜索一位姓 Li 的用户，然后我们写下了，如下的 SQL 123select * from `user` where MATCH(name) AGAINST('Li' IN NATURAL LANGUAGE MODE) 然后我们发现，什么结果都没有？？？？？WTF？Why？ 原因在于分词粒度，在我们进行录入新数据的时候，MySQL 会将我们的索引字段中的数据按照一定的分词基准长度进行分词，然后存储以待查询，其有四个参数控制分词的长度 innodb_ft_min_token_size innodb_ft_max_token_size ft_min_word_len 作用同上，不过是针对 MyISAM 引擎 ft_max_word_len 以 InnoDB 为例，其默认的 innodb_ft_min_token_size 的值是 3，换句话说在我们之前的录入的数据中，我们数据中存储的分词后的单元是 Jeff Wang 所以我们第二次搜索没有结果，现在我们将 MySQL 的参数修改一下后，重新执行一下？ 123select * from `user` where MATCH(name) AGAINST('Li' IN NATURAL LANGUAGE MODE) 还还还是不行？？？？ 查了下官方文档后，我们发现有这样的描述 Some variable changes require that you rebuild the FULLTEXT indexes in your tables. Instructions for doing so are given later in this section. 而索引分词粒度也包含在其中，，所以我们需要删除/rebuild索引，，然后重新执行（有点坑。。） 123select * from `user` where MATCH(name) AGAINST('Li' IN NATURAL LANGUAGE MODE) 好了，现在正常的返回结果了 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 现在让我们来聊聊另一种匹配模式，BOOLEAN MODE 逻辑模式允许我们用一些操作符来检索一些数据，我们举一些常见的例子，剩下大家可以去看看 MYSQL 官方文档 AGAINST(‘Jeff Li’ IN BOOLEAN MODE) 表示，要么存在 Jeff 要么存在 Li AGAINST(‘+Jeff’ IN BOOLEAN MODE) 表示，必须存在 Jeff AGAINST(‘+Jeff -Li’ IN BOOLEAN MODE) 表示 必须存在 Jeff 且 Li 必须不存在 我们来执行下这几个 SQL 123select * from `user` where MATCH(name) AGAINST('Jeff Li' IN BOOLEAN MODE) 结果 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 123select *from `user` where MATCH(name) AGAINST('+Jeff' IN BOOLEAN MODE) 结果 id name age updateTime createTime 1 Jeff Li 18 2020-03-01 15:38:07 2020-03-01 15:38:07 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 123select * from `user` where MATCH(name) AGAINST('+Jeff -Li' IN BOOLEAN MODE) 结果 id name age updateTime createTime 2 Jeff.S.Wang 18 2020-03-01 15:42:28 2020-03-01 15:42:28 好，现在我们有一些中文搜索的需求，我们先来插入数据 12insert into `user` (name, age, createTime, updateTime)values ('奥特曼', 18, current_timestamp, current_timestamp); 现在我们来搜索姓奥的用户，我们按照之前的 Guide 写出了如下的 SQL 123select *from `user`where MATCH(name) AGAINST('+奥' IN BOOLEAN MODE) 然后我们惊喜的发现，又又又没有结果？？？Why？？？ 其实还是之前提到过的一个问题，分词，MySQL 的默认的分词引擎，只支持英文的分词，而不支持中文分词，那么没有分词，没有搜索？怎么办？ 在 MySQL 5.7 之后，MySQL 提供了 ngram 这个组件来帮助我们进行中文分词，使用很简单 12alter table `user` add fulltext index name_index (`name`) with parser ngram; 这里有几点要注意： ngram 不仅适用于中文，按照官方文档，韩文，日文也都支持 一个字段上只能有一个全文索引，所以需要删除原有全文索引 同时，如同默认的分词一样，ngram 也受分词粒度的限制，不过 ngram 的设置参数是 ngram_token_size 我们按照需要设置即可 总结全文搜索对于日常开发来讲，是一个很常见的需求，在我们的 infra 没法让我们去安心的使用外部组件的时候，利用数据库提供的能力也许是个不错的选项。不过还是有很多的坑要踩，有很多的参数要优化。。BTW 阿里云的 RDS 设置真的难用（小声吐槽 好了。。我的拖延症实在没救了。。而且这两天牙疼真的无奈，呜呜呜呜呜","link":"/posts/2020/03/01/talk-about-full-text-search-in-mysql/"},{"title":"「最简单」的 Core Data 上手指南","text":"原文地址：The Easiest Core Data 原文作者：Alberto De Bortoli 译文出自：掘金翻译计划 译者：Zheaoli 校对者：Kulbear, cbangchen 在过去的几个月里，我花费了大量的时间在研究 Core Data 之上，我得去处理一个使用了很多陈旧的代码，糟糕的 Core Data 以及违反了多线程安全的项目。讲真，Core Data 学习起来非常的困难，在学习 Core Data 的时候，你肯定会感到迷惑和一种深深的挫败感。正是因为这些原因，我决定给出一种超级简单的解决方案。这个方案的特点就是简洁，线程安全，非常易于使用，这个方案能满足你大部分对于 Core Data 的需求。在经过若干次的迭代后，我所设计的方案最终成为一个成熟的方案。 OK，女士们，先生们，现在请允许我隆重向您介绍 Skiathos 和 Skopelos。其中 Skiathos 是基于 Objective-C 所开发的，而 Skopelos 则基于 Swift 所开发的。这两个框架的名字来源于希腊的两个岛，在这里，我渡过了2016年的夏天，同时，在这里完成了两个框架的编写工作。 写在前面的话整个项目的目的就是能够让您以及其简便的方式在您的 App 中引入 Core Data。 我们将从如下几个方面来进行一个介绍: CoreDataStack AppStateReactor DALService (Data Access Layer) CoreDataStack如果你有过使用 Core Data 的经验，那么你应该知道创建一个堆栈是一个充满陷阱的过程。这个组件是用于创建堆栈（用于管理 Obejct Context ），具体的设计说明可以参看 Marcus Zarra 所写的这篇文章。 其中一个和 Magical Record 或者其余第三方插件不同的是，整个存储过程都是在一个方向上发起的，可能是从某个子节点向下或者向上传递来进行持久化储存。其余的组件允许你创建以 private context 作为父节点的子节点，这将会导致 main context 不能被更新，同时只能通过通知的方式来进行合并更新。main context 是相对固定的并与 UI 进行了绑定：这样较为简单的方式可以帮助开发者更好的去完成一个 APP 的开发。 AppStateReactor唔，其实你可以忽略这一段。这个组件属于 CoreDataStack ，在 App 切换至后台，失去节点，或者即将退出时，它负责监视相对应的修改，并把其保存。 DALService (Data Access Layer) / (Skiathos/Skopelos)如果你拥有使用 Core Data 的经验，那么你也应该知道，我们大部分操作都是重复的，我们经常在一个 context 中调用 performBlock:/performBlockAndWait: 函数，而这个 Context 提供了一个最终会调用 save: 作为最终语句的 block 。数据库的所有操作都是基于 API 中所提供的 read: 和 write: ：这两个协议提供了 CQRS （命令和查询分离） 的实现。用于读取的代码块将在主体中进行运行（因为这被认为是一个已确定的单个资源）。用于写入的代码块将会在一个子线程中运行，这样可以保证实时的进行数据储存，变化的数据将会在不会阻塞主线程的情况下通过异步的方式进行储存。write:completion: 方法将会程序运行完后来对数据的更改进行持久化储存。 换句话说，写入的数据在 main managed object context 和最后持久化过程中都会保证其一致性。在 主要管理对象的 context 中，相应的数据也能保证其可用性。 Skiathos/Skopelos 是 DALService 的子类, 这样可以给这个组件一个比较好听的名字。 使用介绍在使用这一系列组件之前，你首先需要创建一个类型为 Skiathos 的属性，然后以下面这种方式去初始化它： 123self.skiathos = [Skiathos setupInMemoryStackWithDataModelFileName:@&quot;&lt;#datamodelfilename&gt;&quot;];// orself.skiathos = [Skiathos setupSqliteStackWithDataModelFileName:@&quot;&lt;#datamodelfilename&gt;&quot;]; 在使用 Skopelos 时，代码如下所示： 123self.skopelos = SkopelosClient(inMemoryStack: &quot;&lt;#datamodelfilename&gt;&quot;)// orself.skopelos = SkopelosClient(sqliteStack: &quot;&lt;#datamodelfilename&gt;&quot;) 你可以通过使用依赖注入的方式来在应用的其余地方使用这些对象。不得不说，为 Core Data 栈上的不同对象创建单例是一种很不错的做法。当然，不断的创建实例的开销是十分巨大的。通常来讲，我们不是很推荐使用单例模式。单例模式的测试性不强，在使用过程中，使用者无法有效的控制其声明周期，这样可能会违背一些最佳实践的编程原则。正是因为如此，在这个库里，我们不推荐使用单例。 由于下面几个原因，你在使用时需要从 Skiathos/Skopelos 进行继承： 创建一个全局可共享的实例。 重载 handleError(error: NSError) 方法，以便在你的程序里出现一些错误时，这个方法能够正常的被调用。 为了创建单例，你应该如下面的示例一样去从 Skiathos/Skopelos 进行继承： 单例12345678910111213141516171819@interface SkiathosClient : Skiathos+ (SkiathosClient *)sharedInstance;@endstatic SkiathosClient *sharedInstance = nil;@implementation SkiathosClient+ (SkiathosClient *)sharedInstance{ static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ sharedInstance = [self setupSqliteStackWithDataModelFileName:@&quot;&lt;#datamodelfilename&gt;&quot;];&lt;/#datamodelfilename&gt; }); return sharedInstance;}- (void)handleError:(NSError *)error{ // clients should do the right thing here NSLog(@&quot;%@&quot;, error.description);}@end 或者是 1234567class SkopelosClient: Skopelos { static let sharedInstance = Skopelos(sqliteStack: &quot;DataModel&quot;) override func handleError(error: NSError) { // clients should do the right thing here print(error.description) }} 读写操作写到这里，让我们同时看看在一个标准 Core Data 的操作方式和我们组件所提供的方式吧。 标准的读取姿势: 1234567891011__block NSArray *results = nil;NSManagedObjectContext *context = ...;[context performBlockAndWait:^{ NSFetchRequest *request = [[NSFetchRequest alloc] init]; NSEntityDescription *entityDescription = [NSEntityDescription entityForName:NSStringFromClass(User) inManagedObjectContext:context]; [request setEntity:entityDescription]; NSError *error; results = [context executeFetchRequest:request error:&amp;error];}];return results; 标准的写入姿势: 12345678910111213NSManagedObjectContext *context = ...;[context performBlockAndWait:^{ User *user = [NSEntityDescription insertNewObjectForEntityForName:NSStringFromClass(User) inManagedObjectContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;; NSError *error; [context save:&amp;error]; if (!error) { // continue to save back to the store }}]; Skiathos 中的读取姿势： 1234[[SkiathosClient sharedInstance] read:^(NSManagedObjectContext *context) { NSArray *allUsers = [User allInContext:context]; NSLog(@&quot;All users: %@&quot;, allUsers);}]; Skiathos 中的写入姿势： 1234567891011121314151617181920212223242526// Sync[[SkiathosClient sharedInstance] writeSync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;}];[[SkiathosClient sharedInstance] writeSync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;} completion:^(NSError *error) { // changes are saved to the persistent store}];// Async[[SkiathosClient sharedInstance] writeAsync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;}];[[SkiathosClient sharedInstance] writeAsync:^(NSManagedObjectContext *context) { User *user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;} completion:^(NSError *error) { // changes are saved to the persistent store}]; Skiathos 当然也支持链式调用： 1234567891011__block User *user = nil;[SkiathosClient sharedInstance].write(^(NSManagedObjectContext *context) { user = [User createInContext:context]; user.firstname = @&quot;John&quot;; user.lastname = @&quot;Doe&quot;;}).write(^(NSManagedObjectContext *context) { User *userInContext = [user inContext:context]; [userInContext deleteInContext:context];}).read(^(NSManagedObjectContext *context) { NSArray *users = [User allInContext:context];}); 如果是在 Swift中，代码将会变成下面这个样子 读取： 1234SkopelosClient.sharedInstance.read { context in let users = User.SK_all(context) print(users)} 写入： 1234567891011121314151617181920212223242526// SyncSkopelosClient.sharedInstance.writeSync { context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}SkopelosClient.sharedInstance.writeSync({ context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot; }, completion: { (error: NSError?) in // changes are saved to the persistent store})// AsyncSkopelosClient.sharedInstance.writeAsync { context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}SkopelosClient.sharedInstance.writeAsync({ context in let user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}, completion: { (error: NSError?) in // changes are saved to the persistent store}) 链式调用： 123456789101112SkopelosClient.sharedInstance.write { context in user = User.SK_create(context) user.firstname = &quot;John&quot; user.lastname = &quot;Doe&quot;}.write { context in if let userInContext = user.SK_inContext(context) { userInContext.SK_remove(context) }}.read { context in let users = User.SK_all(context) print(users)} NSManagedObject 类所提供了非常清楚的 CRUD 方法。在作为读/写代码块的参数传递之时，对象应该被作为一个整体进行处理。你应该优先使用这些内建的方法。主要的方法有下面这些： 1234567+ (instancetype)SK_createInContext:(NSManagedObjectContext *)context;+ (NSUInteger)SK_numberOfEntitiesInContext:(NSManagedObjectContext *)context;- (void)SK_deleteInContext:(NSManagedObjectContext *)context;+ (void)SK_deleteAllInContext:(NSManagedObjectContext *)context;+ (NSArray *)SK_allInContext:(NSManagedObjectContext *)context;+ (NSArray *)SK_allWithPredicate:(NSPredicate *)pred inContext:(NSManagedObjectContext *)context;+ (instancetype)SK_firstInContext:(NSManagedObjectContext *)context; 1234567static func SK_create(context: NSManagedObjectContext) -&gt; Selfstatic func SK_numberOfEntities(context: NSManagedObjectContext) -&gt; Intfunc SK_remove(context: NSManagedObjectContext) -&gt; Voidstatic func SK_removeAll(context: NSManagedObjectContext) -&gt; Voidstatic func SK_all(context: NSManagedObjectContext) -&gt; [Self]static func SK_all(predicate: NSPredicate, context:NSManagedObjectContext) -&gt; [Self]static func SK_first(context: NSManagedObjectContext) -&gt; Self? 注意，在使用 SK_inContext(context: NSManagerObjectContext) 时，不同的读写代码块可能会得到同一个对象。 线程安全所有 DALService 所产生的实例都可以认为是线程安全的。 我们特别建议你在项目中进行这样的设置 -com.apple.CoreData.ConcurrencyDebug 1 ，这可以确保你不会在多线程和并发的情况下滥用 Core Data。 这个组件不是为了通过隐藏 ManagedObjectContext: 的概念来达到接口引入的目的：它将会在客户端中引入更多的线程问题，因为开发者有责任去检查所调用线程的类型（而那将会是在忽视 Core Data 所带给我们的好处）。","link":"/posts/2016/09/01/the-easiest-core-data/"},{"title":"Supervisor 的一个隐藏坑","text":"本垃圾 API 搬运工程师又来了啊，= =今天因为 Supervisor 一个隐藏的参数配置，造成了一个重要项目的线上崩溃。= =我觉得还是有必要分享一波，所以写了一篇垃圾水文。 起因写着写着代码，突然接到一堆报警邮件，让我直接觉得世界不那么可爱 然后定睛一看异常信息？卧槽？新建连接就马上传说中的 [Errno 24] Too many open files ？？这搞你xxx啊，开始搞呗。 查 bug首先，众所周知，Linux 中万物皆文件= =，于是我们操作网络链接的过程，其实也就是操作 File Descriptor 的问题= =，诶，既然 Too many open files 那就优先考虑，是不是系统设置的阀值太小了，于是 ulimit -a 一把梭？？ 诶？open files 一栏数字不小啊？足够啊？那这特么是什么鬼啊？ 行吧，查一下网络连接吧， 一把梭，netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 统计下，当时处于各个状态的连接数量吧 诶？有点意思，TIME_WAIT 数量太多了吧？诶？有意思，那就祭出老夫的内核网络参数的半吊子功夫，魔改一下呗？ 123456789101112#参数优化#表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间net.ipv4.tcp_fin_timeout = 30#表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒，net.ipv4.tcp_keepalive_time = 300#表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭net.ipv4.tcp_syncookies = 1#表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间net.ipv4.tcp_tw_reuse = 1#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭，当前TIME-WAIT 过多，所以开启快速回收net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 5000 65000 在 /etc/sysctl.conf 中新增如上一些配置项。然后 sysctl -p 生效一波。开始看效果吧 诶！报错数量是在减小，TIME_WAIT 数量也逐渐正常 诶？？？等等？？？其余机器没 TIME_WAIT 过多的问题啊，那这尼玛是什么鬼？而且快速回收 TIME_WAIT 的连接也会带来其余的副作用（后面单章说） = =好吧，现在怀疑，是不是 Supervisor 的问题，好的，文档翻阅大赛，开始 恩，，翻了半天，查到原因了，跟一个叫做 minfds 的参数相关 描述如下 The minimum number of file descriptors that must be available before supervisord will start successfully. A call to setrlimit will be made to attempt to raise the soft and hard limits of the supervisord process to satisfy minfds. The hard limit may only be raised if supervisord is run as root. supervisord uses file descriptors liberally, and will enter a failure mode when one cannot be obtained from the OS, so it’s useful to be able to specify a minimum value to ensure it doesn’t run out of them during execution. These limits will be inherited by the managed subprocesses. This option is particularly useful on Solaris, which has a low per-process fd limit by default. 大意为，Supervisor 启动时，将根据 minfds 的值来确定系统中是否有足够的空余 fd 供其使用。同时因为我们跑在 Supervisor 中的服务，都是由 Supervisord fork 而来，因为父子关系，同时保证安全，单个进程开启的描述符最多不允许超过 minfds 设置的值，默认为 1024。然后，它补了一个刀，如果你用 root 用户运行的话，我们默认给你搞到系统最大的哦！ 卧槽。。。原来是这啊，你谁没事用 root 跑服务啊= =简直药丸。。。 行吧，改参数，改参数 后续最后这事就这样的结束了，趟一个雷，顺便复习了下内核的网络参数，虽然感觉美滋滋，不过感觉，贵 Supervisor 吃枣药丸！","link":"/posts/2017/12/28/what-the-fuck-supvisor/"},{"title":"Sanic 的若干吐槽","text":"Sanic 的若干吐槽刚刚和红姐，在 哪些 Python 库让你相见恨晚？ 这个答案下面讨论了一下 Sanic 的优劣。 突然想起，我司算是国内应该比较少见的把 Sanic 用在正式生产线上的公司了，作为一个主力推（da）动（shui）者（bi），我这个辣鸡文档工程师觉得有必要来说一下我们在使用 Sanic 过程中所采用的一系列深坑。 正文首先 Sanic 官方 的口号是一个 Flask Like 的 web framework 。这回让很多人有一种错觉，就是 Sanic 内部的实现和 Flask 近乎一致，但是事实真的是这样么？ 我们首先来看一下一组 Hello World 123456789101112# Flaskfrom flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 1234567891011121314# Sanicfrom sanic import Sanicapp = Sanic()@app.route(&quot;/&quot;)async def hello_world(request): return &quot;Hello World!&quot;if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, port=8000) 大家有没有发现什么不同之处？嗯？是不是 Sanic 的 View 函数多了一个参数是为什么呢？ Flask 众所周知的一个最典型的 Feature 就是，它有个 Global Variable 的概念，比如全局的 g 变量，以及 request 变量，这个是借助 werkzurg 里面独立实现的一套类似于 Thread.Local 的机制。在一个请求周期内，在我们业务逻辑，我们可以通过 from flask import request 来获取当前的 request 变量。我们也可以通过这样的机制，在上面挂一些数据来实现数据的全局使用。 但是 Sanic 则没有这个 Global Variable 这个概念，也就是说，我们需要在业务逻辑中使用 request 变量的话，就需要不断的传递一个 request 变量，直到一个请求周期的终结。 这样方式处理，有好，也有坏，不过我们的吐槽刚刚开始 坑点一：扩展极为不方便比如，我们现在有个需求，我们需要写一个插件，提供给其余部门的同事使用，在插件中，我们需要给原本的 Request 类以及 Response 类新增一些功能，在 Flask 中我们可以这么做 1234567891011from flask import Request,Responsefrom flask import Flaskclass APIRequest(Request): passclass APIResponse(Response): passclass NewFlask(Flask): request_class = APIRequest response_class = APIResponse Flask 中可以通过设置 Flask 类中的两个属性 request_class 以及 response_class 来替换原本的 Request 类，以及 Response 类。 就如同上面这段代码一样，我们很轻松的就可以为 Request 以及 Response 添加一些额外的功能。 但是在 Sanic 中呢？很蛋疼 12345678910111213141516171819202122232425262728293031323334353637class Sanic: def __init__(self, name=None, router=None, error_handler=None, load_env=True, request_class=None, strict_slashes=False, log_config=None, configure_logging=True): # Get name from previous stack frame if name is None: frame_records = stack()[1] name = getmodulename(frame_records[1]) # logging if configure_logging: logging.config.dictConfig(log_config or LOGGING_CONFIG_DEFAULTS) self.name = name self.router = router or Router() self.request_class = request_class self.error_handler = error_handler or ErrorHandler() self.config = Config(load_env=load_env) self.request_middleware = deque() self.response_middleware = deque() self.blueprints = {} self._blueprint_order = [] self.configure_logging = configure_logging self.debug = None self.sock = None self.strict_slashes = strict_slashes self.listeners = defaultdict(list) self.is_running = False self.is_request_stream = False self.websocket_enabled = False self.websocket_tasks = set() # Register alternative method names self.go_fast = self.run 这是 Sanic 中 Sanic 类的初始化代码，首先在 Sanic 中，我们没办法很轻松的替换 Response ,其次，我们通过查看其 __init__ 方法，我们就可以知道，如果要替换默认的 Request 我们需要给其初始化的时候传递一个参数 request_class。这就是让人感觉很迷的地方，这个东西，怎么可以让用传入呢？ 诚然我们可以通过重载 Sanic 类的 __init__ 方法，修改其默认的参数来解决这个问题。 但是新的问题也来了，我一直觉得写组件要默认一个假设，就是所有用你东西的人，智商emmmm都不太高。 好了，因为我们是提供的是插件，如果用户在使用的时候重新继承了我们的定制的 Sanic 类，同时没有使用 super 调用我们魔改后的 __init__ 方法。那么这个时候，就会出一些很有趣的乱子。 同时，Sanic 内部耦合严重，也会造成我们构建插件的时候的困难。 坑点二: 内部耦合严重现在，我们写插件，想在生成 Response 的时候进行一些额外的处理，在 Flask 中，我们可以这样做 123456from flask import Flaskclass NewFlask(Flask): def make_response(self): pass 我们直接可以重载 Flask 类中的 make_response 方法来完成我们 Response 生成的时候新增的一些额外操作。 这个看似简单的操作，在 Sanic 中就变得很恶心 Sanic 中没有像 Flask 这样，一个请求周期内的不同阶段的数据流的处理有着各自独立的方法，比如 dispatch_request,after_request , teardown_request 等等，Request 的处理和 Response 的处理也有着很清晰的界限，我们按需重载就好 Sanic 将一个请求周期类的 Request 数据和 Response 数据的处理，都统一包裹在一个大的 handle_request 方法内 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class Sanic: #..... async def handle_request(self, request, write_callback, stream_callback): &quot;&quot;&quot;Take a request from the HTTP Server and return a response object to be sent back The HTTP Server only expects a response object, so exception handling must be done here :param request: HTTP Request object :param write_callback: Synchronous response function to be called with the response as the only argument :param stream_callback: Coroutine that handles streaming a StreamingHTTPResponse if produced by the handler. :return: Nothing &quot;&quot;&quot; try: # -------------------------------------------- # # Request Middleware # -------------------------------------------- # request.app = self response = await self._run_request_middleware(request) # No middleware results if not response: # -------------------------------------------- # # Execute Handler # -------------------------------------------- # # Fetch handler from router handler, args, kwargs, uri = self.router.get(request) request.uri_template = uri if handler is None: raise ServerError( (&quot;'None' was returned while requesting a &quot; &quot;handler from the router&quot;)) # Run response handler response = handler(request, *args, **kwargs) if isawaitable(response): response = await response except Exception as e: # -------------------------------------------- # # Response Generation Failed # -------------------------------------------- # try: response = self.error_handler.response(request, e) if isawaitable(response): response = await response except Exception as e: if self.debug: response = HTTPResponse( &quot;Error while handling error: {}\\nStack: {}&quot;.format( e, format_exc())) else: response = HTTPResponse( &quot;An error occurred while handling an error&quot;) finally: # -------------------------------------------- # # Response Middleware # -------------------------------------------- # try: response = await self._run_response_middleware(request, response) except BaseException: error_logger.exception( 'Exception occurred in one of response middleware handlers' ) # pass the response to the correct callback if isinstance(response, StreamingHTTPResponse): await stream_callback(response) else: write_callback(response) 这就造成了一个现象，我们只需要对于某一个阶段数据进行额外的操作的时候，我们势必要重载 handle_request 这个大方法。就比如前面说的，我们只需要在 Response 生成的时候，进行一些额外操作，在 Flask 中我们只需要重载对应的 make_response 方法即可，而在 Sanic 中我们需要重载整个 handle_request 。可谓牵一发动全身。 同时，Sanic 不像 Flask 一样，做到了 WSGI 层的请求处理和 Framework 层的逻辑相互分离。这样一种分离，有时会给我们带来很多方便。 比如我之前写过这样一篇辣鸡文章你所不知道的 Flask Part1:Route 初探，里面提到了这样一个场景。 之前遇到一个很奇怪的需求，需要在flask中支持正则表达式比如，@app.route('/api/(.*?)') 这样，在视图函数被调用的时候，能传入 URL 中正则匹配的值。不过 Flask 路由中默认不支持这样的方法，那么我们该怎么办？ 解决方案很简单 123456789101112from flask import Flaskfrom werkzeug.routing import BaseConverterclass RegexConverter(BaseConverter): def __init__(self, map, *args): self.map = map self.regex = args[0]app = Flask(__name__)app.url_map.converters['regex'] = RegexConverter 在经过这样的设置后我们便可以按照我们刚才的需求写代码了 1234@app.route('/docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt;')def hello(url=None): print(url) 大家可以看到，由于 Flask 的 WSGI 层的处理是基于 Werkzurg 来做的，也就是说，我们有些时候对于 URL 或者其余涉及到 WSGI 层的东西的时候，我们只需要重载/使用 Werkzurg 给我们提供的相关的类或者函数就可以了。同时 app.url_map.converters['regex'] = RegexConverter 这个操作，看了源码的同学就知道，url_map 这个是 werkzurg.routing 类中的 Map 类的一个子类，我们对它的操作，其实本质上也是对于 Werkzurg 的操作，而与 Flask 的框架逻辑无关。 但是在 Sanic 中，并没有这样的分离机制，比如就上面这个场景而言 12345678910class Sanic: def __init__(self, name=None, router=None, error_handler=None, load_env=True, request_class=None, strict_slashes=False, log_config=None, configure_logging=True): #.... self.router = router or Router() #.... Sanic 中对 URL 的解析是由 Router() 实例来触发的，我们如果需要定制我们自己的 URL 解析，我们需要替换 self.router ，这实际上是对 Sanic 本身进行了修改，感觉略有不妥。 同时这里的 Router 类中，如果我们需要定制自己的解析，需要重载 Router 中的 1234567class Router: routes_static = None routes_dynamic = None routes_always_check = None parameter_pattern = re.compile(r'&lt;(.+?)&gt;') parameter_pattern 属性及其余几个解析方法。这里的 Router 并没有像 Werkzurg 中的 Router 一样，实现 Route 和 Parser 以及 Forammter（就是 Converter) 彼此相互分离的特性，我们只需要按需重构添加即可，如同文中所举的例子。 整个这一部分，其实就在吐槽，Sanic 内部耦合严重，如果想实现一些额外的操作，可以说牵一发动全身。 坑点三：细节以及其余的坑这一部分大概有几方面要说。 第一，Sanic 依赖的库，其实，emmmmmm，不太稳定，比如 10 月份的时候，触发了一个 bug ，其所依赖的 ujson 在序列化一些特定数据的时候，会抛出异常，这个问题，14年就已经爆出来了，不过到目前没修，2333333，同时当时的版本，如果要使用内置的函数的话，是不可以让用户选择具体的 parser 的，具体可以参考当时我提的 PR 第二，Sanic 一些东西实现的并不严谨，比如这篇文章有吐槽过日常辣鸡水文:一个关于 Sanic 的小问题的思考 第三，Sanic 现在不支持 UWSGI ，同时和 Gunicorn 配合部署的话，是自己实现了一套 Gunicorn Worker ，在我们生产环境下，会有一些诸如未知原因 504 这样的玄学 BUG，不过我们还在追查（另外有消息声称，Sanic 的 Server 部分并不严格遵守 PEP333即 WSGI 协议，= =我改天核查一下） 总结Sanic 的性能的确很棒，当时技术验证时，测试的时候，不同业务逻辑下，基本都能保证其性能在 Flask 的 1.5 倍以上。但是就目前的使用经验来说 Sanic 距离真正生产可用，还有相当长一段路要走。无论是内部的架构，还是周边的生态，亦或者是其他。大家可以没事拿来玩玩，但是如果要上生产线，请做好被坑的准备。 最后祝大家新年快乐，Live Long And Prosper!","link":"/posts/2018/02/23/why-i-dont-recommend-sanic/"},{"title":"去 async&#x2F;await 之路","text":"去 async/await 之路看到彭总写的文章这破 Python，感慨颇多，我也来灌水吧。 首先，我司算是在国内比较敢于尝试新东西的公司吧，最直接的提现就在于我们会及时跟进社区相关基础服务的迭代，并且敢于去尝试新的东西。嗯，从去年6月到现在，我司在线上推行了很长一段时间的 async/await ，并且引入新的注入 Sanic 这样全新的框架，但是不得不说，我们现在要对 async/await 暂时的说再见了。 我们为什么选用 async/await ？和我们组具体场景有关，我们组有相当一部分场景，是根据不同的 URL 去不同的子服务请求数据，组合之后，再进行下一步的统一处理。那么这个时候，传统的同步的方式在数据源越来越杂的情况下就显得很无奈。 我们当时有这样几个选择： 维护进程/线程池，利用通用的进程/线程来处理请求 利用 Gevent 这样第三方的 coroutine+EventLoop 方案 使用 async/await + asyncio 这一套 首先，1被我们排除了，原因很简单，太重了。2最开始也被我们暂时性的排除，当时我们对于 monkey-patch 这样看起来不太清真的方式心存畏惧 于是我们就很欢喜鼓舞的选择了3，利用 async/await + asyncio 这一套方案 事实上最开始的效果还是很美妙的。然而，在后面会发现这一套操作其实是在吃屎QwQ 去 async/await我们为什么放弃 async/await?其实几个老生重谈的问题 1. 代码层面的传染性Python 官方的 coroutine 实现，其实是基于 yield/yield from 这一套生成器的魔改的，那么这也意味着你需要入口处开始，往下逐渐的遵循 async/await 的方式进行使用。那么在同一个项目里，充斥着同步/异步的代码，相信我，维护起来，某种意义上来讲算是一种灾难。 2. 生态与兼容性async/await 目前的兼容性真的让人很头大，目前 async/await 的生效范围仅限于 Pure Python Code。这里有个问题，我们很多在项目中使用的诸如 mysqlclient 这样的 C Extension ，async/await 并不能覆盖。 同时，目前而言，async/await 的周边真的堪称一个非常非常大的问题，可以说处于一个 Bug 随处见，发现没人修的状态。比如 aiohttp 的对于 https 链接所存在的链接泄漏的问题。再比如 Sanic 的一团乱麻的设计结构。 我们在为生产项目调研一门新的技术的时候，我们往往会着重去考察一个新的东西，它对于现有的技术是否能覆盖我们的服务，它的周边是否能满足我们日常的需求？目前而言 async/await 周边一套并不能满足 3. 性能问题目前而言，PEP 3156 提出的 asyncio 是 async/await 官方推荐的事件循环的搭配。但是目前而言官方的实现欠缺很多，比如之前 aiohttp 针对于 https 的链接泄漏的问题，底层其实可以追溯至 asyncio 的 SSL 相关的实现。所以我们在使用的时候，往往会选用第三方的 loop 进行搭配。而目前而言第三方的 Loop 而言目前主流的实现方式均是基于 libuv/libev 进行魔改。而这样一来，其性能和 Gevent 不相上下，甚至更低（毕竟 Greenlet 避免了维护 PyFrameObject 的开销） 所以，为了我们的头发着想，目前我们将选择逐渐的将 async/await 从我们的线上代码中退役，最迟今年年底前，完成我们的去 async/await 的操作。 我们替代品是什么？目前而言，我们准备使用 Gevent 作为替代品（嗯，真香） 原因很简单： 目前发展成熟，无明显大的 Bug 周边发展成熟，对于 Pure Python Code，可以 Monkey-Patch 一把梭迁移存量代码，对于 C Extension 有豆瓣内部生产验证过的 Greenify 来作为解决方案 底层的 Greenlet 提供了对应的 API ，在必要的时候可以方便的对协程的切换做上下文的 trace。 关于 async/await 其他一些想说的东西首先而言，async/await 是个好东西，但是现在不实用。这一点其实要看社区去进一步摸索相关的使用方法。 说到这里，很多人又想问我，你对于 ASGI 和 Django Channel 这样的东西怎么看？ 首先我们要明确一点 ASGI 其实并不是为了 async/await 所设计，其最初的设计思路，是为了解决 PEP333/PEP3333 WSGI 协议在面对越来越复杂的网络协议模型力不从心的问题。而 Django Channel 也是为了解决这个问题，从而对于 ASGI 进行实现的产物（最开始是解决 Websocket？）。这一套的确解决了很多问题，比如 Django Channel 2.0 中可以很方便的实现 WebSocket Boardcast，但是他们和 async/await 其实关联并不大。 今年 PyCon 2018 上，Django 组的 Core 来介绍说，Channel 2.0 增加了对 async/await 的支持。未来 Django 也可能会增加对应的支持。但是问题在于，一旦到了使用 async/await 的时候，目前整体的生态，依旧是让人最为担心的，也是最为薄弱的点 。 所以，你好 async/await，再见 async/await！","link":"/posts/2018/10/05/why-i-dont-use-async/"},{"title":"为什么有些时候 Python 中乘法比位运算更快","text":"我本来以为我不再会写水文了，但是突然发现自己现在也只能勉强写写水文才能维持生活这样子。那就继续写水文吧 某天，一个技术群里老哥提出了这样一个问题，为什么在一些情况下，Python 中的简单乘/除法比位运算要慢 首先秉持着实事求是的精神，我们先来验证一下 1234567891011In [33]: %timeit 1073741825*2 7.47 ns ± 0.0843 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)In [34]: %timeit 1073741825&lt;&lt;1 7.43 ns ± 0.0451 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)In [35]: %timeit 1073741823&lt;&lt;1 7.48 ns ± 0.0621 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)In [37]: %timeit 1073741823*2 7.47 ns ± 0.0564 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each) 我们发现几个很有趣的现象 在值 x&lt;=2^30 时，乘法比直接位运算要快 在值 x&gt;2^32 时，乘法显著慢于位运算 这个现象很有趣，那么这个现象的 root cause 是什么？实际上这和 Python 底层的实现有关 简单聊聊PyLongObject 的实现在 Python 2.x 时期，Python 中将整型分为两类，一类是 long, 一类是 int 。在 Python3 中这两者进行了合并。目前在 Python3 中这两者做了合并，仅剩一个 long 首先来看看 long 这样一个数据结构底层的实现 1234struct _longobject { PyObject_VAR_HEAD digit ob_digit[1];}; 在这里不用关心，PyObject_VAR_HEAD 的含义，我们只需要关心 ob_digit 即可。 在这里，ob_digit 是使用了 C99 中的“柔性数组”来实现任意长度的整数的存储。这里我们可以看一下官方代码中的文档 Long integer representation.The absolute value of a number is equal to SUM(for i=0 through abs(ob_size)-1) ob_digit[i] * 2*(SHIFTi)Negative numbers are represented with ob_size &lt; 0; zero is represented by ob_size == 0.In a normalized number, ob_digit[abs(ob_size)-1] (the most significant digit) is never zero. Also, in all cases, for all valid i,0 &lt;= ob_digit[i] &lt;= MASK.The allocation function takes care of allocating extra memory so that ob_digit[0] … ob_digit[abs(ob_size)-1] are actually available.CAUTION: Generic code manipulating subtypes of PyVarObject has to aware that ints abuse ob_size’s sign bit. 简而言之，Python 是将一个十进制数转为 2^(SHIFT) 进制数来进行存储。这里可能不太好了理解。我来举个例子，在我的电脑上，SHIFT 为 30 ，假设现在有整数 1152921506754330628 ，那么将起转为 2^30 进制表示则为: 4*(2^30)^0+2*(2^30)^1+1*(2^30)^2 。那么此时 ob_digit 是一个含有三个元素的数组，其值为 [4,2,1] OK，在明白了这样一些基础知识后，我们回过头去看看 Python 中的乘法运算 Python 中的乘法运算Python 中的乘法运算，分为两部分，其中关于大数的乘法，Python 使用了 Karatsuba 算法1，具体实现如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167static PyLongObject *k_mul(PyLongObject *a, PyLongObject *b){ Py_ssize_t asize = Py_ABS(Py_SIZE(a)); Py_ssize_t bsize = Py_ABS(Py_SIZE(b)); PyLongObject *ah = NULL; PyLongObject *al = NULL; PyLongObject *bh = NULL; PyLongObject *bl = NULL; PyLongObject *ret = NULL; PyLongObject *t1, *t2, *t3; Py_ssize_t shift; /* the number of digits we split off */ Py_ssize_t i; /* (ah*X+al)(bh*X+bl) = ah*bh*X*X + (ah*bl + al*bh)*X + al*bl * Let k = (ah+al)*(bh+bl) = ah*bl + al*bh + ah*bh + al*bl * Then the original product is * ah*bh*X*X + (k - ah*bh - al*bl)*X + al*bl * By picking X to be a power of 2, &quot;*X&quot; is just shifting, and it's * been reduced to 3 multiplies on numbers half the size. */ /* We want to split based on the larger number; fiddle so that b * is largest. */ if (asize &gt; bsize) { t1 = a; a = b; b = t1; i = asize; asize = bsize; bsize = i; } /* Use gradeschool math when either number is too small. */ i = a == b ? KARATSUBA_SQUARE_CUTOFF : KARATSUBA_CUTOFF; if (asize &lt;= i) { if (asize == 0) return (PyLongObject *)PyLong_FromLong(0); else return x_mul(a, b); } /* If a is small compared to b, splitting on b gives a degenerate * case with ah==0, and Karatsuba may be (even much) less efficient * than &quot;grade school&quot; then. However, we can still win, by viewing * b as a string of &quot;big digits&quot;, each of width a-&gt;ob_size. That * leads to a sequence of balanced calls to k_mul. */ if (2 * asize &lt;= bsize) return k_lopsided_mul(a, b); /* Split a &amp; b into hi &amp; lo pieces. */ shift = bsize &gt;&gt; 1; if (kmul_split(a, shift, &amp;ah, &amp;al) &lt; 0) goto fail; assert(Py_SIZE(ah) &gt; 0); /* the split isn't degenerate */ if (a == b) { bh = ah; bl = al; Py_INCREF(bh); Py_INCREF(bl); } else if (kmul_split(b, shift, &amp;bh, &amp;bl) &lt; 0) goto fail; /* The plan: * 1. Allocate result space (asize + bsize digits: that's always * enough). * 2. Compute ah*bh, and copy into result at 2*shift. * 3. Compute al*bl, and copy into result at 0. Note that this * can't overlap with #2. * 4. Subtract al*bl from the result, starting at shift. This may * underflow (borrow out of the high digit), but we don't care: * we're effectively doing unsigned arithmetic mod * BASE**(sizea + sizeb), and so long as the *final* result fits, * borrows and carries out of the high digit can be ignored. * 5. Subtract ah*bh from the result, starting at shift. * 6. Compute (ah+al)*(bh+bl), and add it into the result starting * at shift. */ /* 1. Allocate result space. */ ret = _PyLong_New(asize + bsize); if (ret == NULL) goto fail;#ifdef Py_DEBUG /* Fill with trash, to catch reference to uninitialized digits. */ memset(ret-&gt;ob_digit, 0xDF, Py_SIZE(ret) * sizeof(digit));#endif /* 2. t1 &lt;- ah*bh, and copy into high digits of result. */ if ((t1 = k_mul(ah, bh)) == NULL) goto fail; assert(Py_SIZE(t1) &gt;= 0); assert(2*shift + Py_SIZE(t1) &lt;= Py_SIZE(ret)); memcpy(ret-&gt;ob_digit + 2*shift, t1-&gt;ob_digit, Py_SIZE(t1) * sizeof(digit)); /* Zero-out the digits higher than the ah*bh copy. */ i = Py_SIZE(ret) - 2*shift - Py_SIZE(t1); if (i) memset(ret-&gt;ob_digit + 2*shift + Py_SIZE(t1), 0, i * sizeof(digit)); /* 3. t2 &lt;- al*bl, and copy into the low digits. */ if ((t2 = k_mul(al, bl)) == NULL) { Py_DECREF(t1); goto fail; } assert(Py_SIZE(t2) &gt;= 0); assert(Py_SIZE(t2) &lt;= 2*shift); /* no overlap with high digits */ memcpy(ret-&gt;ob_digit, t2-&gt;ob_digit, Py_SIZE(t2) * sizeof(digit)); /* Zero out remaining digits. */ i = 2*shift - Py_SIZE(t2); /* number of uninitialized digits */ if (i) memset(ret-&gt;ob_digit + Py_SIZE(t2), 0, i * sizeof(digit)); /* 4 &amp; 5. Subtract ah*bh (t1) and al*bl (t2). We do al*bl first * because it's fresher in cache. */ i = Py_SIZE(ret) - shift; /* # digits after shift */ (void)v_isub(ret-&gt;ob_digit + shift, i, t2-&gt;ob_digit, Py_SIZE(t2)); Py_DECREF(t2); (void)v_isub(ret-&gt;ob_digit + shift, i, t1-&gt;ob_digit, Py_SIZE(t1)); Py_DECREF(t1); /* 6. t3 &lt;- (ah+al)(bh+bl), and add into result. */ if ((t1 = x_add(ah, al)) == NULL) goto fail; Py_DECREF(ah); Py_DECREF(al); ah = al = NULL; if (a == b) { t2 = t1; Py_INCREF(t2); } else if ((t2 = x_add(bh, bl)) == NULL) { Py_DECREF(t1); goto fail; } Py_DECREF(bh); Py_DECREF(bl); bh = bl = NULL; t3 = k_mul(t1, t2); Py_DECREF(t1); Py_DECREF(t2); if (t3 == NULL) goto fail; assert(Py_SIZE(t3) &gt;= 0); /* Add t3. It's not obvious why we can't run out of room here. * See the (*) comment after this function. */ (void)v_iadd(ret-&gt;ob_digit + shift, i, t3-&gt;ob_digit, Py_SIZE(t3)); Py_DECREF(t3); return long_normalize(ret); fail: Py_XDECREF(ret); Py_XDECREF(ah); Py_XDECREF(al); Py_XDECREF(bh); Py_XDECREF(bl); return NULL;} 这里不对 Karatsuba 算法1 的实现做单独解释，有兴趣的朋友可以参考文末的 reference 去了解具体的详情。 在普通情况下，普通乘法的时间复杂度位 n^2 (n 为位数），而 K 算法的时间复杂度为 3n^(log3) ≈ 3n^1.585 ，看起来 K 算法的性能要优于普通乘法，那么为什么 Python 不全部使用 K 算法呢？ 很简单，K 算法的优势实际上要在当 n 足够大的时候，才会对普通乘法形成优势。同时考虑到内存访问等因素，当 n 不够大时，实际上采用 K 算法的性能将差于直接进行乘法。 所以我们来看看 Python 中乘法的实现 12345678910111213141516171819202122static PyObject *long_mul(PyLongObject *a, PyLongObject *b){ PyLongObject *z; CHECK_BINOP(a, b); /* fast path for single-digit multiplication */ if (Py_ABS(Py_SIZE(a)) &lt;= 1 &amp;&amp; Py_ABS(Py_SIZE(b)) &lt;= 1) { stwodigits v = (stwodigits)(MEDIUM_VALUE(a)) * MEDIUM_VALUE(b); return PyLong_FromLongLong((long long)v); } z = k_mul(a, b); /* Negate if exactly one of the inputs is negative. */ if (((Py_SIZE(a) ^ Py_SIZE(b)) &lt; 0) &amp;&amp; z) { _PyLong_Negate(&amp;z); if (z == NULL) return NULL; } return (PyObject *)z;} 在这里我们看到，当两个数皆小于 2^30-1 时，Python 将直接使用普通乘法并返回，否则将使用 K 算法进行计算 这个时候，我们来看一下位运算的实现，以右移为例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960static PyObject *long_rshift(PyObject *a, PyObject *b){ Py_ssize_t wordshift; digit remshift; CHECK_BINOP(a, b); if (Py_SIZE(b) &lt; 0) { PyErr_SetString(PyExc_ValueError, &quot;negative shift count&quot;); return NULL; } if (Py_SIZE(a) == 0) { return PyLong_FromLong(0); } if (divmod_shift(b, &amp;wordshift, &amp;remshift) &lt; 0) return NULL; return long_rshift1((PyLongObject *)a, wordshift, remshift);}static PyObject *long_rshift1(PyLongObject *a, Py_ssize_t wordshift, digit remshift){ PyLongObject *z = NULL; Py_ssize_t newsize, hishift, i, j; digit lomask, himask; if (Py_SIZE(a) &lt; 0) { /* Right shifting negative numbers is harder */ PyLongObject *a1, *a2; a1 = (PyLongObject *) long_invert(a); if (a1 == NULL) return NULL; a2 = (PyLongObject *) long_rshift1(a1, wordshift, remshift); Py_DECREF(a1); if (a2 == NULL) return NULL; z = (PyLongObject *) long_invert(a2); Py_DECREF(a2); } else { newsize = Py_SIZE(a) - wordshift; if (newsize &lt;= 0) return PyLong_FromLong(0); hishift = PyLong_SHIFT - remshift; lomask = ((digit)1 &lt;&lt; hishift) - 1; himask = PyLong_MASK ^ lomask; z = _PyLong_New(newsize); if (z == NULL) return NULL; for (i = 0, j = wordshift; i &lt; newsize; i++, j++) { z-&gt;ob_digit[i] = (a-&gt;ob_digit[j] &gt;&gt; remshift) &amp; lomask; if (i+1 &lt; newsize) z-&gt;ob_digit[i] |= (a-&gt;ob_digit[j+1] &lt;&lt; hishift) &amp; himask; } z = maybe_small_long(long_normalize(z)); } return (PyObject *)z;} 在这里我们能看到，在两侧都是小数的情况下，位移动算法将比普通乘法，存在更多的内存分配等操作。这样也会回答了我们文初所提到的一个问题，“为什么一些时候乘法比位运算更快”。 总结本文差不多就到这里了，实际上通过这次分析我们能得到一些很有趣但是也很冷门的知识。实际上我们目前看到这样一个结果，是 Python 对于我们常见且高频的操作所做的一个特定的设计。而这也提醒我们，Python 实际上对于很多操作都存在自己内建的设计哲学，在日常使用的时候，其余语言的经验，可能无法复用 差不多就这样吧，只能勉强写水文苟活了（逃 Reference [1]. Karatsuba 算法","link":"/posts/2020/11/06/why-mul-faster-than-bit-shift-sometimes-in-python/"},{"title":"关于PostCSS的一点小科普","text":"原文链接 : PostCSS – What It Is And What It Can Do 原文作者 : Jake Bresnehan 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: aidistan, JolsonZhu PostCSS起源于2013年9月，发展到现在，已经有很多开发者在工作中使用它。如果你尚未接触过PostCSS，这篇文章正适合你。 PostCSS是一个使用JavaScript插件来转换CSS的工具。 PostCSS本身很小，其只包含CSS解析器，操作CSS节点树的API，资源生成器（译者注1：原文是source map），以及一个节点树字符串化工具。所有的黑魔法都是通过利用插件实现的。 截止目前，PostCSS的生态圈内已经拥有超过100种插件。这些插件可以做太多的事情，比如lint（译者注2：一种用来检测CSS代码的工具），添加vendor prefixes（译者注3：添加浏览器内核前缀，可以使用浏览器的一些独有特性），允许使用最新的CSS特性，在你的CSS里提供统计数据，或者是允许你使用Sass，Less或是Stylus等CSS预处理器。 让我们看看以下十种插件Autoprefixer 根据用户的使用场景来解析CSS和添加vendor prefixes（前文注2）。 PostCSS Focus 一种利用键盘操作为每个**:hover添加:focus选择器的PostCSS**插件。 PreCSS 一个允许你在代码中使用类似Sass标记的插件。 Stylelint 一种强大的，先进的可以使你在CSS样式中保持一致性，避免错误的CSS linter工具。 PostCSS CSS Variables 一种将用户自定义CSS变量（CSS variables）转化为静态样式的插件。 PostCSS Flexbugs Fixes 一种用于修复flexbug的bug的插件。 PostCSS CSSnext 一种可以让你使用CSS最新特性的插件。它通过将最新的CSS特性转变为现阶段浏览器所兼容的特性，这样你不用再等待浏览器对某一特定新特性的支持。 PostCSS CSS Stats 一种支持cssstats的插件。这个插件将会返回一个cssstatus对象，这样你可以使用它来进行CSS分析。 PostCSS SVGO 优化在PostCSS中内联SVG。 PostCSS Style Guide 一种可以自动生成风格指导的插件。将会在Markdown中生成CSS注释，并在生成的HTML文档中显示。 如果你想编写自己的插件，并希望将其贡献给社区的话，请确保你是先看过guidelines这篇文档还有PostCSS Plugin Boilerplate这篇官方文档。 在你的工作中使用PostCSSPostCSS是用JavaScript所编写的，这使得我们在Grunt，Gulp或Webpack等常用的前端构建工具中使用它变得非常方便。 下面是我们使用Autoprefixer插件的示例。 npm install autoprefixer --save-dev Gulp如果你使用Gulp，那么你需要安装gulp-postcss。 npm install --save-dev gulp-postcss gulp.task('autoprefixer', function () { var postcss = require('gulp-postcss'); var autoprefixer = require('autoprefixer'); return gulp.src('./src/*.css') .pipe(postcss([ autoprefixer({ browsers: ['last 2 versions'] }) ])) .pipe(gulp.dest('./dest')); }); Grunt如果你使用Grunt，那么你需要安装grunt-postcss。 npm install grunt-postcss --save-dev module.exports = function(grunt) { grunt.loadNpmTasks('grunt-postcss'); grunt.initConfig({ postcss: { options: { map: true, processors: [ require('autoprefixer')({ browsers: ['last 2 versions'] }) ] }, dist: { src: 'css/*.css' } } }); grunt.registerTask('default', ['postcss:dist']); }; Webpack如果你使用Webpack，那么你需要安装postcss-loader。 npm install postcss-loader --save-dev var autoprefixer = require('autoprefixer'); module.exports = { module: { loaders: [ { test: /\\.css$/, loader: &quot;style-loader!css-loader!postcss-loader&quot; } ] }, postcss: function () { return [autoprefixer]; } } 关于怎么整合PostCSS，你可以从这里PostCSS repo获取到帮助。 最后最后的诚心安利~在有些时候，在新技术，新工具，新框架发布的时候，去使用并观察其发展趋势无疑是一种明智的行为。现在，PostCSS已经发展到一个相当成熟的阶段，我强烈建议你在你的工作中使用它。因为它现在已经在工程中被广泛的使用，同时在未来一段时间内它不会发生太大的变化。","link":"/posts/2016/07/23/%E5%85%B3%E4%BA%8EPostCSS%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E7%A7%91%E6%99%AE/"},{"title":"在Swift中实现撤销功能","text":"原文链接 : Undo History in Swift 原文作者 : chriseidhof 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: xcc3641, Jaeger 在过去的一段时间里，有很多的Blog推出了关于他们想在Swift中所添加的动态特性的文章。事实上Swift 已经成为了一门具有相当多动态特性的语言：它拥有泛型，协议， 头等函数（译者注1：first-class function指函数可以向类一样作为参数传递），和包含很多可以的动态操作的函数的标准库，比如map和filter等（这意味着我们可以利用更安全更灵活的函数来代替 KVC 来使用 字符串）（译者注2：KVC指Key-Value-Coding一个非正式的 Protocol，提供一种机制来间接访问对象的属性）。对于大多数人而言，特别希望介绍反射这一特性，这意味着他们可以在程序运行时进行观察和修改。 在Swift中，反射机制受到很多的限制，但是你仍然你可以在代码运行的时候动态的生成和插入一些东西。 比如这里是怎样为NSCoding或者是JSON动态生成字典的实例。 今天在这里，我们将一起看一下在Swift中怎样去实现撤销功能。 其中一种方法是通过利用Objective-C中基于的反射机制所提供的NSUndoManager。通过利用struct，我们可以利用不同的方式在我们的APP中实现撤销这一功能。 在教程开始之前，请务必确保你自己已经理解了Swift中struct的工作机制(最重要的是理解他们都是独立的拷贝)。首先要声明的一点是，这篇文章并不是想告诉大家我们不需要对runtime进行操作，或者我们提供的是一种NSUndoManager的替代品。这篇文章只是告诉了大家一种不同的思考方式而已。 我们首先创建一个叫做UndoHistory的struct。 通常而言，创建UndoHistory时会伴随一个警告，提示只有当A是一个struct的时才会生效。为了保存所有状态信息，我们需要将其存放入一个数组之中。当我们修改了什么时，我们只需要将其push进数组中，当我们希望进行撤回时，我们将其从数组中pop出去。我们通常希望有一个初试状态，所以我们需要建立一个初始化方法： 1234567struct UndoHistory&lt;A&gt; { private let initialValue: A private var history: [A] = [] init(initialValue: A) { self.initialValue = initialValue }} 举个例子，如果我们想在一个tableViewController中通过数组的方式提供撤销操作，我们可以创建这样一个struct： 1var history = UndoHistory(initialValue: [1, 2, 3]) 对于不同情境下的撤销操作，我们可以创建不同的struct来实现: 1234struct Person { var name: String var age: Int} 1var personHistory = UndoHistory(initialValue: Person(name: &quot;Chris&quot;, age: 31)) 当然，我们希望获得当前的状态，同时设置当前状态。(换句话说：我们希望实时地操作我们的历史记录）。我们可以从history数组中的最后一项值来获取我们的状态，同时如果数组为空的话，我们便返回我们的初始值。 我们可以通过将当前状态添加至history数组来改变我们的操作状态。 12345678910extension UndoHistory { var currentItem: A { get { return history.last ?? initialValue } set { history.append(newValue) } }} 比如，如果我们想修改个人年龄（译者注3：指前面作者编写的Person结构体中的age属性）， 我们可以通过重新计算属性来很轻松的做到这一点： 12personHistory.currentItem.age += 1personHistory.currentItem.age // Prints 32 当然，undo 方法的编写并未完成。对于从数组中移出最后一个元素来讲是非常简单的。 根据你自己的声明，你可以在数组为空的时候抛出一个异常，不过，我没有选择这样一种做法。 123456extension UndoHistory { mutating func undo() { guard !history.isEmpty else { return } history.removeLast() }} 很简单的使用它（译者注4：这里指作者前面所编写的undo相关代码） 123456789101112131415161718192021222324252627282930 personHistory.undo() personHistory.currentItem.age // Prints 31 again~~~~当然，我们到现在的**UndoHistory**操作只是基于一个很简单的**Person**类。比如，如果我们想利用**Array**来实现一个**tableviewcontroller**的**undo**操作，我们可以利用**属性**来获取从数组中得到的元素：~~~ Swift final class MyTableViewController&lt;item&gt;: UITableViewController { var data: UndoHistory&lt;[item]&gt; init(value: [Item]) { data = UndoHistory(initialValue: value) super.init(style: .Plain) } override func tableView(tableView: UITableView, numberOfRowsInSection section: Int) -&gt; Int { return data.currentItem.count } override func tableView(tableView: UITableView, cellForRowAtIndexPath indexPath: NSIndexPath) -&gt; UITableViewCell { let cell = tableView.dequeueReusableCellWithIdentifier(&quot;Identifier&quot;, forIndexPath: indexPath) let item = data.currentItem[indexPath.row] // configure `cell` with `item` return cell } override func tableView(tableView: UITableView, commitEditingStyle editingStyle: UITableViewCellEditingStyle, forRowAtIndexPath indexPath: NSIndexPath) { guard editingStyle == .Delete else { return } data.currentItem.removeAtIndex(indexPath.row) } } 在struct中另一个非常爽的特性是：我们可以自由的使用监听者模式。 比如,我们可以修改data的值： 12345var data: UndoHistory&lt;[item]&gt; { didSet { tableView.reloadData() }} 我们即使是修改数组内很深的值（比如：data.currentItem[17].name = “John”**），我们通过didSet也能很方便地定位到修改的地方。当然,我们可能希望做一些例如reloadData**这样方便的事情。比如， 我们可以利用Changeset 库来计算变化，然后来根据插入/删除/移动/等不同的操作来添加动画。 很明显的是, 这种方法有着它自身的缺点。例如，它保存了整个状态的历史操作，不是每次状态变化之间的不同点。 这种方法只使用了struct来实现undo操作 （更为准确的讲：是只使用了struct中值的一些特性）。这意味着，你并不需要去阅读 runtime编程指导这本书， 你只需要对struct和generics（译者注5：generics指泛型）有足够的了解。 为data.currentItem提供了一个可计算的属性 items 来进行获取和设置操作，是一个不错的想法。这使得data-source和delegate等方法的实现变得更为容易。 如果你想更进一步优化，这里有一些非常有意思的想法：添加恢复功能，或者是编辑功能。你可以在tableView中去实现, 如果你真的很天真的按照这个去做了，那么你会发现在你的undo历史中会存在重复记录。","link":"/posts/2016/07/23/%E5%9C%A8Swift%E4%B8%AD%E5%AE%9E%E7%8E%B0%E6%92%A4%E9%94%80%E5%8A%9F%E8%83%BD/"},{"title":"好与坏，Swift面面观 Part2","text":"原文链接 : Good Swift, Bad Swift — Part 2 原文作者 : Kristian Andersen 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者: owenlyn, yifili09 不久之前，在我写的好与坏，Swift面面观 Part1一文中，我介绍了一些关于在 Swift 里怎样去写出优秀代码的小技巧。在 Swift 发布到现在的两年里，我花费了很长时间去牢牢掌握最佳的实践方法。欲知详情，请看这篇文章：好与坏，Swift面面观 Part1. 在这个系列的文章中，我将尝试提炼出我认为的 Swift 语言中好与不好的部分。唔，我也希望在未来有优秀的 Swift 来帮助我征服 Swift （唔，小伙子，别看了，中央已经决定是你了，快念两句诗吧）。如果你有什么想法，或者想告诉我一点作为开发者的人生经验什么的话，请在 Twitter 上联系我，我的账号是 ksmandersen。 好了废话不多说，让我们开始今天的课程吧。 guard 大法好，入 guard 保平安在 Swift 2.0 中， Swift 新增了一组让开发者有点陌生的的特性。Guard 语句在进行防御性编程的时候将会起到不小的作用。（译者注1：防御性编程（Defensive programming）是防御式设计的一种具体体现，它是为了保证，对程序的不可预见的使用，不会造成程序功能上的损坏。它可以被看作是为了减少或消除墨菲定律效力的想法。防御式编程主要用于可能被滥用，恶作剧或无意地造成灾难性影响的程序上。来源自wiki百科）。每个 Objective-C 开发者可能对防御性编程都不陌生。通过使用这种技术，你可以预先确定你的代码在处理不可预期的输入数据时，不会发生异常。 Guard 语句允许你为接下来的代码设定一些条件和规则，当然你也必须钦定当这些条件（或规则）不被满足时要怎么处理。另外，guard 语句必须要返回一个值。在早期的 Swift 编程中，你可能会使用 if-else 语句来对这些情况进行预先处理。但是如果你使用 guard 语句的话，编译器会在你没有考虑到某些情况下时帮你对异常数据进行处理。 接下来的例子有点长，但是这是一个非常好的关于 guard 作用的实例。 didPressLogIn 函数在屏幕上的 button 被点击时被调用。我们期望这个函数被调用时，如果程序产生了额外的请求时，不会产生额外的日志。因此，我们需要提前对代码进行一些处理。然后我们需要对日志进行验证。如果这个日志不是我们所需要的，那么我们不在需要发送这段日志。但是更为重要的是，我们需要返回一段可执行语句来确保我们不会发送这段日志。guard 将会在我们忘记返回的时候抛出异常。 1234567891011121314@objc func didPressLogIn(sender: AnyObject?) { guard !isPerformingLogIn else { return } isPerformingLogIn = true let email = contentView.formView.emailField.text let password = contentView.formView.passwordField.text guard validateAndShowError(email, password: password) else { isPerformingLogIn = false return } sendLogInRequest(ail, password: password)} 当 let 和 guard 配合使用的时候将会有奇效。下面这个例子中，我们将把请求的结果绑定到一个变量 user ，之后通过 finishSignUp 方法函数使用(这个变量)。如果 result.okValue 为空，那么 guard 将会产生作用，如果不为空的话，那么这个值将对 user 进行赋值。我们通过利用 where 来对 guard 进行限制。 123456789currentRequest?.getValue { [weak self] result in guard let user = result.okValue where result.errorValue == nil else { self?.showRequestError(result.errorValue) self?.isPerformingSignUp = false return } self?.finishSignUp(user)} 讲道理 guard 非常的强大。唔，如果你还没有使用的话，那么你真应该慎重考虑下了。 在使用 subviews 的时候，将声明和配置同时进行。如前面一系列文章中所提到的，开发 viwe 的时候，我比较习惯于用代码生成。因为对 view 的配置套路很熟悉，所以在出现布局问题或者配置不当等问题时，我总是能很快的定位出错的地方。 在开发过程中，我发现将不同的配置过程放在一起非常的重要。在我早期的 Swift 编程经历中，我通常会声明一个 configureView 函数，然后在初始化时将配置过程放在这里。但是在 Swift 中我们可以利用 属性声明代码块 来配置 view （其实我也不知道这玩意儿怎么称呼啦（逃）。 唔，下面这个例子里，有一个包含两个 subviews 、 bestTitleLabel 、 和 otherTitleLabel 的 AwesomeView 视图。两个 subviews 都在一个地方进行配置。我们将配置过程都整合在 configureView 方法中。因此，如果我想去改变一个 label 的 textColor 属性，我很清楚的知道到哪里去进行修改。 1234567891011121314151617181920cclass AwesomeView: GenericView { let bestTitleLabel = UILabel().then { $0.textAlignment = .Center $0.textColor = .purpleColor()tww } let otherTitleLabel = UILabel().then { $0.textAlignment = . $0.textColor = .greenColor() } override func configureView() { super.configureView() addSubview(bestTitleLabel) addSubview(otherTitleLabel) // Configure constraints }} 对于上面的代码，我很不喜欢的就是在声明 label 时所带的类型标签，然后在代码块里进行初始化并返回值。通过使用 Then这个库 ，我们可以进行一点微小的改进。你可以利用这个小函数去在你的项目里将代码块与对象的声明进行关联。这样可以减少重复声明。 1234567891011121314151617181920class AwesomeView: GenericView { let bestTitleLabel = UILabel().then { $0.textAlignment = .Center $0.textColor = .purpleColor()tww } let otherTitleLabel = UILabel().then { $0.textAlignment = . $0.textColor = .greenColor() } override func configureView() { super.configureView() addSubview(bestTitleLabel) addSubview(otherTitleLabel) // Configure constraints }} 通过不同访问级别来对类成员进行分类。唔，对我来讲，最近发生的一件比较重要的事儿就是，我利用一种比较特殊的方法来将类和结构体的成员结合在一起。这是我之前在利用 Objective-C 进行开发的时候养成的习惯。我通常将私有方法放置在最下面，然后公共及初始化方法放在中间。然后将属性按照公共属性到私有属性的顺序放置在代码上层。唔，你可以按照下面的结构在组织你的代码。 公共属性 内联属性 私有属性 初始化容器 公共方法 内联方法 私有方法 你也可以按照静态/类属性/固定值的方式进行排序。可能不同的人会在此基础上补充一些不同的东西。不过对于我来讲，我无时不刻都在按照上面的方法进行编程。 好了，本期节目就到此结束。如果你有什么好的想法，或者什么想说的话，欢迎通过屏幕下方的联系方式联系我。当然欢迎通过这样的方式丢硬币丢香蕉打赏并订阅我的文章（大雾）。 下期预告：将继续讲诉 Swift 里的点点滴滴，不要走开，下期更精彩 。","link":"/posts/2016/07/23/%E5%A5%BD%E4%B8%8E%E5%9D%8F%EF%BC%8CSwift%E9%9D%A2%E9%9D%A2%E8%A7%82-Part2/"},{"title":"如何检测iPhone处于低电量模式","text":"原文链接 : Detecting low power mode 原文作者 : useyourloaf 译文出自 : 掘金翻译计划 译者 : Zheaoli 校对者 : LoneyIsError, wild-flame 这个星期，我阅读了一篇关于Uber怎样检测手机处于省电模式的文章。（注：文章连接是Uber found people more likely to pay） 在人们手机快要关机时，使用Uber可能会面临更高的价格。 这家公司（注：指Uber）宣称他们不会利用手机是否处于节能模式这一数据来进行定价， 但是这里我想知道 我们怎么知道用户的iPhone处于低电量模式 低电量模式在iOS 9中，苹果为iPhone手机新添加了 低电量模式 功能。在你能充电之前，低电量模式通过关闭诸如邮件收发，Siri，后台消息推送能耗电功能来延长你的电池使用时间。 在这里面，很重要的一点是，是否进入低电量模式是由用户自行决定的。 你需要进入电池设置中去开启低电量模式。当你进入低电量模式的时候，状态栏上的电池图标会变成黄色。 当你充电至80%以上时，系统会自动关闭低电量模式。 低电量模式检测事实证明，在iOS 9中获取低电量模式信息是很容易的一件事。 你可以通过NSProcessInfo这个类来判断用户是否进入了低电量模式： 1234if NSProcessInfo.processInfo().lowPowerModeEnabled { // stop battery intensive actions} 如果你想用Objective-C来实现这个功能: 1234if ([[NSProcessInfo processInfo] isLowPowerModeEnabled]) { // stop battery intensive actions} 如果你监听了NSProcessInfoPowerStateDidChangeNotification通知，在用户切换进入低电量模式的时候你将接收到一个消息。比如，在视图控制器中的viewDidLoad方法中: 1234NSNotificationCenter.defaultCenter().addObserver(self, selector: #selector(didChangePowerMode(_:)), name: NSProcessInfoPowerStateDidChangeNotification, object: nil) 1234[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(didChangePowerMode:) name:NSProcessInfoPowerStateDidChangeNotification object:nil]; 在我第一次发布这篇文章后，很多人提醒我：对于只对iOS 9.X适配的开发者而言，没有必要在 ViewController 消失时去移除 Observer 。 接着在这个方法会监视电池模式并在切换的时候给予一个响应。 1234567func didChangePowerMode(notification: NSNotification) { if NSProcessInfo.processInfo().lowPowerModeEnabled { // low power mode on } else { // low power mode off }} 1234567- (void)didChangePowerMode:(NSNotification *)notification { if ([[NSProcessInfo processInfo] isLowPowerModeEnabled]) { // low power mode on } else { // low power mode off }} 小贴士: 这个通知方法和NSProcessInfo里的属性是在iOS 9系统中新提供的方法。如果你想让你的APP兼容iOS8或者更早版本的系统，你需要去这个网站 test for availability测试你的代码是否能正常运行。 低电量模式是iPhone独有的特性，如果你在iPad上测试前面的代码，会一直返回false。 只有在你的 App 能够采取一些节能措施来延长电池寿命的情况下，检测用户开启了低电量模式才是有用的。这里，苹果给了一些建议： 停止更新位置 减少用户交互动画 关闭数据流量这样的后台操作 关闭特效","link":"/posts/2016/07/23/%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8BiPhone%E5%A4%84%E4%BA%8E%E4%BD%8E%E7%94%B5%E9%87%8F%E6%A8%A1%E5%BC%8F/"},{"title":"Swift 声明式程序设计","text":"原文地址：Declarative API Design in Swift 原文作者：Benjamin Encz 译文出自：掘金翻译计划 译者：Zheaoli 校对者：luoyaqifei, Edison-Hsu 在我第一份 iOS 开发工程师的工作中，我编写了一个 XML 解析器和一个简单的布局工具，两个东西都是基于声明式接口。XML 解析器是基于 .plist 文件来实现 Objective-C 类关系映射。而布局工具则允许你利用类似 HTML 一样标签化的语法来实现界面布局（不过这个工具使用的前提是已经正确使用 AutoLayout &amp; CollectionViews）。 尽管这两个库都不完美，它们还是展现了声明式代码的四大优点： 关注点分离: 我们在使用声明式风格编写的代码时声明了意图，从而无需关注具体的底层实现，可以说这样的分离是自然发生的。 减少重复的代码: 所有声明式代码都共用一套样式实现，这里面很多属于配置文件，这样可以减少重复代码所带来的风险。 优秀的 API 设计: 声明式 API 可以让用户自行定制已有实现，而不是将已有实现做一种固定的存在看待。这样可以保证修改程度降至最小。 良好的可读性: 讲真，按照声明式 API 所写出来的代码简直优美无比。 这些天我写的大多数 Swift 代码非常适用于声明式编程风格。 不管是对于某一种数据结构的描述，或者是对某个功能的实现，在编写过程中，我最常使用的类型还是一些简单的结构体。声明不同的类型，主要是基于泛型类，然后这些东西负责实现具体的功能或者完成必要的工作。我们在 PlanGrid 开发过程中采用这种方法来编写我们得 Swift 代码。这种开发方式已经对对代码可读性的提升还有开发人员的效率提升上产生了巨大的影响。 本文我想讨论的是 PlanGrid 应用中所使用的 API 设计，它原本使用 NSOperationQueue 实现，现在使用了一种更接近声明式的方法－讨论这个 API 应该可以展示声明式编程风格在各方面的好处。 在 Swift 中构建一个声明式请求序列我们重新设计的 API 用来将本地变化（也可能是离线发生的）与 API 服务器进行同步。我不会讨论这种变化追踪方法的细节，而是将精力放在网络请求的生成和执行上。 在这篇文章里，我想专注于一个特定的请求类型上：上传本地生成的图片。出于多种因素的考虑（超出本文讨论范围），上传图片的操作包括三次请求： 向 API 服务器发起请求，API 服务器将会响应，响应内容为向 AWS 服务器上传图片所需信息。 上传图片至 AWS （使用上次请求得到的信息）。 向 API 服务器发起请求以确认图片上传成功。 既然我们有包括这些请求序列的上传任务，我们决定将其抽象成一个特殊的类型，并让我们的上传架构支持它。 定义请求序列协议我们决定引入一个单独的类型来对网络请求序列进行描述。这个类型将被我们的上传者类使用，上传者类的作用是将描述转化为实在的网络请求(要提醒你们的是我们不会在本篇文章中讨论上传者类的实现）。 接下来这个类型是我们控制流的精髓：我们有一个请求序列，序列中的每个请求都可能依赖于前一个请求的结果。 小贴士: 接下来的代码里的一些类型的命名方式看起来有点奇怪，但是它们中大多数是根据应用专属术语集来命名的（如： Operation ）。 12345678910111213141516public typealias PreviousRequestTuple = ( request: PushRequest, response: NSURLResponse, responseBody: JsonValue?)/// A sequence of push requests required to sync this operation with the server./// As soon as a request of this sequence completes,/// `PushSyncQueueManager` will poll the sequence for the next request./// If `nil` is returned for the `nextRequest` then/// this sequence is considered complete.public protocol OperationRequestSequence: class { /// When this method returns `nil` the entire `OperationRequestSequence` /// is considered completed. func nextRequest(previousRequest: PreviousRequestTuple?) throws -&gt; PushRequest?} 通过调用 nextRequest: 方法来让请求序列生成一个请求时，我们提供了一个对前一个请求的引用，包括 NSURLResponse 和 JSON 响应体（如果存在的话）。每一个请求的结果都可能在下一次请求时产生（（将会返回一个 PushRequest 对象），除了没有下一次请求（返回 nil ）或者在请求过程中发生了一些以外的情况导致没有返回必要的响应以外（请求序列在该情况下 throws ）。 值得注意的是， PushRequest 并不是这个返回值类型的理想名。这个类型只是描述一个请求的详情（结束符，HTTP 方法等等），其并不参与任何实质性的工作。这是声明式设计中很重要的一个方面。 你可能已经注意到了这个协议依赖于一个特定 class ，我们这样做是因为我们意识到 OperationRequestSequence 其是一个状态描述类型。它需要能够捕获并使用前面的请求所产生的结果（比如：在第三个请求里可能需要获取第一个请求的响应结果）。这个做法参考了 mutating 方法的结构，不得不说这样的行为貌似让这部分有关上传操作的代码变得更为复杂了（所以说重新赋值变化结构体并不是一件那么简单的事儿） 在基于 OperationRequestSequence 协议实现了我们第一个请求序列后，我们发现相比实现 nextRequest 方法来说，简单地提供一个数组来保存请求链更合适。于是我们便添加了 ArrayRequestSequence 协议来提供了一个请求数组的实现： 1234567891011121314public typealias RequestContinuation = (previous: PreviousRequestTuple?) throws -&gt; PushRequest?public protocol ArrayRequestSequence: OperationRequestSequence { var currentRequestIndex: Int { get set } var requests: [RequestContinuation] { get }}extension ArrayRequestSequence { public func nextRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { let nextRequest = try self.requests[self.currentRequestIndex](previous: previous) self.currentRequestIndex += 1 return nextRequest }} 这个时候，我们定义了一个新的上传序列，这只是很微小的一点工作。 实现请求序列协议作为一个小例子，让我们看看用来上传快照的上传序列吧（在 PlanGrid 中，快照指的是在图片中绘制的可导出的蓝图或者注释）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/// Describes a sequence of requests for uploading a snapshot.final class SnapshotUploadRequestSequence: ArrayRequestSequence { // Removed boilerplate initializer &amp; // instance variable definition code... // This is the definition of the request sequence lazy var requests: [RequestContinuation] = { return [ // 1\\. Get AWS Upload Package from API self._allocationRequest, // 2\\. Upload Snapshot to AWS self._awsUploadRequest, // 3\\. Confirm Upload with API self._metadataRequest ] }() // It follows the detailed definition of the individual requests: func _allocationRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { // Generate an API request for this file upload // Pass file size in JSON format in the request body return PushInMemoryRequestDescription( relativeURL: ApiEndpoints.snapshotAllocation(self.affectedModelUid.value), httpMethod: .POST, jsonBody: JsonValue(values: [ &quot;filesize&quot; : self.imageUploadDescription.fullFileSize ] ), operationId: self.operationId, affectedModelUid: self.affectedModelUid, requestIdentifier: SnapshotUploadRequestSequence.allocationRequest ) } func _awsUploadRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { // Check for presence of AWS allocation data in response body guard let allocationData = previous?.responseBody else { throw ImageCreationOperationError.MissingAllocationData } // Attempt to parse AWS allocation data self.snapshotAllocationData = try AWSAllocationPackage(json: allocationData[&quot;snapshot&quot;]) guard let snapshotAllocationData = self.snapshotAllocationData else { throw ImageCreationOperationError.MissingAllocationData } // Get filesystem path for this snapshot let thumbImageFilePath = NSURL(fileURLWithPath: SnapshotModel.pathForUid( self.imageUploadDescription.modelUid, size: .Full ) ) // Generate a multipart/form-data request // that uploads the image to AWS return AWSMultiPartRequestDescription( targetURL: snapshotAllocationData.targetUrl, httpMethod: .POST, fileURL: thumbImageFilePath, filename: snapshotAllocationData.filename, operationId: self.operationId, affectedModelUid: self.affectedModelUid, requestIdentifier: SnapshotUploadRequestSequence.snapshotAWS, formParameters: snapshotAllocationData.fields ) } func _metadataRequest(previous: PreviousRequestTuple?) throws -&gt; PushRequest? { // Generate an API request to confirm the completed upload return PushInMemoryRequestDescription( relativeURL: ApiEndpoints.snapshotAllocation(self.affectedModelUid.value), httpMethod: .PUT, jsonBody: self.snapshotMetadata, operationId: self.operationId, affectedModelUid: self.affectedModelUid, requestIdentifier: SnapshotUploadRequestSequence.metadataRequest ) }} 在实现的过程中你应该注意这样几件事情： 这里面几乎没有命令式代码。大多数的代码都通过实例变量和前次请求的结果来描述网络请求。 代码并不调用网络层，也没有任何上传操作的类型信息。它们只是对每个请求的详情进行了描述。事实上，这段代码没有能被观测到的副作用，它只更改了内部状态。 这段代码里可以说没有任何的错误处理代码。这个类型只负责处理该请求序列中发生的特定错误（比如前次请求并未返回任何结果等）。而其余的错误通常都在网络层予以处理了。 我们使用 PushInMemoryRequestDescription/AWSMultipartRequestDescription 来对我们对自己的 API 服务器或者是对 AWS 服务器发起请求的行为进行抽象。我们的上传代码将会根据情况在两者之前进行切换，对两者使用不同的 URL 会话配置，以免将我们自有 API 服务器的认证信息发送至 AWS 。 我不会详细讨论整个代码，但是我希望这个例子能充分展现我之前提到过的声明式设计方法的一系列优点： 关注点分离: 上面编写的类型只有描述一系列请求这一单一功能。 减少重复的代码: 上面编写的类型里面只包含对请求进行描述的代码，并不包含网络请求及错误处理的代码。 优秀的 API 设计: 这样的 API 设计能有效的减轻开发者的负担，他们只需要实现一个简单的协议以确保后续产生的请求是基于前一个请求结果的即可。 良好的可读性: 再次声明，以上代码非常集中；我们不需要在样板代码的海洋里游泳，就可以找到代码的意图。那也说明，为了更快地理解这段代码，你需要对我们的抽象方式有一定的了解。 现在可以想想如果利用 NSOperationQueue 来替代我们的方案会怎么样？ 什么是 NSOperationQueue ？采用 NSOperationQueue 的方案复杂了很多，所以在这篇文章里给出相对应的代码并不是一个很好的选择。不过我们还是可以讨论下这种方案。 关注点分离在这种方案中难以实现。和对请求序列进行简单抽象不同的是，NSOperationQueue 中的 NSOperations 对象将负责网络请求的开关操作。这里面包含请求取消和错误处理等特性。在不同的位置都有相似的上传代码，同时这些代码很难进行复用。在大多数上传请求被抽象成一个 NSOperation 的情况下，使用子类并不是一个好选择，虽然说我们得上传请求队列被抽象成为一个被 NSOperationQueue 所装饰的 NSOperation 。 NSOperationQueue 中的无关信息相当多。。代码中随处可见对网络层的操作和调用 NSOperation 中的特定方法，比如 main 和 finish 方法。在没有深入了解具体的 API 调用规则前，很难知道具体操作是用来做什么的 这种 API 所采用的处理方式，某种意义上让开发者的开发体验变得更差了。和简单的实现相对应的协议不同的是，在 Swift 中如果采用上述的开发方式，人们需要去了解一些约定俗成的规定，尽管这些规定可能并不强制要求你遵守。 这种处理方式将会显著增加开发者的负担。与实现一个简单协议不同的是，在新版本的 Swift 中实现这样的代码的话，我们需要去理解一些特有的约定。尽管很多被记载下来的约定并不是与编程相关的。 由于一些其他原因，该 API 可能会导致一些与网络请求的错误报告相关的 bug 。为了避免每个请求操作都执行自己的错误报告代码，我们将其集中在一个地方进行处理。错误处理代码将会在请求结束之后开始执行。然后代码将会检查请求类型中的 error 属性的值是否存在。为了及时地反馈错误信息，开发者需要及时在操作完成之前设置 NSOperation 中的 error 属性的值。由于这是一个非强制性约定导致一堆新代码忘记设置其属性的值，可能会导致诸多错误信息的遗失。 所以啊，我们很期待我们介绍的这样一种新的方式能帮助开发者们在未来编写上传及其余功能的代码。 总结声明式的编程方法已经对我们的编程技能和开发效率产生了巨大的影响。我们提供了一种受限的 API ，这种 API 用途单一且不会留下一堆迷之 Bug 。我们可以避免使用子类及多态等一系列手段，转而使用基于泛型类型的声明式风格代码来替代它。我们可以写出优美的代码。我们所编写的代码都是能很方便的进行测试的(关于这点，编程爱好者们可能觉得在声明式风格代码中测试可能不是必要的。）所以你可能想问：“别告诉我这是一种完美无瑕的编程方式？” 首先，在具体的抽象过程中，我们可能会花费一些时间与精力。不过，这种花费可以通过仔细设计 API ，并并通过提供一些测试，代替用例实现功能，为使用者提供参考。 其次，请注意，声明式编程并不是适用于任何时间任何业务的。要想适用声明式编程，你的代码库里至少要有一个用相似方法解决了多次的问题。如果你尝试在一个需要高度可定制化的应用里使用声明式编程， 然后你又对整个代码进行了错误的抽象，那么最后你会得到如同乱麻一般的半声明式代码。对于任何的抽象过程而言，过早地进行抽象都会造成一大堆令人费解的问题。 声明式 API 有效地将 API 使用者身上的压力转移至 API 开发者身上，对于命令式 API 则不需要这样。为了提供一组优秀的声明式 API ，API 的开发者必须确保接口的使用与接口的实现细节进行严格的隔离。不过严格遵循这样要求的 API 是很少的。React 和 GraphQL 证明了声明式 API 能有效提升团队编码的体验。 其实我觉得，这只是一个开端，我们会慢慢发现在复杂的库中所隐藏复杂的细节和对外提供的简单易用的接口。期待有一天，我们能利用一个基于声明式编程的 UI 库来构建我们的 iOS 程序。","link":"/posts/2016/10/26/Declarative-API-Design-in-Swift/"},{"title":"详解模板引擎工作机制","text":"原文地址：How a template engine works 原文作者：Shipeng Feng 译文出自：掘金翻译计划 译者： Zheaoli 校对者：Kulbear, hpoenixf 我已经使用各种模版引擎很久了，现在终于有时间研究一下模版引擎到底是如何工作的了。 简介简单的说，模版引擎是一种可以用来完成涉及大量文本数据的编程任务的工具。一般而言，我们经常在一个 web 应用中利用模板引擎来生成 HTML 。在 Python 中，当你想使用模板引擎的时候，你会发现你有不少的选择，比如jinja 或者是mako。从现在开始，我们将利用 tornado 中的模板引擎来讲解模板引擎的工作原理，在 tornado 中，自带的模板引擎相对的简单，能方便我们去深入的剖析其原理。 在我们研究（模版引擎）的实现原理之前，先让我们来看一个简单的接口调用例子。 123456789101112131415from tornado import templatePAGE_HTML = &quot;&quot;&quot;&lt;html&gt; Hello, {{ username }}! &lt;ul&gt; {% for job in job_list %} &lt;li&gt;{{ job }}&lt;/li&gt; {% end %} &lt;/ul&gt;&lt;/html&gt; &quot;&quot;&quot;t = template.Template(PAGE_HTML)print t.generate(username='John', job_list=['engineer']) 这段代码里的 username 将会动态的生成，job 列表也是如此。你可以通过安装 tornado 并运行这段代码来看看最后的效果。 详解如果你仔细观察 PAGE_HTML ，你会发现这段模板字符串由两个部分组成，一部分是固定的字符串，另一部分是将会动态生成的内容。我们将会用特殊的符号来标注动态生成的部分。在整个工作流程中，模板引擎需要正确输出固定的字符串，同时需要将正确的结果替换我们所标注的需要动态生成的字符串。 使用模板引擎最简单的方式就是像下面这样用一行 python 代码就可以解决： 12deftemplate_engine(template_string, **context):# process herereturn result_string 在整个工作过程中，模板引擎将会分为如下两个阶段对我们的字符串进行操作： 解析 渲染 在解析阶段，我们将我们准备好的字符串进行解析，然后格式化成可被渲染的格式，其可能是能被 rendered.Consider 所解析的字符串，解析器可能是一个语言的解释器或是一个语言的编译器。如果解析器是一种解释器的话，在解析过程中将会生成一种特殊的数据结构来存放数据，然后渲染器会遍历整个数据结构来进行渲染。例如 Django 的模板引擎中的解析器就是一种基于解释器的工具。除此之外，解析器可能会生成一些可执行代码，渲染器将只会执行这些代码，然后生成对应的结果。在 Jinja2 ， Mako ，Tornado 中，模板引擎都在使用编译器来作为解析工具。 编译如同上面所说的一样，我们需要解析我们所编写的模板字符串，然后 tornado 中的模板解析器将会将我们所编写的模板字符串编译成可执行的 Python 代码。我们的解析工具负责生成Python代码，而仅仅由单个Python函数构成： 123def parse_template(template_string): # compilation return python_source_code 在我们分析 parse_template 的代码之前，让我们先看个模板字符串的例子： 12345678&lt;html&gt; Hello, { { username } }! &lt;ul&gt; { % for job in jobs % } &lt;li&gt;{ { job.name } }&lt;/li&gt; { % end % } &lt;/ul&gt;&lt;/html&gt; 模板引擎里的 parse_template 函数将会将上面这个字符串编译成 Python 源码，最简单的实现方式如下： 12345678910111213def _execute(): _buffer = [] _buffer.append('\\n&lt;html&gt;\\n Hello, ') _tmp = username _buffer.append(str(_tmp)) _buffer.append('!\\n &lt;ul&gt;\\n ') for job in jobs: _buffer.append('\\n &lt;li&gt;') _tmp = job.name _buffer.append(str(_tmp)) _buffer.append('&lt;/li&gt;\\n ') _buffer.append('\\n &lt;/ul&gt;\\n&lt;/html&gt;\\n') return''.join(_buffer) 现在我们在 _execute 函数里处理我们的模版。这个函数将可以使用全局命名空间里的所有有效变量。这个函数将创建一个包含多个 string 的列表并将他们合并后返回。显然找到一个局部变量比找一个全局变量要快多了。同时，我们对于其余代码的优化也在这个阶段完成，比如： 12345_buffer.append('hello')_append_buffer = _buffer.append# faster for repeated use_append_buffer('hello') 在 { { ... } } 中的表达式将会被提取出来，然后添加进 string 列表中。在 tornado 模板模块中，在 { { ... } } 所编写的表达式没有任何的限制，if 和 for 代码块都可以准确地转换成为 Python 代码。 让我们来看看具体的代码实现吧让我们来看看模板引擎的具体实现吧。我们在 Template 类中编声明核心变量，当我们创建一个 Template 对象后，我们便可以编译我们所编写的模板字符串，随后我们便可以根据编译的结果来对其进行渲染。我们只需要对我们所编写的模板字符串进行一次编译，然后我们可以缓存我们的编译结果，下面是 Template 类的简化版本的构造器： 1234class Template(object): def__init__(self, template_string): self.code = parse_template(template_string) self.compiled = compile(self.code, '&lt;string&gt;', 'exec') 上段代码里的 compile 函数将会将字符串编译成为可执行代码，我们可以稍后调用 exec 函数来执行我们生成的代码。现在，让我们来看看 parse_template 函数的实现，首先，我们需要将我们所编写的模板字符串转化成一个个独立的节点，为我们后面生成 Python 代码做好准备。在这过程中，我们需要一个 _parse 函数，我们先把它放在一边，等下在回来看看这个函数。现，我们需要编写一些辅助函数来帮助我们从模板文件里读取数据。现在让我们来看看 _TemplateReader 这个类，它用于从我们自定义的模板中读取数据： 123456789101112131415161718192021222324252627282930313233343536373839class _TemplateReader(object): def __init__(self, text): self.text = text self.pos = 0 def find(self, needle, start=0, end=None): pos = self.pos start += pos if end is None: index = self.text.find(needle, start) else: end += pos index = self.text.find(needle, start, end) if index != -1: index -= pos return index def consume(self, count=None): if count is None: count = len(self.text) - self.pos newpos = self.pos + count s = self.text[self.pos:newpos] self.pos = newpos return s def remaining(self): return len(self.text) - self.pos def __len__(self): return self.remaining() def __getitem__(self, key): if key &lt; 0: return self.text[key] else: return self.text[self.pos + key] def __str__(self): return self.text[self.pos:] 为了生成 Python 代码，我们需要去看看 _CodeWriter 这个类的源码，这个类可以编写代码行和管理缩进，同时它也是一个 Python 上下文管理器： 123456789101112131415161718192021222324252627class _CodeWriter(object): def __init__(self): self.buffer = cStringIO.StringIO() self._indent = 0 def indent(self): return self def indent_size(self): return self._indent def __enter__(self): self._indent += 1 return self def __exit__(self, *args): self._indent -= 1 def write_line(self, line, indent=None): if indent == None: indent = self._indent for i in xrange(indent): self.buffer.write(&quot; &quot;) print self.buffer, line def __str__(self): return self.buffer.getvalue() 在 parse_template 函数里，我们先要创建一个 _TemplateReader 对象： 123456def parse_template(template_string): reader = _TemplateReader(template_string) file_node = _File(_parse(reader)) writer = _CodeWriter() file_node.generate(writer) return str(writer) 然后，我们将我们所创建的 _TemplateReader 对象传入 _parse 函数中以便生成节点列表。这里生成的所有节点都是模板文件的子节点。接着，我们创建一个 _CodeWriter 对象，然后 file_node 对象会把生成的 Python 代码写入 _CodeWriter 对象中。然后我们返回一系列动态生成的 Python 代码。_Node 类将会用一种特殊的方法去生成 Python 源码。这个先放着，我们等下再绕回来看。 现在先让我们回头看看前面所说的 _parse 函数： 123456789101112131415161718192021222324252627def _parse(reader, in_block=None): body = _ChunkList([]) while True: # Find next template directive curly = 0 while True: curly = reader.find(&quot;{&quot;, curly) if curly == -1 or curly + 1 == reader.remaining(): # EOF if in_block: raise ParseError(&quot;Missing { %% end %% } block for %s&quot; % in_block) body.chunks.append(_Text(reader.consume())) return body # If the first curly brace is not the start of a special token, # start searching from the character after it if reader[curly + 1] not in (&quot;{&quot;, &quot;%&quot;): curly += 1 continue # When there are more than 2 curlies in a row, use the # innermost ones. This is useful when generating languages # like latex where curlies are also meaningful if (curly + 2 &lt; reader.remaining() and reader[curly + 1] == '{' and reader[curly + 2] == '{'): curly += 1 continue break 我们将在文件中无限循环下去来查找我们所规定的特殊标记符号。当我们到达文件的末尾处时，我们将文本节点添加至列表中然后退出循环。 123# Append any text before the special tokenif curly &gt; 0: body.chunks.append(_Text(reader.consume(curly))) 在我们对特殊标记的代码块进行处理之前，我们先将静态的部分添加至节点列表中。 1start_brace = reader.consume(2) 在遇到 { { 或者 { % 的符号时，我们便开始着手处理相应的的表达式： 1234567891011# Expressionif start_brace == &quot;{ {&quot;: end = reader.find(&quot;} }&quot;) if end == -1 or reader.find(&quot;\\n&quot;, 0, end) != -1: raise ParseError(&quot;Missing end expression } }&quot;) contents = reader.consume(end).strip() reader.consume(2) if not contents: raise ParseError(&quot;Empty expression&quot;) body.chunks.append(_Expression(contents)) continue 当遇到 { { 之时，便意味着后面会跟随一个表达式，我们只需要将表达式提取出来，并添加至 _Expression 节点列表中。 1234567891011121314151617181920212223# Blockassert start_brace == &quot;{ %&quot;, start_braceend = reader.find(&quot;% }&quot;)if end == -1 or reader.find(&quot;\\n&quot;, 0, end) != -1: raise ParseError(&quot;Missing end block % }&quot;)contents = reader.consume(end).strip()reader.consume(2)if not contents: raise ParseError(&quot;Empty block tag ({ % % })&quot;)operator, space, suffix = contents.partition(&quot; &quot;)# End tagif operator == &quot;end&quot;: if not in_block: raise ParseError(&quot;Extra { % end % } block&quot;) return bodyelif operator in (&quot;try&quot;, &quot;if&quot;, &quot;for&quot;, &quot;while&quot;): # parse inner body recursively block_body = _parse(reader, operator) block = _ControlBlock(contents, block_body) body.chunks.append(block) continueelse: raise ParseError(&quot;unknown operator: %r&quot; % operator) 在遇到模板里的代码块的时候，我们需要通过递归的方式将代码块提取出来，并添加至 _ControlBlock 节点列表中。当遇到 { % end % } 时，意味着这个代码块的结束，这个时候我们可以跳出相对应的函数了。 好了现在，让我们看看之前所提到的 _Node 节点，别慌，这其实是很简单的： 123456789101112class _Node(object): def generate(self, writer): raise NotImplementedError()class _ChunkList(_Node): def __init__(self, chunks): self.chunks = chunks def generate(self, writer): for chunk in self.chunks: chunk.generate(writer) _ChunkList 只是一个节点列表而已。 12345678910class _File(_Node): def __init__(self, body): self.body = body def generate(self, writer): writer.write_line(&quot;def _execute():&quot;) with writer.indent(): writer.write_line(&quot;_buffer = []&quot;) self.body.generate(writer) writer.write_line(&quot;return ''.join(_buffer)&quot;) 在 _File 中，它会将 _execute 函数写入 CodeWriter。 1234567891011121314151617class _Expression(_Node): def __init__(self, expression): self.expression = expression def generate(self, writer): writer.write_line(&quot;_tmp = %s&quot; % self.expression) writer.write_line(&quot;_buffer.append(str(_tmp))&quot;)class _Text(_Node): def __init__(self, value): self.value = value def generate(self, writer): value = self.value if value: writer.write_line('_buffer.append(%r)' % value) _Text 和 _Expression 节点的实现也非常简单，它们只是将我们从模板里获取的数据添加进列表中。 123456789class _ControlBlock(_Node): def __init__(self, statement, body=None): self.statement = statement self.body = body def generate(self, writer): writer.write_line(&quot;%s:&quot; % self.statement) with writer.indent(): self.body.generate(writer) 在 _ControlBlock 中，我们需要将我们获取的代码块按 Python 语法进行格式化。 现在让我们看看之前所提到的模板引擎的渲染部分，我们通过在 Template 对象中实现 generate 方法来调用从模板中解析出来的 Python 代码。 123456def generate(self, **kwargs): namespace = { } namespace.update(kwargs) exec self.compiled in namespace execute = namespace[&quot;_execute&quot;] return execute() 在给予的全局命名空间中， exec 函数将会执行编译过的代码对象。然后我们就可以在全局中调用 _execute 函数了。 最后经过上面的一系列操作，我们便可以尽情的编译我们的模板并得到相对应的结果了。其实在 tornado 模板引擎中，还有很多特性是我们没有讨论到的，不过，我们已经了解了其最基础的工作机制，你可以在此基础上去研究你所感兴趣的部分，比如： 模板继承 模板包含 其余的一些逻辑控制语句，比如 else , elfi , try 等等 空白控制 特殊字符转译 更多没讲到的模板指令（译者注：请参考 tornado 官方文档","link":"/posts/2016/08/13/How-a-template-engine-works/"},{"title":"她曾以为自己能逃开教授的手丨人间","text":"她曾以为自己能逃开教授的手丨人间 《不能说的夏天》剧照 之前听舍友笑薇被教授性骚扰时，小柯还以为那只是老师对好学生的亲昵行为，安慰她说：“这应该是老师表达欣赏你的一种方式吧。”但几天后她就也被教授性骚扰了。 陈静越来越焦虑。 她又梦见去上课，楼梯里遇到教授张鹏，转身想跑，对方一把手抓住她，恶毒地问：你为什么举报我？你把我逼急了，我也不让你活…… 在惊恐中醒来，陈静大汗淋漓。 早在今年“五四”青年节，她们五个女生给中大纪委发去了举报信，指控张鹏从2011年到2017年持续性骚扰女学生和女老师，是田野中名副其实的“叫兽”。 张鹏，中山大学社会学与人类学学院（下称“社人院”）兼生命科学大学院教授，跨学科博士生导师（生态学、社会学方向），兼任国际自然保护联盟（IUCN）物种生存委员会委员，2016年青年长江学者。在网络上搜索他的名字，无论是文艺青年的社交网路，或者是著名的科普网站，他会经常跟“灵长类动物研究”出现在一起。 1陈静的噩梦始于2016年1月底的内伶仃猕猴种群数量田野调查。 田野调查是中大社人院每年组织的特色研究活动。张鹏每年寒假都会带队去不同的岛“蹲点”和“环岛”，以此来评估岛上有多少猕猴。陈静喜欢观察猴子的行为模式，田野中一直在蹲点观测。 田野调查最后一天，学员们相互体验，陈静从蹲点观测转为环岛两圈。第一圈时，她遇到在其中一段路蹲点观测的张鹏。 注意到独自环岛的陈静，张鹏说：“我陪你走一段吧。”当时的陈静觉得，陈教授“真亲民”——大一时她听过张鹏的人类学系列讲座，感觉“内容丰富，氛围活跃”，对灵长类研究产生了浓烈的兴趣，也对授课的张鹏心怀敬意。 路上，陈静话不多，张鹏一会儿称赞陈静的长相，“你长得真可爱”，一会儿又分析她的性格，“有南方姑娘温柔气质，又有北方人的豪爽”。逐渐地，张鹏向陈静靠得越来越近，聊得越来越具体，“你的头发真好”，顺势把玩起陈静的长发，还时不时闻一下说，“真香啊。” 陈静先是觉得尴尬，后来越来越不舒服，她隐隐觉得这不应该是一个教授应有的举动，却又不知如何是好，只能加快脚步赶到下一段有同学在的地方。 环岛第二圈，陈静再次遇上张鹏时，张鹏又上前提出“一起走吧”，还很自然地把手搭在陈静肩膀上。陈静不适，碍于师生情面，并未明确拒绝。走过泥泞路段，张鹏突然拉着陈静比起了身高，“感觉你没1米6啊”，比完身高又说“想看看你有多重”，就在陈静不知要如何回答还未及时拒绝时，张鹏一把抱起了陈静，还顺势把头埋到陈静胸口深深吸了一口气。 “那时候整个人完全懵了，不敢相信。”陈静说，想起他在自己胸口闻的那个动作，至今仍想干呕，有一股强烈的羞耻感。陈静挣扎起来，张鹏才把她放下，一放下，陈静就快步跑开，拿出手机，紧张地给她姐姐以及姐姐的男朋友所在的群发信息：“感觉张教授是‘叫兽’。” 陈静姐姐的男友、中山大学2013届历史系学生陈翰元，向我佐证了陈静的讲述。他坦言，当时看到陈静的那条信息，并没太当回事，“以为张鹏也就是在她面前讲了个黄色笑话”。等陈静回到住处，详细和他们讲起具体细节时，陈翰元才意识到这是性骚扰。当时的陈翰元也只是从男性的角度猜测，问：“张鹏是不是很喜欢你啊？” 陈翰元建议陈静去了解一下张鹏的为人，他听过张鹏不少光鲜的头衔，主持如国家自然科学基金及中日国际交流等不少国家级科研项目，也听说过张鹏还有个被广为流传事迹：据说，做研究时，张鹏曾给一个猴群里最漂亮的一只母猴以他女朋友的名字命名，靠此来排解在山上几个月里对女朋友的思念。后来者也跟着张鹏叫起这个名字，于是就在观察笔记写：xxx（张鹏女朋友名字）今天和一只雄猴打情骂俏，明天和另一只雄猴交配。张鹏也津津乐道。 陈翰元担心陈静因为张鹏一次偶尔的“低级错误”影响她对学业的追求，曾尝试劝解。但是，后来好几次聚餐，陈静都会跟姐姐和陈翰元谈起被张鹏性骚扰的经历，越发变得压抑，情绪低落，还持续做噩梦。 陈静无法理解和接受张鹏的举动，也不能让别人理解自己的感受，“感觉自己在孤岛里转”，只好选择了暂时性遗忘。此后她再遇到张鹏，就能躲多远就躲多远，即便上曾经最喜欢的“灵长类进化论”，都变成一种煎熬。 后来，陈静陆陆续续听到张鹏性骚扰其他女生的事情。 2其实，张鹏对女学生更频繁的性骚扰集中在2015年。 那一年暑假，张鹏带队到海南南湾猴岛进行为期一个月的野外实习。当时大二的笑薇和小柯在这次田野实习中，先后遭受了张鹏的性骚扰。 电话里寒暄时，笑薇说话轻快爽朗；谈到张鹏，她的语速变得缓慢而坚定：“张鹏真的伤害了一群女生，他没有资格做中大教授。” 笑薇向我缓缓了讲述她遭遇张鹏性骚扰的经历： 一天开完组会后，约深夜11点，张鹏喊住她“来办公室改论文”。笑薇想，白天都在外观察猴群，晚上讨论也是情理之中；而且张鹏从日本回来，看起来对科研十分严格，于是毫无担忧地去了。 笑薇原本坐在张鹏对面，张鹏指着电脑屏幕招呼她坐到自己同侧。开始笑薇还跟他保持了半米的礼貌距离，张鹏又叫她坐近一点，“坐过来看得清楚”。 出于对张鹏教授身份的尊敬，笑薇没多想，论文讲到一半，张鹏指出她论文问题，同时右手环住她的肩膀久久不拿开，笑薇感到窘迫。 “他先是说了论文这里不对、那里有问题，然后拍拍我的肩膀，拍完手就放着不走了。”笑薇当时思绪复杂，一边想着论文，一边莫名害怕，“还安慰自己，长辈拍晚辈肩膀是常有的，是自己胡思乱想”。 然而，张鹏的话暧昧起来，“你长得真漂亮啊”，手还拍起笑薇的手背。“那时我真是害怕，紧张，窘迫，他却表现得很自然。”笑薇说，回想起来她才发现张鹏的恐怖，“他一边指出你论文各种问题，让你害怕，一边又似乎安慰你，挽肩膀拍手，让你难以理解他动作的真实含义；他控制着你的情绪，让你的注意力都在论文问题上，一时辨识不了他行为的性质。” 在笑薇此前的认知中，张鹏教授常年在野外，年轻有为，风趣幽默——“眼前这个人跟课堂上谈笑风生的让人尊敬的教授完全不是一个人啊！怎么会这样？他不是日本回来的学者吗？” 在坐立不安中笑薇艰难度过了讲论文的一个小时。回到寝室，她跟几个舍友说起张鹏对她的举动。 舍友小柯问她：“是不是你想多了？” 小柯后来在接受采访时向我解释，她当时这样问，并不是质疑笑薇——她从小学到高中，一直都幸运地遇上各种好老师，所以一直觉得老师都是高山仰止，会爱护学生，形象正直高大。所以当笑薇说张鹏“有点不对劲”、对自己动手动脚的时候，小柯还以为那只是老师对好学生的亲昵行为，安慰她说：“这应该是老师表达欣赏你的一种方式吧。” 笑薇的另一位舍友对我说，那晚她听到笑薇说张鹏讲暧昧的话、还摸手搭背时十分惊讶，“我虽没听过张鹏的课，但很多人都说他课讲得好，没想到是这样的老师”。田野回来后，笑薇也曾多次向她透露对张鹏的反感，“她说不想写这个田野报告，不喜欢张鹏，不想见到他”。 笑薇和小柯都坦率告诉我，她们那时未曾意识到、或者意识到了却不愿意相信备受尊敬的教授会性骚扰自己，“如果是陌生人，他随便搭着你的肩膀，摸你的背，拍你的手，闻你的头发，又说‘你很漂亮’此类的话我肯定知道这是性骚扰，但这个人是老师啊，是自己原本尊敬的教授，他那么威严，怎么去辨识他的行为呢？” 女孩们讨论最终得出的解决方案是：不再单独与张鹏相处，找张鹏改论文的时候舍友们要在门口等着。 3然而，小柯很快就“被现实啪啪啪地打脸了”：张鹏也性骚扰了她。 几天后的一次组会结束，小柯让两舍友在门外等，自己带着报告进去张鹏办公室。问完问题小柯想走，张鹏却开始跟她聊起无关田野实习和论文的事，还紧挨着她坐下，一边笑着说奉承话，一边抓起她手腕看，“你的手好细啊”，等小柯把手抽开，张鹏又摸起小柯的头发，“你的头发发质好好啊”，还抓起她一缕头发把玩起来。 小柯尴尬极了，忐忑不已，却手足无措。突然，张鹏起身，走到门口探出身子左右张望——直到后来，小柯才意识到他当时探出头是为了看外面有没有人。 当时小柯还安慰自己，舍友就在外面，不用害怕。然而，她却见张鹏以“外面虫子多”为由把门关上了。关上门的那一刻，小柯懵了。后来她才知道，由于张鹏拖得时间过长，舍友们先行回了宿舍。 关了门后的张鹏言语越发露骨：“我看你这么努力，总让我想到我小时候，也这么努力。你就像一个小妹妹……让我抱一下……”不等小柯拒绝，张鹏便一把抱住了她，“我脑袋一片空白，他一松开的时候，我就赶紧跑走了”。 小柯满脸通红跑出张鹏办公室的一幕，正好被路过的笑薇看见。两个女孩难以理解，张鹏作为一名已婚教授，行为为何如此不堪？她们也不懂，事后张鹏为什么可以没有丝毫避讳，还毫无廉耻之心出现在女学生们面前。 “他看起来那么理直气壮，那么自然，让你怀疑，是不是自己太敏感了？”小柯曾猜测，女学生或许是张鹏心血来潮的戏弄对象？——可她后来才知道是自己后知后觉，张鹏对她的骚扰行为其实早就有了苗头。2015年春，小柯曾和同学们跟随张鹏到上川岛进行一个“小田野”。傍晚休息时，同学们和张鹏商量看第二天的日出，小柯应和并着手查询次日的天气。就在此时，张鹏走到她后面，把手心贴在小柯后背心上，久久不曾放开。当时，小柯按下内心的不适，将这一动作看作长辈对晚辈的一种亲昵，“没想到他是一步步试探”。 即便看到了张鹏的“叫兽”面目，女生们也不知道如何是好：岛上只有他们一个调研组，只有张鹏一个教授，她们不知道该向谁申诉。而且，田野报告需要张鹏打分，她们还有张鹏的课，甚至已经选了张鹏做论文指导导师。 她们能做的，只是不再与张鹏正面接触。 “那段日子有巨大的阴影笼罩在身上，世界仿佛到处都是黑暗。”电话那头，原本激动着控诉张鹏恶行的小柯突然放低了声音，“你知道吗？黄记者，那时感觉自己在地狱。” 4其实，女生们也曾做出力所能及的反抗。 因为项目和论文，小柯还是需要时常与张鹏接触，她曾认为掌握了张鹏的“套路”，“感觉可以保护自己了”。 每次要向张鹏当面汇报时，小柯都会提高警惕：“他把手放在了我大腿上，我直接把腿移开了；他用眼神从上到下扫一遍，那种眼神让人很不舒服，但我没办法控制他的眼神；他问你买了新衣服了？是不是烫头发了、变漂亮了？我都会说‘不是’，并且把话题立刻转移到论文或项目上。” 强硬起来对抗老师，小柯觉得结果没有想象中那么糟糕——那时，她并不知道还有更多女生受到了张鹏更为放肆的性骚扰。 张鹏也并未收敛。 小柯清楚记得，2017年4月的一个夜里，晚上8点多，张鹏走到她自习的桌前当面邀约，“再谈谈论文”，小柯不好当面拒绝，也不好当张鹏的面拿出手机提前给舍友发信息通风，只能跟着他去了办公室。 在办公室里留到快10点，张鹏就坐到了小柯身边，她担心起来，偷偷地快速给舍友发信息：“等我。” 张鹏看到小柯发信息的动作，立刻火了：“老师专门辅导，你竟玩手机？！”然后，张鹏开始用带有侮辱性言语攻击她：“没教养，自私自利”，“老师为了你的论文到现在都没吃饭，你呢？为老师做了什么？我把实验室的资源都提供给你，你又为实验室做了什么？” 小柯被张鹏的翻脸吓坏了，只得道歉，然而，张鹏并未停止责骂：“老师为你付出那么多，你是不是把老师当工具？是不是想快毕业了，可以远走高飞，翻脸不认人，什么都不为我做了？” 小柯难以相信，一个教授，为人师表，竟然说出这样的话。她后来和其他女生交流才得知，张鹏会抓住不同的女生们犯下的各种“错误”，在性骚扰不遂或被拒绝后都复制着一样的骂人模式，试图控制女生们的思想。 “他骂很多女生自私自利，可是，这个实验室本身就是人类学系系里为学生教学投资的，我们应当都有权力正当使用。然而，每次在实验室时，张鹏都要让我们觉得，（能使用实验室）这都是因为他的好心和慷慨，这个道德包袱太重了。” 后来，被张鹏性骚扰过的女生们聚集起来才发现，张鹏通常会选择性格温和、家庭背景普通、独立无援的女学生为骚扰对象。他的性骚扰行为在多人身上重复出现，呈现某种模式化特点： 他不明目张胆地胁迫，而是策划和利用情境（如修改论文、做田野项目），逐步拉近距离；他还会操控受害者心理，找到不同理由和借口严厉训斥，先打击、摧毁女生的自尊自信，使得学生战战兢兢；然后柔声抚慰，诉说欣赏、喜欢之情，打着“师长的关爱”的幌子借机拍背、捏手、拥抱、甚至亲吻，让惊慌的女生无法辨识其动作的真实目的。 小柯和笑薇感到庆幸——她俩及时毕业了，并未遭遇张鹏更为严重的骚扰。 5一次偶然借书机会，陈静认识了师姐小柯。熟悉后，小柯叮嘱师妹，“要小心张鹏”。两人细聊才知道，原来张鹏的性骚扰对象涉及各个年级的女生。 这个结论让她们感到更大的震惊，不约而同萌发了举报张鹏的念头，尤其是后来听到消息后：张鹏性骚扰了2017届的大一师妹，情节严重，接近性侵害。女孩告知了父母，其父亲来到中大评理，因有视频佐证，张鹏无法抵赖，被党内处分。 两个中大2017届的人类学专业的学生向我佐证了这个消息，他们承认“级里都在传”。其中一位学生透露，事发2018年4月3日晚上约10点半，张鹏与受害者女生单独在实验室，张鹏关了灯，对女生进行了严重的性骚扰。第二天，受害者女生父亲气冲冲地来到学校，他们刚好那时在同一栋大楼，看到有两名保安前往实验室取证，还调取了实验室走廊的视频。有当时在实验室的学生匿名向我证实，确实有保安前来调取视频。 知情学生透露说，视频画面里，张鹏先是从他办公室出来，到其他办公室敲了敲门，然后关了灯，又回到了自己的办公室，近半小时后，张鹏先走出办公室，就在走廊里提了提裤子，并把露在外面的衣角重新塞进裤子里，随后女生出来，两人一同离开。 办公室里具体发生了什么，该学生并不清楚，但知道第二天女孩的父亲就来了学校，去了纪委办公室。 张鹏终于被处分，这让学生们看到了一丝希望；但一个“党内处分”，并未平息学生们的愤怒。 “张鹏性骚扰学生的消息就没停过，但是他还是一直在性骚扰学生，而且情况越来越严重”。陈静说，她感觉“不能发生了不可逆转的性侵才举报，那就太迟了”。 小柯也气愤学校的保守处理：“张鹏的行为越来越大胆，一年比一年严重，真的要造成性侵这样实实在在的伤害、有视频证据，才能处罚他吗？” 得知情况的陈翰元也坐不住了：“他不是一次性的冲动，而是一而再再而三性骚扰学生，是一个惯犯，中大怎么能容忍这样的教授？” 受过张鹏性骚扰的女学生们自觉组成举报联盟，笑薇在班级群里实名告知师妹们：“如果选了张鹏的课和‘田野’，一定要格外小心保护自己。” 这引来了更多当事人响应。举报人们收集到了4封实名举报信和1封匿名举报信，让她们没想到的是，举报信中，竟然有一封是一名女老师写的。 6女老师的举报把张鹏最初的性骚扰行为时点提前到2011年，而且张鹏对这位女老师的性骚扰更为直接、严重。 因为这位女老师已经给纪委实名举报交代，并签下协议不再向外透露其他信息，我只能引用此前早已掌握的举报信材料。 女老师在举报信里称，2011年她刚入职中大外国语学院，在往返于中大南校区与东校区（大学城）的校车上，张鹏与她搭讪，“（他）坐我旁边座位，没说几句就开始摸，先是肩膀，再到大腿和大腿内侧，那个时候我很怕，车上有老师有学生，我不敢喊。只能闪避，比如背对他或者甩开他的手。” 当天晚上，她坐校车返回南校时，又遇到张鹏。张鹏借机坐在她身边，“他先是不断找我攀谈，讲述家里的烦心事，妻子不了解他等等。看我没有怎么搭理，就又开始动手动脚，把我的头拉向他的肩膀，并试图亲吻我的耳朵，并继续向胸部和大腿内侧摸。我跟他说，张老师，您这么做可不妥当。他说，我就是很喜欢你啊！当时校车已近校门，我赶紧甩开他下车了！” 之后，张鹏尾随她，并变本加厉进行骚扰，“上课下课都跟着我，找到机会就凑过来……动作越来越过分，往耳朵吹气，抚摸胸部，语言上多次要求发生男女关系，我没有办法，只好每次课都尽可能地约学生陪同搭车。当时我认识了一个住在南校、跟我一样需要搭校车往返的女学生。下课后，留意到张又在尾随我，为了不引起他的注意，我都用外语小声告诉学生这件事，希望她保护我。至此之后，该女生便一直陪同我，每次都坐在我的座位旁边。她也亲眼看到了张鹏的一些性骚扰行为。” 经多方打听，我只得知这位随行的女生是当时旅游管理学院的一位学生，但后来去了法国留学，没有留下有效的联系方式，我至今没有联系到对方进行佐证。 张鹏的性骚扰后来越发露骨和恶劣。另一知情的学生透露，女老师在写举报信前曾跟她说过：2012年初，张鹏平均每天发两三条短信或者打电话给她，言辞暧昧，直接要求去酒店开房或去办公室约会。 2012年春季，女老师调到珠海校区上课，张鹏不知道通过什么方式找到她在珠海校区的教师公寓住址，来到她门口不断敲门，“一敲就敲差不多一个小时，教师公寓的住客比较少，周围没什么人，我吓得不敢动弹。他又不断给我发短信，用词非常露骨，我只能不断地删除，并且把他拉黑名单，因为他，我换了三次号码。” 这段骚扰持续了几年，女老师已经结婚生子，直到2017年，张鹏依然尝试添加她的微信，纠缠不止。 72017届女生和女老师的遭遇给学子们敲醒了警钟：若继续沉默，只能成为待宰羔羊。2018年5月4日，青年节的时候，女生们实名给中大纪委寄出了五个当事人的举报信。 女生们的举报在学院里传开，支持当事人的学生们自主成立了“中山大学人类学系反性骚扰小组”（下称小组），草拟了建议信并半公开征集联署，很快把《人类学系学子关于本系的舆论事件及加快建立本院反性骚扰机制的建议信》发送至院长、副院长、系主任的邮箱。 学生们提出，他们查阅了《中山大学学生手册》、《中山大学学生申诉处理暂行办法》以及附录中的《高等学校校园秩序管理若干规定》和《学生伤害事故处理办法》，都没有找到关于性骚扰问题向何处申诉、哪个机构/部门负责处理、如何处理等信息。“文件多，却都没有实质操作意义”。 他们还查阅了《中山大学关于建立健全师德建设长效机制的实施办法》，文件中确实有指出纪检监察部门负责接收师德相关的举报：“只是，我们发现，里面邮箱负责人都不知道是谁；查到的联系电话，大多是党政办公室的电话，也是外部联系社人院的联系通路，其繁忙程度可想而知。在无专人负责、事务繁杂的情况下，我们有理由质疑，通过这部电话进行的申诉能否得到重视和处理。” 小组建议，尽快出台有效的校园反性骚扰机制，进行性骚扰的师生教育、田野行前培训；在院系层面设立公开渠道接受关于性骚扰的投诉举报、设立专门的负责人受理相关事宜等。 还有中大学生在“为学校发展规划建言献策之‘十大提案’活动”中上交了“关于中山大学师风师德规范细则的建议”的提案，提案详细分析了《中山大学关于建立健全师德建设长效机制的实施办法》和《中山大学教师考核实施办法（试行）》，指出，“中山大学在制度层面上已经有师德建设与师德考核制度，但既存的不同制度之间的重叠、区分甚至相互矛盾，不同渠道的程序的复杂，都让受害者望而却步，让既有的好制度失去其应有的作用。” 然而，提案上交后，一些校领导多次找了提交提案的学子们谈话，表示提案做得很好，但“这个话题太敏感不宜公开讨论”，甚至拒绝了让学生公开对提案进行答辩，在提案优秀奖的奖状上，也不能出现提案具体名称。 我联系中大相关部门对张鹏性骚扰事件以及学子们对反性骚扰机制的建议和提案进行回应，对方表示“不能接受采访，所有采访通过中大宣传部”。我于7月5日、6日的上午和下午的办公时间，分别打了4次电话致电中大宣传部，一直无人接听。 8陈翰元告诉我，最终触发他实名站出来的原因是，他看到张静的焦虑以及她对学术的失望。 “她以前对灵长类很有兴趣的，说起猴子的属性，观察它们群居生活，给它们一一命名，说起来她都是神飞色舞的。她去岛上田野，住得不好，吃得不好，蚊子多，被咬得一身包，她一句怨言都没有，都是兴致勃勃的。现在呢，因为张鹏，她都放弃了原本最感兴趣的研究。对学术的热爱一下子被打破了。张鹏这样性骚扰女生，是断了女孩子一条学术路，伤害女孩子平等受教育的权利。”陈翰元十分惋惜，“她那么聪明、勤奋，如果遇到的是个好导师，肯定会继续做科研的。” 陈静坦言，后来张鹏叫她参加暑假期间印尼苏门答腊的一个研究项目，她因不想再与张鹏接触，放弃了机会。 小柯原本也想尝试在同一个方向做科研，但课题结束后，完全失去了对灵长类的兴趣。她发现自己对张鹏有很强的心理抵触，“国内的灵长类研究领域里张鹏有一定的权威性，教授都这样了，还有什么意思呢？”多种原因下，她放弃了这个研究方向，甚至放弃了在中大的读研机会。 笑薇也告诉我，虽然张鹏的性骚扰不是导致她放弃灵长类研究的唯一原因，但也是主要原因之一。2017级的女孩子，在被张鹏性侵害后，同样选择了放弃。 “你说，他伤害了那么多的女学生，为什么还能继续留在中大？”陈静久久不能释怀，“为什么学校会认为‘老师对学生的捏捏抱抱、亲亲吻吻是小事一桩’呢？” 举报两个月了。中大纪委与女孩们一一座谈了，但对张鹏的处罚仍是未知之数。 女生们说，纪委调查期间也问话了张鹏，张鹏把一切都否认了。 我打电话采访张鹏，问他女孩们举报信上的内容是否属实？张鹏说了一句“你没有工作单位，我不认识你”，便挂掉了我的电话，不再回应。 张鹏仍如往常一样，在实验室里来来回回，若无其事。张鹏的妻子也走进了实验室，要求实验室学生们写一份“张鹏老师无不当行为”的证明，但遭到实验室学生的拒绝。 陈静和其他还留在学校的当事人担心事件再一次被压下去，她们害怕如果学校继续纵容，那以后张鹏必然会更加肆无忌惮伤害更多的人。 更让她们寒心的是，在一个课程群里，仍有老师把名为《你还敢报中山大学人类学的在职研究生吗》的帖子贴到群里，还公开发表了“不要过分纠结”、“有些社会对带色笑话能够容忍，可以舒缓工作压力”等言论。 举报者们好不容易积累起来的勇气和信心正在一点点流逝。 “难道真的需要用生命做祭品，像庆阳女孩一样，才能让他的行为看起来恶劣吗？我们该怎么办？” 陈静又焦虑起来，这一天她的噩梦里，张鹏拿着刀，准备杀了她。 （应受访者要求：陈静、笑薇、小柯为化名） 编辑：许智博 点击联系人间编辑","link":"/posts/2018/07/08/She-thought-she-can-survive-from-sexual-assualt/"},{"title":"听说你会 Python （2）：Python 高阶数据结构解析","text":"前言之前写过一篇《听说你会 Python ？》的文章，大家反响都还不错，那么我想干脆把这个文章做成一个系列，继续讲解一下 Python 当中那些不为人知的细节吧。然后之前在和师父川爷讨论面试的时候，川爷说了一句“要是我，我就考考你们怎么去实现一个 namedtuple ，好用，方便，又能区分人”，说者无心，听者有意，我于是决定在这次的文章中，和大家聊一聊 Python 中一个特殊的高阶数据结构， namedtuple 的实现。 Let’s beginnamedtuple介绍tuple 是 Python 中 build-in 的一种特殊的数据结构，它是一种 immutable 的数据集合，我们经常会这样使用它 123456789def test(): a = (1, 2) print(a) return aif __name__ == '__main__': b, c = test() print(a) Right，很多时候我们会直接使用 tuple 来进行一些数据的 packing/unpacking 的操作。OK，关于 tuple 的科普就到这里。那么什么是 namedtuple 呢，恩，前面不是说了 tuple 是一种特殊的数据集合么，那么 namedtuple 是其一个进阶（这不是废话么）。它将会基础的 tuple 抽象成一个类，我们将自行定义变量的名称和类的名称，这样我们可以很方便的将其复用并管理。具体的用法我们可以看看下面这个例子 12345if __name__ == '__main__': fuck=namedtuple(&quot;fuck&quot;, ['x', 'y']) a=fuck(1,2) print(a.x) print(a.y) 恩，这样看起来貌似更直观了点，但是，但是，但是，我猜你肯定想知道 namedtuple 是怎么实现的，那么我们先来看看代码吧 详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150_class_template = '''\\class {typename}(tuple): '{typename}({arg_list})' __slots__ = () _fields = {field_names!r} def __new__(_cls, {arg_list}): 'Create new instance of {typename}({arg_list})' return _tuple.__new__(_cls, ({arg_list})) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new {typename} object from a sequence or iterable' result = new(cls, iterable) if len(result) != {num_fields:d}: raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result)) return result def __repr__(self): 'Return a nicely formatted representation string' return '{typename}({repr_fmt})' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values' return OrderedDict(zip(self._fields, self)) def _replace(_self, **kwds): 'Return a new {typename} object replacing specified fields with new values' result = _self._make(map(kwds.pop, {field_names!r}, _self)) if kwds: raise ValueError('Got unexpected field names: %r' % kwds.keys()) return result def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) __dict__ = _property(_asdict) def __getstate__(self): 'Exclude the OrderedDict from pickling' pass{field_defs}'''_repr_template = '{name}=%r'_field_template = '''\\ {name} = _property(_itemgetter({index:d}), doc='Alias for field number {index:d}')'''def namedtuple(typename, field_names, verbose=False, rename=False): &quot;&quot;&quot;Returns a new subclass of tuple with named fields. &gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y']) &gt;&gt;&gt; Point.__doc__ # docstring for the new class 'Point(x, y)' &gt;&gt;&gt; p = Point(11, y=22) # instantiate with positional args or keywords &gt;&gt;&gt; p[0] + p[1] # indexable like a plain tuple 33 &gt;&gt;&gt; x, y = p # unpack like a regular tuple &gt;&gt;&gt; x, y (11, 22) &gt;&gt;&gt; p.x + p.y # fields also accessible by name 33 &gt;&gt;&gt; d = p._asdict() # convert to a dictionary &gt;&gt;&gt; d['x'] 11 &gt;&gt;&gt; Point(**d) # convert from a dictionary Point(x=11, y=22) &gt;&gt;&gt; p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22) &quot;&quot;&quot; # Validate the field names. At the user's option, either generate an error # message or automatically replace the field name with a valid name. if isinstance(field_names, basestring): field_names = field_names.replace(',', ' ').split() field_names = map(str, field_names) typename = str(typename) if rename: seen = set() for index, name in enumerate(field_names): if (not all(c.isalnum() or c=='_' for c in name) or _iskeyword(name) or not name or name[0].isdigit() or name.startswith('_') or name in seen): field_names[index] = '_%d' % index seen.add(name) for name in [typename] + field_names: if type(name) != str: raise TypeError('Type names and field names must be strings') if not all(c.isalnum() or c=='_' for c in name): raise ValueError('Type names and field names can only contain ' 'alphanumeric characters and underscores: %r' % name) if _iskeyword(name): raise ValueError('Type names and field names cannot be a ' 'keyword: %r' % name) if name[0].isdigit(): raise ValueError('Type names and field names cannot start with ' 'a number: %r' % name) seen = set() for name in field_names: if name.startswith('_') and not rename: raise ValueError('Field names cannot start with an underscore: ' '%r' % name) if name in seen: raise ValueError('Encountered duplicate field name: %r' % name) seen.add(name) # Fill-in the class template class_definition = _class_template.format( typename = typename, field_names = tuple(field_names), num_fields = len(field_names), arg_list = repr(tuple(field_names)).replace(&quot;'&quot;, &quot;&quot;)[1:-1], repr_fmt = ', '.join(_repr_template.format(name=name) for name in field_names), field_defs = '\\n'.join(_field_template.format(index=index, name=name) for index, name in enumerate(field_names)) ) if verbose: print class_definition # Execute the template string in a temporary namespace and support # tracing utilities by setting a value for frame.f_globals['__name__'] namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename, OrderedDict=OrderedDict, _property=property, _tuple=tuple) try: exec class_definition in namespace except SyntaxError as e: raise SyntaxError(e.message + ':\\n' + class_definition) result = namespace[typename] # For pickling to work, the __module__ variable needs to be set to the frame # where the named tuple is created. Bypass this step in environments where # sys._getframe is not defined (Jython for example) or sys._getframe is not # defined for arguments greater than 0 (IronPython). try: result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__') except (AttributeError, ValueError): pass return result 这，这，这，这特么什么玩意儿啊！没事,我们慢慢来看。首先，下面这一部分代码，将会校验我们传入的数据是否符合要求 1234567891011121314151617181920212223242526272829303132333435if isinstance(field_names, basestring): field_names = field_names.replace(',', ' ').split()field_names = map(str, field_names)typename = str(typename)if rename: seen = set() for index, name in enumerate(field_names): if (not all(c.isalnum() or c=='_' for c in name) or _iskeyword(name) or not name or name[0].isdigit() or name.startswith('_') or name in seen): field_names[index] = '_%d' % index seen.add(name)for name in [typename] + field_names: if type(name) != str: raise TypeError('Type names and field names must be strings') if not all(c.isalnum() or c=='_' for c in name): raise ValueError('Type names and field names can only contain ' 'alphanumeric characters and underscores: %r' % name) if _iskeyword(name): raise ValueError('Type names and field names cannot be a ' 'keyword: %r' % name) if name[0].isdigit(): raise ValueError('Type names and field names cannot start with ' 'a number: %r' % name)seen = set()for name in field_names: if name.startswith('_') and not rename: raise ValueError('Field names cannot start with an underscore: ' '%r' % name) if name in seen: raise ValueError('Encountered duplicate field name: %r' % name) seen.add(name) 接着，便是我们 namedtuple 的核心代码 12345678910111213141516171819202122class_definition = _class_template.format( typename = typename, field_names = tuple(field_names), num_fields = len(field_names), arg_list = repr(tuple(field_names)).replace(&quot;'&quot;, &quot;&quot;)[1:-1], repr_fmt = ', '.join(_repr_template.format(name=name) for name in field_names), field_defs = '\\n'.join(_field_template.format(index=index, name=name) for index, name in enumerate(field_names)))if verbose: print class_definition# Execute the template string in a temporary namespace and support# tracing utilities by setting a value for frame.f_globals['__name__']namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename, OrderedDict=OrderedDict, _property=property, _tuple=tuple)try: exec class_definition in namespaceexcept SyntaxError as e: raise SyntaxError(e.message + ':\\n' + class_definition)result = namespace[typename] 你是不是想说，what the fuck！我知道，class_definition 、 _repr_template 和 _field_template 是前面所定义的字符串模板 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253_class_template = '''\\class {typename}(tuple): '{typename}({arg_list})' __slots__ = () _fields = {field_names!r} def __new__(_cls, {arg_list}): 'Create new instance of {typename}({arg_list})' return _tuple.__new__(_cls, ({arg_list})) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new {typename} object from a sequence or iterable' result = new(cls, iterable) if len(result) != {num_fields:d}: raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result)) return result def __repr__(self): 'Return a nicely formatted representation string' return '{typename}({repr_fmt})' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values' return OrderedDict(zip(self._fields, self)) def _replace(_self, **kwds): 'Return a new {typename} object replacing specified fields with new values' result = _self._make(map(kwds.pop, {field_names!r}, _self)) if kwds: raise ValueError('Got unexpected field names: %r' % kwds.keys()) return result def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) __dict__ = _property(_asdict) def __getstate__(self): 'Exclude the OrderedDict from pickling' pass{field_defs}'''_repr_template = '{name}=%r'_field_template = '''\\ {name} = _property(_itemgetter({index:d}), doc='Alias for field number {index:d}')''' 但是其余的是什么鬼啊！别急，字符串模板我们先放在一边，我们先来看看后面的一段代码 1234567namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename, OrderedDict=OrderedDict, _property=property, _tuple=tuple)try: exec class_definition in namespaceexcept SyntaxError as e: raise SyntaxError(e.message + ':\\n' + class_definition)result = namespace[typename] 在这段代码中，首先 namespace 变量是一个字典，里面设置了一些变量的存在，紧接就是 exec class_definition in namespace 。众所周知，Python 是一门动态语言，在 Python 中，解释器允许我们在运行时，生成一些包含了符合 Python 语法语句的字符串，并用 exec 将其作为 Python 代码进行执行。同时在我们生成一些语句字符串的时候，我们可能会使用一些自定义的变量，于是，我们需要提供一个 dict 供其进行变量的查找。知道前面这些知识点后，exec class_definition in namespace 的作用是不是就很清楚了捏。好了，我们再回过头去看 class_definition 定义。不过我们直接看未格式化之前的模板未免的太过于枯燥和难懂了，我们干脆以前面举过的一个例子来看看格式化后的 class_definition 吧~ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class fuck(tuple): 'fuck(x, y)' __slots__ = () _fields = ('x', 'y') def __new__(_cls, x, y): 'Create new instance of fuck(x, y)' return _tuple.__new__(_cls, (x, y)) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new fuck object from a sequence or iterable' result = new(cls, iterable) if len(result) != 2: raise TypeError('Expected 2 arguments, got %d' % len(result)) return result def __repr__(self): 'Return a nicely formatted representation string' return 'fuck(x=%r, y=%r)' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values' return OrderedDict(zip(self._fields, self)) def _replace(_self, **kwds): 'Return a new fuck object replacing specified fields with new values' result = _self._make(map(kwds.pop, ('x', 'y'), _self)) if kwds: raise ValueError('Got unexpected field names: %r' % kwds.keys()) return result def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) __dict__ = _property(_asdict) def __getstate__(self): 'Exclude the OrderedDict from pickling' pass x = _property(_itemgetter(0), doc='Alias for field number 0') y = _property(_itemgetter(1), doc='Alias for field number 1') 好了，让我们一点点来分析，首先 class fuck(tuple) 指明我们创建的 fuck 类是继承自 tuple 。紧接着 __new__ 是 Python 对象系统中的一个特殊方法，用于我们的实例化的操作，其在 __init__ 之前便被触发，其是一个特殊的静态方法，我们可以将其用于实例缓存等特殊的功能。在这里，__new__ 将会返回一个 tuple 的实例。接下来的是是一些特殊的私有方法，代码很好懂，我们就不细讲了，接着我们来看看这样一段代码 123x = _property(_itemgetter(0), doc='Alias for field number 0')y = _property(_itemgetter(1), doc='Alias for field number 1') 你可能还不知道这两段代码用来是干什么的233，没事儿，我们慢慢来。还记得前面我们举过的一个例子么 12345if __name__ == '__main__': fuck=namedtuple(&quot;fuck&quot;, ['x', 'y']) a=fuck(1,2) print(a.x) print(a.y) 你可能会突发奇想，要是我们执行 a.x=1 这样的操作会怎样呢？OK，你会发现，Python 会抛出一个异常叫做 AttributeError: can't set attribute ，嗯哼，讲到这里，你可能就知道前面提到的包含 property 的两行代码作用就是保证 namedtuple 的 immutable 的特性。那么你可能还是不知道这是为什么。这和 Python 增加的描述符机制有关 扩展（1）：Python 中的描述符首先我们要明确一点，描述符指的是实现了描述符协议的特殊的类，三个描述符协议指的是 __get__ , ‘set‘ , __delete__ 以及 Python 3.6 中新增的 __set_name__ 方法，其中实现了 __get__ 以及 __set__ / __delete__ / __set_name__ 的是 Data descriptors ，而只实现了 __get__ 的是 Non-Data descriptor 。那么有什么区别呢，前面说了， 我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。 但是，这里没有考虑描述符的因素进去，如果将描述符因素考虑进去，那么正确的表述应该是我们如果调用一个属性，那么其顺序是优先从实例的 __dict__ 里查找，然后如果没有查找到的话，那么一次查询类字典，父类字典，直到彻底查不到为止。其中如果在类实例字典中的该属性是一个 Data descriptors ，那么无论实例字典中存在该属性与否，无条件走描述符协议进行调用，在类实例字典中的该属性是一个 Non-Data descriptors ，那么优先调用实例字典中的属性值而不触发描述符协议，如果实例字典中不存在该属性值，那么触发 Non-Data descriptor 的描述符协议。 可能这讲完了，你还是不清楚和前面问题有什么关联，没事儿，我们接下来会讲讲 property 的实现 扩展（2）：Property 详解首先我们来看看关于 Property 的实现 1234567891011121314151617181920212223242526272829303132333435class Property(object): &quot;Emulate PyProperty_Type() in Objects/descrobject.c&quot; def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(&quot;unreadable attribute&quot;) return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError(&quot;can't set attribute&quot;) self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(&quot;can't delete attribute&quot;) self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__) 当我们执行完这两句语句时 123x = _property(_itemgetter(0), doc='Alias for field number 0')y = _property(_itemgetter(1), doc='Alias for field number 1') 我们的 x 和 y 就变成了一个 property 对象的实例，它们也是一个描述符，还记得我们前面讲的么，当一个变量/成员成为一个描述符后，它将改变正常的调用逻辑，现在当我们 a.x=1 的时候，因为我们的x是一个 Data descriptors ，那么不管我们的实例字典中是否有 x 的存在，我们都会触发其 __set__ 方法，由于在我们初始化 x 和 y 两个变量时，没有给予其传入 fset 的方法，因此，我们 __set__ 方法在运行过程中将会抛出 AttributeError(&quot;can't set attribute&quot;) 的异常，这也保证了 namedtuple 遵循了 tuple 的 immutable 的特性！是不是很优美！Amazing！ 吐槽向其实很多人不知道我为什么选择 namedtuple 来作为本期的主题，其实很简单呀，namedtuple 中预定义模板，格式化，然后用 exec 函数进行执行这一套方法，是目前 Python 中主流模板引擎的核心原理。某种意义上讲，你在吃透这一点后，你也掌握了怎样去实现一个简易模板引擎的方法，如果大家有兴趣，我们可以下次一起来写一个简单的模板引擎。还有就是在 namedtuple 对于 Python 中的一些高阶特性使用的简直优美无比，这也是我们学习的好例子。 最后的最后，作为另一个写的非常优美的例子，我将 orderdict 的代码贴出来，大家可以下来看看，然后评论区我们讨论一个！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207class OrderedDict(dict): 'Dictionary that remembers insertion order' # An inherited dict maps keys to values. # The inherited dict provides __getitem__, __len__, __contains__, and get. # The remaining methods are order-aware. # Big-O running times for all methods are the same as regular dictionaries. # The internal self.__map dict maps keys to links in a doubly linked list. # The circular doubly linked list starts and ends with a sentinel element. # The sentinel element never gets deleted (this simplifies the algorithm). # Each link is stored as a list of length three: [PREV, NEXT, KEY]. def __init__(*args, **kwds): '''Initialize an ordered dictionary. The signature is the same as regular dictionaries, but keyword arguments are not recommended because their insertion order is arbitrary. ''' if not args: raise TypeError(&quot;descriptor '__init__' of 'OrderedDict' object &quot; &quot;needs an argument&quot;) self = args[0] args = args[1:] if len(args) &gt; 1: raise TypeError('expected at most 1 arguments, got %d' % len(args)) try: self.__root except AttributeError: self.__root = root = [] # sentinel node root[:] = [root, root, None] self.__map = {} self.__update(*args, **kwds) def __setitem__(self, key, value, dict_setitem=dict.__setitem__): 'od.__setitem__(i, y) &lt;==&gt; od[i]=y' # Setting a new item creates a new link at the end of the linked list, # and the inherited dictionary is updated with the new key/value pair. if key not in self: root = self.__root last = root[0] last[1] = root[0] = self.__map[key] = [last, root, key] return dict_setitem(self, key, value) def __delitem__(self, key, dict_delitem=dict.__delitem__): 'od.__delitem__(y) &lt;==&gt; del od[y]' # Deleting an existing item uses self.__map to find the link which gets # removed by updating the links in the predecessor and successor nodes. dict_delitem(self, key) link_prev, link_next, _ = self.__map.pop(key) link_prev[1] = link_next # update link_prev[NEXT] link_next[0] = link_prev # update link_next[PREV] def __iter__(self): 'od.__iter__() &lt;==&gt; iter(od)' # Traverse the linked list in order. root = self.__root curr = root[1] # start at the first node while curr is not root: yield curr[2] # yield the curr[KEY] curr = curr[1] # move to next node def __reversed__(self): 'od.__reversed__() &lt;==&gt; reversed(od)' # Traverse the linked list in reverse order. root = self.__root curr = root[0] # start at the last node while curr is not root: yield curr[2] # yield the curr[KEY] curr = curr[0] # move to previous node def clear(self): 'od.clear() -&gt; None. Remove all items from od.' root = self.__root root[:] = [root, root, None] self.__map.clear() dict.clear(self) # -- the following methods do not depend on the internal structure -- def keys(self): 'od.keys() -&gt; list of keys in od' return list(self) def values(self): 'od.values() -&gt; list of values in od' return [self[key] for key in self] def items(self): 'od.items() -&gt; list of (key, value) pairs in od' return [(key, self[key]) for key in self] def iterkeys(self): 'od.iterkeys() -&gt; an iterator over the keys in od' return iter(self) def itervalues(self): 'od.itervalues -&gt; an iterator over the values in od' for k in self: yield self[k] def iteritems(self): 'od.iteritems -&gt; an iterator over the (key, value) pairs in od' for k in self: yield (k, self[k]) update = MutableMapping.update __update = update # let subclasses override update without breaking __init__ __marker = object() def pop(self, key, default=__marker): '''od.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. ''' if key in self: result = self[key] del self[key] return result if default is self.__marker: raise KeyError(key) return default def setdefault(self, key, default=None): 'od.setdefault(k[,d]) -&gt; od.get(k,d), also set od[k]=d if k not in od' if key in self: return self[key] self[key] = default return default def popitem(self, last=True): '''od.popitem() -&gt; (k, v), return and remove a (key, value) pair. Pairs are returned in LIFO order if last is true or FIFO order if false. ''' if not self: raise KeyError('dictionary is empty') key = next(reversed(self) if last else iter(self)) value = self.pop(key) return key, value def __repr__(self, _repr_running={}): 'od.__repr__() &lt;==&gt; repr(od)' call_key = id(self), _get_ident() if call_key in _repr_running: return '...' _repr_running[call_key] = 1 try: if not self: return '%s()' % (self.__class__.__name__,) return '%s(%r)' % (self.__class__.__name__, self.items()) finally: del _repr_running[call_key] def __reduce__(self): 'Return state information for pickling' items = [[k, self[k]] for k in self] inst_dict = vars(self).copy() for k in vars(OrderedDict()): inst_dict.pop(k, None) if inst_dict: return (self.__class__, (items,), inst_dict) return self.__class__, (items,) def copy(self): 'od.copy() -&gt; a shallow copy of od' return self.__class__(self) @classmethod def fromkeys(cls, iterable, value=None): '''OD.fromkeys(S[, v]) -&gt; New ordered dictionary with keys from S. If not specified, the value defaults to None. ''' self = cls() for key in iterable: self[key] = value return self def __eq__(self, other): '''od.__eq__(y) &lt;==&gt; od==y. Comparison to another OD is order-sensitive while comparison to a regular mapping is order-insensitive. ''' if isinstance(other, OrderedDict): return dict.__eq__(self, other) and all(_imap(_eq, self, other)) return dict.__eq__(self, other) def __ne__(self, other): 'od.__ne__(y) &lt;==&gt; od!=y' return not self == other # -- the following methods support python 3.x style dictionary views -- def viewkeys(self): &quot;od.viewkeys() -&gt; a set-like object providing a view on od's keys&quot; return KeysView(self) def viewvalues(self): &quot;od.viewvalues() -&gt; an object providing a view on od's values&quot; return ValuesView(self) def viewitems(self): &quot;od.viewitems() -&gt; a set-like object providing a view on od's items&quot; return ItemsView(self) 参考目录 Descriptor HowTo Guide Python 描述符入门指北 collections","link":"/posts/2016/12/28/Someone-tell-me-that-you-think-Python-is-simple-2/"},{"title":"简单聊聊 Maglev ，来自 Google 的软负载均衡实践","text":"好久没博客了，来写个简单的读论文笔记吧，这篇文章是来自 Google 2016 年发表的一篇论文 Maglev: A Fast and Reliable Software Network Load Balancer 分享了他们内部从08年开始大规模使用的软负载均衡系统的实现。里面很多很有趣的细节，我看我能写多少，算多少吧 背景负载均衡的概念大家肯定都比较熟悉了，再次不再赘述。现在我们需要考虑 Google 的场景。设计之初，Google 需要一种高性能的 LB 来承担 Google 一些重头服务的流量，比如 Google 搜索，Gmail 等等。由于流量非常庞大，那么 LB 需要非常强大的性能来处理大量的流量。 在这里，传统的想法可能说，我直接上专业的硬件负载均衡，能用钱解决的问题，都不算事（笑。但是这样的方案有着不小的问题 硬件负载均衡单点的性能决定了整个网络能承担的请求 在 HA 上存在缺陷。为了保证单点失效的时候，整个网络集群不陷入瘫痪。那么我们通常需要 1:1 的做冗余 灵活性和编程性欠缺，想做骚操作的时候没有切入点 太贵了。贵到 Google 都承受不了（逃 在这样一种情况下，Google 开始考虑自行构建一种 SLB (Software Load Balance) 系统。去构建这样一种系统。好处也很明显。比如方便的 Scale ，为了保证 HA 所需的冗余从之前的 1:1 可以降至 N+1 ，方便的定制性等。架构就演变成下图了 但是挑战也很明显。首先需要有足够的性能，这样保证集群有足够的吞吐。同时需要做 connection tracking ，这样保证同一个连接的数据包能妥投到同一个机器上。也许要保证能有透明的 failover 的能力。 这样一些要件结合起来，这也就是我们今天要聊的 Maglev。Google 从 08 年开始大规模的应用的 LB 系统 Maglev 初窥背景知识在继续聊 Maglev 之前，我们需要去了解 Google 现在怎么样去去使用 Maglev 的，下面是一个简化后的示意图 同时这里我们需要介绍一个很重要的概念叫做 VIP(Virtual IP Address) 。 用过 Kubernetes 的同学肯定对这个概念并不陌生。VIP 并不是一个实际与网卡绑定的物理 IP。近似来讲它可以作为后端一组 Endpoint 的抽象，当你访问这个 VIP 的时候，实际上是在访问后端的 Endpoint 。这里举个更方便理解的例子，以 Kubernetes 为例，我们在创建完一组 Pod 后，为了暴露 Pod 中提供的服务，我们通常会创建一个 Service 来关联对应的 Pod。Service 通常会有一个 IP，那么这个 IP 就是一个 VIP 。当我们访问 Service 的 IP 的时候，通常会随机从后面的 Pod 中选择一个承接请求。 好了，回到 Maglev ，我们现在来看下整个的一个流程。Maglev 会和 VIP 关联，然后将 VIP 透传给一组 Router。 当用户在浏览器中输入 https://www.google.com 并按下回车的时候，浏览器会进行 DNS 解析。而 DNS 解析将由 Google 的 DNS 服务器进行处理。DNS 服务器会根据用户的区域选择一个最近集群的 VIP 返回给用户，然后浏览器会根据获取到的 VIP 建立连接。 当 Router 收到对应包时，会将包转发给 VIP 所属的 Maglev 集群中的任意节点。集群中的每个节点权重都是平衡。Maglev 节点在接受到包的时候，会利用 GRE(Generic Routing Encapsulation) 进行封包。然后传输给对应的后端端点。 当后端端点接收到数据包的时候，会进行接包并处理请求。当响应数据准备就绪的时候，会进行封包操作，会将 VIP 的作为源地址，用户的 IP 作为目标地址，然后响应数据作为数据包操作。这个时候，后端端点会利用 DSR(Direct Server Return) 将数据包绕过 Maglev 直接返回。这样避免响应过大的时候对 Maglev 造成额外的负担。实际上 DSR 在 L4 的 LB 实现，如 HAProxy，Envoy 等都得到了比较多的应用。改天有时间写篇博客来聊聊。 Maglev 配置如前面所说， Maglev 接收来自 Router 的 VIP 请求，然后将对应流量转发到对应的后端端点上。每个 Maglev 将由 Controller 和 Forwarder 组成，其架构如下所示 而 Controller 和 Forwarder 都利用 Configuration Object 管理相关 VIP。Configuration Object 这一套实际上又是另外一套系统（可以近似的认为是注册中心），彼此之间通过 RPC 来通信。 在 Maglev 机器上，Controller 会定期对 Forwarder 进行检查。根据检查结果来确定是否通过 BGP 提交/撤回所有 VIP 的注册（要么全部成功，要么全部失败，其实还是为了保障系统的一致性）。这样确保从 Router 过来的流量都能扔到健康的机器上 而从 Router 过来的 VIP 流量将会由 Forwarder 进行处理。在 Forwarder 中，每个 VIP 都会和一个或多个 backend pool 关联。除非特殊处理，Maglev 中的 backend 都是服务端点。一个 backend pool 可以包含一组服务端点的的物理 IP ，也可以是其余的 backend pool。每个 backend pool 都会根据其特定需求，设计若干个监控检查器，数据包只会转发给健康的服务。如之前所说，同一个服务可能会被包含在多个 backend pool 中，因此 Forwarder 将会根据具体的地址进行去重，避免额外的开销。 Forwarder 的 Config Manager 将负责从 Configuration Object 中拉取，解析并验证相关的配置。所有配置的提交都是具备原子性（要么全部成功，要么全部失败）。在推送和解析到生效的过程中，存在一个非常短暂的 gap，在此期间，一个 Maglev 集群之间的配置可能存在不同步的情况。不过因为一致性 Hash 的存在，在这个非常短的 Gap 内，大部分请求还是能成功妥投。 Maglev 实现好了，扯了这么多，来看一下 Maglev 整个系统的一些实践细节 概述总所周知（如前面所说），Maglev 由 Forwarder 来实际承担流量相关的转发工作，我们用一张图来说明一下它的结构 Forwarder 将直接从 NIC(Network Interface Card) 拿到数据包，然后直接扔入 NIC 转发到后端。期间所有操作都不会过内核（实际上过内核会有额外的 cost） 从 NIC 中捞出的包，会先由 Steering Module 进行处理，在处理过程中，Steering Module 将会根据五元组（协议，目标地址，目标端口，源地址，源端口）进行 hash 计算。然后将其转入对应的 Receiving Queue 中。每个 Receiving Queue 都会对应一个处理线程。处理线程将过滤掉目标 VIP 和本机注册 VIP 不匹配的包。然后会重新计算五元组 hash，然后从 Connection Tracking Table 中查找对应的值。 在 Connection Tracking Table 中存放之前五元组 Hash 所对应的 Backend，然后如果查找命中，那么直接复用，如果未命中，则为这个包选择一个新的 Backend, 然后将键值对加入 Connection Tracking Table。如果此时没有 Backend 可用，那么这个包会被丢弃。当这个包完成查找操作后，如前面所说，会改写这个包，然后将其放入 transmission queue 中去。最后将 muxing module 会将 transmission queue 的包直接通过 NIC 发送出去。 这里有个问题，在 Steering Module 中为啥不考虑根据 round-robin 这种常见的策略来做？大家都知道每个线程的处理速度是不一致的，如果直接裸 round-robin ，那么面对这种情况，可能会导致数据包重排的情况发生，如果是引入权重的概念来改良，又会引入新的复杂度，毕竟线程的处理速度是动态变化的。另外一种是 connnection tracking 的情况，假设我们有个需要持久化的连接，我们需要保证每个包都能扔到同样的机器上，这个时候用 round-robin 就会引入额外的复杂性。不过对于一些特殊情况，比如 receive queue 满了，一致性 Hash 处理不过来的时候，我们会利用 round-robin 作为 backup 的手段来替代一致性 Hash，这种情况对于同时存在同样5元组包的时候比较好用。 高效处理数据包前面已经花了很多时间讲述了，Maglev 是直接对 TCP 的数据包进行操作，同时因为 Google 的流量极为庞大，那么这个时候实际上是需要 Maglev 有着良好的转发性能。不然在大规模场景下，其吞吐能力会无法满足需求。Google 怎么做的？答：直接对网卡操作。。 我们都知道在 Linux 中进行网络编程的时候，将数据包从内核态拷贝到用户态实际上是一件开销非常大的事，所以对于一些极端需求性能的场景，如 L4 的负载均衡等，大家可能更倾向于将东西做到内核里，避免跨态拷贝。这也是 LVS 等工具的思路。但是实际上对于更大规模的流量，来讲，从网卡到内核，经过内核中的一堆 filter 处理也是一件开销非常大的事，而如同前面所说，Maglev 只依赖数据包中的五元组，对于包序列号，包 payload ，都不需要关心。于是 Google：我有一个大胆的想法！好了，来看张图 Google 选择直接在 NIC (即网卡) 上进行编程。让 Forwarder 和 NIC 共享一片内存。内存中维护的是一个环状的数据包池子。然后 Forwarder 中的 steering module 和 muxing module 各自维护三个指针来处理这些数据包，下面详细描述一下 首先而言 steering module 维护了三个指针 received ，管理接收数据包 reserved, 管理已接收未处理的数据包 processed, 管理处理完成的数据包 那么流程是这样的，当 NIC 接受到新的数据包后，那么 received 指针指向的内存会被修改。然后当一个数据包被分发给线程完成相关操作后，那么 processed 指针指向的内存地址会被修改。因为是个环状结构嘛， received 和 processed 中间存在的数据包就是已接收但未完成处理的包，由 reserved 指针进行管理。 于此对应的，muxing module 也维护了三个指针 sent，管理已发送完毕的数据包 ready，管理已经就绪等待发送的数据包 recycled, 管理已回收的数据包 那么对应的流程是这样的，当 steering module 完成相关包的处理的时候，ready 指针指向的内存会被修改，然后等待发送。当一个数据包发送后，sent 指向的内存地址被修改。在 ready 和 sent 之外有另一个状态 recycled 管理已经回收的数据包。 我们可以看到在这个过程中，没有发生数据拷贝的操作，实际上这减小了一部分复制数据带来的时延。不过这种方法存在的问题就是，当指针越界后，会带来很大的额外开销。所以 Google 采用的做法是批处理，比如接收 3000 个小包集中处理一次，这样的骚操作 另外需要做一些额外的优化，比如包处理线程之间不共享数据以避免竞态。比如需要将线程与具体 CPU Core 绑定来保证性能等等 目前来看，Google 这一套的做法效率非常的出色，平均每个包的处理只需要 300 ns($10^{-9}$s)。如同前面所说，Google 采用批处理的方式来处理包，这样的问题是每当一些例如硬件中断的情况发生的时候，可能到达处理阈值的时间会比大部分情况长很多，所以 Google 设计了一个 50μs($10^{-6}$s) 的 Timer 来处理这种情况。换句话说，当因为硬件或者其余问题时，整体的包处理时长可能会增加 50μs 的时间（其实这里感觉 Google 怎么是在得瑟，你看我们性能超棒的噢，只有硬件是我们的瓶颈喔（逃 后端选择如同前面所说的一样，Forwarder 会为数据包选择一个后端。对于 TCP 这种常见来说，将相同五元组的数据包转发到同一个后端节点上非常重要。Google 采取在 Maglev 中维护一个 connction tracking table 来解决这个问题。当一个包抵达的时候，Maglev 会计算其五元组 Hash ，然后确定在 table 中是否存在，如果不存在，则选择一个节点作为后端，然后将记录值添加到 table 中。如果存在则直接复用 这样看起来没有问题了对吧？Google：不，不是，还有问题！ 我们首先考虑这样一种场景：如前面所说，Maglev 前面挂了一个/组 Router，而 Router 是不提供连接亲和的，即不保证把同一个连接的包发送到同一个机器上。所以可能存在的情况是同一个连接的不同数据包会被仍在不同的机器上。再比如，我们假设 Router 是具有连接亲和的，但是也会存在如果机器发生重启后，connection tracking table 被清空的情况。 再来一个例子，我们都知道 connection tracking table 它所能使用的内存，必定是有一个阈值的。这样在面对一些流量非常大，或者 SYN Flood 这种非正常情景的时候。当 connection tracking table 的容量到达阈值的时候，我们势必会清理一些数据。那么在这个时候，一个连接的 tracking 信息就很有可能被清理。那么在这种情况下，我们怎么样去做 connection tracking ？ Google 选择的做法是引入一致性 Hash 一致性 Hash：Maglev Hash整体算法其实有很多细节，这里只说明大概，具体细节大家可以去阅读原文查找 首先，我们要确定经过预处理后的产物 lookup table 的长度 M。所有 Key 都会被 hash 到这个 lookup table 中去，而 lookup table 中的每个元素都会被映射到一个 Node 上 而计算 lookup table 的计算分为两步 计算每一个 node 对于每一个 lookup table 项的一个取值（也就是原文中提到的 permutation）； 根据这个值，去计算每一个 lookup table 项所映射到的 node（放在 entry 中，此处 entry 用原文的话来讲就是叫做 the final lookup table）。 permutation 是一个 M×N 的矩阵，列对应 lookup table，行对应 node。 为了计算 permutation，需要挑选两个 hash 算法，分别计算两个值 offset 与 skip 。最后根据 offset 和 skip 的值来填充 permutation，计算方式描述如下： offset ← h 1 (name[i]) mod M skip ← h 2 (name[i]) mod (M − 1)+ 1 permutation[i][j] ← (offset+ j × skip) mod M 其中 i 是 Node Table 中 Node 的下标，j 是 lookup table 下标 在计算完 permutation 后，我们就可以计算最后的 lookup table 了，这个 table 用一维的数组表示 这里贴一张图，大家可以配合下面的代码一起看一下 123456789101112131415161718192021222324252627from typing import List# 根据已经计算好的 permutation 来计算 lookup_tabledef calculate_lookup_table(n: int, m: int, permutation: List[List[int]]) -&gt; List[int]: # result 是最终记录分布的 Hash 表 result: List[int] = [-1] * m # next 是用来解决冲突的，在遍历过程中突然想要填入的 entry 表已经被占用， # 则通过 next 找到下一行。一直进行该过程直到找到一个空位。 # 因为每一列都包含有 0~M-1 的每一个值，所以最终肯定能遍历完每一行。 # 计算复杂度为 O(M logM) ~ O(M^2) next: List[int] = [0] * n flag = 0 while True: for i in range(n): x = permutation[i][next[i]] while True: # 找到空位，退出查找 if result[x] == -1: break next[i] += 1 x = permutation[i][next[i]] result[x] = i next[i] += 1 flag += 1 # 表已经填满，退出计算 if flag == m: return result 在这里我们能看到，这段循环代码必然结束，而最坏情况下，复杂度会非常高，最坏的情况可能会到 O(M^2)。原文中建议找一个远大于 N 的 M （To avoid this happening we always choose M such that M ≫ N.）可以使平均复杂度维持在 O(MlogM) 而 Maglev 中 Google 自研的一致性算法性能怎么样呢？论文中也做了测试 可以看到，对于不同大小的 lookup table，Maglev 表现出了更好的均衡性 说实话，Maglev 在我看来本质上是一个带虚节点的 Hash，说实话，我没想到为什么 Google 不用 Dynamo 等已经比较成熟的 Hash ？难道是因为政策原因？（毕竟 Dynamo 是 AWS 家的嘛（逃。BTW Enovy 也实现了 Maglev 。参见 Evaluate other consistent hash LB algorithms ，而且引入了权重，实现的挺不错，有兴趣的同学可以去看看（逃 说实话，Maglev Hash 还有很多细节没有讲，不过实在懒得写了，，等后面出一个一致性 Hash 的分析博客吧，Flag++ Maglev 优化前面我们已经把 Maglev 这一套的基本原理讲的差不多了。但是如果作为一个生产上大规模使用的 LB ，那么势必还需要针对细节做很多优化，由于这里涉及到很多方面，我这里只简单介绍一下，剩下的还是建议大家直接去读原文 分段数据包的处理熟悉网络的同学都知道，在基于 IP 协议传输报文的时候，受限于 MTU 的大小，在传输的时候，可能会存在数据分片传输的情况，而这些分片后的数据不一定会带有完整的五元组信息。比如一个数据被切分为两段，那么第一段将带有 L3 和 L4 的头部信息，而第二段只带有 L3 的信息。而在传输过程中，因为网络关系，Maglev 无法完全保证对接收到的数据作出正确的处理 这样问题就大了，因为数据分段的情况实际上是非常场景的。那么对于这样的场景，Maglev 应该怎么样去处理？首先我们需要确定怎么样才能保证所有数据都能妥投 保证一个数据报文的不同数据段都需要由同一个 Maglev 实例处理 对于同一个数据报文的不同数据段需要能保证后端选择结果是一致的 OK，那么我们来看看 Google 是怎么解决这个问题的。 首先，每个 Maglev 实例中都会有一个特殊的 backend pool ，池子中是该 Maglev 集群中所有的实例。当接收到数据后，Maglev 会先根据三元组（源地址，目标地址，协议簇）计算 hash ，然后选择一个 Maglev 实例进行转发，这样就能保证同一数据报文的不同分段能传输到同一个 Maglev 实例上。当然这里需要利用 GRE 的递归控制来避免无限循环。 好了我们来看看条件2怎么满足。在每个 Maglev 实例上会维护一个特殊的表，记录数据分片后第一个数据端的转发结果。以前面的例子为例，当一个报文的第二个分段抵达的时候，Maglev 会查询表中是否存在第一个数据段的转发结果。如果存在则直接转发，如果不存在，则将这个数据段缓存，直到第一个数据段抵达，或者到达超时阈值 监控与调试真正的用时都是不需要调试（划掉）（笑，Google 为了这一套系统设计了辅助的监控与调试手段来帮助日常的开发迭代。 在监控这边，分为黑盒和白盒两种监控手段。比如遍布全球的特定监控节点，以确认 VIP 的健康状态。当然与之配套的还有一整套白盒监控。Google 会监控具体的服务器指标，同时会监控 Maglev 本身的指标 当然与之配套的还有一些调试工具。比如 Google 开发了一套类似 X-Trace 的 packettracer。可以通过 packettracer 来发送一些带有特定标头和 payload 的信息。当 Maglev 接到这样一些特殊的数据包后，除了照常转发数据包以外，也会讲一些关键信息上报到指定位置 这其实也体现了软负载均衡相较于硬件负载均衡的一个好处，无论可调试性还是可迭代性都是硬件负载均衡无法媲美的 总结这篇文章其实我读了挺久，里面很多细节挺值得慢慢深究的，所以再次建议大家一定要去找原文读一下，非常不错。另外顺便推荐一篇文章，是美团技术团队的作品，他们也参考了 Maglev 来实现自己的高性能 L4 负载均衡，参见MGW——美团点评高性能四层负载均衡 好了，这篇文章，就先到这里吧，这篇文章应该是我写的最耗时的一篇文章了。。不过想想后面还有几篇文章要写，头就很大 溜了溜了","link":"/posts/2020/05/23/a-simple-introduction-about-maglev/"},{"title":"简单聊聊容器中的一号进程","text":"新年了，决定趁着有时间的时候多写几篇技术水文。今天的话，准备来简单聊聊容器中我们每天都会接触，但是时常又会被我们忽略的一号进程 正文容器技术发展到现在，其实形态上已经发生了很大的变化。根据不同的场景，既有传统的 Docker1, containterd2 这样传统基于 CGroup + Namespace 的容器形态，也有像 Kata3 这样基于 VM 的新型的容器形态。本文主要着眼在传统容器中一号进程上。 我们都知道，传统容器依赖的 CGroup + Namespace 进行资源隔离，本质上来说，还是 OS 内的一个进程。所以在继续往下聊容器相关的内容之前，我们需要先来简单聊聊 Linux 中的进程管理 Linux 中的进程管理简单聊聊进程Linux 中的进程实际上是个非常大的话题，如果要展开聊，实际上这个话题可以聊一整本书= =，\b所以为了时间着想，我们还是把目光聚集在最核心的一部分上面（实际上是因为很多东西我也不懂。 首先来讲，在内核中利用一个特殊的结构体来维护进程有关的相关信息，比如常见的 PID，进程状态，打开的文件描述符等信息。在内核代码中，这个结构体是 task_struct4, 其大概结构大家可以看一下下图 而通常而言，我们会在系统上跑很多个进程。所以内核用一个进程表(实际上 Linux 中管理进程表的有多个数据结构，这里我们用 PID Hash Map 来举例）来维护所有 Process Descriptor 相关的信息，详见下图 OK， 这里我们大概了解了进程中的基本结构，现在我们来看我们常见使用进程的一个场景：父子进程。我们都知道，我们有时会在一个进程中，通过 fork5 这个 sys call 来创建出一个新的进程。通常来说，我们创建的新的进程是当前进程的子进程。那么在内核中怎么表达这种父子关系呢？ 回到刚刚提到 task_struct, 在这个结构体中存在这样几个字段来描述父子关系 real_parent：一个 task_struct 指针，指向父进程 parent: 一个 task_struct 指针，指向父进程。在大多数情况下，这个字段的值和 real_parent 一致。在有进程对当前进程使用 ptrace6 等情况的时候，和 real_parent 字段不一致 children：list_head, 其指向一个由当前进程所创建的所有子进程的双向链表 这里大家可能还有点抽象的话，给大家一个图就能看清楚了 实际上，我们发现，不同进程之间的父子关系，反应到具体的数据结构之上，就形成了一个完整的树形结构（先记住这点，我们稍后会再提到这里） 到现在为止，我们已经对 Linux 中的进程，有了最简单一个概念，那么接下来，我们会聊聊我们在进程使用中常遇到的两个问题：孤儿进程&amp;&amp;僵尸进程 孤儿进程 &amp;&amp; 僵尸进程首先来聊聊 僵尸进程 这个概念。 如前面所说，我们内核有进程表来维护 Process Descriptor 相关信息。那么在 Linux 的设计中，当一个子进程退出后，将保存自己的进程相关的状态以供父进程使用。而父进程将调用 waitpid7 来获取子进程状态，并清理相关资源。 那么如上所说，父进程是有可能需要拿到子进程相关的状态的。那么也就导致为了满足这一设计，内核中的进程表将一直保存相关资源。当僵尸进程多了以后，那么将造成很大的资源浪费。 首先来看一个简单的僵尸进程的例子 1234567891011121314#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main() { int pid; if ((pid = fork()) == 0) { printf(&quot;Here's child process\\n&quot;); } else { printf(&quot;the child process pid is %d\\n&quot;, pid); sleep(20); } return 0;} 然后我们编译执行这段代码，然后配合 ps 命令查看一下，发现我们的确造了一个 z 进程 OK 我们再来看一个正确处理子进程退出的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/signalfd.h&gt;#include &lt;sys/wait.h&gt;#define MAXEVENTS 64void deletejob(pid_t pid) { printf(&quot;delete task %d\\n&quot;, pid); }void addjob(pid_t pid) { printf(&quot;add task %d\\n&quot;, pid); }int main(int argc, char **argv) { int pid; struct epoll_event event; struct epoll_event *events; sigset_t mask; sigemptyset(&amp;mask); sigaddset(&amp;mask, SIGCHLD); if (sigprocmask(SIG_SETMASK, &amp;mask, NULL) &lt; 0) { perror(&quot;sigprocmask&quot;); return 1; } int sfd = signalfd(-1, &amp;mask, 0); int epoll_fd = epoll_create(MAXEVENTS); event.events = EPOLLIN | EPOLLEXCLUSIVE | EPOLLET; event.data.fd = sfd; int s = epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) { abort(); } events = calloc(MAXEVENTS, sizeof(event)); while (1) { int n = epoll_wait(epoll_fd, events, MAXEVENTS, 1); if (n == -1) { if (errno == EINTR) { fprintf(stderr, &quot;epoll EINTR error\\n&quot;); } else if (errno == EINVAL) { fprintf(stderr, &quot;epoll EINVAL error\\n&quot;); } else if (errno == EFAULT) { fprintf(stderr, &quot;epoll EFAULT error\\n&quot;); exit(-1); } else if (errno == EBADF) { fprintf(stderr, &quot;epoll EBADF error\\n&quot;); exit(-1); } } printf(&quot;%d\\n&quot;, n); for (int i = 0; i &lt; n; i++) { if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) { printf(&quot;%d\\n&quot;, i); fprintf(stderr, &quot;epoll err\\n&quot;); close(events[i].data.fd); continue; } else if (sfd == events[i].data.fd) { struct signalfd_siginfo si; ssize_t res = read(sfd, &amp;si, sizeof(si)); if (res &lt; 0) { fprintf(stderr, &quot;read error\\n&quot;); continue; } if (res != sizeof(si)) { fprintf(stderr, &quot;Something wrong\\n&quot;); continue; } if (si.ssi_signo == SIGCHLD) { printf(&quot;Got SIGCHLD\\n&quot;); int child_pid = waitpid(-1, NULL, 0); deletejob(child_pid); } } } if ((pid = fork()) == 0) { execve(&quot;/bin/date&quot;, argv, NULL); } addjob(pid); }} OK, 我们现在都知道了，子进程退出后需要由父进程正确的回收相关的资源。那么问题来了，我们父进程先于子进程退出了怎么办。实际上这是一个很常见的场景。比如说大家去用两次 fork 实现守护进程。 我们常规的认知来说，我们父进程退出后，这个进程所属的所有子进程会进行 re-parent 到当前 PID Namespace 的一号进程上，那么这样的答案是正确的么？对，也不对，我们首先来看一个例子 1234567891011121314151617181920212223#include &lt;stdio.h&gt;#include &lt;sys/prctl.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main() { int pid; int err = prctl(PR_SET_CHILD_SUBREAPER, 1); if (err != 0) { return 0; } if ((pid = fork()) == 0) { if ((pid = fork()) == 0) { printf(&quot;Here's child process1\\n&quot;); sleep(20); } else { printf(&quot;the child process pid is %d\\n&quot;, pid); } } else { sleep(40); } return 0;} 这是一个很典型的两次 fork 创建守护进程的代码（除了我没写 SIGCHLD 处理（逃）。我们来看下这段代码的输出 我们能看到守护进程的 PID 是 449920 然后我们执行 ps -efj 和 ps auf 两个命令看一下结果 我们能看到，449920 这个进程在父进程退出后没有 re-parent 到当前空间的一号进程上。这是为什么呢？可能眼尖的同学已经注意到，这段代码中一个特殊的 sys call prctl8。我们给当前进程设置了 PR_SET_CHILD_SUBREAPER 的属性。 这里我们来看一下内核里的实现 1234567891011121314151617181920212223242526272829303132333435363738394041/* * When we die, we re-parent all our children, and try to: * 1. give them to another thread in our thread group, if such a member exists * 2. give it to the first ancestor process which prctl'd itself as a * child_subreaper for its children (like a service manager) * 3. give it to the init process (PID 1) in our pid namespace */static struct task_struct *find_new_reaper(struct task_struct *father, struct task_struct *child_reaper){ struct task_struct *thread, *reaper; thread = find_alive_thread(father); if (thread) return thread; if (father-&gt;signal-&gt;has_child_subreaper) { unsigned int ns_level = task_pid(father)-&gt;level; /* * Find the first -&gt;is_child_subreaper ancestor in our pid_ns. * We can't check reaper != child_reaper to ensure we do not * cross the namespaces, the exiting parent could be injected * by setns() + fork(). * We check pid-&gt;level, this is slightly more efficient than * task_active_pid_ns(reaper) != task_active_pid_ns(father). */ for (reaper = father-&gt;real_parent; task_pid(reaper)-&gt;level == ns_level; reaper = reaper-&gt;real_parent) { if (reaper == &amp;init_task) break; if (!reaper-&gt;signal-&gt;is_child_subreaper) continue; thread = find_alive_thread(reaper); if (thread) return thread; } } return child_reaper;} 这里我们总结一下，当父进程退出后，所属的子进程，将按照如下顺序进行 re-parent 线程组里其余可用线程（这里的线程有所不一样，可以暂时忽略） 在当前所属的进程树上不断寻找设置了 PR_SET_CHILD_SUBREAPER 进程 在前面两者都无效的情况下，re-parent 到当前 PID Namespace 中的 1 号进程上 到这里，我们关于 Linux 中进程管理的基础介绍就完成了。那么我们将来聊聊容器中的情况 容器中的一号进程这里，我们将利用，Docker 作为背景聊聊这个话题。首先，在 Docker 1.11 之后，其架构发生了比较大的变化，如下图所示 那么我们拉起一个容器的的流程如下 Docker Daemon 向 containerd 发送指令 containerd 创建一个 containterd-shim 进程 containerd-shim 创建一个 runc 进程 runc 进程将根据 OCI 标准，设置相关环境（创建 cgroup，创建 ns 等），然后执行 entrypoint 中的设定的命令 runc 在执行完相关设置后，将自我退出，此时其子进程（即容器命名空间内的1号进程）将被 re-parent 给 containerd-shim 进程。 OK，上面 step 5 操作，就需要依赖我们上节中讲到的 prctl 和 PR_SET_CHILD_SUBREAPER 。 自此，containerd-shim 将承担容器内进程相关的操作，即便其父进程退出，子进程也会根据 re-parent 的流程托管到 containerd-shim 进程上。 那么，这样是不是就没有问题了呢？ 答案很明显不是。来给大家举一个实际上的场景：假设我一个服务需要实现一个需求叫做优雅下线。通常而言，我们会在暴力杀死进程之前，利用 SIGTERM 信号实现这个功能。但是在容器时期有个问题，我们一号进程，可能不是程序本身（比如大家习惯性的会考虑在 entrypoint 中用 bash 去裹一层），或者经过一些特殊场景，容器中的进程，全部已经托管在 containerd-shim 上了。而 contaninerd-shim 是不具备信号转发的能力的。 所以在这样一些场景下，我们就需要考虑额外引入一些组件来完成我们的需求。这里以一个非常轻量级的专门针对容器的设计的一号进程项目 tini9 来作为介绍 我们这里看一下核心的一些代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455int register_subreaper () { if (subreaper &gt; 0) { if (prctl(PR_SET_CHILD_SUBREAPER, 1)) { if (errno == EINVAL) { PRINT_FATAL(&quot;PR_SET_CHILD_SUBREAPER is unavailable on this platform. Are you using Linux &gt;= 3.4?&quot;) } else { PRINT_FATAL(&quot;Failed to register as child subreaper: %s&quot;, strerror(errno)) } return 1; } else { PRINT_TRACE(&quot;Registered as child subreaper&quot;); } } return 0;}int wait_and_forward_signal(sigset_t const* const parent_sigset_ptr, pid_t const child_pid) { siginfo_t sig; if (sigtimedwait(parent_sigset_ptr, &amp;sig, &amp;ts) == -1) { switch (errno) { case EAGAIN: break; case EINTR: break; default: PRINT_FATAL(&quot;Unexpected error in sigtimedwait: '%s'&quot;, strerror(errno)); return 1; } } else { /* There is a signal to handle here */ switch (sig.si_signo) { case SIGCHLD: /* Special-cased, as we don't forward SIGCHLD. Instead, we'll * fallthrough to reaping processes. */ PRINT_DEBUG(&quot;Received SIGCHLD&quot;); break; default: PRINT_DEBUG(&quot;Passing signal: '%s'&quot;, strsignal(sig.si_signo)); /* Forward anything else */ if (kill(kill_process_group ? -child_pid : child_pid, sig.si_signo)) { if (errno == ESRCH) { PRINT_WARNING(&quot;Child was dead when forwarding signal&quot;); } else { PRINT_FATAL(&quot;Unexpected error when forwarding signal: '%s'&quot;, strerror(errno)); return 1; } } break; } } return 0;} 这里我们能很清楚看到两个核心点 tini 会通过 prctl 和 PR_SET_CHILD_SUBREAPER 来接管容器内的孤儿进程 tini 在收到信号后，会将信号转发给子进程或者是所属的子进程组 当然其实 tini 本身也有一些小问题（不过比较冷门）这里留一个讨论题：假设我们有这样一个服务，在创建10个守护进程后自己退出。在这十个守护进程中，我们都会设置一个全新的进程组 ID （所谓进程组逃逸）。那么我们怎么样将信号转发到这十个进程上（仅供讨论，生产上这么干的人早被打死了） 总结可能看到这里，可能有人要喷我不讲武德，说好的容器内一号进程，但是花了大半篇幅来讲 Linux 进程233333. 实际上传统容器基本可以认为是在 OS 中执行的一个完整进程。讨论容器中的一号进程离不开讨论 Linux 中进程管理的相关知识点。 希望通过这篇技术水文能帮大家对容器中一号进程有个大概的认知，并能正确的使用和管理他。 最后祝大家新年快乐！（希望新年我能不以写水文为生，呜呜呜呜） Reference [1]. Docker [2]. containerd [3]. kata [4]. task_struct [5]. Linux Man Page: fork [6]. Linux Man Page: ptrace [7]. Linux man page: waitpid [8]. Linux man page: prctl [9]. tini","link":"/posts/2021/02/13/a-simple-introduction-about-the-init-process-in-container/"},{"title":"继续爆论容器中的一号进程","text":"上周的文章聊了关于容器中的一号进程的一些概况后，在我师父某川(可以去 GitHub 找他玩，jschwinger23) 的指导与配合下，我们一起对目前主流的被广泛使用的两个容器中一号进程的实现 dumb-init 和 tini 做了一番探究，继续写个水文来爆论一番。 正文我们为什么需要一个一号进程，我们希望的一号进程需要承担怎样的职责？在继续聊关于 dumb-init 和 tini 的相关爆论之前，我们需要来 review 一个问题。我们为什么需要一个一号进程？以及我们所选择的一号进程需要承担怎么样的职责 其实我们在容器场景下需要一号进程托管在前面实际上有两种主要的场景， 对于容器内 Graceful Upgrade 二进制这种场景，主流的一种做法之一是 fork 一个新的进程，exec 新的二进制文件，新进程处理新链接，老进程处理老链接。（Nginx 就采用这种方案） 没有正确的处理信号转发以及进程回收的情况 一些如同 calico-node 的场景么，我们出于方便打包的考虑，将多个二进制运行在同一容器中 对于第一种其实需要说的没有太多，我们来看一下第二点的测试 我们先准备一个最简单 Python 文件，demo1.py 123import timetime.sleep(10000) 然后依照常规，我们开始用一个 bash 脚本裹一下 123#!/bin/bashpython /root/demo1.py 最后编写 Dockerfile 123456FROM python:3.9ADD demo1.py /root/demo1.pyADD demo1.sh /root/demo1.shENTRYPOINT [&quot;bash&quot;, &quot;/root/demo1.sh&quot;] 构建后开始执行，我们先来看一下进程结构 没有问题，现在我们用 strace 来 trace 一下，2049962、2050009 这两个进程，然后对 2049962 这个 bash 进程发 *SIGTERM＊ 信号 我们来看下结果 我们能清晰看到 2049962 进程在接到 SIGTERM 的时候，没有将其转发给 2050009 进程。在我们手动 SIGKILL 2049962 后， 2050009 也随即退出，这里可能有人会有点疑惑，为什么 2049962 退出后，2050009 也会退出呢？ 这里是由于 pid namespace 本身的特性，我们来看看，pid_namespaces 中的相关介绍 If the “init” process of a PID namespace terminates, the kernel terminates all of the processes in the namespace via a SIGKILL signal. 当当前 pid ns 内的一号进程退出的时候，内核直接 SIGKILL 伺候该 pid ns 内的剩余进程 OK，在我们结合容器调度框架后，那么在生产上实际会出现很多的坑，来看一段我之前的吐槽 我们一个测试服务，Spring Cloud 的，在下线后，节点无法从注册中心摘除，然后百思不得其解，最后查到问题，，本质上是这样，POD 被摘除的时候，K8S Scheduler 会给 POD 的 ENTRYPOINT 发一个 SIGTERM 信号，然后等待三十秒（默认的 graceful shutdown 超时实践)，还没响应就会 SIGKILL 直接杀问题在于，我们 Eureka 版的服务是通过 start.sh 来启动的，ENTRYPOINT [“/home/admin/start.sh”]，容器里默认是 /bin/sh 是 fork/exec 模式，导致我服务进程没法正确的收到 SIGTERM 信号，然后一直没结束就被 SIGKILL 了 刺激不刺激。除了信号转发无法正常处理以外，我们应用程序常见的一个常见处理的问题就是 Z 进程的出现，即子进程结束之后，无法正确的回收。比如早期 puppeteer 臭名昭著的 Z 进程问题。 在这种情况下，除了应用程序本身的问题以外，另外可能的原因是在守护进程这样的场景下，孤儿进程 re-parent 之后的进程，不具备回收子进程的功能 OK 在回顾完上面我们常见的问题后，我们来 review 一下我们对于容器内一号进程所需要承担的职责 信号的转发 Z 进程的回收 而在目前，在容器场景下，大家主要使用两个方案来作为自己的容器内一号进程，dumb-init 和 tini。这两个方案对于容器内孤儿与 Z 进程的处理都算是 OK。但是信号转发的实现上一言难尽。那么接下来 爆论时间！ 拉跨的 dumb-init某种程度上来说，dumb-init 这货完全是属于虚假宣传的典范。代码实现非常糙 来看看官方的宣传 dumb-init runs as PID 1, acting like a simple init system. It launches a single process and then proxies all received signals to a session rooted at that child process. 这里，dumb-init 说自己使用了 Linux 中的进程 Session，我们都知道，一个进程 Session 在默认情况下，共享一个 Process Group Id 。那么我们这里可以理解为，dumb-init 能将信号完全转发到进程组中的每个进程上。听起来很美好是不是？ 我们先来测试一下吧 测试代码如下，demo2.py 1234567import osimport timepid = os.fork()if pid == 0: cpid = os.fork()time.sleep(1000) Dockerfile 如下 12345678910FROM python:3.9RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.5/dumb-init_1.2.5_x86_64RUN chmod +x /usr/local/bin/dumb-initADD demo2.py /root/demo2.pyENTRYPOINT [&quot;/usr/local/bin/dumb-init&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo2.py&quot;] 构建，开跑，先来看下进程结构 然后老规矩，strace 2103908、2103909、2103910 这三个进程，然后我们对 dumb-init 的进程做一下发送 SIGTERM 的操作吧 诶？dumb-init 老师，发生了甚么事？为什么 2103909 直接被 SIGKILL 了，而没有收到 SIGTERM 这里我们要来看下 dumb-init 的关键实现 123456789101112131415161718192021222324252627282930313233void handle_signal(int signum) { DEBUG(&quot;Received signal %d.\\n&quot;, signum); if (signal_temporary_ignores[signum] == 1) { DEBUG(&quot;Ignoring tty hand-off signal %d.\\n&quot;, signum); signal_temporary_ignores[signum] = 0; } else if (signum == SIGCHLD) { int status, exit_status; pid_t killed_pid; while ((killed_pid = waitpid(-1, &amp;status, WNOHANG)) &gt; 0) { if (WIFEXITED(status)) { exit_status = WEXITSTATUS(status); DEBUG(&quot;A child with PID %d exited with exit status %d.\\n&quot;, killed_pid, exit_status); } else { assert(WIFSIGNALED(status)); exit_status = 128 + WTERMSIG(status); DEBUG(&quot;A child with PID %d was terminated by signal %d.\\n&quot;, killed_pid, exit_status - 128); } if (killed_pid == child_pid) { forward_signal(SIGTERM); // send SIGTERM to any remaining children DEBUG(&quot;Child exited with status %d. Goodbye.\\n&quot;, exit_status); exit(exit_status); } } } else { forward_signal(signum); if (signum == SIGTSTP || signum == SIGTTOU || signum == SIGTTIN) { DEBUG(&quot;Suspending self due to TTY signal.\\n&quot;); kill(getpid(), SIGSTOP); } }} 这是 dumb-init 老师处理信号的代码，在收到信号后，将除 SIGCHLD 的信号做转发（注意 SIGKILL 是不可 handle 信号），我们来看看信号转发的逻辑 123456789void forward_signal(int signum) { signum = translate_signal(signum); if (signum != 0) { kill(use_setsid ? -child_pid : child_pid, signum); DEBUG(&quot;Forwarded signal %d to children.\\n&quot;, signum); } else { DEBUG(&quot;Not forwarding signal %d to children (ignored).\\n&quot;, signum); }} 默认情况下直接 kill 发送信号，其中 -child_pid 是这样一个特性： If pid is less than -1, then sig is sent to every process in the process group whose ID is -pid. 直接转发进程组，看起来没啥问题啊？那么上面是甚么原因呢？我们再来复习下上一段话，kill 给进程组发信号的逻辑是 sig is sent to every process ，懂了，一个 O(N) 的遍历嘛。没啥问题啊？好了，不卖关子，这里 dumb-init 的实现存在一个 race-condition 我们刚刚说了，kill 进程组的行为是一个 O(N) 的遍历，那么必然会有进程先收到信号，而有进程后收到信号。以 SIGTERM 为例，假设我们 dumb-init 的子进程先收到 SIGTERM，优雅退出后，dumb-init 收到 SIGCHLD 的信号，然后 wait_pid 拿到子进程 ID，判断是自己直接托管的进程后，自杀退出。好了，由于 dumb-init 是我们当前 pid ns 内的 init 进程，再来复习下 pid ns 的特性。 If the “init” process of a PID namespace terminates, the kernel terminates all of the processes in the namespace via a SIGKILL signal. 在 dumb-init 自杀以后，剩余进程将直接被内核 SIGKILL 伺候。也就导致了我们上面看到的，子进程没有收到转发的信号！ 所以这里加粗处理一下，dumb-init 所承诺的，能将信号转发到所有进程上，完全是虚假宣传！ 而且请注意，dumb-init 宣称自己能管理一个 Session 内的进程！但是实际上他们只做了一个进程组的信号转发！完全是虚假宣称！Fake News！ 而且如上面所提到的，在我们热更新二进制这样的场景下，dumb-init 在进程退出后直接自杀。和不使用一号进程完全没有差别！ 我们可以来测试一下，测试代码 demo3.py 12345import osimport timepid = os.fork()time.sleep(1000) fork 一个进程，总共两个进程 Dockerfile 如下 12345678910FROM python:3.9RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.5/dumb-init_1.2.5_x86_64RUN chmod +x /usr/local/bin/dumb-initADD demo3.py /root/demo3.pyENTRYPOINT [&quot;/usr/local/bin/dumb-init&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo3.py&quot;] 构建，执行，先看看进程结构 然后模拟老进程退出，我们直接 SIGKILL 掉 2134836，然后我们看看 2134837 的 strace 的结果 如预期一样，在 dumb-init 自杀后，2134837 被内核 SIGKILL 了 所以跟我复习一遍 dumb-init 拉跨！好了，我们接着聊 tini 的实现 态度友好的聊聊 tini平心而论，tini 的实现，虽然也还有坑，但是比 dumb-init 细腻到不知道哪里去了，我们直接来先看下代码 12345678910111213141516while (1) { /* Wait for one signal, and forward it */ if (wait_and_forward_signal(&amp;parent_sigset, child_pid)) { return 1; } /* Now, reap zombies */ if (reap_zombies(child_pid, &amp;child_exitcode)) { return 1; } if (child_exitcode != -1) { PRINT_TRACE(&quot;Exiting: child has exited&quot;); return child_exitcode; }} 首先 tini 没有设置 signal handler ，不断循环 wait_and_forward_signal 和 reap_zombies 这两个函数 12345678910111213141516171819202122232425262728293031323334353637383940int wait_and_forward_signal(sigset_t const* const parent_sigset_ptr, pid_t const child_pid) { siginfo_t sig; if (sigtimedwait(parent_sigset_ptr, &amp;sig, &amp;ts) == -1) { switch (errno) { case EAGAIN: break; case EINTR: break; default: PRINT_FATAL(&quot;Unexpected error in sigtimedwait: '%s'&quot;, strerror(errno)); return 1; } } else { /* There is a signal to handle here */ switch (sig.si_signo) { case SIGCHLD: /* Special-cased, as we don't forward SIGCHLD. Instead, we'll * fallthrough to reaping processes. */ PRINT_DEBUG(&quot;Received SIGCHLD&quot;); break; default: PRINT_DEBUG(&quot;Passing signal: '%s'&quot;, strsignal(sig.si_signo)); /* Forward anything else */ if (kill(kill_process_group ? -child_pid : child_pid, sig.si_signo)) { if (errno == ESRCH) { PRINT_WARNING(&quot;Child was dead when forwarding signal&quot;); } else { PRINT_FATAL(&quot;Unexpected error when forwarding signal: '%s'&quot;, strerror(errno)); return 1; } } break; } } return 0;} 用 sigtimedwait 这个函数来接收信号，然后过滤掉 SIGCHLD 转发。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364int reap_zombies(const pid_t child_pid, int* const child_exitcode_ptr) { pid_t current_pid; int current_status; while (1) { current_pid = waitpid(-1, &amp;current_status, WNOHANG); switch (current_pid) { case -1: if (errno == ECHILD) { PRINT_TRACE(&quot;No child to wait&quot;); break; } PRINT_FATAL(&quot;Error while waiting for pids: '%s'&quot;, strerror(errno)); return 1; case 0: PRINT_TRACE(&quot;No child to reap&quot;); break; default: /* A child was reaped. Check whether it's the main one. If it is, then * set the exit_code, which will cause us to exit once we've reaped everyone else. */ PRINT_DEBUG(&quot;Reaped child with pid: '%i'&quot;, current_pid); if (current_pid == child_pid) { if (WIFEXITED(current_status)) { /* Our process exited normally. */ PRINT_INFO(&quot;Main child exited normally (with status '%i')&quot;, WEXITSTATUS(current_status)); *child_exitcode_ptr = WEXITSTATUS(current_status); } else if (WIFSIGNALED(current_status)) { /* Our process was terminated. Emulate what sh / bash * would do, which is to return 128 + signal number. */ PRINT_INFO(&quot;Main child exited with signal (with signal '%s')&quot;, strsignal(WTERMSIG(current_status))); *child_exitcode_ptr = 128 + WTERMSIG(current_status); } else { PRINT_FATAL(&quot;Main child exited for unknown reason&quot;); return 1; } // Be safe, ensure the status code is indeed between 0 and 255. *child_exitcode_ptr = *child_exitcode_ptr % (STATUS_MAX - STATUS_MIN + 1); // If this exitcode was remapped, then set it to 0. INT32_BITFIELD_CHECK_BOUNDS(expect_status, *child_exitcode_ptr); if (INT32_BITFIELD_TEST(expect_status, *child_exitcode_ptr)) { *child_exitcode_ptr = 0; } } else if (warn_on_reap &gt; 0) { PRINT_WARNING(&quot;Reaped zombie process with pid=%i&quot;, current_pid); } // Check if other childs have been reaped. continue; } /* If we make it here, that's because we did not continue in the switch case. */ break; } return 0;} 然后在 reap_zombies 函数中，不断利用 waitpid 这个函数来处理进程，在没有子进程等待处理或者遇到其余系统错误时退出循环。 注意这里 tini 和 dumb-init 的的实现差异，dumb-init 在回收自己的入口子进程后便会自杀。而 tini 将会在所有自己的子进程退出之后，结束循环，然后判断是否自杀。 那么我们这里来测试一下 还是 demo2 的例子，我们来测试一下孙进程的例子 123456789FROM python:3.9ADD demo2.py /root/demo2.pyENV TINI_VERSION v0.19.0ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tiniRUN chmod +x /tiniENTRYPOINT [ &quot;/tini&quot;,&quot;-s&quot;, &quot;-g&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo2.py&quot;] 然后构建，执行，进程结构如下 然后，老规矩，strace , kill 发 SIGTERM 看一下， 嗯，如预期一样，那么 tini 的实现是不是没有问题了呢，我们再来准备一个例子,demo4.py 12345678import osimport timeimport signalpid = os.fork()if pid == 0: signal.signal(15, lambda _, __: time.sleep(1)) cpid = os.fork()time.sleep(1000) 这里我们用 time.sleep(1) 来模拟，程序接到 SIGTERM 后需要优雅处理，然后我们还是准备下 dockefile 123456789FROM python:3.9ADD demo4.py /root/demo4.pyENV TINI_VERSION v0.19.0ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tiniRUN chmod +x /tiniENTRYPOINT [ &quot;/tini&quot;,&quot;-s&quot;, &quot;-g&quot;, &quot;--&quot;]CMD [&quot;python&quot;, &quot;/root/demo4.py&quot;] 然后构建，允许，看进程结构，啪的一下很快啊 然后 strace ，发 SIGTERM 一条龙服务， 然后我们发现，2173316 和 2173317 这两个进程，成功接收到 SIGTERM 的信号后，在处理中，被 SIGKILL 了。那么这是为甚么呢？实际上这里也存在一个潜在的 race condition 当我们开启 tini 的使用。2173315 退出后，2173316 将被 re-parent ， 按照内核的 re-parent 流程，2173317 re-parent 到 tini 进程。 但是，tini 在使用 waitpid 的时候，使用了 WNOHANG 这个选项，那么这里如果在执行 waitpid 时，子进程还未结束，那么将立刻返回0。从而退出循环，开始自杀流程。 刺激不刺激，关于这点，我师父和我提了一个 issue: tini Exits Too Early Leading to Graceful Termination Failure 然后，我也做了一版修复，具体可以参考use new threading to run waipid（还在 PoC，没写单测，处理也有点糙） 实际上思路很简单 ，我们不使用 waitpid 中的 WNOHANG 选项，将其变为阻塞的调用，然后用一个新的线程来做 waitpid 的处理 构建一版测试效果如下 嗯，如预期一样，测试没有问题。 当然这里实际上可能细心的朋友发现，原本的 tini 也没法处理二进制更新的情况，原因和 demo5 里的原因一致。这里大家可以去测试一下 实际上这里我的处理很过于粗糙和暴力，我们实际上只要保证让 tini 的退出条件变成一定要等到 waitpid()=-1 &amp;&amp; errno==EHILD再退出。具体的实现手段大家可以一起来思考（实际上还不少 最后来总结一下问题的核心： 无论是 dumb-init 还是 tini 在现行的实现里，都犯了同一个错误，即在容器这个特殊的场景下，都没有等待所有子孙进程的退出再退出。其实解决方案很简单，退出条件一定要是 waitpid()=-1 &amp;&amp; errno==EHILD 总结本文吐槽了 dumb-init 和 tini。dumb—init 实现属实拉跨，tini 的实现细腻了很多。但是 tini 依旧存在不可靠的行为，以及我们所期待的 fork 二进制更新这种使用一号进程的场景在 dumb-init 和 tini 上都没法实现。而且 dumb-init 和 tini 目前也还有一个共通的局限性。即无法处理子进程进程组逃逸的情况。（比如十个子进程各自逃逸到一个进程组中）。 而且在文中的测试中，我们用 time.sleep(1) 来模拟 Graceful Shutdown 的行为，tini 也已经无法满足需求了。。So。。。。 所以归根到底一句话，应用的信号，进程回收这些基础行为应该应用自决。任何管杀不管埋而寄托于一号进程的行为，都是对于生产的不负责任。（如果你们实在想要一个一号进程，还是用 tini 吧，千万别用 dumb-init) 所以 exec 裸起大法好，不用一号进程平安保！ 差不多水文就这样吧，这篇水文从提出问题到验证结论，到 patch PoC 报销了我快一个星期的业余时间（本文初稿在凌晨4点过写完）。最后感谢某川同学和我一起搞了几个凌晨三点过。最后，祝大家看的愉快。","link":"/posts/2021/02/28/damn-the-init-process/"},{"title":"云原生时代的几个爆论","text":"从去年调转到现在，做了一段时间的云原生，我突发奇想，想发表几个爆论来论述下我眼中的云原生来作为今年最后一篇技术博客。本文纯属个人向吐槽，与本人公司立场无关 概述云原生大概在 2014-2015 年开始左右，开始正式的提出了这个概念。2015 年 Google 主导成立了云原生计算基金会（Cloud Native Computing Foundation aka CNCF)。在 2018 年，CNCF 在 CNCF Cloud Native Definition v1.01 首次对云原生的概念有了一个认定 Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil. 其中文翻译如下： 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 从官方的定义来看，我更愿意将其称为一个愿景(vision/landscape)而不是一个定义(definition)，因为在上述的表达中，并没有清晰明确的表述出云原生这一新生概念的具体的范围与边界，也没有阐述清楚 Cloud Native 和 Non-Cloud Native 之间的差异。 如果以个人的视角来看，一个云原生应用具备以下特质 容器化 服务化 而一个践行云原生的组织，那么应该具备以下特质 重度 Kubernetes 或其余容器调度平台（如 Shopee 自研的 eru22 具备完整的监控体系 具备完整的 CI/CD 体系 在这个基础上，最近看到很多人都在讨论云原生这一新生概念，所以我想在这里聊聊个人向的四个爆论（爆论中的数据是个人主观判断，轻喷） 百分之95以上的公司，没有完成 CI/CD 体系的建立。也没有完成线上服务进程的收敛 百分之90以上的公司，没有能微服务化的技术储备 百分之90以上的公司，没有能撑起容器化的技术储备 开始爆论1. 百分之95以上的公司，没有完成 CI/CD 体系的建立。也没有完成线上服务进程的收敛CI 指持续集成（Continuous Integration aka CI），而 CD 指持续交付(Continuous Delivery aka CD)，通常来讲 CI 与 CD 的定义如下（此处引用 Brent Laster 在 What is CI/CD?3 中给出的定义 Continuous integration (CI) is the process of automatically detecting, pulling, building, and (in most cases) doing unit testing as source code is changed for a product. CI is the activity that starts the pipeline (although certain pre-validations—often called “pre-flight checks”—are sometimes incorporated ahead of CI).The goal of CI is to quickly make sure a new change from a developer is “good” and suitable for further use in the code base.Continuous deployment (CD) refers to the idea of being able to automatically take a release of code that has come out of the CD pipeline and make it available for end users. Depending on the way the code is “installed” by users, that may mean automatically deploying something in a cloud, making an update available (such as for an app on a phone), updating a website, or simply updating the list of available releases. 通常在我们的实践中，CI 和 CD 的边界并不明显。以常见的基于 Jenkins 的实践为例，我们通常的一套路径是 创建一个 Jenkins 的项目，设定一个 Pipeline（其中包含代码拉取，构建，单元测试等 task），设置触发条件 当指定代码仓库存在主分支代码合入等操作时，执行 Pipeline ，然后生成产物 在生成产物后的，常见有两种做法 在生成产物的下一个阶段触发自动的 deploy 流程，按照 deploy script 直接将生成的产物/镜像直接部署到目标服务器上 将生成的产物上传到中间平台，由人通过部署平台手动触发部署任务 在上面描述的过程中，如果有着完备的流程的公司还会有着其余的辅助流程（如 PR/MR 时的 CI 流程，CR 流程等） 而在面对目标平台的部署时，我自己的另外一个观点是大部分的公司没有完成线上服务进程的收敛。讲个笑话： Q: 你们怎么部署线上服务呀？A；nohup，tmux，screen 对于当下而言，一个规范化的 CI/CD 流程，收口的线上的服务进程的管理，至少在当下，有着可以遇见的几个好处 尽可能的降低人为手动变更带来的风险 能够较好的完成基础运行依赖配置的收口 依托目前主流的开源的 systemd, supervisor, pm2 等进程管理工具，能对进程提供基础的 HA 的保证（包括进程探活，进程重拉等） 为后续的服务化，容器化等步骤打下基础 2. 百分之90以上的公司，没有能微服务化的技术储备如果说，对于爆论1 提到的 CI/CD 等手段，我更多的觉得这是一个制度障碍大于技术障碍的现实。那么接下来的几个爆论，我更愿意用没有技术储备来形容 先来说说爆论2: 百分之90以上的公司，没有能微服务化的技术储备 首先来聊聊微服务的概念吧，微服务实际上在计算机历史上有着不同的论述，在2014年 Martin Fowler 和 James Lewis 正式在 Microservices a definition of this new architectural term4 一文中正式的提出了微服务（Microservice）这一概念。此处引用维基百科的一段概述 微服务是由以单一应用程序构成的小服务，自己拥有自己的行程与轻量化处理，服务依业务功能设计，以全自动的方式部署，与其他服务使用HTTP API通信。同时服务会使用最小的规模的集中管理 (例如 Docker) 能力，服务可以用不同的编程语言与数据库等组件实现 那么我们来用研发的话来尝试描述下关于微服务和与之对应的传统单体服务（Monolith） 之间显著性的差异 微服务的 scope 更小，其更多的专注在某一个功能，或者某一类的功能上 由于其 scope 更小的特性，其变更，crash 所带来的影响相较于传统的单体来说更小 对于多语言多技术栈团队来说更为友好 ”符合“现在互联网所需求的小步快跑，快速迭代的大目标 那么我们这里需要思考一下，微服务这一套体系，如果我们想要去进行落地和实践，那么我们需要怎么样的技术储备？我觉得主要是两个方面，架构和治理 首先来聊聊架构吧，我觉得对于微服务来说，最麻烦的一个问题在于从传统单体应用上进行拆分（当然要是最开始创始之初就开始搞微服务的当我没说，虽然这样也有其余的问题） 如前面所说，微服务相较于传统的单体应用来说，，其 scope 更小，更专注在某一个功能或者某一类的功能上。那么这里所引申出来我觉得做微服务最大的一个问题在于合理的划分功能边界并进行拆分 如果拆分不合理那么将导致服务之间相互耦合，比如我将用户鉴权放置在商城服务中，导致我论坛服务需要依赖其不需要的商城服务。如果拆分的过细，那么将导致出现一个很有趣的现象，一个规模不大的业务拆了100多个服务 repo 出来（我们把这种情况称为： 微服务难民2333） 我们践行落地微服务这一套理念，是因为我们在业务和团队规模扩大后，面对多样化的需求与团队成员技术栈时，传统单体应用在其持续维护上的成本将会是一个不小的开支。我们希望引入微服务来尽可能减少维护成本，降低风险。但是不合理的拆分，将会重新让我们的维护成本远超继续践行单体化的方案 而我觉得阻碍微服务继续践行的另外一个问题是治理问题。我们来看一下在微服务化后我们所面临的几个问题 可观测性的问题。如前面所说，微服务化后的单个服务 scope 更小，更多的专注在某一个功能或者某一类功能上。那么这可能导致的问题是，我们在完成一个业务请求所需要经历的请求链路更长。那么按照通用的观点来看，链路更长，其风险更大。那么在在当服务存在异常时（比如业务 RT 的突然增高）我们怎么样去定位具体服务的问题？ 配置框架的收口。在微服务化的场景中，我们可能会选择将一些基础的功能下沉至具体的内部框架中（如服务注册，发现，路由等），那么意味着我们需要维护自己的框架，同时完成配置的收敛 老生常谈的服务治理（注册、发现、熔断）等 由于微服务化后，对于一个完备 CI/CD 机制的需求将变得更为迫切。那么如果存在爆论1的情况，将会成为践行微服务这一理念的障碍 诚然，目前无论开源社区（如 Spring Cloud，Go—Micro 等）还是四大云厂商（AWS，Azure，阿里云，GCP）都在尝试提供一种开箱即用的微服务方案，但是除了没法很好的解决如上面所说的诸如架构这样的问题外，其也存在自己的问题 无论是依赖开源社区的方案，还是云厂商的方案，都需要使用者具备一定的技术素养，来定位特定情况下框架中的问题 Vendor Lock-in，目前开箱即用的微服务方案并没有一个通用的开源事实标准。那么依赖某一个开源社区或者云厂商的方案将存在 vendor lock-in 的问题 无论是开源社区的方案还是云厂商的方案，都存在多语言不友好的问题（大家貌似现在都喜欢 Java 一点（Python 没人权.jpg 所以爆论2想表明的一个最核心的观点就是：微服务化并不是一个无代价的行为，与之相反的是一个需要不低技术储备与人力投入的的行为。所以请不要认为微服务是万能良药。请按需使用 3. 百分之90以上的公司，没有能撑起容器化的技术储备目前很主流的一个观点，是能上容器尽可能上容器，说实话这个想法实际上是有一定的合理性的，去 review 这个想法，我们需要去看一下容器这个东西，给我们带来了什么样的改变 容器首先毫无疑问，会给我们带来非常多的好处： 真正意义上让开发与生产环境保持一致是一种非常方便的事，换句话说，开发说的“这个服务在我本地没啥问题”是一句有用的话了 让部署一些服务变的更为方便，无论是分发，还是部署， 能做到一定程度上的资源隔离与分配 那么，看起来我们是不是可以无脑用容器？不，不是，我们需要再来 Review 一下，容器化后我们可能所要面临的一些弊端： 容器安全性问题，目前最主流的容器实现（此处点名 Docker）本质上而言还是基于 CGroups + NS 来进行资源与进程隔离。那么其安全性将会是一个非常值得考量的问题。毕竟 Docker 越权与逃逸漏洞年年有，年年新。那么这意味着我们是需要去有一个系统的机制去规范我们容器的使用，来保证相关的越权点能被把控在一个可控的范围内。而另一个方向是镜像安全问题，大家都是面向百度/CSDN/Google/Stackoverflow 编(fu)程(zhi)选手，那么势必会出现一个情况，当我们遇到一个问题，搜索一番，直接复制点 Dockerfile 下来，这个时候，将会存在很大的风险点，毕竟谁也不知道 base image 里加了啥料不是？ 容器的网络问题。当我们启动若干个镜像后，那么容器之间的网络互通怎么处理？而大家生产环境，肯定不止一个机器那么少，那么跨主机的情况下，怎么样去进行容器间的通信，同时保证网络的稳定性？ 容器的调度与运维的问题，当我一个机器高负载的时候，怎么样去将该机器上的一些容器调度到其余的机器上？而怎么样去探知一个容器是否存活？如果一个容器 crash 了，怎么样重新拉起？ 容器具体的细节问题，比如镜像怎么样构建与打包？怎么样上传？（又回到了爆论1）乃至说怎么样去排查一些 corner case 的问题？ 对于一些特定的 large size 的镜像（如机器学习同学常用的 CUDA 官方镜像，打包了字典模型等大量数据的镜像等）怎么样去快速下载，快速发布？ 可能这里又会有一种观点，没事，我们上 Kubernetes 就好啦，上面这些很多问题就能解决啦！好吧，我们再来聊聊这个问题 首先我已经忽略掉自建 Kubernetes 集群的场景了，因为那不是一般人能 Hold 住的。那么我们来看一下，依托公有云使用的情况吧，以阿里云为例，点开页面，然后我们见到这样张图 好了，提问： VPC 是什么？ Kubernetes 1.16.9 和 1.14.8 有什么区别 Docker 19.03.5 和阿里云安全沙箱 1.1.0 是什么，有什么区别 专有网络是什么？ 虚拟交换机是什么？ 网络插件是什么？Flannel 和 Terway 又是什么？有什么区别？当你翻了翻文档，然后文档告诉你，Terway 是阿里云基于 Calico 魔改的 CNI 插件。那么 CNI 插件是什么？Calico 是什么？ Pod CIDR 是什么怎么设？ Service CIDR 是什么怎么设？ SNAT 是什么怎么设？ 安全组怎么配置？ Kube-Proxy 是什么？iptables 和 IPVS 有什么区别？怎么选？ 大家能看到上面的问题涵盖了这样几方面 Kubernetes 本身的深入了解（CNI，runtime，kube-proxy 等） 一个合理网络规划 对于云厂商特定功能的熟悉 在我看来，这三方面任何一方面对于一个技术团队的技术储备以及对于业务的理解（广义的技术储备）都需要有一个不浅的需求。 当然这里在碎碎念一下，实际上搞 Kubernetes 这一套开销实际上很大的（有点偏题，但是还是继续说吧） 你得有个镜像仓库吧，不贵，中国区基础版780一个月 你集群内的服务需要暴露出去用吧？行叭，买个最低规格的 SLB，简约型，每个月200 好了，你每个月日志得花钱吧？假设你每个月20G日志，不多吧？行，39.1 你集群监控要不要？好，买，每天50w条日志上报吧？行，不贵，975 一个月 算一下，一个集群吧，(780+200+39.1+975)*12=23292.2一年，不算集群基础的 ENI，ECS 等费用，美滋滋 而且 Kubernetes 会有很多的玄学的问题，也需要技术团队有足够的技术储备来进行排查（我想想啊，我遇到过 CNI 一号进程 crash 了没重拉，特定版本上的内核 cgroup 泄漏，ingress OOM 等问题），大家可以去 Kubernetes 的 Issue 区看一下盛况（说多了都是泪） 总结我知道这篇文章写出来会存在很多的争议。但是我始终想表述的一个观点是对于云原生时代这一套东西（实际上也更多是之前传统技术的延伸），他们的引入并不是无代价，并不是无成本的。对于有着足够规模与痛点的公司来说，这样的成本对于他们的业务增长来说是一个正向的促进，而对于更多中小企业来说，可能这一套对于业务的提升将会是非常小乃至说是负作用。 我希望我们技术人员在做技术决策的时候，一定是在评估自己的团队的技术储备乃至对于业务的收益后再引入某一种技术与理念，而不是引入一个技术只是因为它看起来够先进，够屌，能够为我的简历背书 最后用之前我分享过的一句话来作为本文的结尾吧 一个企业奔着技术先进性去搞技术，就是死 Reference CNCF Cloud Native Definition v1.0 projecteru2 What is CI/CD? Microservices a definition of this new architectural term","link":"/posts/2020/12/31/fuck-the-cloud-native/"},{"title":"利用动态 tracing 技术来 trace 内核中的网络请求","text":"这周帮朋友用 eBPF/SystemTap 这样的动态 tracing 工具做了一些很有趣的功能。这篇文章算是一个总结 开篇实际上这周的一些想法，最开始是实际上来源于某天一个朋友问我的一个问题 我们能不能监控机器上哪些进程在发出 ICMP 请求？需要拿到 PID，ICMP 包出口地址，目标地址，进程启动命令 很有趣的问题。实际上首先拿到这个问题时候，我们第一反应肯定是 “让机器上的进程在发 ICMP 包的时候”直接往一个地方写日志不就好了，emmmm，用一个 meme 镇楼吧 嗯，可能大家都知道我想说什么了，我们这种场景实际上只能选择旁路，无侵入的方式去做。 那么涉及到包的旁路的 trace，大家第一反应肯定是 tcpdump 去抓包。但是在我们今天的问题下，tcpdump 只能拿到包信息， 但是拿不到具体的 PID，启动命令等信息。 所以我们可能需要用另外一些方式去实现我们的需求 在需求最开始之初，我们还可能的选择的方式有这样一些 走 /proc/net/tcp 去拿具体的 socket 的 inode 信息，然后反查 pid 关联 eBPF + kprobe 内核打点做监控 SystemTap + kprobe 内核打点做监控 第一种方式，实际上只能拿到 TCP 一层的信息，但是 ICMP 并不是 TCP 协议啊（衰（虽然同属 L4 那么看到最后，我们貌似就只有用 eBPF/SystemTap 配合 kprobe 的一条路可以走了 基础的 traceKprobe在继续下面的代码实际操作之前，我们首先要来认识一下 Kprobe 先引用一段官方文档的介绍 Kprobes enables you to dynamically break into any kernel routine and collect debugging and performance information non-disruptively. You can trap at almost any kernel code address 1, specifying a handler routine to be invoked when the breakpoint is hit.There are currently two types of probes: kprobes, and kretprobes (also called return probes). A kprobe can be inserted on virtually any instruction in the kernel. A return probe fires when a specified function returns.In the typical case, Kprobes-based instrumentation is packaged as a kernel module. The module’s init function installs (“registers”) one or more probes, and the exit function unregisters them. A registration function such as register_kprobe() specifies where the probe is to be inserted and what handler is to be called when the probe is hit. 简单来说，kprobe 是内核的一个提供的一个 trace 机制，在执行我们所设定特定的内核函数时/后，会按照我们所设定的规则触发我们的回调函数。用官方的话来说，“You can trap at almost any kernel code address” 在我们今天的场景下，不管利用 eBPF 还是 SystemTap 都需要依赖 Kprobe 并选择合适的 hook 点来完成我们内核调用的 trace 那么，在我们今天的场景下，我们应该选择在什么函数上加上对应的 hook 呢？ 首先我们来想一下，ICMP 是一个四层的包，最终封装在一个 IP 报文中分发出去，那么我们来看一下，内核中 IP 报文发送中的关键调用，参见下图 在这里我选择将 ip_finish_output 作为我们的 hook 点。 OK，Hook 点确认后，在开始正式编码前，我们来大概介绍下 ip_finish_output ip_finish_output首先来看下这个函数 123456789101112131415static int ip_finish_output(struct net *net, struct sock *sk, struct sk_buff *skb){ int ret; ret = BPF_CGROUP_RUN_PROG_INET_EGRESS(sk, skb); switch (ret) { case NET_XMIT_SUCCESS: return __ip_finish_output(net, sk, skb); case NET_XMIT_CN: return __ip_finish_output(net, sk, skb) ? : ret; default: kfree_skb(skb); return ret; }} 具体细节先不在这里展开（因为实在是太多了Orz），在系统调用 ip_finish_output 时，会触发我们设定的 kprobe 的钩子，在我们所设定的 hook 函数中会收到 net, sk, skb 三个参数（这三个参数也是调用 ip_finish_output 时的值。 在这三个参数中，我们主要来将视线放在 struct sk_buff *skb 上。 熟悉 Linux Kernel 协议栈实现的同学肯定对 sk_buff 这个数据结构非常非常熟悉了。这个数据结构是 Linux Kernel\b 中网络相关的核心数据结构。通过不断的偏移指针，这个数据结构能够很方便帮助我们确认我们待发送/已接收的数据在内存中所存放的位置。 空口直说好像有点抽象，我们来看个图 以发送一个 TCP 包为例，我们能看到这个图中，sk_buff 经历了六个阶段 a. 根据 TCP 中的一些选项如 MSS 等，分配一个 bufferb. 根据 MAX_TCP_HEADER 在我们申请好的内存 buffer 中预留一段足够容纳所有网络层的 header 的空间（TCP/IP/Link等）c. 填入 TCP 的 payloadd. 填入 TCP headere. 填入 IP headerd. 填入 link header 可以参照一下 TCP 报文结构，这样大家会有一个更直观的理解 大家能看到，通过 sk_buff 的一些指针的操作，我们就能很方便的获取到其中不同 layer 的 header 和具体的 payload OK，现在让我们正式的来开始实现我们所需要的功能 eBPF + KProbe首先简单介绍下 eBPF。BPF 指 Berkeley Packet Filter ，最早期是用来设计在内核中实现一些网络包过滤的功能。但是后续社区对其做了非常多的强化增强，使其不仅能应用于网络目地。这也是名字中 e 的来历（extend） 本质上而言，eBPF 在内核维护了一层 VM，可以加载特定规则生成的代码，让内核变得更具有可编程性（后面我争取写一篇 eBPF 从入门到入土的介绍文章） Tips: Tcpdump 的背后就是 BPF 然后在这次实现中，我们使用了 BCC 来简化我们 eBPF 相关的编写难度 OK，先上代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889from bcc import BPFimport ctypesbpf_text = &quot;&quot;&quot;#include &lt;linux/ptrace.h&gt;#include &lt;linux/sched.h&gt; /* For TASK_COMM_LEN */#include &lt;linux/icmp.h&gt;#include &lt;linux/ip.h&gt;#include &lt;linux/netdevice.h&gt;struct probe_icmp_sample { u32 pid; u32 daddress; u32 saddress;};BPF_PERF_OUTPUT(probe_events);static inline unsigned char *custom_skb_network_header(const struct sk_buff *skb){ return skb-&gt;head + skb-&gt;network_header;}static inline struct iphdr *get_iphdr_in_icmp(const struct sk_buff *skb){ return (struct iphdr *)custom_skb_network_header(skb);}int probe_icmp(struct pt_regs *ctx, struct net *net, struct sock *sk, struct sk_buff *skb){ struct iphdr * ipdata=get_iphdr_in_icmp(skb); if (ipdata-&gt;protocol!=1){ return 1; } u64 __pid_tgid = bpf_get_current_pid_tgid(); u32 __pid = __pid_tgid; struct probe_icmp_sample __data = {0}; __data.pid = __pid; u32 daddress; u32 saddress; bpf_probe_read(&amp;daddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;daddr); bpf_probe_read(&amp;saddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;saddr); __data.daddress=daddress; __data.saddress=saddress; probe_events.perf_submit(ctx, &amp;__data, sizeof(__data)); return 0;}&quot;&quot;&quot;class IcmpSamples(ctypes.Structure): _fields_ = [ (&quot;pid&quot;, ctypes.c_uint32), (&quot;daddress&quot;, ctypes.c_uint32), (&quot;saddress&quot;, ctypes.c_uint32), ]bpf = BPF(text=bpf_text)filters = {}def parse_ip_address(data): results = [0, 0, 0, 0] results[3] = data &amp; 0xFF results[2] = (data &gt;&gt; 8) &amp; 0xFF results[1] = (data &gt;&gt; 16) &amp; 0xFF results[0] = (data &gt;&gt; 24) &amp; 0xFF return &quot;.&quot;.join([str(i) for i in results[::-1]])def print_icmp_event(cpu, data, size): # event = b[&quot;probe_icmp_events&quot;].event(data) event = ctypes.cast(data, ctypes.POINTER(IcmpSamples)).contents daddress = parse_ip_address(event.daddress) print( f&quot;pid:{event.pid}, daddress:{daddress}, saddress:{parse_ip_address(event.saddress)}&quot; )bpf.attach_kprobe(event=&quot;ip_finish_output&quot;, fn_name=&quot;probe_icmp&quot;)bpf[&quot;probe_events&quot;].open_perf_buffer(print_icmp_event)while 1: try: bpf.kprobe_poll() except KeyboardInterrupt: exit() OK，这段代码严格意义上来说是混编的，一部分是 C，一部分是 Python，。Python 部分大家肯定都很熟悉，BCC 帮我们加载我们的 C 代码，并 attch 到 kprobe 上。然后不断输出我们从内核中往外传输的数据 那我们重点来看看 C 部分的代码（实际上这严格来说不算标准 C，算是 BCC 封装的一层 DSL） 首先看一下我们辅助的两个函数 123456789static inline unsigned char *custom_skb_network_header(const struct sk_buff *skb){ return skb-&gt;head + skb-&gt;network_header;}static inline struct iphdr *get_iphdr_in_icmp(const struct sk_buff *skb){ return (struct iphdr *)custom_skb_network_header(skb);} 如前面所说，我们可以根据 sk_buff 中的 head 和 network_header 就能计算出我们 IP 头部在内存中的地址，然后我们将其 cast 成一个 iphdr 结构体指针 我们还得再来看一下 iphdr 123456789101112131415161718192021struct iphdr {#if defined(__LITTLE_ENDIAN_BITFIELD) __u8 ihl:4, version:4;#elif defined (__BIG_ENDIAN_BITFIELD) __u8 version:4, ihl:4;#else#error &quot;Please fix &lt;asm/byteorder.h&gt;&quot;#endif __u8 tos; __be16 tot_len; __be16 id; __be16 frag_off; __u8 ttl; __u8 protocol; __sum16 check; __be32 saddr; __be32 daddr; /*The options start here. */}; 熟悉 IP 报文结构的同学肯定就很眼熟了对吧，其中 saddr 和 daddr 就是我们的源地址和目标地址，protocol 代表着我们 L4 协议的类型，其中为1的时候代表着 ICMP 协议 OK 然后来看一下我们的 trace 函数 123456789101112131415161718int probe_icmp(struct pt_regs *ctx, struct net *net, struct sock *sk, struct sk_buff *skb){ struct iphdr * ipdata=get_iphdr_in_icmp(skb); if (ipdata-&gt;protocol!=1){ return 1; } u64 __pid_tgid = bpf_get_current_pid_tgid(); u32 __pid = __pid_tgid; struct probe_icmp_sample __data = {0}; __data.pid = __pid; u32 daddress; u32 saddress; bpf_probe_read(&amp;daddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;daddr); bpf_probe_read(&amp;saddress, sizeof(ipdata-&gt;daddr), &amp;ipdata-&gt;saddr); __data.daddress=daddress; __data.saddress=saddress; probe_events.perf_submit(ctx, &amp;__data, sizeof(__data)); return 0;} 如前面所说，kprobe 触发调用时，会将 ip_finish_output 的三个参数传入到我们的 trace 函数中来，那我们就可以根据传入的数据做很多的事了，现在来介绍下上面的代码中所做的事 将 sk_buff 转换成对应的 iphdr 判断当前报文是否为 ICMP 协议 利用内核 BPF 提供的 helper bpf_get_current_pid_tgid 获取当前调用 ip_finish_output 进程的 pid 获取 saddr 和 daddr。注意我们这里用的 bpf_probe_read 也是 BPF 提供的 helper function，原则上来讲，在 eBPF 中为了保证安全，我们所有从内核中读取数据的行为都应该利用 bpf_probe_read 或 bpf_probe_read_kernel 来实现 通过 perf 将数据提交出去 这样一来，我们就能排查到机器上具体什么进程在发送 ICMP 请求了 来看下效果 OK，我们的需求基本上达到了，不过这里算是留了一个小问题，大家可以思考下，我们怎么样根据 pid 获取启动进程时的 cmdline ? SystemTap + kprobeeBPF 的版本实现了，但是有个问题啊，eBPF 只能在高版本的内核中使用。一般而言，在 xb86_64 上，Linux 3.16 中支持了 eBPF。而我们依赖的 kprobe 对于 eBPF 的支持则是在 Linux 4.1 中实现的。通常而言，我们一般推荐使用 4.9 及以上内核来配合 eBPF 使用 那么问题来了。实际上我们现在有很多 Centos 7 + Linux 3.10 这样的传统的搭配，那么他们怎么办呢？ Linux 3.10 live’s matter! Centos 7 live’s matter! 那没办法，只能换一个技术栈来做了。这个时候，我们就首先考虑由 RedHat 开发，贡献进入社区，低版本可用的 SystemTap 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485%{#include&lt;linux/byteorder/generic.h&gt;#include&lt;linux/if_ether.h&gt;#include&lt;linux/skbuff.h&gt;#include&lt;linux/ip.h&gt;#include&lt;linux/in.h&gt;#include&lt;linux/tcp.h&gt;#include &lt;linux/sched.h&gt;#include &lt;linux/list.h&gt;#include &lt;linux/pid.h&gt;#include &lt;linux/mm.h&gt;%}function isicmp:long (data:long)%{ struct iphdr *ip; struct sk_buff *skb; int tmp = 0; skb = (struct sk_buff *) STAP_ARG_data; if (skb-&gt;protocol == htons(ETH_P_IP)){ ip = (struct iphdr *) skb-&gt;data; tmp = (ip-&gt;protocol == 1); } STAP_RETVALUE = tmp;%}function task_execname_by_pid:string (pid:long) %{ struct task_struct *task; task = pid_task(find_vpid(STAP_ARG_pid), PIDTYPE_PID);// proc_pid_cmdline(p, STAP_RETVALUE); snprintf(STAP_RETVALUE, MAXSTRINGLEN, &quot;%s&quot;, task-&gt;comm); %}function ipsource:long (data:long)%{ struct sk_buff *skb; struct iphdr *ip; __be32 src; skb = (struct sk_buff *) STAP_ARG_data; ip = (struct iphdr *) skb-&gt;data; src = (__be32) ip-&gt;saddr; STAP_RETVALUE = src;%}/* Return ip destination address */function ipdst:long (data:long)%{ struct sk_buff *skb; struct iphdr *ip; __be32 dst; skb = (struct sk_buff *) STAP_ARG_data; ip = (struct iphdr *) skb-&gt;data; dst = (__be32) ip-&gt;daddr; STAP_RETVALUE = dst;%}function parseIp:string (data:long) %{ sprintf(STAP_RETVALUE,&quot;%d.%d,%d.%d&quot;,(int)STAP_ARG_data &amp;0xFF,(int)(STAP_ARG_data&gt;&gt;8)&amp;0xFF,(int)(STAP_ARG_data&gt;&gt;16)&amp;0xFF,(int)(STAP_ARG_data&gt;&gt;24)&amp;0xFF);%}probe kernel.function(&quot;ip_finish_output&quot;).call { if (isicmp($skb)) { pid_data = pid() /* IP */ ipdst = ipdst($skb) ipsrc = ipsource($skb) printf(&quot;pid is:%d,source address is:%s, destination address is %s, command is: '%s'\\n&quot;,pid_data,parseIp(ipsrc),parseIp(ipdst),task_execname_by_pid(pid_data)) } else { next }} 实际上大家可以看到，我们思路还是一样，利用 ip_finish_output 来作为 kprobe 的 hook 点，然后我们获取对应的 iphdr 然后进行操作。 嗯，我们的需求的基础功能差不多就是这样了，大家可以在额外进行一些功能增强，比如获取完整的进程 cmdline 等等 更近一步的想法和实验大家可能对于 ICMP 这样的冷门协议没有太明显的感觉，那么我们换个需求大家可能就更为有感觉了 监控机器上哪些进程在发出 HTTP 1.1 请求 嗯，一如往的，我们先来看一下系统中的关键调用 嗯，这里我们选择 tcp_sendmsg 来作为我们的切入点 12345678910int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size){ int ret; lock_sock(sk); ret = tcp_sendmsg_locked(sk, msg, size); release_sock(sk); return ret;} 嗯，其中 sock 是包含我们一些关键元数据的结构体 12345678910111213141516171819202122232425262728293031323334struct sock { /* * Now struct inet_timewait_sock also uses sock_common, so please just * don't add nothing before this first member (__sk_common) --acme */ struct sock_common __sk_common; ...}struct sock_common { /* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned * address on 64bit arches : cf INET_MATCH() */ union { __addrpair skc_addrpair; struct { __be32 skc_daddr; __be32 skc_rcv_saddr; }; }; union { unsigned int skc_hash; __u16 skc_u16hashes[2]; }; /* skc_dport &amp;&amp; skc_num must be grouped as well */ union { __portpair skc_portpair; struct { __be16 skc_dport; __u16 skc_num; }; }; ...} 大家可以看到，我们能在 sock 中获取到我们端口的五元组数据，然后我们从 msghdr 中能获取到具体的数据 那么，以我们需求中的 HTTP 为例，我们实际上只需要判断，我们获取到的 TCP 包中是否包含 HTTP/1.1 ，便可粗略判断，这个请求是否是 HTTP 1.1 请求（很暴力的做法Hhhhh OK，我们来看下代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102from bcc import BPFimport ctypesimport binasciibpf_text = &quot;&quot;&quot;#include &lt;linux/ptrace.h&gt;#include &lt;linux/ip.h&gt;#include &lt;linux/tcp.h&gt;#include &lt;uapi/linux/ptrace.h&gt;#include &lt;net/sock.h&gt;#include &lt;bcc/proto.h&gt;#include &lt;linux/socket.h&gt;struct ipv4_data_t { u32 pid; u64 ip; u32 saddr; u32 daddr; u16 lport; u16 dport; u64 state; u64 type; u8 data[300]; u16 data_size;};BPF_PERF_OUTPUT(ipv4_events);int trace_event(struct pt_regs *ctx,struct sock *sk, struct msghdr *msg, size_t size){ if (sk == NULL) return 0; u32 pid = bpf_get_current_pid_tgid() &gt;&gt; 32; // pull in details u16 family = sk-&gt;__sk_common.skc_family; u16 lport = sk-&gt;__sk_common.skc_num; u16 dport = sk-&gt;__sk_common.skc_dport; char state = sk-&gt;__sk_common.skc_state; if (family == AF_INET) { struct ipv4_data_t data4 = {}; data4.pid = pid; data4.ip = 4; //data4.type = type; data4.saddr = sk-&gt;__sk_common.skc_rcv_saddr; data4.daddr = sk-&gt;__sk_common.skc_daddr; // lport is host order data4.lport = lport; data4.dport = ntohs(dport); data4.state = state; struct iov_iter temp_iov_iter=msg-&gt;msg_iter; struct iovec *temp_iov=temp_iov_iter.iov; bpf_probe_read_kernel(&amp;data4.data_size, 4, &amp;temp_iov-&gt;iov_len); u8 * temp_ptr; bpf_probe_read_kernel(&amp;temp_ptr, sizeof(temp_ptr), &amp;temp_iov-&gt;iov_base); bpf_probe_read_kernel(&amp;data4.data, sizeof(data4.data), temp_ptr); ipv4_events.perf_submit(ctx, &amp;data4, sizeof(data4)); } return 0;}&quot;&quot;&quot;bpf = BPF(text=bpf_text)filters = {}def parse_ip_address(data): results = [0, 0, 0, 0] results[3] = data &amp; 0xFF results[2] = (data &gt;&gt; 8) &amp; 0xFF results[1] = (data &gt;&gt; 16) &amp; 0xFF results[0] = (data &gt;&gt; 24) &amp; 0xFF return &quot;.&quot;.join([str(i) for i in results[::-1]])def print_http_payload(cpu, data, size): # event = b[&quot;probe_icmp_events&quot;].event(data) # event = ctypes.cast(data, ctypes.POINTER(IcmpSamples)).contents event= bpf[&quot;ipv4_events&quot;].event(data) daddress = parse_ip_address(event.daddr) # data=list(event.data) # temp=binascii.hexlify(data) body = bytearray(event.data).hex() if &quot;48 54 54 50 2f 31 2e 31&quot;.replace(&quot; &quot;, &quot;&quot;) in body: # if &quot;68747470&quot; in temp.decode(): print( f&quot;pid:{event.pid}, daddress:{daddress}, saddress:{parse_ip_address(event.saddr)}, {event.lport}, {event.dport}, {event.data_size}&quot; )bpf.attach_kprobe(event=&quot;tcp_sendmsg&quot;, fn_name=&quot;trace_event&quot;)bpf[&quot;ipv4_events&quot;].open_perf_buffer(print_http_payload)while 1: try: bpf.perf_buffer_poll() except KeyboardInterrupt: exit() OK，我们来看下效果 实际上这个我们还可以再扩展一下。比如针对 Go 这样，所发出的 HTTPS 连接有着固定特征的语言，我们也可以用相对简单的做法去完成机器上的包来源的溯源（大家可以参考下无辄的这篇文章，为什么用 Go 访问某网站始终会 503 Service Unavailable ？) 我自己也做了一个测试，大家可以参考下代码：https://github.com/Zheaoli/linux-traceing-script/blob/main/ebpf/go-https-tracing.py 总结实际上无论是 eBPF 还是 SystemTap ，这类动态 tracing 技术可以 Linux Kernel 变得更具被可编程性。相较于传统的 recompile kernel 这些手段来说，更为方便快捷。而 BCC/BPFTrace 这类的更进一步的封装框架的出现，更进一步的降低了我们去观测内核的难度 很多时候我们很多需求都可以选择旁路的方式去更快捷的实现。但是要注意的一点是，动态 tracing 技术的引入势必增加了内核的不稳定性，而且一定程度上会影响性能。所以我们需要根据具体的场景去做 trade-off 好了，这篇文章差不多就水到这里，后面有时间争取出一个 eBPF 从入门到入土的系列文章（flag++","link":"/posts/2021/04/17/how-to-tracing-package-in-the-linux-kernel/"},{"title":"你所不知道的 Flask Part1:Route 初探","text":"前言我自己都记不清楚上一次写博客是什么时候了（笑），上一次挖的坑现在还没填完，干脆，开个新坑吧，你不知道的 Flask ，记录下自己用 Flask 过程中一些很好玩的东西，当然很大可能我又会中途弃坑 开篇引子之前遇到一个很奇怪的需求，需要在flask中支持正则表达式比如，@app.route('/api/(.*?)') 这样，在视图函数被调用的时候，能传入 URL 中正则匹配的值。不过 Flask 路由中默认不支持这样的方法，那么我们该怎么办？我们先思考五分钟吧？ 好了，我先给出解决方案吧 12345678910from flask import Flaskfrom werkzeug.routing import BaseConverterclass RegexConverter(BaseConverter): def __init__(self, map, *args): self.map = map self.regex = args[0]app = Flask(__name__)app.url_map.converters['regex'] = RegexConverter 在经过这样的设置后我们便可以按照我们刚才的需求写代码了 12345@app.route('/docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt;')def hello(url=None): print(url) 在这里，我们函数中传入的url变量，就是我们代码中所匹配到的值 但是为什么这样就OK了呢？ 详解首先，我们要弄清楚一个东西，Flask 是 基于 Werkzurg 的一个框架，Flask 的 Route 机制基于 Werkzurg 上更进一步封装所得到的，OK，我们上面所以实现的 Converter 便是利用了 Werkzurg 中的 Route 的特性 好了，我先给出官方文档 custom-converters 然后我们来仔细讲讲， 首先，Werkzurg 中存在着一种机制叫做 Converter ，简而言之就是通过一定的特殊语法，将 URL 中的特定部分，转化成特定的 Python 变量，其语法格式为 /url/&lt;converter_name(&quot;表达式&quot;):变量名&gt; 看起来有点复杂对吧，OK 用我们之前的例子来讲一下吧，你看，我们之前定义了一个 '/docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt;' 的 URL ，其中后面部分就是利用了我们提到的 Converter 语法。具体的含义是，这个部分的 url 交给 regex 这个 Converter 来处理，最终生成的变量名为 url。 好了，我们来说说自定义 Converter 参数中的注意事项，在构建一个自己的 Converter 过程中，我们将按照如下的方式编写代码 1234class RegexConverter(BaseConverter): def __init__(self, map, regex,*args): self.map = map self.regex = regex map 是指 werkzurg.routing 中的 Map 对象，而 regex 则是指你所写的表达式。其中 map 的作用我们将放在下一章进行讲解，（又立flag了，笑）。 好了这里差不多完成了，我们来看看 Flask 喔，不，werkzurg 中怎么实现的这样的方法吧 简明代码剖析最前面，你首先得有一点 flask 装饰器路由的知识，详情可以参考这篇文章，菜鸟阅读 Flask 源码系列（1）：Flask的router初探 首先在 werkzurg 框架的 routing 文件中，存在着这样一段代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374_rule_re = re.compile(r''' (?P&lt;static&gt;[^&lt;]*) # static rule data &lt; (?: (?P&lt;converter&gt;[a-zA-Z_][a-zA-Z0-9_]*) # converter name (?:\\((?P&lt;args&gt;.*?)\\))? # converter arguments \\: # variable delimiter )? (?P&lt;variable&gt;[a-zA-Z_][a-zA-Z0-9_]*) # variable name &gt;''', re.VERBOSE)_simple_rule_re = re.compile(r'&lt;([^&gt;]+)&gt;')_converter_args_re = re.compile(r''' ((?P&lt;name&gt;\\w+)\\s*=\\s*)? (?P&lt;value&gt; True|False| \\d+.\\d+| \\d+.| \\d+| \\w+| [urUR]?(?P&lt;stringval&gt;&quot;[^&quot;]*?&quot;|'[^']*') )\\s*,''', re.VERBOSE | re.UNICODE)def parse_converter_args(argstr): argstr += ',' args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group('stringval') if value is None: value = item.group('value') value = _pythonize(value) if not item.group('name'): args.append(value) else: name = item.group('name') kwargs[name] = value return tuple(args), kwargsdef parse_rule(rule): &quot;&quot;&quot;Parse a rule and return it as generator. Each iteration yields tuples in the form ``(converter, arguments, variable)``. If the converter is `None` it's a static url part, otherwise it's a dynamic one. :internal: &quot;&quot;&quot; pos = 0 end = len(rule) do_match = _rule_re.match used_names = set() while pos &lt; end: m = do_match(rule, pos) if m is None: break data = m.groupdict() if data['static']: yield None, None, data['static'] variable = data['variable'] converter = data['converter'] or 'default' if variable in used_names: raise ValueError('variable name %r used twice.' % variable) used_names.add(variable) yield converter, data['args'] or None, variable pos = m.end() if pos &lt; end: remaining = rule[pos:] if '&gt;' in remaining or '&lt;' in remaining: raise ValueError('malformed url rule: %r' % rule) yield None, None, remaining 首先，_rule_re 以及 _converter_args_re 两段是很骚的正则表达式，不过作者已经给出了足够的注释，大家可以对照着正则表达式的语法进行学习一个，然后 parse_converter_args 以及 parse_rule 则是利用正则表达式对其进行解析操作。 OK，我们紧接着往下查看 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def compile(self): &quot;&quot;&quot;Compiles the regular expression and stores it.&quot;&quot;&quot; assert self.map is not None, 'rule not bound' if self.map.host_matching: domain_rule = self.host or '' else: domain_rule = self.subdomain or '' self._trace = [] self._converters = {} self._weights = [] regex_parts = [] def _build_regex(rule): for converter, arguments, variable in parse_rule(rule): if converter is None: regex_parts.append(re.escape(variable)) self._trace.append((False, variable)) for part in variable.split('/'): if part: self._weights.append((0, -len(part))) else: if arguments: c_args, c_kwargs = parse_converter_args(arguments) else: c_args = () c_kwargs = {} convobj = self.get_converter( variable, converter, c_args, c_kwargs) regex_parts.append('(?P&lt;%s&gt;%s)' % (variable, convobj.regex)) self._converters[variable] = convobj self._trace.append((True, variable)) self._weights.append((1, convobj.weight)) self.arguments.add(str(variable)) _build_regex(domain_rule) regex_parts.append('\\\\|') self._trace.append((False, '|')) _build_regex(self.is_leaf and self.rule or self.rule.rstrip('/')) if not self.is_leaf: self._trace.append((False, '/')) if self.build_only: return regex = r'^%s%s$' % ( u''.join(regex_parts), (not self.is_leaf or not self.strict_slashes) and '(?&lt;!/)(?P&lt;__suffix__&gt;/?)' or '' ) self._regex = re.compile(regex, re.UNICODE) 这是 werkzurg 框架的 routing 文件中 Rule 类种的一部分的源码，其中在 def _build_regex(rule): 之前的是一些准备代码，然后我们接着往下看，for converter, arguments, variable in parse_rule(rule): 这一段代码，就是 URL 解析，通过调用 parse_rule 函数来实现对我们之前提到的 converter 语法进行解析，紧接着，如果 URL 里不存在我们 Converter 的语法，则 converter 为空，我们执行处理其余 URL 的逻辑，如果 converter 存在，进行下面的流程，首先，如果我们在 Converter 语法中设定了解析表达式，那么我们利用 parse_converter_args 函数来处理我们的表达式，方便后续的操作，处理完成后，我们利用 get_converter 方法来初始化我们的 Converter , 代码如下： 123456789def get_converter(self, variable_name, converter_name, args, kwargs): &quot;&quot;&quot;Looks up the converter for the given parameter. .. versionadded:: 0.9 &quot;&quot;&quot; if converter_name not in self.map.converters: raise LookupError('the converter %r does not exist' % converter_name) return self.map.converters[converter_name](self.map, *args, **kwargs) 以我们之前的 demo 为例， 12345678910from flask import Flaskfrom werkzeug.routing import BaseConverterclass RegexConverter(BaseConverter): def __init__(self, map, *args): self.map = map self.regex = args[0]app = Flask(__name__)app.url_map.converters['regex'] = RegexConverter 我们已经添加了一个名为 regex 的 Converter 对象，在 get_converter 方法中我们传入了值为 regex 的 converter_name 变量，紧接着，我们初始化了一个 RegexConverter 对象的实例，然后返回这个实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 def compile(self): &quot;&quot;&quot;Compiles the regular expression and stores it.&quot;&quot;&quot; assert self.map is not None, 'rule not bound' if self.map.host_matching: domain_rule = self.host or '' else: domain_rule = self.subdomain or '' self._trace = [] self._converters = {} self._weights = [] regex_parts = [] def _build_regex(rule): for converter, arguments, variable in parse_rule(rule): if converter is None: regex_parts.append(re.escape(variable)) self._trace.append((False, variable)) for part in variable.split('/'): if part: self._weights.append((0, -len(part))) else: if arguments: c_args, c_kwargs = parse_converter_args(arguments) else: c_args = () c_kwargs = {} convobj = self.get_converter( variable, converter, c_args, c_kwargs)############################################################# 无耻分割线 regex_parts.append('(?P&lt;%s&gt;%s)' % (variable, convobj.regex)) self._converters[variable] = convobj self._trace.append((True, variable)) self._weights.append((1, convobj.weight)) self.arguments.add(str(variable)) _build_regex(domain_rule) regex_parts.append('\\\\|') self._trace.append((False, '|')) _build_regex(self.is_leaf and self.rule or self.rule.rstrip('/')) if not self.is_leaf: self._trace.append((False, '/')) if self.build_only: return regex = r'^%s%s$' % ( u''.join(regex_parts), (not self.is_leaf or not self.strict_slashes) and '(?&lt;!/)(?P&lt;__suffix__&gt;/?)' or '' ) self._regex = re.compile(regex, re.UNICODE) 在分割线后面的代码中，我们对处理后的 url 进行一些收尾的操作，以我们之前的 demo 为例，我们设定的 /docs/model_utils/&lt;regex(&quot;.*&quot;):url&gt; URL 最终转化成 /docs/model_utils/(?P&lt;url&gt;.*) ，编译成 re 对象后赋值给 Rule 实例中的 _regex 变量 好了，我们知道处理的部分后，我们大致来看一下怎么匹配并生成值的吧 12345678910111213141516171819202122232425262728293031323334353637383940414243def match(self, path, method=None): &quot;&quot;&quot;Check if the rule matches a given path. Path is a string in the form ``&quot;subdomain|/path&quot;`` and is assembled by the map. If the map is doing host matching the subdomain part will be the host instead. If the rule matches a dict with the converted values is returned, otherwise the return value is `None`. :internal: &quot;&quot;&quot; if not self.build_only: m = self._regex.search(path) if m is not None: groups = m.groupdict() # we have a folder like part of the url without a trailing # slash and strict slashes enabled. raise an exception that # tells the map to redirect to the same url but with a # trailing slash if self.strict_slashes and not self.is_leaf and \\ not groups.pop('__suffix__') and \\ (method is None or self.methods is None or method in self.methods): raise RequestSlash() # if we are not in strict slashes mode we have to remove # a __suffix__ elif not self.strict_slashes: del groups['__suffix__'] result = {} for name, value in iteritems(groups): try: value = self._converters[name].to_python(value) except ValidationError: return result[str(name)] = value if self.defaults: result.update(self.defaults) if self.alias and self.map.redirect_defaults: raise RequestAliasRedirect(result) return result 这也是 werkzurg 框架的 routing 文件中 Rule 类种的一部分的源码，在这段代码中，首先利用 re 对象中的 search 方法，检测当前传入的 Path 是否匹配，如果匹配的话，进入后续的处理流程，还记得我们之前最终生成的 /docs/model_utils/(?P&lt;url&gt;.*) 么，这里面利用了正则表达式命名组的语法糖，在这里，匹配成功后，Python 的 re 库里给我们提供了一个 groupdict 让我们取出命名组里所代表的值。然后我们调用 conveter 实例里面的 to_python 方法来对我们匹配出来的值进行处理（注：这是 Converter 系列对象中的一个可重载方法，我们可以通过重载这个方法，来对我们匹配到的值进行一些逻辑处理，这个我们还是后面再讲吧，flag++），然后我们把最终的 result 值返回。 最后的最后，Flask 在获取 werkzurg 给出的匹配结果后，将匹配的值，放在 request 实例中的 view_args 变量上，最后通过 dispatch_request 对象传递给我们的视图函数，代码如下 123456789101112131415161718192021def dispatch_request(self): &quot;&quot;&quot;Does the request dispatching. Matches the URL and returns the return value of the view or error handler. This does not have to be a response object. In order to convert the return value to a proper response object, call :func:`make_response`. .. versionchanged:: 0.7 This no longer does the exception handling, this code was moved to the new :meth:`full_dispatch_request`. &quot;&quot;&quot; req = _request_ctx_stack.top.request if req.routing_exception is not None: self.raise_routing_exception(req) rule = req.url_rule # if we provide automatic options for this URL and the # request came with the OPTIONS method, reply automatically if getattr(rule, 'provide_automatic_options', False) \\ and req.method == 'OPTIONS': return self.make_default_options_response() # otherwise dispatch to the handler for that endpoint return self.view_functions[rule.endpoint](**req.view_args) 好了，我们的代码剖析就到此结束 最后想说几句Flask + Werkzurg 是一套设计实现的非常精妙的组合，不过我们在日常的使用中常常忽略了里面的美丽的风景，所以这也是我想写这样剖析代码笔记的文章的原因 好了，给老铁们留几个思考题，欢迎评论区讨论 Flask 为什么不默认支持正则表达式的输入 诸如 PathConverter 这样 Werkzurg 内置的 Converter 为什么在写表达式的时候可以这样 /&lt;path:wikipage&gt;/edit 写，而忽略其中的表达式 前面提到的 parse_converter_args 方法的代码详解 好了，就先这样吧2333 对了，保佑我文章里立的 Flag 都能实现（笑）","link":"/posts/2017/08/13/what-the-fuck-about-flask-part1/"},{"title":"详解Swift的类型检查器","text":"原文链接: Exponential time complexity in the Swift type checker 原文作者: Matt Gallagher 译文出自: 掘金翻译计划 译者: Zheaoli 校对者: geeeeeeeeek, Graning 这篇文章将围绕曾不断使我重写代码的一些 Swift 编译器的报错信息展开： 错误：你的表达式太过于复杂，请将其分解为一些更为简单的表达式。（译者注：原文是 error: expression was too complex to be solved in reasonable time; consider breaking up the expression into distinct sub-expressions） 我会看那个触发错误的例子，谈谈以后由相同底层问题引起以外的编译错误的负面影响。我将会带领你看看在编译过程中发生了什么，然后告诉你，怎样在短时间内去解决这些报错。 我将为编译器设计一种时间复杂度为线性算法来代替原本的指数算法来彻底的解决这个问题，而不需要采用其余更复杂的方法。 正确代码的编译错误如果你尝试在 Swift 3 中编译这段代码，那么将会产生报错信息： 1let a: Double = -(1 + 2) + -(3 + 4) + 5 这段代码无论从哪方面来讲都是合法且正确的代码，从理论上讲，在编译过程中，这段代码将会被优化成一个固定的值。 但是这段代码在编译过程中没有办法通过 Swift 的类型检查。编译器会告诉你这段代码太复杂了。但是，等等，这段代码看起来一点都不复杂不是么。里面包含 5 个变量， 4 次加法操作， 2 次取负值操作和一次强制转换为 Double 类型的操作。 但是，编译器你怎么能说这段仅包含 12 个元素的语句相当复杂呢？ 这里有非常多的表达式在编译的时候会出现同样的问题。大多数表达式包含一些变量，基础的数据操作，可能还有一些重载之类的操作。接下来的表达式在编译时会面对同样的错误信息： 1234567891011let b = String(1) + String(2) + String(3) + String(4)let c = 1 * sqrt(2.0) * 3 * 4 * 5 * 6 * 7let d = [&quot;1&quot; + &quot;2&quot;].reduce(&quot;3&quot;) { &quot;4&quot; + String($0) + String($1) }let e: [(Double) -&gt; String] = [ { v in String(v + v) + &quot;1&quot; }, { v in String(-v) } + &quot;2&quot;, { v in String(Int(v)) + &quot;3&quot; }] 上面的代码都是符合 Swift 语法及编程规则的，但是在编译过程中，它们都没有办法通过类型检查。 需要较长的编译时间编译报错只是 Swift 类型检查器缺陷带来的副作用之一，比如，你可以试试下面这个例子： 1let x = { String(&quot;\\($0)&quot; + &quot;&quot;) + String(&quot;\\($0)&quot; + &quot;&quot;) }(0) 这段代码编译时不会报错，但是在我的电脑上，使用 Swift 2.3 将花费 4s 的时间，如果是使用 Swift 3 将会花费 15s 时间。编译过程中，将会花费大量的时间在类型检查上。 现在，你可能不会遇到太多需要耗费这么多时间的问题，但是一个大型的 Swift 项目中，你将会遇到很多 expression was too complex to be solved in reasonable time 这样的报错信息。 不可预知的操作接下来，我将讲一点 Swift 类型检查器的特性：类型检查器选择尽可能的解决非泛型重载的问题。 编译器中处理这种特定行为的路径下的代码注释对此给出了解释，这是一种避免性能问题的优化手段，用于优化造成 expression was too complex 报错的性能问题。 接下来是一些具体的例子： 1let x = -(1) 这段代码将会编译失败，我们会得到一个 Ambiguous use of operator ‘-‘ 的报错信息。 这段代码并不算很模糊，编译器将会明白我们想要使用一个整数类型的变量，它将会把 1 作为一个 Int 进行处理，同时从标准库中选择如下的重载方式： 1prefix public func -&lt;T : SignedNumber&gt;(x: T) -&gt; T 然而，Swift 只能进行非泛型重载。在这个例子中，Float 、 Double 、 Float80 类型的实现并不完善，编译器无法根据上下文选择使用哪种实现，从而导致了这个报错信息。 某些特定的优化可以对操作符进行优化，但是可能导致如下的一些问题： 12345678910func f(_ x: Float) -&gt; Float { return x }func f&lt;I: Integer&gt;(_ x: I) -&gt; I { return x }let x = f(1)prefix operator %% {}prefix func %%(_ x: Float) -&gt; Float { return x }prefix func %%&lt;I: Integer&gt;(_ x: I) -&gt; I { return x }let y = %%1 在这段代码里，我们定义了两个函数（ f 和一个自定的操作 prefix %% ）。每个函数都进行了两次重载，一个参数为 (Float) -&gt; Float ，另一个是 &lt;I: Integer&gt;(I) -&gt; I。 当调用 f(1) 的时候，将会选择使用 &lt;I: Integer&gt;(I) -&gt; If(1) 的实现，然后 x 将会作为 Int 类型进行处理。这应该是你所期待的方式。 当调用 %%1 时，将会使用 (Float) -&gt; Float 的实现，同时会将 y 作为 Float 类型处理，这和我们所期望的恰恰相反。在编译过程中，编译器选择将 1 作为 Float 处理，而不是作为 Int 处理，虽然作为 Int 处理也同样能正常工作。造成这样情况的原因是，编译器在对方法的进行泛型重载之前就已经先行确定变量的类型。这不是基于前后文一致性的做法，这是编译器对于避免类似于 expression was too complex to be solved 等报错信息以及性能优化上的一种妥协。 在代码中解决上诉问题通常来讲，Swift 里的显示代码太过复杂的缺陷并不是一个太大的问题，当然前提是你不会在单个表达式里使用两个或两个以上的下面列出的特性： 方法重载（包括操作符重载） 常量 不明确类型的闭包 会引导 Swift 进行错误类型转换的表达式 一般而言，如果你不使用如上面所述的特性，那么你一般不会遇到类似于 expression was too complex 的报错信息。然而，如果选择是用了上面所诉的特性，那么你可能会面临一些让你感到困惑的问题。通常，在编写一个足够大小的方法和其余常规代码的时候，将会很容易用到上面这些特性，这意味着有些时候我们可能要仔细考虑怎样避免大量使用上面这些特性。 你肯定是想只通过一点细微的修改来通过编译，而不是大量修改你的代码。接下来的一点小建议可能帮得上一些忙。 当上面所诉的编译报错信息出现时，编译器可能建议你将原表达式分割成不同的子表达式： 123let x_1: Double = -(1 + 2)let x_2: Double = -(3 + 4)let x: Double = x_1 + x_2 + 5 好了，从结果上来看，这样的修改是有效的，但是却让人有点蛋疼，特别是在分解成子表达式的时候会明显破坏代码可读性的时候。 另一个建议是通过显示类型转换，减少编译器在编译过程中对方法和操作符重载的选取次数。 1let x: Double = -(1 + 2) as Double + -(3 + 4) as Double + 5 上面这种做法避免了在使用 (Float) -&gt; Float 或者是 (Float80) -&gt; Float80 编译器需要去查找相对应的负号重载。这样的做法很有效的将编译过程中编译器的6次查找相对应的方法重载过程降至4次。 在上面的处理方式中有一个点要注意一下：不同于其余语言，在 Swift 中 Double(x) 并不等同于 x as Double。构造函数通常会如同普通方法一样，当有不同参数的重载需求时，编译器还是会将构造函数的各种重载加入到搜索空间中（尽管这些重载可能在代码中的不同的位置）。在前面所举的例子里，通过在括号前用 Double 进行显示类型转换会解决一部分问题（这种方法有利于编译器进行类型检查），同时在一些情况下，采用这种方法会导致出现一些其余的问题（请参见本文开始所举的关于 String 的例子）。最终， 使用as 操作符是在不增加复杂度的情况下解决这类的问题的最好方式。幸运的是，as 操作符的优先级比大多数二元运算符更高，这样我们可以在大多数的情况下使用它。 另一种方法是使用一个独立命名的自定义函数： 1let x: Double = myCustomDoubleNegation(1 + 2) + myCustomDoubleNegation(3 + 4) + 5 这种方法可以解决之前方法重载所带来的一系列问题。然而，在一系列轻量级的代码里使用这种方式会让我们的代码显得格外的丑陋。 好了，让我们来说说最后的方法，在很多情况下，你可以根据情况自行替换方法和操作符： 1let x: Double = (1 + 2).negated() + (3 + 4).negated() + 5 因为在使用对应方法时，和使用常见算数运算符相比，会有效的减少重载次数，同时使用 . 操作符时其效率相较于直接调用方法更高，因此，这种方法能有效解决我们前面所提到的问题。 Swift类型约束系统简析编译时出现的 expression was too complex 错误是由 Swift 编译器的语义分析系统所抛出的。语义分析系统的意义在于解决整个代码里的类型问题，从而确保输入表达式的类型是正确且安全的。 最重要的是，整个报错信息是由the constraints system solver (CSSolver.cpp)里所编写的语义分析系统所定义的。类型约束系统将从 Swift 的表达式里构建一个由类型和方法组成的图，并根据节点之间的关系来对代码进行约束。约束系统将对每个节点进行推算直至每个节点都已获得明确的类型约束。 讲真，上面的东西可能太抽象了，让我们看点具体的例子吧。 1let a = 1 + 2 类型约束系统将表达式解析成下面这个样子： 每个节点的名字都以 T 开头（意味着需要待确定明确的类型），然后它们用来代表需要解决的类型约束或者方法重载。在这个图里，这些节点被如下的规则所约束： T1 是 ExpressibleByIntegerLiteral 类型 T2 是 ExpressibleByIntegerLiteral 类型 T0 是一个传入 (T1,T2) 返回 T3 的方法 T0 是 infix + ，其在 Swift 里有28种实现 T3 与 T4 之间可以进行交换 小贴士: 在 Swift 2.X 中，ExpressibleByIntegerLiteral 的替代者是 IntegerLiteralConvertible 在这个系统中，类型约束系统遵循着 最小分离 原则。分割出来的单元被这样一个规则所约束着，即，每个单元都是一个拥有一套独立值的个体。在上面的这个例子里，实际上只有一个最小单元：在上述的约束 4 里，T0 发生了重载。在重载之时，编译器选择了 infix + 实现列表里第一种实现：即签名是 (Int, Int) -&gt; Int 的实现。 通过上述这个最小的单元，类型约束系统开始对元素进行类型约束：根据约束 3 T1、 T2 、 T3 被确定为 Int 类型，根据约束 4 ， T4 同样被确认为 Int 类型。 在 T1 、 T2 被确定为 Int 之后（最开始它们被认为是 ExpressibleByIntegerLiteral）， infix + 的重载方式便已经确定，这个时候编译器便不需要再考虑其余可行性，并把其当做最终的解决方案。我们在确定每个节点对应的类型后，我们便可以选择我们所需要的重载方法了。 让我们看点复杂的例子吧！到目前为止，并没有什么超出我们意料之中的异常出现，你可能想象不到当表达式开始变得复杂之时， Swift 的编译系统将会开始不断的出现错误信息。来让我们修改下上面的例子：第一·将 2 放在括号里，第二·添加负号操作符，第三·规定返回值为 Double 类型。 1let a: Double = 1 + -(2) 整个节点结构如下图所述： 节点约束如下： T1 是 ExpressibleByIntegerLiteral 类型 T3 是 ExpressibleByIntegerLiteral 类型 T2 是一个传入 T3 返回 T4 的方法 T2 是 prefix -，其在 Swift 里有6种实现 T0 是一个传入 T1、T4，返回 T5 的方法 T0 是 infix + ，其在 Swift 里有28种实现 T5 是 Double 类型 相较于上面的例子，这里多了两个约束，让我们看看类型约束系统会怎样处理这个例子。 第一步：选择最小分离单元。这次是约束 4 ：“ T2 是 prefix -，在 Swift 里有6种实现”。最后系统选择了签名为 (Float) -&gt; Float 的实现。 第二步：和第一步一样，选择最小分离单元，这次是约束 6 ：“T0 是 infix + ，其在 Swift 里有28种实现”。系统选择了签名为 (Int, Int) -&gt; Int 的实现。 最后一步是：利用上述的类型约束确定所有节点的类型。 然而，这里出现了点问题：在第一步里我们选择的签名为 (Float) -&gt; Float 的 prefix - 实现和第二步里我们选择的签名为 (Int, Int) -&gt; Int 的 infix + 实现和我们的约束 5 （T0 是一个传入 T1、T4，返回 T5 的方法）发生了冲突。解决方法是放弃当前的选择，然后重新回滚至第二步，为 T0 最终，系统将遍历所有的 infix + 实现，然后发现没有一种实现同时满足约束 5 和约束 7 （T5 是 Double 类型）。 所以，类型约束系统将回滚至第一步，为 T2 选取了签名为 (Double, Double) -&gt; Double 的实现。最后，这种实现也满足了 T0 的约束。 然而，在发现 Double 类型和 ExpressibleByIntegerLiteral 相互不匹配后，类型约束系统将继续回滚，寻找合适的重载方法。 T2 总共有6种实现，但是最后3种实现不能被优化(因为它们是通用的实现，因此优先级高于显示声明参数为 Double 的实现）。 在类型约束系统里，这种特殊优化是我曾经在Unexpected behaviors一文中提到的快速重载的一些特性。 拜这种特殊的“优化”所赐，类型约束系统需要76次查询才能找到一个合理的解决方案。如果我们添加了其余的一些新的重载，那么这个数字会变得超出我们的想象。例如，我们我们在例子里添加另外一个 infix + 操作符，比如： let a: Double = 0 + 1 + -(2) ，那么将需要1190次查询才能找到合理的解决方案。 查询解决方案的这个过程是一个典型的具有指数时间复杂度的操作。在分离单元里进行搜索的范围称为“笛卡尔积”，然后，对于图中的 n 个分离单元，算法将会在 n 维笛卡尔乘积的范围内进行查找（这是一个空间复杂度同样为指数的操作）。 根据我的测试，单语句内拥有6个分离单元，便足以触发 Swift 中的 expression was too complex 的错误。 线性化的类型约束系统针对本文所反复提到的这个问题，最好的解决方法就是在编译器中进行修复。 类型约束系统之所以采用时间复杂度为指数算法来解决方法重载的问题，是因为 Swift 需要对方法重载所生成的 n 维“笛卡尔乘积”空间里的元素进行遍历并搜索从而确定一个合适的选项（在没有更好方案之前，这应该是最好的方案）。 为了避免生成 n 维笛卡尔乘积空间，我们需要设计一个方法来实现相关逻辑实现的独立性，而不让它们彼此依赖。 在开始之前我必须给你们一个很重要的提醒： 友情提醒，这些东西仅代表我的个人观点：接下来的一些讨论，都是我从理论的角度上来分析怎样在 Swift 的类型约束系统中怎样去解决函数重载的问题。我并没有写一些东西来证明我提出的解决方案，这可能意味着我会忽略某些非常重要的东西。 前提我们想实现如下两个目标： 限制一个节点不应该与其余节点相互依赖或引用 从前一个方法分析出来的分离单元应该与后一个方法分离出来的存在着交集，并进一步简化分离单元的两个约束条件。 第一个目标，可以通过限制节点的约束路径实现。在 Swift 中，每个节点的约束是双向的，每个节点的约束都从表达式的每一个分支开始，然后依照着遍历主干-&gt;线性遍历子节点的方式不断传播。在这个过程中，我们可以有选择性的简单地合并相同的约束逻辑来组合这些约束，而不是从其余节点引用相对应的类型约束。 第二个目标里，支持前面通过减少类型约束的传播复杂度来进一步简化相关约束条件。每个重载方法的分离单元之间最重要的交叉点是一个重载函数的输出，可能会作为另一个重载函数的输入。这个操作应该根据参数相互交叉的两个重载方法所产生的2维笛卡尔积来进行计算。对于其余的可能存在的交叉点来说，给出一个真正意义上的数学上的严格交叉证明是非常困难的，同时这样的证明是没有必要的，我们只需要复制 Swift 里在复杂情况下的对于类型选择的时所采用的贪婪策略即可。 让我们重新看看之前的例子让我们看看如果我们实现了前文所讲的两个目标后，类型约束系统将会变成什么样子。首先让我们复习下之前所生成的节点图： 1let a: Double = 1 + -(2) 然后让我们也复习下以下节点约束： T1 是 ExpressibleByIntegerLiteral 类型 T3 是 ExpressibleByIntegerLiteral 类型 T2 是一个传入 T3 返回 T4 的方法 T2 是 prefix -，其在 Swift 里有6种实现 T0 是一个传入 T1、T4，返回 T5 的方法 T0 是 infix + ，其在 Swift 里有28种实现 T5 是 Double 类型 将节点约束从右至左传递我们从右至左进行遍历（从叶子节点向主干遍历）。 在节点约束从 T3 向 T2 传播时，添加了这样一个新的约束：“ T2 节点的输入值必须是一个由 ExpressibleByIntegerLiteral 转化而来的值”。现在在新的约束规则和原有规则同时发生作用后，一旦我们确认所有拥有 T2 的节点都被新规则约束成功之后，或者是与“特定操作重载优先于通用操作重载（比如在 prefix - 中 Double、 Float 或者是 Float80 会被优先重载）”这条规则冲突之时，便可以丢弃我们新建立的节点约束规则。在节点约束从 T2 向 T4 中传播的过程中，添加新约束为：“ T4 必须是 prefix - 所返回的6中类型的值之一，其中 Double、Float 或 Float80 优先被考虑）。在节点约束从 T4 朝 T0 传播的过程中，添加新约束为：“ T0 的第二个参数必须是从 prefix - 返回的6种参数里的任意一种演变而来，其中 Double、 Float 或 Float80 类型优先）。在结合 T0 已有的节点约束后，T0 的节点约束变为：“ T0 是 infix + 的6种实现之一，同时从右侧传入的参数是来自 prefix - 返回参数中的任意一种，在这个过程中类型是 Double、 Float 或者 Float80 的参数优先被考虑）。在节点约束从 T1 朝 T0 传递之时，没有新的约束条件需要添加（在这里，T0 已经被我们所增加的约束条件严格约束了，同时，原本所使用的 ExpressibleByIntegerLiteral 类型已经被 Double、 Float 或者 Float80 中的任意一种类型所替代了）。在节点约束从 T0 向 T5 传播时，需新增加约束为：“ T5 是 infix + 的6种返回值中的一种，且 infix + 的第二个参数是来自 prefix - 的返回值，在这个过程中，Double、 Float 或者 Float80 类型优先被考虑）。在上述约束的共同作用下，我们可以最终确认 T5 的类型为 Double。 经过上述过程的变动之后，整个节点约束集迭代成下面这个样子： T1 是 ExpressibleByIntegerLiteral 类型 T3 是 ExpressibleByIntegerLiteral 类型 T2 是一个传入 T3 返回 T4 的方法 T2 是 prefix - 的6种实现之一，同时为了满足在 Swift 中特殊操作重载优先级高于通用运算重载的原则，类型为 Double、 Float 或者 Float80 的 prefix - 重载优先被考虑。 T4 是 prefix - 的六种返回值之一，同样为了满足在 Swift 中特殊操作重载优先级高于通用运算重载的原则，类型为 Double、 Float 或者 Float80 的 prefix - 重载优先被考虑。 T0 是一个传入 T1、T4，返回 T5 的方法 T0 是 infix + 的6种实现之一，同时从右侧传入的参数是来自 prefix - 返回参数中的任意一种，在这个过程中为了满足在 Swift 中特殊操作重载优先级高于通用运算重载的原则，类型是 Double、 Float 或者 Float80 的参数优先被考虑 T5 是 Double 类型 将节点约束从左至右传递现在我们开始从左至右进行遍历（先遍历主干，后遍历叶子节点）。 首先从 T5 开始遍历，约束 5 是：“ T5 是 Double 类型的节点”。这时我们为 T0 添加新的约束：“ T0 的返回值类型一定要是 Double 类型的”。在这个约束生效后，我们就可以排除除 (Double, Double) -&gt; Double 之外的 infix + 的重载了。节点约束继续从 T0 朝 T1 传递，根据 infix + 的(Double, Double) -&gt; Double 重载的参数要求，我们为 T1 创建一个新的约束： T1 一定是 Double 类型的。在多种约束的作用下，之前所提到的“T1 是 ExpressibleByIntegerLiteral 类型”变为“T1 是 Double 类型”。在节点约束从 T0 朝 T4 ，根据 infix + 的第二个参数的要求，我们确定 T4 的类型为 Double。节点约束从 T4 朝 T2 传播的过程中，我们新增加一个约束：“ T2 的返回值一定为 Double 类型”。在以上规则共同作用下，我们可以确定 T2 为 prefix - 的参数类型为 (Double) -&gt; Double 重载。最后根据以上的约束，我们可以得知 ‘T3’ 的类型为 ‘Double’。 最后整个类型约束系统编程下面这个样子： T1 为 Double 类型。 T3 为 Double 类型。` T2 是 prefix - 的参数为 (Double) -&gt; Double 类型的重载 T0 是 infix + 的参数为 (Double, Double) -&gt; Double 类型的重载 T5 为 Double 类型。 好了，现在整个类型约束操作便已经告一段落了。 唔，我提出这算法的目的是改善方法重载的相关操作，因此，我将方法重载的次数用 n 表示。然后我将平均每个函数重载次数用 m 表示。 如我前面所述，在 Swift 中，编译器是通过在一个 n 维的笛卡尔积空间内进行搜索来确定最终的结果。它的时间复杂度是 O(m^n) 。 而我所提出的算法，是在一个2维的空间内去搜索 n-1 个分离单元来实现的。其执行时间是 *m^2n**.因为 m 是和 n 相关联的，我们可以得到其最终的时间复杂度为 O(n) 。 通常来讲，在 n 为很大的时候，线性复杂度的算法比指数时间复杂度的算法更能适应当前的状况，不过我们得搞清楚什么样的情况才能被称之为 n 为很大的数。在这个例子中，3 已经是一个非常 “大” 的数了。正如我前面所提到的一样，Swift 自带的类型约束系统将进行1190次搜索来确认最后的结果。而我设计的算法只需要336次搜索。这可以说很明显的降低了最后的耗时。 我做了一个很有趣的实验：在之前所提到的 let a: Double = 1 + -(2) 这个例子里，不管是 Swift 里的类型约束系统，还是我所设计的算法，它们都是在一个2维的笛卡尔积空间内进行搜索，里面都包含了168中可能性。 Swift 里现在所采用的类型约束算法选取了在 prefix - 和 infix + 重载生成的2维笛卡尔积空间内的168种可能性的76种。但是这样做的话，整个过程里会产生567次对 ConstraintSystem::matchTypes的调用，其中546次是用于搜索相适应的重载函数。 我所设计的算法，搜索了全部168种可能性，但是根据我的分析，其最后只产生了22次对 ConstraintSystem::matchTypes 的调用。 去确定一个非公开的算法，需要进行很多次的猜测，所以知道某一种算法的的具体细节是一件非常困难的事儿。但是我想，我的算法在任意数量级的情况下，其表现优于或与现在已有的算法持平并不是一件不可能的事儿。 Swfit 很快会改进他的类型系统么？虽然我很想说：“我一个人就把所有工作做完了，看看这些代码运行的多么完美啊”，但是这也只能是想想罢了。一个整个系统由成千上万个逻辑和单元组成，并不能单独抽出某一个节点来进行讨论。 你觉得 Swift 开发团队是不是在尝试把类型约束系统进行线性化处理呢？我对此持否定看法。 在这篇文章里“[swift-dev] A type-checking performance case study”表明官方开发者认为类型约束系统采用时间复杂度为指数的算法是一件很正常的事儿。与其将时间放在优化算法上，还不如去重构标准库，使其更为合理。 一点吐槽： 现在看来本文的前面两章简直就是在做无用功，我应该静静的将其删除。 我觉得我想法是正确的，类型约束系统应该进行大幅度改进，这样我们次啊不会被上面所提到的问题所困扰。 友情提醒: 理论上将类型约束系统并不是整个语言最主要的一部分，因此如果其进行了改进，应该是在一个小版本迭代中进行发布，而不是一个大版本更新。 结论在我使用 Swift 的经历里, expression was too complex to be solved in reasonable time 是一个经常出线的错误，而且我并不认为这是一个简单的错误。如果你在单个例子中是用了大量的方法或者是数学操作的时候，你应该定期看看这篇文章。 Swift 里所采用的时间复杂度为指数的算法也可能导致编译时间较长的问题。 尽管我没有确切的统计整个编译里的时间分配，但是不出意外的话，系统应该将大部分时间放在了类型约束器的相关计算上。 这个问题可以再我们编写的代码的时候予以避免，但是讲真，没有必要这么做。如果编译器能采用我所提出的线性时间的算法的话，我敢肯定，这些问题都不在是问题。 在编译器做出具体的改变之前，本文所提到的问题会一直困扰着我们，与编译器的斗争还要持续下去。","link":"/posts/2016/08/02/%E8%AF%A6%E8%A7%A3Swift%E7%9A%84%E7%B1%BB%E5%9E%8B%E6%A3%80%E6%9F%A5%E5%99%A8/"},{"title":"用 Python 实现一个最简单的对象模型","text":"原文地址：A Simple Object Model 原文作者：Carl Friedrich Bolz 译文出自：掘金翻译计划 译者：Zheaoli 校对者：Yuze Ma, Gran 一个简单的对象模型Carl Friedrich Bolz 是一位在伦敦国王大学任职的研究员，他沉迷于动态语言的实现及优化等领域而不可自拔。他是 PyPy/RPython 的核心开发者之一，于此同时，他也在为 Prolog, Racket, Smalltalk, PHP 和 Ruby 等语言贡献代码。这是他的 Twitter @cfbolz 。 开篇面向对象编程是目前被广泛使用的一种编程范式，这种编程范式也被大量现代编程语言所支持。虽然大部分语言给程序猿提供了相似的面向对象的机制，但是如果深究细节的话，还是能发现它们之间还是有很多不同的。大部分的语言的共同点在于都拥有对象处理和继承机制。而对于类来说的话，并不是每种语言都完美支持它。比如对于 Self 或者 JavaScript 这样的原型继承的语言来说，是没有类这个概念的，他们的继承行为都是在对象之间所产生的。 深入了解不同语言的对象模型是一件非常有意思的事儿。这样我们可以去欣赏不同的编程语言的相似性。不得不说，这样的经历可以在我们学习新的语言的时候，利用上我们已有的经验，以便于我们快速的掌握它。 这篇文章将会带领你实现一套简单的对象模型。首先我们将实现一个简单的类与其实例，并能够通过这个实例去访问一些方法。这是被诸如 Simula 67 、Smalltalk 等早期面向对象语言所采用的面向对象模型。然后我们会一步步的扩展这个模型，你可以看到接下来两步会为你展现不同语言的模型设计思路，然后最后一步是来优化我们的对象模型的性能。最终我们所得到的模型并不是哪一门真实存在的语言所采用的模型，不过，硬是要说的话，你可以把我们得到的最终模型视为一个低配版的 Python 对象模型。 这篇文章里所展现的对象模型都是基于 Python 实现的。代码在 Python 2.7 以及 Python 3.4 上都可以完美运行。为了让大家更好的了解模型里的设计哲学，本文也为我们所设计的对象模型准备了单元测试，这些测试代码可以利用 py.test 或者 nose 来运行。 讲真，用 Python 来作为对象模型的实现语言并不是一个好的选择。一般而言，语言的虚拟机都是基于 C/C++ 这样更为贴近底层的语言来实现的，同时在实现中需要非常注意很多的细节，以保证其执行效率。不过，Python 这样非常简单的语言能让我们将主要精力都放在不同的行为表现上，而不是纠结于实现细节不可自拔。 基础方法模型我们将以 Smalltalk 中的实现的非常简单的对象模型来开始讲解我们的对象模型。Smalltalk 是一门由施乐帕克研究中心下属的 Alan Kay 所带领的小组在 70 年代所开发出的一门面向对象语言。它普及了面向对象编程，同时在今天的编程语言中依然能看到当时它所包含的很多特性。在 Smalltalk 核心设计原则之一便是：“万物皆对象”。Smalltalk 最广为人知的继承者是 Ruby，一门使用类似 C 语言语法的同时保留了 Smalltalk 对象模型的语言。 在这一部分中，我们所实现的对象模型将包含类，实例，属性的调用及修改，方法的调用，同时允许子类的存在。开始前，先声明一下，这里的类都是有他们自己的属性和方法的普通的类 友情提示：在这篇文章中，“实例”代表着“不是类的对象”的含义。 一个非常好的习惯就是优先编写测试代码，以此来约束具体实现的行为。本文所编写的测试代码由两个部分组成。第一部分由常规的 Python 代码组成，可能会使用到 Python 中的类及其余一些更高级的特性。第二部分将会用我们自己建立的对象模型来替代 Python 的类。 在编写测试代码时，我们需要手动维护常规的 Python 类和我们自建类之间的映射关系。比如，在我们自定类中将会使用 obj.read_attr(&quot;attribute&quot;) 来作为 Python 中的 obj.attribute 的替代品。在现实生活中，这样的映射关系将由语言的编译器/解释器来进行实现。 在本文中，我们还对模型进行了进一步简化，这样看起来我们实现对象模型的代码和和编写对象中方法的代码看起来没什么两样。在现实生活中，这同样是基本不可能的，一般而言，这两者都是由不同的语言实现的。 首先，让我们来编写一段用于测试读取求改对象字段的代码： 1234567891011121314151617181920212223242526def test_read_write_field(): # Python code class A(object): pass obj = A() obj.a = 1 assert obj.a == 1 obj.b = 5 assert obj.a == 1 assert obj.b == 5 obj.a = 2 assert obj.a == 2 assert obj.b == 5 # Object model code A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;a&quot;, 1) assert obj.read_attr(&quot;a&quot;) == 1 obj.write_attr(&quot;b&quot;, 5) assert obj.read_attr(&quot;a&quot;) == 1 assert obj.read_attr(&quot;b&quot;) == 5 obj.write_attr(&quot;a&quot;, 2) assert obj.read_attr(&quot;a&quot;) == 2 assert obj.read_attr(&quot;b&quot;) == 5 在上面这个测试代码中包含了我们必须实现的三个东西。Class 以及 Instance 类分别代表着我们对象中的类以及实例。同时这里有两个特殊的类的实例：OBJECT 和 TYPE。 OBJECT 对应的是作为 Python 继承系统起点的 object 类（译者注：在 Python 2.x 版本中，实际上是有两套类系统，一套被统称为 new style class , 一套被称为 old style class ，object 是 new style class 的基类）。TYPE 对应的是 Python 类型系统中的 type 。 为了给 Class 以及 Instance 类的实例提供通用操作支持，这两个类都会从 Base 类这样提供了一系列方法的基础类中进行继承并实现： 123456789101112131415161718192021222324252627class Base(object): &quot;&quot;&quot; The base class that all of the object model classes inherit from. &quot;&quot;&quot; def __init__(self, cls, fields): &quot;&quot;&quot; Every object has a class. &quot;&quot;&quot; self.cls = cls self._fields = fields def read_attr(self, fieldname): &quot;&quot;&quot; read field 'fieldname' out of the object &quot;&quot;&quot; return self._read_dict(fieldname) def write_attr(self, fieldname, value): &quot;&quot;&quot; write field 'fieldname' into the object &quot;&quot;&quot; self._write_dict(fieldname, value) def isinstance(self, cls): &quot;&quot;&quot; return True if the object is an instance of class cls &quot;&quot;&quot; return self.cls.issubclass(cls) def callmethod(self, methname, *args): &quot;&quot;&quot; call method 'methname' with arguments 'args' on object &quot;&quot;&quot; meth = self.cls._read_from_class(methname) return meth(self, *args) def _read_dict(self, fieldname): &quot;&quot;&quot; read an field 'fieldname' out of the object's dict &quot;&quot;&quot; return self._fields.get(fieldname, MISSING) def _write_dict(self, fieldname, value): &quot;&quot;&quot; write a field 'fieldname' into the object's dict &quot;&quot;&quot; self._fields[fieldname] = valueMISSING = object() Base 实现了对象类的储存，同时也使用了一个字典来保存对象字段的值。现在，我们需要去实现 Class 以及 Instance 类。在Instance 的构造器中将会完成类的实例化以及 fields 和 dict 初始化的操作。换句话说，Instance 只是 Base 的子类，同时并不会为其添加额外的方法。 Class 的构造器将会接受类名、基础类、类字典、以及元类这样几个操作。对于类来讲，上面几个变量都会在类初始化的时候由用户传递给构造器。同时构造器也会从它的基类那里获取变量的默认值。不过这个点，我们将在下一章节进行讲述。 123456789101112class Instance(Base): &quot;&quot;&quot;Instance of a user-defined class. &quot;&quot;&quot; def __init__(self, cls): assert isinstance(cls, Class) Base.__init__(self, cls, {})class Class(Base): &quot;&quot;&quot; A User-defined class. &quot;&quot;&quot; def __init__(self, name, base_class, fields, metaclass): Base.__init__(self, metaclass, fields) self.name = name self.base_class = base_class 同时，你可能注意到这点，类依旧是一种特殊的对象，他们间接的从 Base 中继承。因此，类也是一个特殊类的特殊实例，这样的很特殊的类叫做：元类。 现在，我们可以顺利通过我们第一组测试。不过这里，我们还没有定义 Type 以及 OBJECT 这样两个 Class 的实例。对于这些东西，我们将不会按照 Smalltalk 的对象模型进行构建，因为 Smalltalk 的对象模型对于我们来说太过于复杂。作为替代品，我们将采用 ObjVlisp1 的类型系统，Python 的类型系统从这里吸收了不少东西。 在 ObjVlisp 的对象模型中，OBJECT 以及 TYPE 是交杂在一起的。OBJECT 是所有类的母类，意味着 OBJECT 没有母类。TYPE 是 OBJECT 的子类。一般而言，每一个类都是 TYPE 的实例。在特定情况下，TYPE 和 OBJECT 都是 TYPE 的实例。不过，程序猿可以从 TYPE 派生出一个类去作为元类： 123456789# set up the base hierarchy as in Python (the ObjVLisp model)# the ultimate base class is OBJECTOBJECT = Class(name=&quot;object&quot;, base_class=None, fields={}, metaclass=None)# TYPE is a subclass of OBJECTTYPE = Class(name=&quot;type&quot;, base_class=OBJECT, fields={}, metaclass=None)# TYPE is an instance of itselfTYPE.cls = TYPE# OBJECT is an instance of TYPEOBJECT.cls = TYPE 为了去编写一个新的元类，我们需要自行从 TYPE 进行派生。不过在本文中我们并不会这么做，我们将只会使用 TYPE 作为我们每个类的元类。 好了，现在第一组测试已经完全通过了。现在让我们来看看第二组测试，我们将会在这组测试中测试对象属性读写是否正常。这段代码还是很好写的。 123456789101112131415def test_read_write_field_class(): # classes are objects too # Python code class A(object): pass A.a = 1 assert A.a == 1 A.a = 6 assert A.a == 6 # Object model code A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;a&quot;: 1}, metaclass=TYPE) assert A.read_attr(&quot;a&quot;) == 1 A.write_attr(&quot;a&quot;, 5) assert A.read_attr(&quot;a&quot;) == 5 isinstance 检查到目前为止，我们还没有将对象有类这点特性利用起来。接下来的测试代码将会自动的实现 isinstance 。 1234567891011121314151617181920def test_isinstance(): # Python code class A(object): pass class B(A): pass b = B() assert isinstance(b, B) assert isinstance(b, A) assert isinstance(b, object) assert not isinstance(b, type) # Object model code A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={}, metaclass=TYPE) B = Class(name=&quot;B&quot;, base_class=A, fields={}, metaclass=TYPE) b = Instance(B) assert b.isinstance(B) assert b.isinstance(A) assert b.isinstance(OBJECT) assert not b.isinstance(TYPE) 我们可以通过检查 cls 是不是 obj 类或者它自己的超类来判断 obj 对象是不是某些类 cls 的实例。通过检查一个类是否在一个超类链上工作，来判断一个类是不是另一个类的超类。如果还有其余类存在于这个超类链上，那么这些类也可以被称为是超类。这个包含了超类和类本身的链条，被称之为方法解析顺序（译者注：简称MRO）。它很容易以递归的方式进行计算： 12345678910111213class Class(Base): ... def method_resolution_order(self): &quot;&quot;&quot; compute the method resolution order of the class &quot;&quot;&quot; if self.base_class is None: return [self] else: return [self] + self.base_class.method_resolution_order() def issubclass(self, cls): &quot;&quot;&quot; is self a subclass of cls? &quot;&quot;&quot; return cls in self.method_resolution_order() 好了，在修改代码后，测试就完全能通过了 方法调用前面所建立的对象模型中还缺少了方法调用这样的重要特性。在本章我们将会建立一个简单的继承模型。 123456789101112131415161718192021222324252627def test_callmethod_simple(): # Python code class A(object): def f(self): return self.x + 1 obj = A() obj.x = 1 assert obj.f() == 2 class B(A): pass obj = B() obj.x = 1 assert obj.f() == 2 # works on subclass too # Object model code def f_A(self): return self.read_attr(&quot;x&quot;) + 1 A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;f&quot;: f_A}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;x&quot;, 1) assert obj.callmethod(&quot;f&quot;) == 2 B = Class(name=&quot;B&quot;, base_class=A, fields={}, metaclass=TYPE) obj = Instance(B) obj.write_attr(&quot;x&quot;, 2) assert obj.callmethod(&quot;f&quot;) == 3 为了找到调用对象方法的正确实现，我们现在开始讨论类对象的方法解析顺序。在 MRO 中我们所寻找到的类对象字典中第一个方法将会被调用： 12345678class Class(Base): ... def _read_from_class(self, methname): for cls in self.method_resolution_order(): if methname in cls._fields: return cls._fields[methname] return MISSING 在完成 Base 类中 callmethod 实现后，可以通过上面的测试。 为了保证函数参数传递正确，同时也确保我们事先的代码能完成方法重载的功能，我们可以编写下面这段测试代码，当然结果是完美通过测试： 123456789101112131415161718192021222324252627282930def test_callmethod_subclassing_and_arguments(): # Python code class A(object): def g(self, arg): return self.x + arg obj = A() obj.x = 1 assert obj.g(4) == 5 class B(A): def g(self, arg): return self.x + arg * 2 obj = B() obj.x = 4 assert obj.g(4) == 12 # Object model code def g_A(self, arg): return self.read_attr(&quot;x&quot;) + arg A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;g&quot;: g_A}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;x&quot;, 1) assert obj.callmethod(&quot;g&quot;, 4) == 5 def g_B(self, arg): return self.read_attr(&quot;x&quot;) + arg * 2 B = Class(name=&quot;B&quot;, base_class=A, fields={&quot;g&quot;: g_B}, metaclass=TYPE) obj = Instance(B) obj.write_attr(&quot;x&quot;, 4) assert obj.callmethod(&quot;g&quot;, 4) == 12 基础属性模型现在最简单版本的对象模型已经可以开始工作了，不过我们还需要去不断的改进。这一部分将会介绍基础方法模型和基础属性模型之间的差异。这也是 Smalltalk 、 Ruby 、 JavaScript 、 Python 和 Lua 之间的核心差异。 基础方法模型将会按照最原始的方式去调用方法： 1result = obj.f(arg1, arg2) 基础属性模型将会将调用过程分为两步：寻找属性，以及返回执行结果： 12method = obj.fresult = method(arg1, arg2) 你可以在接下来的测试中体会到前文所述的差异： 12345678910111213141516171819202122232425262728293031def test_bound_method(): # Python code class A(object): def f(self, a): return self.x + a + 1 obj = A() obj.x = 2 m = obj.f assert m(4) == 7 class B(A): pass obj = B() obj.x = 1 m = obj.f assert m(10) == 12 # works on subclass too # Object model code def f_A(self, a): return self.read_attr(&quot;x&quot;) + a + 1 A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;f&quot;: f_A}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;x&quot;, 2) m = obj.read_attr(&quot;f&quot;) assert m(4) == 7 B = Class(name=&quot;B&quot;, base_class=A, fields={}, metaclass=TYPE) obj = Instance(B) obj.write_attr(&quot;x&quot;, 1) m = obj.read_attr(&quot;f&quot;) assert m(10) == 12 我们可以按照之前测试代码中对方法调用设置一样的步骤去设置属性调用，不过和方法调用相比，这里面发生了一些变化。首先，我们将会在对象中寻找与函数名对应的方法名。这样一个查找过程结果被称之为已绑定的方法，具体来说就是，这个结果一个绑定了方法与具体对象的特殊对象。然后这个绑定方法会在接下来的操作中被调用。 为了实现这样的操作，我们需要修改 Base.read_attr 的实现。如果在实例字典中没有找到对应的属性，那么我们需要去在类字典中查找。如果在类字典中查找到了这个属性，那么我们将会执行方法绑定的操作。我们可以使用一个闭包来很简单的模拟绑定方法。除了更改 Base.read_attr 实现以外，我们也可以修改 Base.callmethod 方法来确保我们代码能通过测试。 1234567891011121314151617181920212223242526class Base(object): ... def read_attr(self, fieldname): &quot;&quot;&quot; read field 'fieldname' out of the object &quot;&quot;&quot; result = self._read_dict(fieldname) if result is not MISSING: return result result = self.cls._read_from_class(fieldname) if _is_bindable(result): return _make_boundmethod(result, self) if result is not MISSING: return result raise AttributeError(fieldname) def callmethod(self, methname, *args): &quot;&quot;&quot; call method 'methname' with arguments 'args' on object &quot;&quot;&quot; meth = self.read_attr(methname) return meth(*args)def _is_bindable(meth): return callable(meth)def _make_boundmethod(meth, self): def bound(*args): return meth(self, *args) return bound 其余的代码并不需要修改。 元对象协议除了常规的类方法之外，很多动态语言还支持特殊方法。有这样一些方法在调用时是由对象系统调用而不是使用常规调用。在 Python 中你可以看到这些方法的方法名用两个下划线作为开头和结束的，比如 __init__ 。特殊方法可以用于重载一些常规操作，同时可以提供一些自定义的功能。因此，它们的存在可以告诉对象模型如何自动的处理不同的事情。Python 中相关特殊方法的说明可以查看这篇文档。 元对象协议这一概念由 Smalltalk 引入，然后在诸如 CLOS 这样的通用 Lisp 的对象模型中也广泛的使用这个概念。这个概念包含特殊方法的集合（注：这里没有查到 coined3 的梗，请校者帮忙参考）。 在这一章中，我们将会为我们的对象模型添加三个元调用操作。它们将会用来对我们读取和修改对象的操作进行更为精细的控制。我们首先要添加的两个方法是 __getattr__ 和 __setattr__， 这两个方法的命名看起来和我们 Python 中相同功能函数的方法名很相似。 自定义属性读写操作__getattr__ 方法将会在属性通过常规方法无法查找到的情况下被调用，换句话说，在实例字典、类字典、父类字典等等对象中都找不到对应的属性时，会触发该方法的调用。我们将传入一个被查找属性的名字作为这个方法的参数。在早期的 Smalltalk4 中这个方法被称为 doesNotUnderstand: 。 在 __setattr__ 这里事情可能发生了点变化。首先我们需要明确一点的是，设置一个属性的时候通常意味着我们需要创建它，在这个时候，在设置属性的时候通常会触发 __setattr__ 方法。为了确保 __setattr__ 的存在，我们需要在 OBJECT 对象中实现 __setattr__ 方法。这样最基础的实现完成了我们向相对应的字典里写入属性的操作。这可以使得用户可以将自己定义的 __setattr__ 委托给 OBJECT.__setattr__ 方法。 针对这两个特殊方法的测试用例如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def test_getattr(): # Python code class A(object): def __getattr__(self, name): if name == &quot;fahrenheit&quot;: return self.celsius * 9\\. / 5\\. + 32 raise AttributeError(name) def __setattr__(self, name, value): if name == &quot;fahrenheit&quot;: self.celsius = (value - 32) * 5\\. / 9. else: # call the base implementation object.__setattr__(self, name, value) obj = A() obj.celsius = 30 assert obj.fahrenheit == 86 # test __getattr__ obj.celsius = 40 assert obj.fahrenheit == 104 obj.fahrenheit = 86 # test __setattr__ assert obj.celsius == 30 assert obj.fahrenheit == 86 # Object model code def __getattr__(self, name): if name == &quot;fahrenheit&quot;: return self.read_attr(&quot;celsius&quot;) * 9\\. / 5\\. + 32 raise AttributeError(name) def __setattr__(self, name, value): if name == &quot;fahrenheit&quot;: self.write_attr(&quot;celsius&quot;, (value - 32) * 5\\. / 9.) else: # call the base implementation OBJECT.read_attr(&quot;__setattr__&quot;)(self, name, value) A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;__getattr__&quot;: __getattr__, &quot;__setattr__&quot;: __setattr__}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;celsius&quot;, 30) assert obj.read_attr(&quot;fahrenheit&quot;) == 86 # test __getattr__ obj.write_attr(&quot;celsius&quot;, 40) assert obj.read_attr(&quot;fahrenheit&quot;) == 104 obj.write_attr(&quot;fahrenheit&quot;, 86) # test __setattr__ assert obj.read_attr(&quot;celsius&quot;) == 30 assert obj.read_attr(&quot;fahrenheit&quot;) == 86 为了通过测试，我们需要修改下 Base.read_attr 以及 Base.write_attr 两个方法： 12345678910111213141516171819202122class Base(object): ... def read_attr(self, fieldname): &quot;&quot;&quot; read field 'fieldname' out of the object &quot;&quot;&quot; result = self._read_dict(fieldname) if result is not MISSING: return result result = self.cls._read_from_class(fieldname) if _is_bindable(result): return _make_boundmethod(result, self) if result is not MISSING: return result meth = self.cls._read_from_class(&quot;__getattr__&quot;) if meth is not MISSING: return meth(self, fieldname) raise AttributeError(fieldname) def write_attr(self, fieldname, value): &quot;&quot;&quot; write field 'fieldname' into the object &quot;&quot;&quot; meth = self.cls._read_from_class(&quot;__setattr__&quot;) return meth(self, fieldname, value) 获取属性的过程变成调用 __getattr__ 方法并传入字段名作为参数，如果字段不存在，将会抛出一个异常。请注意 __getattr__ 只能在类中调用（Python 中的特殊方法也是这样），同时需要避免这样的 self.read_attr(&quot;__getattr__&quot;) 递归调用，因为如果 __getattr__ 方法没有定义的话，上面的调用会造成无限递归。 对属性的修改操作也会像读取一样交给 __setattr__ 方法执行。为了保证这个方法能够正常执行，OBJECT 需要实现 __setattr__ 的默认行为，比如： 123def OBJECT__setattr__(self, fieldname, value): self._write_dict(fieldname, value)OBJECT = Class(&quot;object&quot;, None, {&quot;__setattr__&quot;: OBJECT__setattr__}, None) OBJECT.__setattr__ 的具体实现和之前 write_attr 方法的实现有着相似之处。在完成这些修改后，我们可以顺利的通过我们的测试。 描述符协议在上面的测试中，我们频繁的在不同的温标之间切换，不得不说，在执行修改属性操作的时候这样真的很蛋疼，所以我们需要在 __getattr__ 和 __setattr__ 中检查所使用的的属性的名称为了解决这个问题，在 Python 中引入了描述符协议的概念。 我们将从 __getattr__ 和 __setattr__ 方法中获取具体的属性，而描述符协议则是在属性调用过程结束返回结果时触发一个特殊的方法。描述符协议可以视为一种可以绑定类与方法的特殊手段，我们可以使用描述符协议来完成将方法绑定到对象的具体操作。除了绑定方法，在 Python 中描述符最重要的几个使用场景之一就是 staticmethod、 classmethod 和 property。 在接下来一点文字中，我们将介绍怎么样来使用描述符进行对象绑定。我们可以通过使用 __get__ 方法来达成这一目标，具体请看下面的测试代码： 1234567891011121314151617181920212223def test_get(): # Python code class FahrenheitGetter(object): def __get__(self, inst, cls): return inst.celsius * 9\\. / 5\\. + 32 class A(object): fahrenheit = FahrenheitGetter() obj = A() obj.celsius = 30 assert obj.fahrenheit == 86 # Object model code class FahrenheitGetter(object): def __get__(self, inst, cls): return inst.read_attr(&quot;celsius&quot;) * 9\\. / 5\\. + 32 A = Class(name=&quot;A&quot;, base_class=OBJECT, fields={&quot;fahrenheit&quot;: FahrenheitGetter()}, metaclass=TYPE) obj = Instance(A) obj.write_attr(&quot;celsius&quot;, 30) assert obj.read_attr(&quot;fahrenheit&quot;) == 86 __get__ 方法将会在属性查找完后被 FahrenheitGetter 实例所调用。传递给 __get__ 的参数是查找过程结束时所处的那个实例。 实现这样的功能倒是很简单，我们可以很简单的修改 _is_bindable 和 _make_boundmethod 方法： 12345def _is_bindable(meth): return hasattr(meth, &quot;__get__&quot;)def _make_boundmethod(meth, self): return meth.__get__(self, None) 好了，这样简单的修改能保证我们通过测试了。之前关于方法绑定的测试也能通过了，在 Python 中 __get__ 方法执行完了将会返回一个已绑定方法对象。 在实践中，描述符协议的确看起来比较复杂。它同时还包含用于设置属性的 __set__ 方法。此外，你现在所看到我们实现的版本是经过一些简化的。请注意，前面 _make_boundmethod 方法调用 __get__ 是实现级的操作，而不是使用 meth.read_attr('__get__') 。这是很有必要的，因为我们的对象模型只是从 Python 中借用函数和方法，而不是展示 Python 的对象模型。进一步完善模型的话可以有效解决这个问题。 实例优化这个对象模型前面三个部分的建立过程中伴随着很多的行为变化，而最后一部分的优化工作并不会伴随着行为变化。这种优化方式被称为 map ,广泛存在在可以自举的语言虚拟机中。这是一种最为重要对象模型优化手段：在 PyPy ，诸如 V8 现代 JavaScript 虚拟机中得到应用（在 V8 中这种方法被称为 hidden classes）。 这种优化手段基于如下的观察：到目前所实现的对象模型中，所有实例都使用一个完整的字典来储存他们的属性。字典是基于哈希表进行实现的，这将会耗费大量的内存。在很多时候，同一个类的实例将会拥有同样的属性，比如，有一个类 Point ，它所有的实例都包含同样的属性 x y。 Map 优化利用了这样一个事实。它将会将每个实例的字典分割为两个部分。一部分存放可以在所有实例中共享的属性名。然后另一部分只存放对第一部分产生的 Map 的引用和存放具体的值。存放属性名的 map 将会作为值的索引。 我们将为上面所述的需求编写一些测试用例，如下所示： 12345678910111213141516171819202122232425def test_maps(): # white box test inspecting the implementation Point = Class(name=&quot;Point&quot;, base_class=OBJECT, fields={}, metaclass=TYPE) p1 = Instance(Point) p1.write_attr(&quot;x&quot;, 1) p1.write_attr(&quot;y&quot;, 2) assert p1.storage == [1, 2] assert p1.map.attrs == {&quot;x&quot;: 0, &quot;y&quot;: 1} p2 = Instance(Point) p2.write_attr(&quot;x&quot;, 5) p2.write_attr(&quot;y&quot;, 6) assert p1.map is p2.map assert p2.storage == [5, 6] p1.write_attr(&quot;x&quot;, -1) p1.write_attr(&quot;y&quot;, -2) assert p1.map is p2.map assert p1.storage == [-1, -2] p3 = Instance(Point) p3.write_attr(&quot;x&quot;, 100) p3.write_attr(&quot;z&quot;, -343) assert p3.map is not p1.map assert p3.map.attrs == {&quot;x&quot;: 0, &quot;z&quot;: 1} 注意，这里测试代码的风格和我们之前的才是代码看起不太一样。之前所有的测试只是通过已实现的接口来测试类的功能。这里的测试通过读取类的内部属性来获取实现的详细信息，并将其与预设的值进行比较。这种测试方法又被称之为白盒测试。 p1 的包含 attrs 的 map 存放了 x 和 y 两个属性，其在 p1 中存放的值分别为 0 和 1。然后创建第二个实例 p2 ，并通过同样的方法网同样的 map 中添加同样的属性。 换句话说，如果不同的属性被添加了，那么其中的 map 是不通用的。 Map 类长下面这样： 123456789101112131415161718class Map(object): def __init__(self, attrs): self.attrs = attrs self.next_maps = {} def get_index(self, fieldname): return self.attrs.get(fieldname, -1) def next_map(self, fieldname): assert fieldname not in self.attrs if fieldname in self.next_maps: return self.next_maps[fieldname] attrs = self.attrs.copy() attrs[fieldname] = len(attrs) result = self.next_maps[fieldname] = Map(attrs) return resultEMPTY_MAP = Map({}) Map 类拥有两个方法，分别是 get_index 和 next_map 。前者用于查找对象储存空间中的索引中查找对应的属性名称。而在新的属性添加到对象中时应该使用后者。在这种情况下，不同的实例需要用 next_map 计算不同的映射关系。这个方法将会使用 next_maps 来查找已经存在的映射。这样，相似的实例将会使用相似的 Map 对象。 Figure 14.2 - Map transitions 使用 map 的 Instance 实现如下： 1234567891011121314151617181920212223class Instance(Base): &quot;&quot;&quot;Instance of a user-defined class. &quot;&quot;&quot; def __init__(self, cls): assert isinstance(cls, Class) Base.__init__(self, cls, None) self.map = EMPTY_MAP self.storage = [] def _read_dict(self, fieldname): index = self.map.get_index(fieldname) if index == -1: return MISSING return self.storage[index] def _write_dict(self, fieldname, value): index = self.map.get_index(fieldname) if index != -1: self.storage[index] = value else: new_map = self.map.next_map(fieldname) self.storage.append(value) self.map = new_map 现在这个类将给 Base 类传递 None 作为字段字典，那是因为 Instance 将会以另一种方式构建存储字典。因此它需要重载 _read_dict 和 _write_dict 。在实际操作中，我们将重构 Base 类，使其不在负责存放字段字典。不过眼下，我们传递一个 None 作为参数就足够了。 在一个新的实例创建之初使用的是 EMPTY_MAP ，这里面没有任何的对象存放着。在实现 _read_dict 后，我们将从实例的 map 中查找属性名的索引，然后映射相对应的储存表。 向字段字典写入数据分为两种情况。第一种是现有属性值的修改，那么就简单的在映射的列表中修改对应的值就好。而如果对应属性不存在，那么需要进行 map 变换（如上面的图所示一样），将会调用 next_map 方法，然后将新的值存放入储存列表中。 你肯定想问，这种优化方式到底优化了什么？一般而言，在具有很多相似结构实例的情况下能较好的优化内存。但是请记住，这不是一个通用的优化手段。有些时候代码中充斥着结构不同的实例之时，这种手段可能会耗费更大的空间。 这是动态语言优化中的常见问题。一般而言，不太可能找到一种万能的方法去优化代码，使其更快，更节省空间。因此，具体情况具体分析，我们需要根据不同的情况去选择优化方式。 在 Map 优化中很有意思的一点就是，虽然这里只有花了内存占用，但是在 VM 使用 JIT 技术的情况下，也能较好的提高程序的性能。为了实现这一点，JIT 技术使用映射来查找属性在存储空间中的偏移量。然后完全除去字典查找的方式。 潜在扩展扩展我们的对象模型和引入不同语言的设计选择是一件非常容易的事儿。这里给出一些可能的方向： 最简单的是添加更多的特殊方法方法，比如一些 __init__, __getattribute__, __set__ 这样非常容易实现和有趣的方法。 扩展模型支持多重继承。为了实现这一点，每一个类都需要一个父类列表。然后 Class.method_resolution_order 需要进行修改，以便支持方法查找。一个简单的 MRO 计算规则可以使用深度优先原则。然后更为复杂的可以采用C3 算法, 这种算法能更好的处理菱形继承结构所带来的一些问题。 一个更为疯狂的想法是切换到原型模式，这需要消除类和实例之间的差别。 总结面向对象编程语言设计的核心是其对象模型的细节。编写一些简单的对象模型是一件非常简单而且有趣的事情。你可以通过这种方式来了解现有语言的工作机制，并且深入了解面向对象语言的设计原则。编写不同的对象模型验证不同对象的设计思路是一个非常棒的方法。你也不在需要将注意力放在其余一些琐碎的事情上，比如解析和执行代码。 这样编写对象模型的工作在实践中也是非常有用的。除了作为实验品以外，它们还可以被其余语言所使用。这种例子有很多：比如 GObject 模型，用 C 语言编写，在 GLib 和 其余 Gonme 中得到使用，还有就是用 JavaScript 实现的各类对象模型。 参考文献 P. Cointe, “Metaclasses are first class: The ObjVlisp Model,” SIGPLAN Not, vol. 22, no. 12, pp. 156–162, 1987.↩ It seems that the attribute-based model is conceptually more complex, because it needs both method lookup and call. In practice, calling something is defined by looking up and calling a special attribute __call__, so conceptual simplicity is regained. This won’t be implemented in this chapter, however.)↩ G. Kiczales, J. des Rivieres, and D. G. Bobrow, The Art of the Metaobject Protocol. Cambridge, Mass: The MIT Press, 1991.↩ A. Goldberg, Smalltalk-80: The Language and its Implementation. Addison-Wesley, 1983, page 61.↩ In Python the second argument is the class where the attribute was found, though we will ignore that here.↩ C. Chambers, D. Ungar, and E. Lee, “An efficient implementation of SELF, a dynamically-typed object-oriented language based on prototypes,” in OOPSLA, 1989, vol. 24.↩ How that works is beyond the scope of this chapter. I tried to give a reasonably readable account of it in a paper I wrote a few years ago. It uses an object model that is basically a variant of the one in this chapter: C. F. Bolz, A. Cuni, M. Fijałkowski, M. Leuschel, S. Pedroni, and A. Rigo, “Runtime feedback in a meta-tracing JIT for efficient dynamic languages,” in Proceedings of the 6th Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems, New York, NY, USA, 2011, pp. 9:1–9:8.↩","link":"/posts/2016/12/15/A-Simple-Object-Model/"},{"title":"关于我自己被性侵经历的访谈记录","text":"这篇文章是我 2020 年 12 月接受华中师范大学关于儿童性侵的采访所产生采访稿。在这次采访中，我完整的复盘了在我12岁那年发生在我身上的强奸事件。在这次采访中，我完整回顾了当时我和我家庭的一些反应，也表达了我一些关于性侵这件事的看法。我希望每个人都能平安顺利的过完一生，但是如果有不好的事情发生的时候，我希望这篇文章能帮到你。Everything is gonna be OK。 采访稿 时间标签 说话人 逐字稿 备忘录 访谈对象： Hello，你好。 访谈者： Hi，你好啊。好，那在开始之前我再说一下我们的知情同意书吧，就是前面也发给你过。 访谈对象： 哦，对，我看了，啊，签名我忘了签了。 访谈者： 没事，等一下发给我也行。知情同意书主要包括研究目标：就是聚焦于探索儿童期性创伤经历者的创伤应对和表露过程。然后你说的话对于我来说，都很重要，所以需要录音，我们的访谈大概持续是三十到六十分钟。如果说这次访谈中断或者没有完成的话，可以协商下一次。参与这个研究，你随时都有权利选择退出，访谈过程中或遇到一些你不想回答的问题，你都可以选择拒绝回答。 访谈对象： 嗯，没问题没问题。 访谈者： 还有一些小小的风险，就是可能会谈到一些触动你情绪的这件事的时候，可能就会引起你的情绪有些波动。 访谈对象： 嗯，没事儿，快乐水我已经准备好了（笑）。 访谈者： 嗯，好，但是呢我会就是照顾到你的情绪，因为我是有做过心理咨询的经验的。嗯，然后还有就是你的个人隐私处理问题。首先就是这个录音我会给它变进行一个变音，我处理好逐字稿之后就会把它删除。然后还有就是你的一些能够辨认出你个人信息的内容，我会进行一个匿名化的处理。 访谈对象： 之后你生成的就说是paper，我能够拿到一份拷贝吗？ 访谈者： 嗯，你是说论文吗？可以。 访谈对象： 对，是的是的是的，没错，就是你的相关研究结果。因为我其实对这个事情很好奇，因为我我其实一直是在做性侵这方面公益的。但是据我所知的话，其实国内这方面对于专门性侵受害者的研究，其实我觉得好像还是一个很偏门的领域。我其实挺好奇的。对，因为当时你在这个过程中说的时候，我其实也挺好奇的你为啥会选择这个方向。 访谈者： 这个通过我看论文的时候我就发现了，因为国内真的很少，就是比较权威的论文，真的非常少。我参考的文献全部都是英文的，就看论文的时候还是挺痛苦的。 访谈对象： 就是因为我自己亲身经历，还有我去跟就是给别人做。因为我之前在做公益嘛，就是校园暴力，其实校园性侵属于校园暴力一种嘛。 访谈者： 嗯。 访谈对象： 然后的话做的时候，我发现其实国内对于这方面，研究特别少。包括就是说起因、结果的研究，包括怎么样去做进行一个系统性的，就是说是受创伤后的心理干涉的介入。我觉得这方面研究好像都很少，我其实挺好奇的。 访谈者： 嗯，确实很少。 访谈对象： 哎，你们导师也是专门做这一块的？ 访谈者： 没有，我们导师主要是做危机干预和自杀预防的。 访谈对象： 啊，对，自杀干预前几年好像也是比较少。做自杀干预很有必要，我的好朋友八月份就离世了，就因为这抑郁症。来吧，我们现在就开始吧。 访谈者： 嗯，行，好，那开始之前我再做一个自我介绍吧。尽管前面已经介绍过了，我是来自河南信阳，然后今年26岁，是现在是在华中师范心理学院的在读研究生。然后很高兴，也很感谢你能够愿意支持，然后参与这个研究。好，在开始之前你也就是简单做一个自我介绍，包括年龄啊，职业还有居住地和婚姻状况。 访谈对象： 啊，我现在很明显未婚，然后的话成都人，然后年龄的话，我九四年的跟你同岁。然后但是不知道我们月份谁大谁小。然后现在程序员然后在阿里。嗯，对，就是差不多这个情况。我受侵犯的年纪。我想想具体年份，啊，零七年到现在已经是快14年了，已经是十多年了。所以当时应该是我13岁的时候。13岁未满，07年2月，我没记错的话。 访谈者： 还是能够记得很清楚的。 访谈对象： 对，因为我其实我自己是选择把我很多事情公开出来。因为我觉得国内对于同性……因为我其实是比较，呃，在常规的…（遭遇性侵）这样是比较少见的，我是同性的性侵。 访谈者： 嗯。 访谈对象： 对对，然后然后大家可能更focus 在就是说我异性的性侵。但是国内的性侵的话，对于同性性侵其实这一块比异性的研究更少，而且是什么，法律也不完善。所以说我是会刻意的记忆，然后把这个事情分享出来。 访谈者： 这比一般女生更需要勇气。 访谈对象： 啊，其实还好，其实还好。对，我觉得其实男生来讲，就拿我自己的亲身经历来讲，其实男生受创伤它是一个持续的过程，它是一个持续的过程，可能你小时候觉得做出这个事情有点无所谓。但是当你大了之后，潜意识了，因为我之前抑郁过嘛，我之前抑郁过，去做过心理诊断。然后当时就是说是心理咨询师就确定了诱因，有一个其中的诱因可能就是这件事引发了一个长期性的一个p t s d 。然后，对，然后可能说比女性来得更猛一些。但是可能说，但是在社会舆论氛围上相对能够获得就更多的，就是说可能说宽容一些，或者说是我也不知道怎么界定啊。 访谈者： 嗯，好，那现在我们就开始吧，就按照我的访谈提纲。我们就像讲故事那样的方式，以时间发展顺序来讲一下。就是当时遭遇侵犯的背景，然后还有你当时是怎么去应对处理的。在这个过程中你经历了怎么样的一个心路历程。 访谈对象： 哦，o k ，其实我的话当时是在住宿学校，然后的话，因为就是这个事情可能就比较长。就是我在住宿学校，然后我当时也不算太合群的一个人。然后就是可能就说校官就界定了，界定了就是说这个是一个小孩子，而且他可能也是有性需求嘛。然后现在我想起来他部队退役的，然后就这是一个性需求。嗯，那么他会长期的培养我一个服服从。就比如说平时就因为我当时是就是有点不听话的小孩子嘛。然后就比如说训练军姿啊训练或者其他的，那么服从之后呢，然后就在事发当天晚上，然后把我拉到一个卧室去，然后去喝了一些酒，然后这个事情。就是我彻底就失去了反抗的这个意识了，就是说，那么这个事情就发生了。至于发生之后的话，就是说发生之后，我第二天早上回去，然后我爸请我吃香肠，当时刚过年不久嘛，我们家还有香肠，然后我就回家，因为那个时候是晚上是周五晚上，然后第二天周六回家，然后当时就觉得不太对劲，然后就很不爽，然后跟我爸说了这事。然后我家里人就发现，就可能说知道我被性侵了，然后就赶紧去报警，然后做了就是精液采集，然后做了笔录。然后后面的话，在大概在一年之后，他定罪之前，又重新做了一次我的d n a 鉴定。因为体液采集是从我身上采集的嘛，然后做了一次d n a 鉴定。然后你要说心路历程的话，其实，嗯，跟女孩子不同的是，男孩子其实可能最开始在小时候，最开始可能他并不会认识到这是一次强奸。 访谈者： 其实女孩子也不知道。 访谈对象： 呃，女孩子可能最开始知道，因为女孩子可能从小父母就教育她，说你那个地方很羞羞的嘛，然后你就，然后不能让男孩子碰。然后或者碰到这个地方对你不好，可能是有这样一个介绍。那么女孩子最开始会就会觉得这个地方是有个耻辱感，就有个很明确的耻辱感，很明确的被侵犯感。而男孩子可能说，就我自己的亲身经验来讲，你最开始其实反应可能可能并不会太强烈。就可能你最开始都不知道这发生了什么，就觉得很不舒服很不爽。 访谈者： 然后你第二天早上就跟你父亲说了？ 访谈对象： 对，因为当时我觉得就很不爽。对，然后又因为香肠那个诱因嘛，然后这个东西大家都理解，然后就是加上香肠这个诱因。然后的话就我父亲就发现我的异样，然后就去报警。对。 访谈者： 那你父亲还挺细致的一个人。 访谈对象： 对，我父亲，其实这一件事情我一直很感谢我父亲。因为如果说是传统家长，可能就会考虑说，啊，出于名誉考虑遮盖子嘛，对，遮盖子，然后就说或者说：哎，事情发生就发生了，过。然后就报警了，然后最后那个哥们儿是以猥亵儿童罪被判4年，我就记得没错。 访谈者： 4年。 访谈对象： 对，因为因为男生很尴尬的一点，就是说他不是…因為國內的法律的性侵，它强奸是针对于女性的一个性器官的插入说，而对于男生来讲它其实是没有强奸这个说法的，而我当时凑巧的是我年龄小于十四岁，然后所以说他以那个侵犯儿童、猥亵儿童罪，刑法二百三十几条，我记得没错的话，然后他就入刑了。 访谈者： 嗯，就是这样看来的话，家庭关系其实也很重要。你和父亲的交流…… 访谈对象： 非常的重要，非常的重要，我觉得其实很……就是说我自己身边也有被性侵的。其实这个事情的伤害程度很取决于家庭对这个事情的处理。如果你家庭觉得说：这是一个性侵。那么我及时报警，然后寻求公权或者说警察的帮助。那么这个事情呢受伤害或者说再配合再更开明一点，配合及时的心理介入干涉。那么这个伤害会控制到非常小的点。 访谈者： 嗯，就是你父亲知道之后，他当时的一个反应是什么样的，还有包括你的家里人反应？ 访谈对象： 呃，我其实我父亲他在我面前其实没有表现太多的焦虑。嗯，对我父亲懂得，家长嘛，就是可能说再天大的事，可能也不能在孩子面前（焦虑，不淡定）…我母亲当时是在成都，然后我在小地方，我小时候出生在小地方，这个钢铁城市嘛，然后，对，我父亲然后当时其实是很淡定的，当时他做了两件事，我没记错的话，第一个他给他朋友打电话，就是说他怕警察不认这个事情。于是他找他朋友咨询：我能不能去其他的诊所？先将精液或者说其他的体液部分固定下来，然后后面发现这些事情好像并不合规。然后我父亲就当时把我带到了那个住宿学校的辖区的派出所去做了那个就是说笔录，包括第一波的体液采集，对。 访谈者： 你父亲真的很厉害。 访谈对象： 嗯，对。然后就是后面的话，然后出于其他考虑，然后他就将我带来成都了嘛。对，然后的话就说是和那个环境做隔离。因为这个事情整个公开报道出去其实还是会对你的成长造成一个影响，因为这个特别是教育系统，就是基本上是教育系统，你一出去，一转学，大家一打听就知道这孩子不太对劲（笑）。 访谈者： 嗯。就是那你对于你父亲当时那个处理，你当时心里面就是有一个什么样的状态？ 访谈对象： 嗯，其实我当时还是懵懂的状态，其实还是懵懂的状态。就是说，包括体液采集，做笔录的时候，我当时其实并没有意识到这个事情是一个强奸。我当时其实心里没有很明确的这个词，这个是对于男生的一个强奸。然后我其实就觉得这可能是个不好的事情。嗯，然后呢那么我就配合警察，把这个事情完全记录、说出来。然后做笔录，其实还是小孩子对于，就是说当时去做这一套，我父亲替我拍了板之后，我当时做这一套核心的一个动机，更可能还是说，呃，小孩子对于长辈，对于就是说是或者说其他的一个服从，就是我当时并没有意识到这个东西发生了什么。 访谈者： 嗯，那你后来什么时候开始意识到这件事情它的一个性质是什么的？ 访谈对象： 唔，其实初步意识到应该是在高中，然后完整的去复盘我自己的这件事儿的话，应该是在大学。应该是在快毕业的时候，当时其实，呃，我完整的复盘其实诱因应该是国内的me too 运动。 访谈者： 嗯嗯。 访谈对象： 对，实际上是应该是me too 运动。然后诱因…，我其实之前有个粗浅的复盘，但是真正的去重新的就是从头到尾的去审视这件事是就应该是me too，对，然后的话对，差不多就是这样一个情况。 访谈者： 就是这件事情对你有什么影响？就是从它发生一直到现在的话，对你有什么影响？ 访谈对象： 呃，其实我觉得你要说真正的影响很难量化。因为就是说很多东西就包括我去做心理咨询之后，其实医生的看法就是说这个事情是潜移默化的，就是说潜移默化的一个伤害，就是说是长期性的一个p t s d。然后当时我去医院去确诊，然后就因为我当时是确诊抑郁症嘛，然后重度抑郁，然后是伴自杀焦虑，然后的话就是自杀倾向。然后的话，呃，当时其实医生当时听我把这个事情清理完整描述过之后，医生的评价就是说是你这个抑郁的有一部分其实就是来源于你这件事儿。嗯，就说，对，对，因为就是什么，因为我是抑郁需要吃药并伴做那个心理干涉嘛，然后，对，但是就是说你要说这件事情有什么明显的，就是说后遗（症），一个就是要很量化的，就是说你觉得这个事情对你影响有多大，我觉得这没量化。我觉得这个东西是一个潜移默化的过程。因为我当时是在受侵害的，最开始他是没有做到一个系统的心理干涉，我是没有做到的。对，可能对于同性的时候，可能说最开始你觉得没什么，但是越想越想觉得有事儿。 访谈者： 那你诊断为抑郁症的时候，大概是多大？ 访谈对象： 啊，应该是在17年。 访谈者： 嗯，在三年前。就是在此之前就没有表现出来一些具体的症状？ 访谈对象： 有，其实有我一直神经衰弱，但是我其实在大学的时候应该是有，但是我是没有去确诊的，因为就是说没有确诊。对，但是正式确认是在17年。对，然后的话，所以说我觉得这个事情可能更可能来讲还是一个长期的一个潜移默化的。就是说它并不会对你就是说你一下子觉得：哎，你自己不是一个干净的人，你一下子觉得你想跳楼了。我觉得这个不存在，但是可能就是说潜移默化的过程。 访谈者： 那你觉得这种潜移默化主要是来自于哪里呢？ 访谈对象： 我觉得可能最开始就是打破你自己的，就说是一个完整感。就是说你自己觉得自己是一个不完整，或者说是你觉得你自己是…用这种传统的话来讲呢，就是你自己是个不干净的，懂我意思吧？ 访谈者： 嗯。 访谈对象： 对，就是我自己猜的话，潜意识可能会给自己加一个这样的，其实我觉得可能说。呃，然后你就会觉得你自己说话会比别人低一头或者是其他。对，我觉得这个可能还是一个社会氛围所造成的吧。 访谈者： 嗯，就是在高中之后慢慢的开始有这种意识。那其实对是小一点的话，你其实还是不太知道这件事情是什么性质？ 访谈对象： 对，对，对，男生其实很麻烦的一件事就在这个地方，最开始如果你是被性侵的那个男生，男生你在小时候你可能会觉得这是一个游戏，你可能会去更觉得这是一个游戏。但是你在大了之后，你会发现慢慢觉得这不是…（一个词，听不清）欸？怎么这么不太对劲啊？ 访谈者： 嗯，就是慢慢的意识到的。 访谈对象： 对，是的，没错。 访谈者： 你就是最开始就已经选择，也不是说你主动告诉你父亲，就是你父亲他自己觉察到这件事情。那再后来你有没有再表露过这件事情？嗯，再一次向别人说这件事情的时候是什么时候，然后是什么原因？ 访谈对象： 我其实应该是还是在大学吧。然后我其实之前其实只跟几个好朋友说过，然后我正式选择公开这件事情，应该还是在大学或者说快毕业，me too事件之后，因为我觉得可能说我的经（历），因为我当时其实有个背景是我也是在做公益，我之前是救援队的。然后的话我之前大学在救援队服务，然后是以私人身份去做一些，然后就包括啊校园校园暴力和校园性侵，资助我也在做。呃，我可能觉得，更觉得这件事的话，可能就是说，我需要让这件事变得有意义。然后于是我选择就是说公开的把我这个经历分享出来。因为我觉得可能说，呃，一个事情如果只能变成你自己的痛苦，或者说你自己的建议，那么你自己是否痛苦其实是没有太大意义的。对，而且我这个事情又是相对来说对别人更容易产生帮助的，因为其实你知道儿童期间的性侵其实是很常见的一个事情。 访谈者： 嗯嗯。 访谈对象： 对，其实是，然后家庭，就是说父母的干涉又是非常重要的。我选择的在知乎，你看到我的那个答案，包括我选择的一些社群里面把这个事情，呃，就是说我自己的当时的经历，以及我家庭所做的一些手段，包括我家庭当时做的不足的地方，就因为认知的关系嘛，没有给我寻求及时的心理介入，这些事情分享出来我就是希望说，我自己能够把这些，啊，把我自己的经历能够变得更为有意义一点，然后不仅限于我自己所受到的伤害。 访谈者： 做的真的很好。 访谈对象： 唉，（深深叹一口气）对，这也是我参加你的这个研究的原因吧。 访谈者： 嗯，我也看到，你说这就是你分享的意义。 访谈对象： 嗯嗯，对，对，我觉得其实就是说你自己每个人受伤害嘛，然后要怎么样去把自己受到的创伤，受到的伤害变得让它具备除了伤害以外更多的意义。我觉得这是一个很好玩的话题。 访谈者： 嗯，很有意义，很有价值的。 访谈对象： 嗯，对，这就是我自己选择的，我自己披露出来的一个啊，心理活动。 访谈者： 好，还有就是你刚才提到也告诉过好朋友，那你是在什么时候告诉好朋友，然后他们又是怎么回应你，他们当时的这个态度和反应是什么样的？ 访谈对象： 他们其实就觉得就是说：不可思议。就是这样子，然后就觉得就觉得说，出于保护我，就告诉我这件事情还是不要跟别人说。出于好意，就是让我这件事情不要跟别人说。然后这个事情不要跟别人说，其实后面相处也是没有什么太大的变化，因为好朋友嘛，对。 访谈者： 那也是因为你非常的信任他们。 访谈对象： 对，因为其实是在闲聊的时候嘛，就是闲聊的时候就说了这事。这个没有什么可以防备的，就说了这事儿，虽然我父母我一直在告诉我说：这个事情你千万不要给别人说啊。但是我从小都不是个好小孩。 访谈者： 嗯，就是在朋友那里，其实也是得到了一定的支持。 访谈对象： 嗯，是的，没错。 访谈者： 还有很多小孩就是选择告诉父母的话，他还会造成二次伤害。所以就是说你父亲的这种处理真的特别可贵。 访谈对象： 对，对，是的，我觉得其实是被性侵伤害的（人），从我自己打过交道来看，其实是很大一部分伤害是来自原生家庭。这个我们毫不避讳的讲的是其实很多包括就觉得说可能说女孩子就觉得说：啊，你嫁不出去了，就直接原生家庭给的压力，就“你嫁不出去了，你怎么这样？”就受害者有罪推定嘛，是吧，受害者有罪推定。“哎，当时谁叫你穿那么少干嘛，然后叫你再穿这么少？”其实我父母也有，我母亲也有一点这种就说：哎，叫你当时不合群，你要是合群，你教官就不会挑上你了嘛！”我觉得这种东西其实是没有必要的那个谴（责），苛责。 访谈者： 对对对，明明就是加害者的错，反过来还要去揪你的毛病。对，所以面对母亲的那种那种责怪，你其实感觉怎么样？。 访谈对象： 我母亲其实也不是恶意的嘛。她其实就是说教育我要去合群嘛。 访谈者： 嗯。 访谈对象： 对，其实我小时候我一直不太想去为了合群而合群，但是他们长辈嘛，她为了教育我去合群，就是说可能说举了一个不恰当的例子。但是不恰当的例子其实呢也是说，目前我跟你说性侵的伤害，就是说遇到了一个现状，很多时候的压力和伤害其实是更多的是，就是说比性侵这件事情伤害更大的，可能说是来自于原生家庭的一个苛责。 访谈者： 嗯，对，确实是。 访谈对象： 好，你觉得有什么是促进你来表述这件事情，有什么是阻碍你来表述这件事情的？ 访谈者： 哦，其实阻碍我去表述这件事情的因素并不多。 访谈对象： 嗯，我也发现了。 访谈者： 对，因为我觉得你们说“关我吊事儿”，就是用术语来说就是就是“关我什么事儿”。然后我说我的，你们看的惯就看，看不惯就不看，你们又不给我发钱，发工资是不是？我这个人的性格是比较，就是比较那个就怎么说，就反常规的吧，所以说我觉得还好。再一个就是说促进我表露的话，可能就还是说，呃，国内就是说是目前就是说是对于性侵受害者态度的一个变化。嗯，就说是一个……就是因为之前，可能说在零几年或者说前几年或者更早的时候，觉得性侵它是一个非常shame的事情，就是说你不光彩。就是说“how shame you “就是说你怎么能这样，就是就刚才我举的例子吧，你被性侵了，你一定是穿的太少了，你一定是穿的太骚了，或者说你一定是穿的太浪了。啊，这种就是受害者有罪论。这几年国内其实是对性侵受害者态度其实是逐渐变好了。而且就是我刚才说了嘛，另外一个契机可能就是说是me too。然后我觉得可能说，啊，把这个事情公布出来，然后让别人，能够帮到别人，我觉得是能够去抚慰自己被性侵。就特别说如果有一些人告诉你说：我孩子，呃，但是我不希望有这样一天，啊，但是就是如果说有一天跟你说，有个当父亲或当母亲跟你说：我孩子被性侵性了，我按照你说的做了。其实我觉得如果说是有这样一件事情，它其实是能够治愈很多东西的。但是我不希望有一天会出现这种情况。 访谈对象： 嗯。 访谈者： 对，然后我觉得这可能说是，呃，一个是算是一个自我的拯救吧。第二个就是说可能说还是希望就是说能够帮…就说是用自己的一些东西去回馈一些什么。 访谈对象： 嗯。当你回忆了整个的这样的一个经历之后，你现在是有什么感受？用一个词来概括一下。 访谈者： 啊！还好。 访谈对象： 还好？用一个词来概括的话？ 访谈者： 啊，还好，就是跟产品经理撕完逼之后的，怎么说，啊，轻松。 访谈对象： 轻松？ 访谈者： 对，轻松吧，我觉得其实，我觉得怎么说呢？我觉得其实这样面对面聊这个问题，其实聊完之后其实还是会有些轻松感的，对，就像跟产品经理撕完批之后，然后你会觉得：哇，爽死了。 访谈对象： 程序员的快乐。 访谈者： 啊，是的。那能再具体一点说一下，为什么是这种感觉呢？ 访谈对象： 因为我觉得其实你在网上以一个虚拟的身份去说，可能说大家都知道迈克萨卡的i d 是我。因为我可能说我觉得我在这个社区比较活跃，大家可能觉得迈克萨卡的i d 是我，我也会在微信群里面说，大家看到啊，这个啊微信i d是迈克萨卡的人是我。但是我觉得可能说跟你就是一个真人的去one on one的去说，就是相对来说是一个陌生人。我跟我女朋友其实是很深入地聊过这件事儿的。就是说，呃，跟你一个陌生人去one on one的去聊这件事儿，其实也是在治愈自己那个心里面的一个东西，对，其实是因为，因为潜意识来讲可能最开始我觉得这还是一件shame的事情。所以我想的是在网上以一个虚拟的身份去做。那我去给你做访谈，就其实说我有一些枷锁，其实还是（非常大的？）（没听清楚）。 访谈者： 嗯嗯，你刚才我联系你时你说去抽根烟，我就觉得，嗯，可能就是就觉得又要去面对这个事情，情绪上需要准备一下。 访谈对象： 哦，其实你是第一个就是说以一个相对陌生人的身份去忘one on one的去聊这件事儿。 访谈者： 那个单词是怎么你能说慢点吗？one什么？ 访谈对象： 就是one on one，就是一对一，就是一对一，面对面。 访谈者： 哦哦。 访谈对象： 对，我们这边应该就是叫one on one，然后就是跟老板one on one的聊天。 访谈者： 哦哦，在我打断你之前，你刚才要说什么，你接着说吧。 访谈对象： 哦，没有没有，其实我觉得，对，就是说你自己以一个虚拟的身份去说这个事情，就是说是有温度的去说这个事情，我觉得其实是两种体验吧。 访谈者： 嗯，确实是。你刚才提到你女朋友，因为你女朋友毕竟是你除了爸妈之外关系最亲密的人嘛，你告诉了她这件事情，那你再具体说一下，就是你告诉她的这个经过吧，还有你做了什么思想工作？ 访谈对象： 就是在晚上休息之前，我给她就聊到了小时候这件事情，其实也没什么，因为我的事情她都知道，她其实没有什么，我就只是在去聊一些东西。她其实也没什么反应，就是觉得很心疼心疼。 访谈者： 嗯，心疼。 访谈对象： 对，对，对，然后其实我觉得跟亲密的人聊这个事情，其实是没有什么。当然前提是你亲密的人不会再苛责你：你当时为什么穿的那么少，那么浪？对，就是……（没听清楚）其实我觉得去聊这种事情，我觉得是，嗯，也是能够促进感情的一种比较好的方式吧。就是说是互相剖析嘛。 访谈者： 嗯，就是那种完全信任的那种状态。 访谈对象： 是的，没错。 访谈者： 你对于儿童性侵犯的一个现状有什么样的认识？ 访谈对象： 很严重，非常严重，因为性侵其实它是一个很大的概念。我可能下面的话会说的比较露骨一点，不要介意啊。就是说性侵，它可能并不仅仅限于就是说，呃，一个强奸的一个行为。就是插入了，它可能是边缘性的一个性侵，可说搂搂抱抱，对于小女孩或小男孩的一个搂搂抱抱，乃至于说是对于性器官的一个接触，对。然后而且这个，其实现在是越发多样了。可能往常的人来说，我觉得就是说，往常的人来说，就觉得说啊，男生对于女的是一个性侵。后面可能就是说，大家发现男的对于男的其实是一个性侵。但是可能现在这些情况可能会更加多样，就可能说，呃，我并不会去做一些实质性的侵入的工作。我可能就只是说，哎，我见到一个萝莉，我去搂搂抱抱，或者说我心生邪念了，我去做一些性器官的接触。或者说我，或者说是…因为我其实见到，如果你有关注，其实见到很多案例，就可能说，呃，我叫一个小女孩用棒棒糖让她来帮我口一次，也就是说是口交一次，对吧，嗯，其实见过这种案例的，对，然后或者说是，呃，或者说是叫一个小男孩来给口一次，或者说是其他，而且这个他身份可能是会有变化的。可能后面会多样的，就是说我一个二十多岁的女生，或者说就是我自己的生活比较开放。嗯，对，然后我去我去勾搭一个小孩，然后我去跟他发生性关系。其实在我看来这种其实也是一种性侵。因为在小男孩性观念没有成熟之前，你这个操作是会对他造成很大影响的，他的性取向或者性操作会有很大影响吧。包括现在其实同性恋也放到台面上来讲了，就是说我同性恋去强行掰弯一个人，或者说我去性侵一个小男孩，其实也是一个，就是说性侵它其实是现在很严重，儿童性侵其实是很严重的一件事。而且现在是多样化。 访谈者： 嗯，但是相关的法律确实还没有跟得上。 访谈对象： 是的，没错。对，就比如说，呃，我一个，呃就比如说我一个二十多岁三十多岁的女生，我去性侵一个男生，一个十多岁的男生，如果说我是有其他身份的，我是一个老师，或者说是一个在其他国家，他是能够得到一个，就可能说是更重的处罚。而国内的话，其实这一块的法律行为是没有完善的。就可能说如果这个男生小于14岁，你可能会以侵犯儿童，猥亵儿童罪的名义去入刑。然后就比如说小于14岁是强奸这种，其实仅限于女性，没有男性，因为国内强奸它采取的还是一个插入的说法，性器官男生对于女生的一个侵犯。所以说我觉得其实我国法律有其实还有很大的空间要走。包括就是说是在被性侵之后的及时的心理介入。 访谈者： 嗯，对对对，真的是急需完善的一个状态。 访谈对象： 对，其实是国内，其实现在性侵其实是多样化的。比如说是儿童性侵，其实多样化。因为其实生活好了嘛，大家其实会想怎么回事？而且其实比如说同性同龄之间的性侵，其实也是非常严重的。就比如说校园暴力的凌辱的行为，就由凌辱转化为性侵，这种事情其实都没有被重视的。 访谈者： 还有很长的路要要走。 访谈对象： 是的，没错，是的，没错。 访谈者： 然后你觉得对于遭遇过儿童期性侵犯的受害者来说，他需要哪些力量的介入和帮助？ 访谈对象： 哦，我觉得最强力的是公权力。 访谈者： 什么？公权力？ 访谈对象： 嗯，公权力的介入，就是政府部门的介入。就我当时其实这个事情实际上还没有太大，是因为派出所的人给我留下很深的印象。对，当时因为没来得及吃晚饭嘛，然后我当时我还记得就是说那个派出所当时应该和我父亲那边还在联系……（信号原因，没听清）因为一般像这种小地方的警察，可能觉得这种事情就很不耐烦：没事没事，你们先回家自己处理嘛。和稀泥嘛，基层的传统玩法，和稀泥嘛。然后那个警察叔叔就很尽责，然后我记得当时我还没吃早饭，然后录笔录录到下午两点多，还没采体液。小孩子嘛，饿嘛，然后我记得我还在当时他办公室吃了一袋苹果片，然后我现在还记得，然后，对，我觉得这种东西，公权力的及时的介入，其实是能够在第一时间对小孩子就说是一个很贴切的保障。因为大家都从小都得到一个认识嘛：警察叔叔是好人。你受到欺负，警察叔叔会来帮你，对吧？然后，对，这个东西公权力的介入一定是第一优先的，就是说家庭和公权力的及时介入第一是优先。第二个其实我觉得很重要的就说是在性侵的一个月以内，就说是我觉得不管他当时有没有表现出异样啊，一定是要寻求及时的心理干涉。对，就是说因为性侵其实跟其他一样，就是你学心理的肯定比我清楚，就是说性侵跟其他的一样，就是说受伤害的这个东西，他或多或少的一定会有p t sd 相关的东西。如果说是你没有去寻求心理的介入，那么你这个东西可能就是说你心里始终是有个梗儿在这儿。就比如说台湾有个作家叫张苑，就是台湾那个作家自杀的那一位我忘了。 访谈者： 林奕含。 访谈对象： 啊，对，林奕含，然后她其实她其实就是这种，呃，就可能说像我这样皮实的人，可能说会选择自我开导，但是选择像她那样的人，可能就是一个p t s d，然后再加上其他各种事情的不理解，抑郁，然后自杀。其实我觉得所以说第一时间的公权力介入，第一时间的心理干涉一定是非常必要的且非常重要的。 访谈者： 嗯嗯，确实是的。 访谈对象： 但是我觉得国内其实好像专门做儿童性侵心理干涉的好像很少。 访谈者： 确实很少。 访谈对象： 对，我觉得就这个东西其实是亟待完善的。因为其实用成人的那套心理干涉的做法去做儿童心理，其实是（一个词，没听清）不同的。 访谈者： 嗯，首先心理学在中国就刚刚发展起来，就是处于起步的那种朝阳状态。 访谈对象： 啊，就是说这些东西还得需要时间去完善吧。 访谈者： 对，确实它需要一个过程。不过也在慢慢的变好。 访谈对象： 其实说实话，我觉得短期之内其实是不一定能看到太好的变好。 访谈者： 是的是的是的。 访谈对象： 唉，任重道远吧。 访谈者： 路漫漫。 访谈对象： 对，其实是我一个（没听清），我觉得对于小孩子来讲，就是说及时的、周边的帮助非常的重要。 访谈者： 对于预防和干预儿童性侵犯你有什么建议？其实这个问题和刚才那个问题是一个性质的。 访谈对象： 预防其实你这个事情没法解，你预防不了，你预防不了。因为你其实性侵它一定是意味着一个事情，就是说是有一方的呃，就是说是，呃，不管是体力还是其他的，他一定是凌驾于另外一方的。对吧？ 访谈者： 是的。 访谈对象： 对。我就举这么一个不太恰当的例子，就是说性侵，它这个事情其实你只能从就是说是一些呃去打一些预防针。就比如说女孩子就是现在常见的：女孩子啊，你不来让男孩子摸啊，然后这种东西。但是如果说，比如说我，就是说我要采取暴力手段了，我举一个不恰当的例子，希望你不要介意。就是比如说我见到一个萝莉了，对，我采取暴力手段，我去搂搂抱抱，来做一个就是说是，呃，过分的事情，你觉得她有抵抗力吗？没有，对吧？ 访谈者： 嗯。 访谈对象： 对，然后你觉得这件事情能预防吗？没有办法。对，所以说我觉得预防其实，其实它一部分其实是个很奢侈的事情，因为它是需要一个去体系化的一个建设，很奢侈的事情。我觉得更重要的可能却需要去告诉孩子说：这个事情不是一个shame的事情。如果你被侵犯了，这不是你的错误，你需要告诉及时告诉爸爸。 访谈者： 嗯嗯，对。 访谈对象： 我觉得这个事情其实是比预防更重要的事情。 访谈者： 很中肯，嗯。 访谈对象： 对对，因为，呃，知乎上你应该也能看到很多匿名的，就觉得说家里人觉得丢脸，很多，特别是女孩子，然后家里人就不告诉她这些。对吧？然后我觉得这个可能说是因为很多事情预防预防不了。因为就比如说走在路上，突然有人心生歹念，就是你怎么预防？防也防不动啊。除非你随身身上带把刀，然后谁敢碰我我割了他。这种事情当然也都是说笑话，然后我觉得这种事情就一定去培养一个观念：就是说这个事情它不是一个shame的事情，不是你的错，你需要（告诉家里人？没听清，不确定）。 访谈者： 嗯，主要是家庭方面影响特别大。 访谈对象： 是的，其实我觉得公权力方面也影响非常大。因为其实很多就是基层嘛，就是其实很多他虽然说他是警察，但是可能他的法律意识没有吧。然后他可能就会下意识的，就是说，因为这种地方是个小地方，可能就会说：啊，为了你女儿着想，这个事情和了吧。咱们就不走中间法吧，你们就都妥一下对吧。我觉得就是说基层，就说是可能就觉得说，呃，反正就觉得，呃，就千万不要和稀泥，就不要受害者有罪推定，就这个事情不是他的错误。我觉得就是建立一个系统的认知，这个事情是非常重要的。 访谈者： 嗯，就是现在来看的话，你回头去看你的那些应对方式，你会有一个什么样的感受？ 访谈对象： 我父亲做很帮，啊，然后但是可能说他可以做的更棒，但是这个东西其实是马后炮的，我也没法去跟07年的时候，那个时候他……啊，对不起，我去看一下吧，可能有人在敲门。反正我觉得他当时做的很棒，那一套行云流水，其实也很感激他。对，但是的话我觉得可能说，如果说我自己从现在一个事后的角度去复盘，我觉得可能还缺了就我刚刚说的心理干涉。 访谈者： 嗯，那包括你自己的应对方式呢，就是你选择把它给说出来。你觉得如果不说的话，你会怎么样？ 访谈对象： 哦，其实我这个东西很多事情没法做假设呀。我也不知道我不说会怎么样。其实这个东西我觉得其实没法做没法。 访谈者： 嗯，那你去看你的这种应对方式的话，你是什么感受？你怎么看待？ 访谈对象： 我其实也没什么特别的感受。我只是觉得我做了一件应该做的事情。没什么特别的感受。 访谈者： 嗯，我还设计了一个问题，就是如果说你可以回到当初那件事情发生的时候，你会做什么？ 访谈对象： 其实说实话，我觉得这个问题其实没有太大意义。因为其实。你像刚才我说的很多时候，他其实是处于弱势的一个地位，校园性侵他其实是对于性一个很懵懂的这个东西，你没法去假设去说你具备一个完全的性知识人。穿越回去附身到他身上，然后说当时会怎么做，我觉得其实这个东西，这个假设其实是没有意义的。我说的比较直白啊。 访谈者： 嗯，因为可能就是男生比较理性嘛，但是就是对我自己而言，我是希望我能够回去的，就是我假想过这样的一个场景，我如果可以回去，那我就可以避免这所有的一切的发生，就是有一个情绪的宣泄口嘛，这就是我问这个问题的原因。 访谈对象： 是，是一个情绪的宣泄口，但是就是可能说，呃，对于更大多数人来讲，其实这个东西其实没有意义。我们只能往前看，就是就像刚才我说的。其实我知道你意思，你其实想说我去总结一套经验出来，就是说避免当时发生。但是其实就像我刚才说的，很多时候儿童性侵的受害者他其实是面对的是成年人，他其实是面对的是成年人，而且是多种多样的成年人。他其实留给他的选择余地不多。我感觉你情绪好像有点波动啊。 访谈者： 没有，是因为我想到我前面访谈的那些女生，加害者全部都不是成年人，啊，有一个是成年人，其余的全都是中学生，十四岁左右的小男生。 访谈对象： 对，唉，其实我觉得这个事情吧，其实这个事情其实说实话，这种东西就是纯粹就是属于飞来横祸的。对，因为这个东西，就比如我走在路上，然后被突然十二楼掉下来一个东西砸死了。然后我也没法说我提前预知到今天三点十分楼上要掉东西，避开它。这个东西没法预测，除非我直接具备超能力。 访谈者： 嗯，好，再追加一个问题呢，就整个访谈下来你有什么感受，还有你对于我这个访谈的感受吧？ 访谈对象： 哦，我觉得其实反正我觉得其实感觉还好吧。我最开始想的就是你的态度会push一些，就稍微有侵略性一些，学院性一点，其实我觉得整个聊下来感觉还不错。然后的话，对，但是我觉得自己更希望就是说我自己希望是能越来越多的人有来做你这方面的研究的。 访谈者： 我也有这样的希望啊。 访谈对象： 对，我觉得可能是我自己更希望说是有更多人来做这个研究。其实我不介意去做访谈，但是我希望能有更多人。其实说实话我也很好奇，为什么你是第一个找我的？为什么没有其他人找我？其实我也很好奇这个问题。 访谈者： 你是觉得在很早之前就应该有人去做这样的一个研究？ 访谈对象： 对，我觉得其实它并不是一个新鲜事儿，它其实并不是一个新鲜事儿。其实我觉得虽然说国内心理学就是说这方面的研究起步晚吧，但是也不至于到了2020年的今天才有人来开始去做这个，马上都要2021年了，才有人来去找当时的事儿。但是也有可能是我不是在圈子内啊，我不知道啊，也有可能我不在圈子内，我不知道。 访谈者： 其实我很早就想写这个，但是我不太敢跟我导师说，怕被否决。他们会说这是一个特别特别敏感的话题，可行性特别低。 访谈对象： o k ，o k ，o k 明白，我其实很好奇，那你最终决定去做这个话题的时候，你导师你老板什么看法？ 访谈者： 我老师就支持我啊。但是他说你要考虑一个问题，就是你可能找不到被试，你找不够，因为这是一个非常敏感的问题，你找研究对象是非常困难的。 访谈对象： 啊，是的。 访谈者： 还有就是我没有提到的，而你想要说的，想要表达的一些东西？ 访谈对象： 哦，其实我当时想聊的，其实基本上你这个题做的还不错。其实我想聊的其实基本上都聊到了。就是说反正我觉得其实我更希望的就是说是国内这方面的研究能够跟上，其实先不提法律层面上了，因为法律必定是滞后于社会发展。对，然后对，我先不提法律上，我就希望心理干涉这一块儿能够及时跟上，就包括大家的观点，就千万不要去面对一个性侵受害者说：你当时穿的太少了，你太骚，你太浪了…这种东西，就被害者有罪论你千万不要再去讲。 访谈者： 对对对。 访谈对象： 就是这些吧，我就是只想说“去你丫的”！反正我觉得其实这个东西，反正我觉得你做这个事情是比较有意义的。就我刚才说，我很好奇为啥为啥会2020年才有人找。当然我不在圈内，可能更早有人做了，我没知道而已，就翻到了我而已。但是我觉得的确应该是非常少见。 访谈者： 嗯，13年、14年也是有人做的，因为我看到一些相关的硕士论文。 访谈对象： 对，反正希望你这边的研究一切顺利吧，反正就是说我自己还是觉得就是其实是反正如果说你对这个，我不知道你对做这个事情有怀疑没有。如果有的话，反正我还明确告诉你：做这个事情非常有意义。 访谈者： 嗯，谢谢，谢谢。 访谈对象： 对，行，看你这边还有啥想问的没？ 访谈者： 目前是没有了。你做的一切也非常非常有意义。 访谈对象： 谢谢。那要不然就先这样？ 访谈者： 嗯，好，那你忙吧，拜拜。 访谈对象： 拜拜。 总结其实想说的话有很多，但是一下不知道怎么说吧。在这里先引用我的采访者在论文中写的一句话作为结尾吧 我始终相信星星之火，可以燎原。 Everything is gonna be OK","link":"/posts/2021/05/07/something-I-want-to-share-about-sex-assault1/"},{"title":"Python concurrent.future 使用教程及源码初剖","text":"垃圾话很久没写博客了，想了想不能再划水，于是给自己定了一个目标，写点 concurrent.future 的内容，于是这篇文章就是来聊聊 Python 3.2 中新增的 concurrent.future 模块。 正文Python 的异步处理有一个 Python 开发工程师小明，在面试过程中，突然接到这样一个需求：去请求几个网站，拿到他们的数据，小明定睛一想，简单啊，噼里啪啦，他写了如下的代码 1234567891011121314import multiprocessingimport timedef request_url(query_url: str): time.sleep(3) # 请求处理逻辑if __name__ == '__main__': url_list = [&quot;abc.com&quot;, &quot;xyz.com&quot;] task_list = [multiprocessing.Process(target=request_url, args=(url,)) for url in url_list] [task.start() for task in task_list] [task.join() for task in task_list] Easy, 好了，现在新的需求来了，我们想获取每一个请求结果，怎么办?小明想了想，又写出如下的代码 123456789101112131415161718import multiprocessingimport timedef request_url(query_url: str, result_dict: dict): time.sleep(3) # 请求处理逻辑 result_dict[query_url] = {} # 返回结果if __name__ == '__main__': process_manager = multiprocessing.Manager() result_dict = process_manager.dict() url_list = [&quot;abc.com&quot;, &quot;xyz.com&quot;] task_list = [multiprocessing.Process(target=request_url, args=(url, result_dict)) for url in url_list] [task.start() for task in task_list] [task.join() for task in task_list] print(result_dict) 好了，面试官说，恩看起来不错，好了，我再改改题目，首先，我们不能阻塞主进程，主进程需要根据任务当前的状态（结束/未结束）来及时的获取对应的结果，怎么改？，小明想了想，要不，我们直接用信号量，让任务完成后，向父进程发送一个信号量？然后直接暴力出奇迹？还有更简单的方法么？貌似没了？最后面试官心理说了一句 naive ，脸上笑而不语，让小明回去慢慢等消息。 从小明的窘境，我们可以看出一个这样的问题，就是我们最常用的 multiprocessing 或者是 threding 两个模块，对于我们想实现异步任务的场景来说，其实略有一点不友好的，我们往往需要做一些额外的工作，才能比较干净的实现一些异步的需求。为了解决这样的窘境，09 年 10 月，Brian Quinlan 先生提出了 PEP 3148 ，在这个提案中，他提出将我们常用的 multiprocessing 和 threding 模块进行进一步封装，达成较好的支持异步操作的目的。最终这个提案在 Python 3.2 中被引入。也就是我们今天要聊聊的 concurrent.future 。 Future 模式在我们正式开始聊新模块之前，我们需要去了解关于 Future 模式的相关姿势 首先 Future 模式，是什么？ Future 其实是生产-消费者模型的一种扩展，在生产-消费者模型中，生产者不关心消费者什么时候处理完数据，也不关心消费者处理的结果。比如我们经常写出如下的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import multiprocessing, Queueimport osimport timefrom multiprocessing import Processfrom time import sleepfrom random import randintclass Producer(multiprocessing.Process): def __init__(self, queue): multiprocessing.Process.__init__(self) self.queue = queue def run(self): while True: self.queue.put('one product') print(multiprocessing.current_process().name + str(os.getpid()) + ' produced one product, the no of queue now is: %d' %self.queue.qsize()) sleep(randint(1, 3)) class Consumer(multiprocessing.Process): def __init__(self, queue): multiprocessing.Process.__init__(self) self.queue = queue def run(self): while True: d = self.queue.get(1) if d != None: print(multiprocessing.current_process().name + str(os.getpid()) + ' consumed %s, the no of queue now is: %d' %(d,self.queue.qsize())) sleep(randint(1, 4)) continue else: break #create queuequeue = multiprocessing.Queue(40) if __name__ == &quot;__main__&quot;: print('Excited!&quot;) #create processes processed = [] for i in range(3): processed.append(Producer(queue)) processed.append(Consumer(queue)) #start processes for i in range(len(processed)): processed[i].start() #join processes for i in range(len(processed)): processed[i].join() 这就是生产-消费者模型的一个简单的实现，我们利用一个 multiprocessing 中的 Queue 来作为通信渠道，我们的生产者负责往队列中传入数据，消费者负责从队列中获取数据并处理。不过就如同上面所说的一样，在这种模式中，生产者并不关心消费者何时处理完数据，也不关心处理的结果。而在 Future 中，我们可以让生产者等待消息处理完成，如果需要的话，我们还可以获取相关的计算结果。 比如，大家可以看看下面这样一段 Java 代码 1234567891011121314package concurrent;import java.util.concurrent.Callable;public class DataProcessThread implements Callable&lt;String&gt; { @Override public String call() throws Exception { // TODO Auto-generated method stub Thread.sleep(10000);//模拟数据处理 return &quot;数据返回&quot;; }} 这是我们负责处理数据的代码。 12345678910111213141516171819202122232425262728293031package concurrent;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.FutureTask;public class MainThread { public static void main(String[] args) throws InterruptedException, ExecutionException { // TODO Auto-generated method stub DataProcessThread dataProcessThread = new DataProcessThread(); FutureTask&lt;String&gt; future = new FutureTask&lt;String&gt;(dataProcessThread); ExecutorService executor = Executors.newFixedThreadPool(1); executor.submit(future); Thread.sleep(10000);//模拟继续处理自身其他业务 while (true) { if (future.isDone()) { System.out.println(future.get()); break; } } executor.shutdown(); }} 这是我们主线程，大家可以看到，我们可以很方便的获取数据处理任务的状态。同时获取相关的结果。 Python 中的 concurrent.futures前面说了，在 Python 3.2 以后，concurrent.futures 是内置的模块，我们可以直接使用 Note: 如果你需要在 Python 2.7 中使用 concurrent.futures , 那么请用 pip 进行安装，pip install futures 好了，准备就绪后，我们来看看怎么使用这个东西呢 12345678910111213141516171819from concurrent.futures import ProcessPoolExecutorimport timedef return_future_result(message): time.sleep(2) return messageif __name__ == '__main__': pool = ProcessPoolExecutor(max_workers=2) # 创建一个最大可容纳2个task的进程池 future1 = pool.submit(return_future_result, (&quot;hello&quot;)) # 往进程池里面加入一个task future2 = pool.submit(return_future_result, (&quot;world&quot;)) # 往进程池里面加入一个task print(future1.done()) # 判断task1是否结束 time.sleep(3) print(future2.done()) # 判断task2是否结束 print(future1.result()) # 查看task1返回的结果 print(future2.result()) # 查看task2返回的结果 首先 from concurrent.futures import ProcessPoolExecutor 从 concurrent.futures 引入 ProcessPoolExecutor 作为我们的进程池，处理我们后面的数据。(在 concurrent.futures 中，为我们提供了两种 Executor ，一种是我们现在用的 ProcessPoolExecutor, 一种是 ThreadPoolExecutor 他们对外暴露的方法一致，大家可以根据自己的实际需求选用。) 紧接着，初始化一个最大容量为 2 的进程池。然后我们调用进程池中的 submit 方法提交一个任务。好了有意思的点来了，我们在调用 submit 方法后，得到了一个特殊的变量，这个变量是 Future 类的实例，代表着一个在未来完成的操作。换句话说，当 submit 返回 Future 实例的时候，我们的任务可能还没有完成，我们可以通过调用 Future 实例中的 done 方法来获取当前任务的运行状态，如果任务结束后，我们可以通过 result 方法来获取返回的结果。如果在执行后续的逻辑时，我们因为一些原因想要取消任务时，我们可以通过调用 cancel 方法来取消当前的任务。 现在新的问题来了，我们如果想要提交很多个任务应该怎么办呢？concurrent.future 为我们提供了 map 方法来方便我们批量添加任务。 123456789101112131415import concurrent.futuresimport requeststask_url = [('http://www.baidu.com', 40), ('http://example.com/', 40), ('https://www.github.com/', 40)]def load_url(params: tuple): return requests.get(params[0], timeout=params[1]).textif __name__ == '__main__': with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor: for url, data in zip(task_url, executor.map(load_url, task_url)): print('%r page is %d bytes' % (url, len(data))) 恩，concurrent.future 中线程/进程池所提供的 map 方法和标准库中的 map 函数使用方法一样。 剖一下 concurrent.futures前面讲了怎么使用 concurrent.futures 后，我们都比较好奇，concurrent.futures 是怎么实现 Future 模式的。里面是怎么将任务和结果进行关联的。我们现在开始从 submit 方法着手来简单看一下 ProcessPoolExecutor 的实现。 首先，在初始化 ProcessPoolExecutor 时，它的 __init__ 方法中进行了一些关键变量的初始化操作。 1234567891011121314151617181920212223242526272829303132333435363738394041class ProcessPoolExecutor(_base.Executor): def __init__(self, max_workers=None): &quot;&quot;&quot;Initializes a new ProcessPoolExecutor instance. Args: max_workers: The maximum number of processes that can be used to execute the given calls. If None or not given then as many worker processes will be created as the machine has processors. &quot;&quot;&quot; _check_system_limits() if max_workers is None: self._max_workers = os.cpu_count() or 1 else: if max_workers &lt;= 0: raise ValueError(&quot;max_workers must be greater than 0&quot;) self._max_workers = max_workers # Make the call queue slightly larger than the number of processes to # prevent the worker processes from idling. But don't make it too big # because futures in the call queue cannot be cancelled. self._call_queue = multiprocessing.Queue(self._max_workers + EXTRA_QUEUED_CALLS) # Killed worker processes can produce spurious &quot;broken pipe&quot; # tracebacks in the queue's own worker thread. But we detect killed # processes anyway, so silence the tracebacks. self._call_queue._ignore_epipe = True self._result_queue = SimpleQueue() self._work_ids = queue.Queue() self._queue_management_thread = None # Map of pids to processes self._processes = {} # Shutdown is a two-step process. self._shutdown_thread = False self._shutdown_lock = threading.Lock() self._broken = False self._queue_count = 0 self._pending_work_items = {} 好了，我们来看看我们今天的入口 submit 方法 1234567891011121314151617def submit(self, fn, *args, **kwargs): with self._shutdown_lock: if self._broken: raise BrokenProcessPool('A child process terminated ' 'abruptly, the process pool is not usable anymore') if self._shutdown_thread: raise RuntimeError('cannot schedule new futures after shutdown') f = _base.Future() w = _WorkItem(f, fn, args, kwargs) self._pending_work_items[self._queue_count] = w self._work_ids.put(self._queue_count) self._queue_count += 1 # Wake up queue management thread self._result_queue.put(None) self._start_queue_management_thread() return f 首先，传入的参数 fn 是我们的处理函数，args 以及 kwargs 是我们要传递 fn 函数的参数。在 submit 函数最开始，首先根据 _broken 和 _shutdown_thread 的值来判断当前进程池中处理进程的状态以及目前进程池的状态。如果处理进程突然被销毁或者进程池已经被关闭，那么将抛出异常表明目前不再接受新的 submit 操作。 如果前面状态没有问题，首先，实例化 Future 类，然后将这个实例和处理函数和相关参数一起，作为参数来实例化 _WorkItem 类，然后将实例 w 作为 value ，_queue_count 作为 key 存入 _pending_work_items 中。然后调用 _start_queue_management_thread 方法开启进程池中的管理线程。现在来看看这部分代码 123456789101112131415161718192021def _start_queue_management_thread(self): # When the executor gets lost, the weakref callback will wake up # the queue management thread. def weakref_cb(_, q=self._result_queue): q.put(None) if self._queue_management_thread is None: # Start the processes so that their sentinels are known. self._adjust_process_count() self._queue_management_thread = threading.Thread( target=_queue_management_worker, args=(weakref.ref(self, weakref_cb), self._processes, self._pending_work_items, self._work_ids, self._call_queue, self._result_queue)) self._queue_management_thread.daemon = True self._queue_management_thread.start() _threads_queues[self._queue_management_thread] = self._result_queue 这一部分很简单，首先运行 _adjust_process_count 方法，然后开启一个守护线程，运行 _queue_management_worker 方法。我们首先来看看 _adjust_process_count 方法。 12345678def _adjust_process_count(self): for _ in range(len(self._processes), self._max_workers): p = multiprocessing.Process( target=_process_worker, args=(self._call_queue, self._result_queue)) p.start() self._processes[p.pid] = p 根据在 __init__ 方法中设定的 _max_workers 来开启对应数量的进程，在进程中运行 _process_worker 函数。 恩，顺藤摸瓜，我们先来看看 _process_worker 函数吧？ 12345678910111213141516171819202122232425262728def _process_worker(call_queue, result_queue): &quot;&quot;&quot;Evaluates calls from call_queue and places the results in result_queue. This worker is run in a separate process. Args: call_queue: A multiprocessing.Queue of _CallItems that will be read and evaluated by the worker. result_queue: A multiprocessing.Queue of _ResultItems that will written to by the worker. shutdown: A multiprocessing.Event that will be set as a signal to the worker that it should exit when call_queue is empty. &quot;&quot;&quot; while True: call_item = call_queue.get(block=True) if call_item is None: # Wake up queue management thread result_queue.put(os.getpid()) return try: r = call_item.fn(*call_item.args, **call_item.kwargs) except BaseException as e: exc = _ExceptionWithTraceback(e, e.__traceback__) result_queue.put(_ResultItem(call_item.work_id, exception=exc)) else: result_queue.put(_ResultItem(call_item.work_id, result=r)) 首先，这里搞了一个死循环，紧接着，我们从 call_queue 队列中获取一个 _WorkItem 实例，然后如果获取的值为 None 的话，那么证明没有新的任务进来了，我们可以把当前进程的 pid 放入结果队列中。然后结束进程。 如果收到了任务，那么执行这个任务。不管是在执行过程中发生异常，亦或者是得到最终的结果，都将其封装为 _ResultItem 实例，并将其放入结果队列中。 好了，我们回到刚刚看了一半的 _start_queue_management_thread 函数， 123456789101112131415161718192021def _start_queue_management_thread(self): # When the executor gets lost, the weakref callback will wake up # the queue management thread. def weakref_cb(_, q=self._result_queue): q.put(None) if self._queue_management_thread is None: # Start the processes so that their sentinels are known. self._adjust_process_count() self._queue_management_thread = threading.Thread( target=_queue_management_worker, args=(weakref.ref(self, weakref_cb), self._processes, self._pending_work_items, self._work_ids, self._call_queue, self._result_queue)) self._queue_management_thread.daemon = True self._queue_management_thread.start() _threads_queues[self._queue_management_thread] = self._result_queue 在执行完 _adjust_process_count 函数后，我们进程池中的 _processes 变量（它是一个 dict ）便关联了一些处理进程。然后我们开启一个后台守护线程，来执行 _queue_management_worker 函数，我们给它传了几个变量，首先 _processes 是我们的进程映射，_pending_work_items 中存放着我们待处理任务，还有 _call_queue 和 _result_queue 。好了还有一个参数大家可能不太理解，就是 weakref.ref(self, weakref_cb) 这货。 首先，Python 是一门具有垃圾回收机制的语言，有着 GC (Garbage Collection) 机制意味着我们在大多数时候，不太需要去关注内存的分配与回收。在 Python 中，什么时候对象会被回收是由其引用计数所决定的。当引用计数为 0 的时候，这个对象会被回收。在有一些情况下，我们对象因为交叉引用或者其余的一些原因，造成引用计数始终不为0，这意味着这个对象无法被回收。造成内存泄露。因此区别于我们普通的引用，Python 中新增了一个引用机制叫做弱引用，弱引用的意义在于，某个变量持有一个对象，却不会增加这个对象的引用计数。因此 weakref.ref(self, weakref_cb) 在大多数而言，等价于 self （至于这里为什么要使用弱引用，我们这里先不讲，会开一个单章来说） 好了，这一部分代码看完，我们来看看，_queue_management_worker 怎么实现的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115def _queue_management_worker(executor_reference, processes, pending_work_items, work_ids_queue, call_queue, result_queue): &quot;&quot;&quot;Manages the communication between this process and the worker processes. This function is run in a local thread. executor_reference: A weakref.ref to the ProcessPoolExecutor that owns Args: process: A list of the multiprocessing.Process instances used as this thread. Used to determine if the ProcessPoolExecutor has been garbage collected and that this function can exit. workers. pending_work_items: A dict mapping work ids to _WorkItems e.g. {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...} work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]). call_queue: A multiprocessing.Queue that will be filled with _CallItems derived from _WorkItems for processing by the process workers. result_queue: A multiprocessing.Queue of _ResultItems generated by the process workers. &quot;&quot;&quot; executor = None def shutting_down(): return _shutdown or executor is None or executor._shutdown_thread def shutdown_worker(): # This is an upper bound nb_children_alive = sum(p.is_alive() for p in processes.values()) for i in range(0, nb_children_alive): call_queue.put_nowait(None) # Release the queue's resources as soon as possible. call_queue.close() # If .join() is not called on the created processes then # some multiprocessing.Queue methods may deadlock on Mac OS X. for p in processes.values(): p.join() reader = result_queue._reader while True: _add_call_item_to_queue(pending_work_items, work_ids_queue, call_queue) sentinels = [p.sentinel for p in processes.values()] assert sentinels ready = wait([reader] + sentinels) if reader in ready: result_item = reader.recv() else: # Mark the process pool broken so that submits fail right now. executor = executor_reference() if executor is not None: executor._broken = True executor._shutdown_thread = True executor = None # All futures in flight must be marked failed for work_id, work_item in pending_work_items.items(): work_item.future.set_exception( BrokenProcessPool( &quot;A process in the process pool was &quot; &quot;terminated abruptly while the future was &quot; &quot;running or pending.&quot; )) # Delete references to object. See issue16284 del work_item pending_work_items.clear() # Terminate remaining workers forcibly: the queues or their # locks may be in a dirty state and block forever. for p in processes.values(): p.terminate() shutdown_worker() return if isinstance(result_item, int): # Clean shutdown of a worker using its PID # (avoids marking the executor broken) assert shutting_down() p = processes.pop(result_item) p.join() if not processes: shutdown_worker() return elif result_item is not None: work_item = pending_work_items.pop(result_item.work_id, None) # work_item can be None if another process terminated (see above) if work_item is not None: if result_item.exception: work_item.future.set_exception(result_item.exception) else: work_item.future.set_result(result_item.result) # Delete references to object. See issue16284 del work_item # Check whether we should start shutting down. executor = executor_reference() # No more work items can be added if: # - The interpreter is shutting down OR # - The executor that owns this worker has been collected OR # - The executor that owns this worker has been shutdown. if shutting_down(): try: # Since no new work items can be added, it is safe to shutdown # this thread if there are no pending work items. if not pending_work_items: shutdown_worker() return except Full: # This is not a problem: we will eventually be woken up (in # result_queue.get()) and be able to send a sentinel again. pass executor = None 熟悉的大循环，循环的第一步，利用 _add_call_item_to_queue 函数来将等待队列中的任务加入到调用队列中去，先来看看这一部分代码 123456789101112131415161718192021222324252627282930313233343536def _add_call_item_to_queue(pending_work_items, work_ids, call_queue): &quot;&quot;&quot;Fills call_queue with _WorkItems from pending_work_items. This function never blocks. Args: pending_work_items: A dict mapping work ids to _WorkItems e.g. {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...} work_ids: A queue.Queue of work ids e.g. Queue([5, 6, ...]). Work ids are consumed and the corresponding _WorkItems from pending_work_items are transformed into _CallItems and put in call_queue. call_queue: A multiprocessing.Queue that will be filled with _CallItems derived from _WorkItems. &quot;&quot;&quot; while True: if call_queue.full(): return try: work_id = work_ids.get(block=False) except queue.Empty: return else: work_item = pending_work_items[work_id] if work_item.future.set_running_or_notify_cancel(): call_queue.put(_CallItem(work_id, work_item.fn, work_item.args, work_item.kwargs), block=True) else: del pending_work_items[work_id] continue 首先，判断调用队列是不是已经满了，如果满了，则放弃这次循环。紧接着从 work_id 队列中取出，然后从等待任务中取出对应的 _WorkItem 实例。紧接着，调用实例中绑定的 Future 实例的 set_running_or_notify_cancel 方法来设置任务的状态，紧接着将其扔入调用队列中。 12345678910111213141516171819202122232425262728293031323334353637383940def set_running_or_notify_cancel(self): &quot;&quot;&quot;Mark the future as running or process any cancel notifications. Should only be used by Executor implementations and unit tests. If the future has been cancelled (cancel() was called and returned True) then any threads waiting on the future completing (though calls to as_completed() or wait()) are notified and False is returned. If the future was not cancelled then it is put in the running state (future calls to running() will return True) and True is returned. This method should be called by Executor implementations before executing the work associated with this future. If this method returns False then the work should not be executed. Returns: False if the Future was cancelled, True otherwise. Raises: RuntimeError: if this method was already called or if set_result() or set_exception() was called. &quot;&quot;&quot; with self._condition: if self._state == CANCELLED: self._state = CANCELLED_AND_NOTIFIED for waiter in self._waiters: waiter.add_cancelled(self) # self._condition.notify_all() is not necessary because # self.cancel() triggers a notification. return False elif self._state == PENDING: self._state = RUNNING return True else: LOGGER.critical('Future %s in unexpected state: %s', id(self), self._state) raise RuntimeError('Future in unexpected state') 这一部分内容很简单，检查当前实例如果处于等待状态，就返回 True ，如果处于被取消的状态，就返回 False , 在 _add_call_item_to_queue 函数中，会将已经处于 cancel 状态的 _WorkItem 从等待任务中移除。 好了，我们继续回到 _queue_management_worker 函数中去， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115def _queue_management_worker(executor_reference, processes, pending_work_items, work_ids_queue, call_queue, result_queue): &quot;&quot;&quot;Manages the communication between this process and the worker processes. This function is run in a local thread. executor_reference: A weakref.ref to the ProcessPoolExecutor that owns Args: process: A list of the multiprocessing.Process instances used as this thread. Used to determine if the ProcessPoolExecutor has been garbage collected and that this function can exit. workers. pending_work_items: A dict mapping work ids to _WorkItems e.g. {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...} work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]). call_queue: A multiprocessing.Queue that will be filled with _CallItems derived from _WorkItems for processing by the process workers. result_queue: A multiprocessing.Queue of _ResultItems generated by the process workers. &quot;&quot;&quot; executor = None def shutting_down(): return _shutdown or executor is None or executor._shutdown_thread def shutdown_worker(): # This is an upper bound nb_children_alive = sum(p.is_alive() for p in processes.values()) for i in range(0, nb_children_alive): call_queue.put_nowait(None) # Release the queue's resources as soon as possible. call_queue.close() # If .join() is not called on the created processes then # some multiprocessing.Queue methods may deadlock on Mac OS X. for p in processes.values(): p.join() reader = result_queue._reader while True: _add_call_item_to_queue(pending_work_items, work_ids_queue, call_queue) sentinels = [p.sentinel for p in processes.values()] assert sentinels ready = wait([reader] + sentinels) if reader in ready: result_item = reader.recv() else: # Mark the process pool broken so that submits fail right now. executor = executor_reference() if executor is not None: executor._broken = True executor._shutdown_thread = True executor = None # All futures in flight must be marked failed for work_id, work_item in pending_work_items.items(): work_item.future.set_exception( BrokenProcessPool( &quot;A process in the process pool was &quot; &quot;terminated abruptly while the future was &quot; &quot;running or pending.&quot; )) # Delete references to object. See issue16284 del work_item pending_work_items.clear() # Terminate remaining workers forcibly: the queues or their # locks may be in a dirty state and block forever. for p in processes.values(): p.terminate() shutdown_worker() return if isinstance(result_item, int): # Clean shutdown of a worker using its PID # (avoids marking the executor broken) assert shutting_down() p = processes.pop(result_item) p.join() if not processes: shutdown_worker() return elif result_item is not None: work_item = pending_work_items.pop(result_item.work_id, None) # work_item can be None if another process terminated (see above) if work_item is not None: if result_item.exception: work_item.future.set_exception(result_item.exception) else: work_item.future.set_result(result_item.result) # Delete references to object. See issue16284 del work_item # Check whether we should start shutting down. executor = executor_reference() # No more work items can be added if: # - The interpreter is shutting down OR # - The executor that owns this worker has been collected OR # - The executor that owns this worker has been shutdown. if shutting_down(): try: # Since no new work items can be added, it is safe to shutdown # this thread if there are no pending work items. if not pending_work_items: shutdown_worker() return except Full: # This is not a problem: we will eventually be woken up (in # result_queue.get()) and be able to send a sentinel again. pass executor = None result_item 变量 我们看看 首先，大家可能在这里有点疑问了 12345sentinels = [p.sentinel for p in processes.values()]assert sentinelsready = wait([reader] + sentinels) 这个 wait 是什么鬼啊，reader 又是什么鬼啊。一步步来。首先，我们看到，前面，reader = result_queue._reader 也会引起大家的疑问，这里我们 result_queue 是 multiprocess 里面的 SimpleQueue 啊，它没有 _reader 方法啊QAQ 1234567891011class SimpleQueue(object): def __init__(self, *, ctx): self._reader, self._writer = connection.Pipe(duplex=False) self._rlock = ctx.Lock() self._poll = self._reader.poll if sys.platform == 'win32': self._wlock = None else: self._wlock = ctx.Lock() 上面这贴出来的，是 SimpleQueue 的部分代码，我们可以很清楚的看到，SimpleQueue 本质是利用一个 Pipe 来进行进程间通信的，然后 _reader 是读取 Pipe 的一个变量。 Note : 大家可以复习下其余几种进程间通信的方法了 好了，这一部分看懂后，我们来看看 wait 方法吧。 1234567891011121314151617181920212223def wait(object_list, timeout=None): ''' Wait till an object in object_list is ready/readable. Returns list of those objects in object_list which are ready/readable. ''' with _WaitSelector() as selector: for obj in object_list: selector.register(obj, selectors.EVENT_READ) if timeout is not None: deadline = time.time() + timeout while True: ready = selector.select(timeout) if ready: return [key.fileobj for (key, events) in ready] else: if timeout is not None: timeout = deadline - time.time() if timeout &lt; 0: return ready 这一部分代码很简单，首先将我们待读取的对象，进行一次注册，然后当 timeout 为 None 的时候，就一直等待到有对象读取数据成功为止 好了，我们继续回到前面的 _queue_management_worker 函数中去，来看看这样一段代码 12345678910111213141516171819202122232425262728ready = wait([reader] + sentinels)if reader in ready: result_item = reader.recv()else: # Mark the process pool broken so that submits fail right now. executor = executor_reference() if executor is not None: executor._broken = True executor._shutdown_thread = True executor = None # All futures in flight must be marked failed for work_id, work_item in pending_work_items.items(): work_item.future.set_exception( BrokenProcessPool( &quot;A process in the process pool was &quot; &quot;terminated abruptly while the future was &quot; &quot;running or pending.&quot; )) # Delete references to object. See issue16284 del work_item pending_work_items.clear() # Terminate remaining workers forcibly: the queues or their # locks may be in a dirty state and block forever. for p in processes.values(): p.terminate() shutdown_worker() return 我们用 wait 函数来读取一系列对象，因为我们没有设置 Timeout ，所以当我们拿到可读取对象的结果时，如果 result_queue._reader 没有在列表中，那么意味着，有处理进程突然异常关闭了，这个时候，我们开始执行后面的语句来执行目前进程池的关闭操作。如果在列表中，我们读取数据，得到 result_item 变量 我们再看看下面的代码 123456789101112131415161718192021if isinstance(result_item, int): # Clean shutdown of a worker using its PID # (avoids marking the executor broken) assert shutting_down() p = processes.pop(result_item) p.join() if not processes: shutdown_worker() returnelif result_item is not None: work_item = pending_work_items.pop(result_item.work_id, None) # work_item can be None if another process terminated (see above) if work_item is not None: if result_item.exception: work_item.future.set_exception(result_item.exception) else: work_item.future.set_result(result_item.result) # Delete references to object. See issue16284 del work_item 首先，如果 result_item 变量是 int 类型的话，不知道大家还记不记得在 _process_worker 函数中有这样一段逻辑 123456call_item = call_queue.get(block=True)if call_item is None: # Wake up queue management thread result_queue.put(os.getpid()) return 当调用队列中没有新的任务时，将进程 pid 放入 result_queue 中。那么我们 result_item 如果值为 int 那么意味着，我们之前任务处理工作已经完毕，于是开始清理，关闭我们的进程池。 如果 result_item 既不为 int 也不为 None , 那么必然是 _ResultItem 的实例，我们根据 work_id 取出 _WorkItem 实例，并将产生的异常或者值和 _WorkItem 实例中的 Future 实例（也就是我们 submit 后返回的那货）进行绑定。 最后，删除这个 work_item ，完事儿，手工 最后洋洋洒洒写了一大篇辣鸡文章，希望大家不要介意，其实我们能看到 concurrent.future 的实现，其实并没有用什么高深的黑魔法，但是其中细节值得我们一一品味，所以这篇文章我们先写到这里。后面有机会的话，我们再去看看 concurrent.future 其余部分代码。也有蛮多值得品味的地方。 Reference1.Python 3 multiprocessing 2.Python 3 weakref 3.并发编程之Future模式 4.Python并发编程之线程池/进程池 5.Future 模式详解（并发使用）","link":"/posts/2017/11/28/something-about-concurrent-future/"}],"tags":[{"name":"社会","slug":"社会","link":"/tags/%E7%A4%BE%E4%BC%9A/"},{"name":"PKU","slug":"PKU","link":"/tags/PKU/"},{"name":"iOS","slug":"iOS","link":"/tags/iOS/"},{"name":"编程","slug":"编程","link":"/tags/%E7%BC%96%E7%A8%8B/"},{"name":"Swift","slug":"Swift","link":"/tags/Swift/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"黑魔法","slug":"黑魔法","link":"/tags/%E9%BB%91%E9%AD%94%E6%B3%95/"},{"name":"编程技巧","slug":"编程技巧","link":"/tags/%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7/"},{"name":"协程","slug":"协程","link":"/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"笔记","slug":"笔记","link":"/tags/%E7%AC%94%E8%AE%B0/"},{"name":"水文","slug":"水文","link":"/tags/%E6%B0%B4%E6%96%87/"},{"name":"Flask","slug":"Flask","link":"/tags/Flask/"},{"name":"源码阅读","slug":"源码阅读","link":"/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"人生","slug":"人生","link":"/tags/%E4%BA%BA%E7%94%9F/"},{"name":"杂记","slug":"杂记","link":"/tags/%E6%9D%82%E8%AE%B0/"},{"name":"秀恩爱","slug":"秀恩爱","link":"/tags/%E7%A7%80%E6%81%A9%E7%88%B1/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"刷题","slug":"刷题","link":"/tags/%E5%88%B7%E9%A2%98/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"输入法","slug":"输入法","link":"/tags/%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"电子产品","slug":"电子产品","link":"/tags/%E7%94%B5%E5%AD%90%E4%BA%A7%E5%93%81/"},{"name":"Apple","slug":"Apple","link":"/tags/Apple/"},{"name":"评测","slug":"评测","link":"/tags/%E8%AF%84%E6%B5%8B/"},{"name":"async","slug":"async","link":"/tags/async/"},{"name":"PEP","slug":"PEP","link":"/tags/PEP/"},{"name":"吐槽","slug":"吐槽","link":"/tags/%E5%90%90%E6%A7%BD/"},{"name":"PEP484","slug":"PEP484","link":"/tags/PEP484/"},{"name":"Unix","slug":"Unix","link":"/tags/Unix/"},{"name":"网络编程","slug":"网络编程","link":"/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"Objective-C","slug":"Objective-C","link":"/tags/Objective-C/"},{"name":"掘金翻译计划","slug":"掘金翻译计划","link":"/tags/%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92/"},{"name":"人间世","slug":"人间世","link":"/tags/%E4%BA%BA%E9%97%B4%E4%B8%96/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"论文","slug":"论文","link":"/tags/%E8%AE%BA%E6%96%87/"},{"name":"随想","slug":"随想","link":"/tags/%E9%9A%8F%E6%83%B3/"},{"name":"云原生","slug":"云原生","link":"/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"eBPF","slug":"eBPF","link":"/tags/eBPF/"},{"name":"SystemTap","slug":"SystemTap","link":"/tags/SystemTap/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"源码剖析","slug":"源码剖析","link":"/tags/%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"}],"categories":[{"name":"社会","slug":"社会","link":"/categories/%E7%A4%BE%E4%BC%9A/"},{"name":"编程","slug":"编程","link":"/categories/%E7%BC%96%E7%A8%8B/"},{"name":"PKU","slug":"社会/PKU","link":"/categories/%E7%A4%BE%E4%BC%9A/PKU/"},{"name":"Linux","slug":"编程/Linux","link":"/categories/%E7%BC%96%E7%A8%8B/Linux/"},{"name":"翻译","slug":"编程/翻译","link":"/categories/%E7%BC%96%E7%A8%8B/%E7%BF%BB%E8%AF%91/"},{"name":"Python","slug":"编程/Python","link":"/categories/%E7%BC%96%E7%A8%8B/Python/"},{"name":"杂记","slug":"编程/杂记","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%9D%82%E8%AE%B0/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"人生","slug":"人生","link":"/categories/%E4%BA%BA%E7%94%9F/"},{"name":"leetcode","slug":"编程/leetcode","link":"/categories/%E7%BC%96%E7%A8%8B/leetcode/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"生活","link":"/categories/%E7%94%9F%E6%B4%BB/"},{"name":"水文","slug":"编程/水文","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%B0%B4%E6%96%87/"},{"name":"随笔","slug":"编程/Python/随笔","link":"/categories/%E7%BC%96%E7%A8%8B/Python/%E9%9A%8F%E7%AC%94/"},{"name":"PEP484","slug":"编程/Python/PEP484","link":"/categories/%E7%BC%96%E7%A8%8B/Python/PEP484/"},{"name":"网络编程","slug":"编程/网络编程","link":"/categories/%E7%BC%96%E7%A8%8B/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"MySQL","slug":"编程/Python/MySQL","link":"/categories/%E7%BC%96%E7%A8%8B/Python/MySQL/"},{"name":"总结","slug":"随笔/总结","link":"/categories/%E9%9A%8F%E7%AC%94/%E6%80%BB%E7%BB%93/"},{"name":"杂记","slug":"人生/杂记","link":"/categories/%E4%BA%BA%E7%94%9F/%E6%9D%82%E8%AE%B0/"},{"name":"刷题","slug":"编程/leetcode/刷题","link":"/categories/%E7%BC%96%E7%A8%8B/leetcode/%E5%88%B7%E9%A2%98/"},{"name":"输入法","slug":"工具/输入法","link":"/categories/%E5%B7%A5%E5%85%B7/%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"电子产品","slug":"生活/电子产品","link":"/categories/%E7%94%9F%E6%B4%BB/%E7%94%B5%E5%AD%90%E4%BA%A7%E5%93%81/"},{"name":"秀恩爱","slug":"随笔/总结/秀恩爱","link":"/categories/%E9%9A%8F%E7%AC%94/%E6%80%BB%E7%BB%93/%E7%A7%80%E6%81%A9%E7%88%B1/"},{"name":"Rime","slug":"工具/输入法/Rime","link":"/categories/%E5%B7%A5%E5%85%B7/%E8%BE%93%E5%85%A5%E6%B3%95/Rime/"},{"name":"人间世","slug":"人间世","link":"/categories/%E4%BA%BA%E9%97%B4%E4%B8%96/"},{"name":"论文","slug":"编程/水文/论文","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%B0%B4%E6%96%87/%E8%AE%BA%E6%96%87/"},{"name":"随想","slug":"编程/随想","link":"/categories/%E7%BC%96%E7%A8%8B/%E9%9A%8F%E6%83%B3/"},{"name":"Kernel","slug":"编程/Linux/Kernel","link":"/categories/%E7%BC%96%E7%A8%8B/Linux/Kernel/"}]}